Logging to /Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME

 ---------------- Iteration 0 ----------------
Obtaining samples...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
----------------------------
| Time/Buffer | 0.00875    |
| Train/Value | -0.0666277 |
----------------------------
Saving snapshot...
Saved

 ---------------- Iteration 1 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
------------------------------------------
| Itr                     | 0            |
| Time/Actor_Time         | 0.0107       |
| Time/B_Format_Time      | 0.00338      |
| Time/B_Original_Form... | 0.00348      |
| Time/Buffer             | 0.000312     |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30401126   |
| Train/Action_magnitu... | 1.8701445    |
| Train/Action_magnitude  | 1.5100642    |
| Train/Action_max        | 0.5661284    |
| Train/Action_std        | 0.96035445   |
| Train/Entropy           | 1.2981236    |
| Train/Entropy_Loss      | -0.0013      |
| Train/Entropy_loss      | -0.0013      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -2.9871986   |
| Train/Loss              | -0.15944457  |
| Train/PolicyClip        | 0.025339937  |
| Train/Policy_loss       | -0.17158066  |
| Train/Ratio             | 1.0435442    |
| Train/Return            | 0.10289925   |
| Train/V                 | -0.059486564 |
| Train/Value             | -0.059486564 |
| Train/control_penalty   | 1.3434213    |
| Train/policy_loss       | -0.17158066  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01         |
------------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
-----------------------------------------
| Itr                     | 1           |
| Time/Actor_Time         | 0.0107      |
| Time/B_Format_Time      | 0.00332     |
| Time/B_Original_Form... | 0.00359     |
| Time/Buffer             | 0.00053     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.36125106  |
| Train/Action_magnitu... | 1.6828892   |
| Train/Action_magnitude  | 1.356437    |
| Train/Action_max        | 0.59197545  |
| Train/Action_std        | 0.83495677  |
| Train/Entropy           | 1.1739454   |
| Train/Entropy_Loss      | -0.00117    |
| Train/Entropy_loss      | -0.00117    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -2.6609576  |
| Train/Loss              | -0.32354277 |
| Train/PolicyClip        | 0.034157224 |
| Train/Policy_loss       | -0.3341368  |
| Train/Ratio             | 1.0740519   |
| Train/Return            | 0.3793746   |
| Train/V                 | 0.06233668  |
| Train/Value             | 0.06233668  |
| Train/control_penalty   | 1.1767977   |
| Train/policy_loss       | -0.3341368  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
-----------------------------------------
| Itr                     | 2           |
| Time/Actor_Time         | 0.0112      |
| Time/B_Format_Time      | 0.00339     |
| Time/B_Original_Form... | 0.00354     |
| Time/Buffer             | 0.000517    |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2864145   |
| Train/Action_magnitu... | 1.4669931   |
| Train/Action_magnitude  | 1.1750432   |
| Train/Action_max        | 0.557982    |
| Train/Action_std        | 0.8372189   |
| Train/Entropy           | 1.1867388   |
| Train/Entropy_Loss      | -0.00119    |
| Train/Entropy_loss      | -0.00119    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -2.396758   |
| Train/Loss              | -0.11678876 |
| Train/PolicyClip        | 0.015558524 |
| Train/Policy_loss       | -0.12611368 |
| Train/Ratio             | 1.081801    |
| Train/Return            | 0.7303493   |
| Train/V                 | 0.617335    |
| Train/Value             | 0.617335    |
| Train/control_penalty   | 1.0511665   |
| Train/policy_loss       | -0.12611368 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02        |
-----------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
-------------------------------------------
| Itr                     | 3             |
| Time/Actor_Time         | 0.0108        |
| Time/B_Format_Time      | 0.00333       |
| Time/B_Original_Form... | 0.00339       |
| Time/Buffer             | 0.000414      |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.12638408    |
| Train/Action_magnitu... | 1.2875708     |
| Train/Action_magnitude  | 1.0508615     |
| Train/Action_max        | 0.6007692     |
| Train/Action_std        | 0.67793775    |
| Train/Entropy           | 0.9484719     |
| Train/Entropy_Loss      | -0.000948     |
| Train/Entropy_loss      | -0.000948     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -2.2217567    |
| Train/Loss              | -0.0069858423 |
| Train/PolicyClip        | 0.0074512935  |
| Train/Policy_loss       | -0.015579325  |
| Train/Ratio             | 0.9701614     |
| Train/Return            | 0.1146999     |
| Train/V                 | 0.10156336    |
| Train/Value             | 0.10156336    |
| Train/control_penalty   | 0.9541956     |
| Train/policy_loss       | -0.015579325  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.005         |
-------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
-----------------------------------------
| Itr                     | 4           |
| Time/Actor_Time         | 0.0112      |
| Time/B_Format_Time      | 0.00322     |
| Time/B_Original_Form... | 0.00349     |
| Time/Buffer             | 0.000397    |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2479454   |
| Train/Action_magnitu... | 1.1557873   |
| Train/Action_magnitude  | 0.9133236   |
| Train/Action_max        | 0.39478797  |
| Train/Action_std        | 0.6703311   |
| Train/Entropy           | 0.97919196  |
| Train/Entropy_Loss      | -0.000979   |
| Train/Entropy_loss      | -0.000979   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -2.0331802  |
| Train/Loss              | -0.10404922 |
| Train/PolicyClip        | 0.03547289  |
| Train/Policy_loss       | -0.11234608 |
| Train/Ratio             | 0.95607924  |
| Train/Return            | 0.32683286  |
| Train/V                 | 0.23176573  |
| Train/Value             | 0.23176573  |
| Train/control_penalty   | 0.92760557  |
| Train/policy_loss       | -0.11234608 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01        |
-----------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
------------------------------------------
| Itr                     | 5            |
| Time/Actor_Time         | 0.0108       |
| Time/B_Format_Time      | 0.00329      |
| Time/B_Original_Form... | 0.00341      |
| Time/Buffer             | 0.000288     |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27801996   |
| Train/Action_magnitu... | 1.0803306    |
| Train/Action_magnitude  | 0.85469645   |
| Train/Action_max        | 0.3910969    |
| Train/Action_std        | 0.65815103   |
| Train/Entropy           | 0.96337897   |
| Train/Entropy_Loss      | -0.000963    |
| Train/Entropy_loss      | -0.000963    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -2.0748389   |
| Train/Loss              | 0.015890187  |
| Train/PolicyClip        | 0.008656586  |
| Train/Policy_loss       | 0.0075941994 |
| Train/Ratio             | 0.86199576   |
| Train/Return            | 0.19605078   |
| Train/V                 | 0.20847973   |
| Train/Value             | 0.20847973   |
| Train/control_penalty   | 0.92593664   |
| Train/policy_loss       | 0.0075941994 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0          |
------------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
------------------------------------------
| Itr                     | 6            |
| Time/Actor_Time         | 0.0104       |
| Time/B_Format_Time      | 0.00333      |
| Time/B_Original_Form... | 0.00334      |
| Time/Buffer             | 0.0004       |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.22617301   |
| Train/Action_magnitu... | 1.3909832    |
| Train/Action_magnitude  | 1.123919     |
| Train/Action_max        | 0.46364403   |
| Train/Action_std        | 0.8530642    |
| Train/Entropy           | 1.2039868    |
| Train/Entropy_Loss      | -0.0012      |
| Train/Entropy_loss      | -0.0012      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -2.4464312   |
| Train/Loss              | -0.028268782 |
| Train/PolicyClip        | 0.009366149  |
| Train/Policy_loss       | -0.038446136 |
| Train/Ratio             | 0.92211634   |
| Train/Return            | 0.20442954   |
| Train/V                 | 0.176287     |
| Train/Value             | 0.176287     |
| Train/control_penalty   | 1.1381341    |
| Train/policy_loss       | -0.038446136 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.005        |
------------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
------------------------------------------
| Itr                     | 7            |
| Time/Actor_Time         | 0.0107       |
| Time/B_Format_Time      | 0.0036       |
| Time/B_Original_Form... | 0.00353      |
| Time/Buffer             | 0.000404     |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.31994346   |
| Train/Action_magnitu... | 1.6170888    |
| Train/Action_magnitude  | 1.2680886    |
| Train/Action_max        | 0.4673533    |
| Train/Action_std        | 0.88463026   |
| Train/Entropy           | 1.250211     |
| Train/Entropy_Loss      | -0.00125     |
| Train/Entropy_loss      | -0.00125     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -2.7814066   |
| Train/Loss              | -0.01908474  |
| Train/PolicyClip        | 0.021323409  |
| Train/Policy_loss       | -0.029304724 |
| Train/Ratio             | 0.9190829    |
| Train/Return            | 0.203857     |
| Train/V                 | 0.18439926   |
| Train/Value             | 0.18439926   |
| Train/control_penalty   | 1.1470194    |
| Train/policy_loss       | -0.029304724 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.005        |
------------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/EXP_NAME
-----------------------------------------
| Itr                     | 8           |
| Time/Actor_Time         | 0.0107      |
| Time/B_Format_Time      | 0.00342     |
| Time/B_Original_Form... | 0.0035      |
| Time/Buffer             | 0.000586    |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31546775  |
| Train/Action_magnitu... | 1.3556155   |
| Train/Action_magnitude  | 1.0982542   |
| Train/Action_max        | 0.4115124   |
| Train/Action_std        | 0.6515461   |
| Train/Entropy           | 0.9144127   |
| Train/Entropy_Loss      | -0.000914   |
| Train/Entropy_loss      | -0.000914   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.9870678  |
| Train/Loss              | -0.5178733  |
| Train/PolicyClip        | 0.01919231  |
| Train/Policy_loss       | -0.52652013 |
| Train/Ratio             | 1.0974048   |
| Train/Return            | 0.62734747  |
| Train/V                 | 0.1285435   |
| Train/Value             | 0.1285435   |
| Train/control_penalty   | 0.9561267   |
| Train/policy_loss       | -0.52652013 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04        |
-----------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples...
RL Training...
