Logging to /Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter

 ---------------- Iteration 0 ----------------
Obtaining samples...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------
| Time/Buffer | 0.0141   |
| Train/Value | 4.205055 |
--------------------------
Saving snapshot...
Saved

 ---------------- Iteration 1 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 0           |
| Time/Actor_Time         | 0.0939      |
| Time/B_Format_Time      | 0.0762      |
| Time/B_Original_Form... | 0.0785      |
| Time/Buffer             | 0.00506     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.7841934   |
| Train/Action_magnitu... | 0.4657955   |
| Train/Action_magnitude  | 0.3628458   |
| Train/Action_max        | 0.16935912  |
| Train/Action_std        | 0.38493264  |
| Train/Entropy           | 0.0753524   |
| Train/Entropy_Loss      | -7.54e-05   |
| Train/Entropy_loss      | -7.54e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -32.016903  |
| Train/Loss              | 0.72333443  |
| Train/PolicyClip        | 0.67136693  |
| Train/Policy_loss       | 0.70944554  |
| Train/Ratio             | 0.038282868 |
| Train/Return            | 3.5747383   |
| Train/V                 | 4.4390264   |
| Train/Value             | 4.4390264   |
| Train/control_penalty   | 1.396424    |
| Train/policy_loss       | 0.70944554  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0105      |
-----------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 1           |
| Time/Actor_Time         | 0.0868      |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00152     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.58587223  |
| Train/Action_magnitu... | 1.2414957   |
| Train/Action_magnitude  | 1.0076302   |
| Train/Action_max        | 0.328312    |
| Train/Action_std        | 0.3770301   |
| Train/Entropy           | 0.2579507   |
| Train/Entropy_Loss      | -0.000258   |
| Train/Entropy_loss      | -0.000258   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.41514012 |
| Train/Loss              | 0.9037898   |
| Train/PolicyClip        | 0.05879315  |
| Train/Policy_loss       | 0.8933794   |
| Train/Ratio             | 0.9137422   |
| Train/Return            | 2.9079392   |
| Train/V                 | 3.8147411   |
| Train/Value             | 3.8147411   |
| Train/control_penalty   | 1.0668391   |
| Train/policy_loss       | 0.8933794   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.00025     |
-----------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 2             |
| Time/Actor_Time         | 0.101         |
| Time/B_Format_Time      | 0.0755        |
| Time/B_Original_Form... | 0.0749        |
| Time/Buffer             | 0.00195       |
| Time/Critic_Time        | 7.15e-07      |
| Train/Action_abs_mean   | 0.5849882     |
| Train/Action_magnitu... | 1.1993872     |
| Train/Action_magnitude  | 0.9611996     |
| Train/Action_max        | -0.2506244    |
| Train/Action_std        | 0.35133502    |
| Train/Entropy           | 0.29019284    |
| Train/Entropy_Loss      | -0.00029      |
| Train/Entropy_loss      | -0.00029      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.5064151    |
| Train/Loss              | 0.19488466    |
| Train/PolicyClip        | -0.0032363082 |
| Train/Policy_loss       | 0.1855713     |
| Train/Ratio             | 1.0024208     |
| Train/Return            | 0.97694474    |
| Train/V                 | 1.1621462     |
| Train/Value             | 1.1621462     |
| Train/control_penalty   | 0.96035653    |
| Train/policy_loss       | 0.1855713     |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.00025       |
-------------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 3             |
| Time/Actor_Time         | 0.0971        |
| Time/B_Format_Time      | 0.0904        |
| Time/B_Original_Form... | 0.0897        |
| Time/Buffer             | 0.00339       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.59357786    |
| Train/Action_magnitu... | 1.2929648     |
| Train/Action_magnitude  | 1.0283107     |
| Train/Action_max        | -0.0101756165 |
| Train/Action_std        | 0.38931447    |
| Train/Entropy           | 0.34893686    |
| Train/Entropy_Loss      | -0.000349     |
| Train/Entropy_loss      | -0.000349     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.62579316   |
| Train/Loss              | 0.309791      |
| Train/PolicyClip        | 0.00027442965 |
| Train/Policy_loss       | 0.2999764     |
| Train/Ratio             | 0.9843181     |
| Train/Return            | 1.3087925     |
| Train/V                 | 1.6140293     |
| Train/Value             | 1.6140293     |
| Train/control_penalty   | 1.0163522     |
| Train/policy_loss       | 0.2999764     |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.002         |
-------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 4             |
| Time/Actor_Time         | 0.0897        |
| Time/B_Format_Time      | 0.075         |
| Time/B_Original_Form... | 0.0732        |
| Time/Buffer             | 0.00148       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.64818865    |
| Train/Action_magnitu... | 1.4437187     |
| Train/Action_magnitude  | 1.1513991     |
| Train/Action_max        | 0.35340598    |
| Train/Action_std        | 0.44245964    |
| Train/Entropy           | 0.4449147     |
| Train/Entropy_Loss      | -0.000445     |
| Train/Entropy_loss      | -0.000445     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.7411945    |
| Train/Loss              | 0.42537105    |
| Train/PolicyClip        | -0.0064662704 |
| Train/Policy_loss       | 0.4139913     |
| Train/Ratio             | 0.9558759     |
| Train/Return            | 1.5618571     |
| Train/V                 | 1.9916835     |
| Train/Value             | 1.9916835     |
| Train/control_penalty   | 1.1824669     |
| Train/policy_loss       | 0.4139913     |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0035        |
-------------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 5             |
| Time/Actor_Time         | 0.0853        |
| Time/B_Format_Time      | 0.0771        |
| Time/B_Original_Form... | 0.0714        |
| Time/Buffer             | 0.00141       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.7366752     |
| Train/Action_magnitu... | 1.5895792     |
| Train/Action_magnitude  | 1.3033949     |
| Train/Action_max        | 0.26270723    |
| Train/Action_std        | 0.542388      |
| Train/Entropy           | 0.5685468     |
| Train/Entropy_Loss      | -0.000569     |
| Train/Entropy_loss      | -0.000569     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.1239485    |
| Train/Loss              | 0.00084976107 |
| Train/PolicyClip        | 0.03713581    |
| Train/Policy_loss       | -0.012161907  |
| Train/Ratio             | 0.9100902     |
| Train/Return            | 2.2710567     |
| Train/V                 | 2.276925      |
| Train/Value             | 2.276925      |
| Train/control_penalty   | 1.3580215     |
| Train/policy_loss       | -0.012161907  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0195        |
-------------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 6             |
| Time/Actor_Time         | 0.086         |
| Time/B_Format_Time      | 0.071         |
| Time/B_Original_Form... | 0.0717        |
| Time/Buffer             | 0.00345       |
| Time/Critic_Time        | 1.19e-06      |
| Train/Action_abs_mean   | 0.5753932     |
| Train/Action_magnitu... | 1.4279734     |
| Train/Action_magnitude  | 1.1389052     |
| Train/Action_max        | 0.18374021    |
| Train/Action_std        | 0.5708816     |
| Train/Entropy           | 0.68387485    |
| Train/Entropy_Loss      | -0.000684     |
| Train/Entropy_loss      | -0.000684     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.2892866    |
| Train/Loss              | 0.43421376    |
| Train/PolicyClip        | -0.0049680923 |
| Train/Policy_loss       | 0.4232921     |
| Train/Ratio             | 0.97009206    |
| Train/Return            | 1.6572655     |
| Train/V                 | 2.0964046     |
| Train/Value             | 2.0964046     |
| Train/control_penalty   | 1.1605542     |
| Train/policy_loss       | 0.4232921     |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.005         |
-------------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 7            |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0761       |
| Time/B_Original_Form... | 0.075        |
| Time/Buffer             | 0.00164      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.31764767   |
| Train/Action_magnitu... | 0.97268593   |
| Train/Action_magnitude  | 0.7644773    |
| Train/Action_max        | -0.12904954  |
| Train/Action_std        | 0.46119285   |
| Train/Entropy           | 0.63334125   |
| Train/Entropy_Loss      | -0.000633    |
| Train/Entropy_loss      | -0.000633    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.2408925   |
| Train/Loss              | 0.41606382   |
| Train/PolicyClip        | -0.015804447 |
| Train/Policy_loss       | 0.40942067   |
| Train/Ratio             | 0.9936578    |
| Train/Return            | 1.3016257    |
| Train/V                 | 1.7080935    |
| Train/Value             | 1.7080935    |
| Train/control_penalty   | 0.7276483    |
| Train/policy_loss       | 0.40942067   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0          |
------------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 8            |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0793       |
| Time/Buffer             | 0.00361      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.6872854    |
| Train/Action_magnitu... | 1.6777537    |
| Train/Action_magnitude  | 1.3914226    |
| Train/Action_max        | 0.17919071   |
| Train/Action_std        | 0.654595     |
| Train/Entropy           | 0.8170902    |
| Train/Entropy_Loss      | -0.000817    |
| Train/Entropy_loss      | -0.000817    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7065824   |
| Train/Loss              | -0.13785689  |
| Train/PolicyClip        | -0.001224076 |
| Train/Policy_loss       | -0.15099889  |
| Train/Ratio             | 0.9995866    |
| Train/Return            | 1.0604819    |
| Train/V                 | 0.9149348    |
| Train/Value             | 0.9149348    |
| Train/control_penalty   | 1.3959093    |
| Train/policy_loss       | -0.15099889  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 9            |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.077        |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00461      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.60878086   |
| Train/Action_magnitu... | 1.4317107    |
| Train/Action_magnitude  | 1.177319     |
| Train/Action_max        | -0.037690964 |
| Train/Action_std        | 0.58644533   |
| Train/Entropy           | 0.7418275    |
| Train/Entropy_Loss      | -0.000742    |
| Train/Entropy_loss      | -0.000742    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.4060091   |
| Train/Loss              | 0.1254611    |
| Train/PolicyClip        | -0.006745236 |
| Train/Policy_loss       | 0.11388884   |
| Train/Ratio             | 0.9945198    |
| Train/Return            | 0.907217     |
| Train/V                 | 1.0264633    |
| Train/Value             | 1.0264633    |
| Train/control_penalty   | 1.2314081    |
| Train/policy_loss       | 0.11388884   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0085       |
------------------------------------------

 ---------------- Iteration 11 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 10            |
| Time/Actor_Time         | 0.0997        |
| Time/B_Format_Time      | 0.0795        |
| Time/B_Original_Form... | 0.0791        |
| Time/Buffer             | 0.00152       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.5183215     |
| Train/Action_magnitu... | 1.4961022     |
| Train/Action_magnitude  | 1.1891328     |
| Train/Action_max        | 0.26678973    |
| Train/Action_std        | 0.6161129     |
| Train/Entropy           | 0.81279707    |
| Train/Entropy_Loss      | -0.000813     |
| Train/Entropy_loss      | -0.000813     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.6148245    |
| Train/Loss              | 0.12746502    |
| Train/PolicyClip        | -0.0014352455 |
| Train/Policy_loss       | 0.11651033    |
| Train/Ratio             | 0.97980815    |
| Train/Return            | 0.7728124     |
| Train/V                 | 0.8938003     |
| Train/Value             | 0.8938003     |
| Train/control_penalty   | 1.1767496     |
| Train/policy_loss       | 0.11651033    |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0025        |
-------------------------------------------

 ---------------- Iteration 12 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 11           |
| Time/Actor_Time         | 0.0833       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.46137485   |
| Train/Action_magnitu... | 1.4766573    |
| Train/Action_magnitude  | 1.1715267    |
| Train/Action_max        | 0.3560527    |
| Train/Action_std        | 0.64208114   |
| Train/Entropy           | 0.8411588    |
| Train/Entropy_Loss      | -0.000841    |
| Train/Entropy_loss      | -0.000841    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7530442   |
| Train/Loss              | -0.01569936  |
| Train/PolicyClip        | 0.0013834664 |
| Train/Policy_loss       | -0.026284    |
| Train/Ratio             | 0.99604714   |
| Train/Return            | 0.8384905    |
| Train/V                 | 0.81745374   |
| Train/Value             | 0.81745374   |
| Train/control_penalty   | 1.1425798    |
| Train/policy_loss       | -0.026284    |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01275      |
------------------------------------------

 ---------------- Iteration 13 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 12            |
| Time/Actor_Time         | 0.0864        |
| Time/B_Format_Time      | 0.072         |
| Time/B_Original_Form... | 0.0707        |
| Time/Buffer             | 0.00292       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.4788274     |
| Train/Action_magnitu... | 1.4655343     |
| Train/Action_magnitude  | 1.1549745     |
| Train/Action_max        | 0.3221899     |
| Train/Action_std        | 0.64510185    |
| Train/Entropy           | 0.8632003     |
| Train/Entropy_Loss      | -0.000863     |
| Train/Entropy_loss      | -0.000863     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.7186853    |
| Train/Loss              | 0.06598688    |
| Train/PolicyClip        | -0.0009924135 |
| Train/Policy_loss       | 0.055139504   |
| Train/Ratio             | 0.9826937     |
| Train/Return            | 0.70800155    |
| Train/V                 | 0.7705496     |
| Train/Value             | 0.7705496     |
| Train/control_penalty   | 1.1710579     |
| Train/policy_loss       | 0.055139504   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0085        |
-------------------------------------------

 ---------------- Iteration 14 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 13            |
| Time/Actor_Time         | 0.0848        |
| Time/B_Format_Time      | 0.0749        |
| Time/B_Original_Form... | 0.0726        |
| Time/Buffer             | 0.00146       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.5126018     |
| Train/Action_magnitu... | 1.6215341     |
| Train/Action_magnitude  | 1.2786577     |
| Train/Action_max        | 0.45026255    |
| Train/Action_std        | 0.69946736    |
| Train/Entropy           | 0.90947145    |
| Train/Entropy_Loss      | -0.000909     |
| Train/Entropy_loss      | -0.000909     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.8345113    |
| Train/Loss              | 0.037948374   |
| Train/PolicyClip        | 0.00020905877 |
| Train/Policy_loss       | 0.026327949   |
| Train/Ratio             | 0.98448783    |
| Train/Return            | 0.5911615     |
| Train/V                 | 0.6230455     |
| Train/Value             | 0.6230455     |
| Train/control_penalty   | 1.2529898     |
| Train/policy_loss       | 0.026327949   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.00525       |
-------------------------------------------

 ---------------- Iteration 15 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 14            |
| Time/Actor_Time         | 0.0835        |
| Time/B_Format_Time      | 0.0731        |
| Time/B_Original_Form... | 0.0716        |
| Time/Buffer             | 0.00573       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.67366654    |
| Train/Action_magnitu... | 1.8485296     |
| Train/Action_magnitude  | 1.5249251     |
| Train/Action_max        | 0.11457022    |
| Train/Action_std        | 0.76526064    |
| Train/Entropy           | 0.95727825    |
| Train/Entropy_Loss      | -0.000957     |
| Train/Entropy_loss      | -0.000957     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.9381628    |
| Train/Loss              | 0.09748891    |
| Train/PolicyClip        | 0.00061728276 |
| Train/Policy_loss       | 0.08373109    |
| Train/Ratio             | 0.9887297     |
| Train/Return            | 0.4889623     |
| Train/V                 | 0.5759225     |
| Train/Value             | 0.5759225     |
| Train/control_penalty   | 1.4715098     |
| Train/policy_loss       | 0.08373109    |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0025        |
-------------------------------------------

 ---------------- Iteration 16 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 15           |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0745       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.0026       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.60303015   |
| Train/Action_magnitu... | 1.6962341    |
| Train/Action_magnitude  | 1.4049747    |
| Train/Action_max        | 0.014659738  |
| Train/Action_std        | 0.7928979    |
| Train/Entropy           | 1.033606     |
| Train/Entropy_Loss      | -0.00103     |
| Train/Entropy_loss      | -0.00103     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.975258    |
| Train/Loss              | 0.10297008   |
| Train/PolicyClip        | -0.002225129 |
| Train/Policy_loss       | 0.09024547   |
| Train/Ratio             | 0.98715824   |
| Train/Return            | 0.3817045    |
| Train/V                 | 0.47462237   |
| Train/Value             | 0.47462237   |
| Train/control_penalty   | 1.3758216    |
| Train/policy_loss       | 0.09024547   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.001        |
------------------------------------------

 ---------------- Iteration 17 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 16            |
| Time/Actor_Time         | 0.0841        |
| Time/B_Format_Time      | 0.074         |
| Time/B_Original_Form... | 0.0739        |
| Time/Buffer             | 0.00278       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.32071963    |
| Train/Action_magnitu... | 1.1892346     |
| Train/Action_magnitude  | 0.9548748     |
| Train/Action_max        | 0.1315812     |
| Train/Action_std        | 0.60397756    |
| Train/Entropy           | 0.8434204     |
| Train/Entropy_Loss      | -0.000843     |
| Train/Entropy_loss      | -0.000843     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.721836     |
| Train/Loss              | -0.0007627662 |
| Train/PolicyClip        | 0.00074985385 |
| Train/Policy_loss       | -0.009157109  |
| Train/Ratio             | 0.9968508     |
| Train/Return            | 0.3776877     |
| Train/V                 | 0.37189656    |
| Train/Value             | 0.37189656    |
| Train/control_penalty   | 0.9237763     |
| Train/policy_loss       | -0.009157109  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.00475       |
-------------------------------------------

 ---------------- Iteration 18 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 17           |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00259      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26633248   |
| Train/Action_magnitu... | 1.109973     |
| Train/Action_magnitude  | 0.88017833   |
| Train/Action_max        | 0.21270366   |
| Train/Action_std        | 0.587554     |
| Train/Entropy           | 0.8373744    |
| Train/Entropy_Loss      | -0.000837    |
| Train/Entropy_loss      | -0.000837    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7298155   |
| Train/Loss              | -0.07079846  |
| Train/PolicyClip        | 0.004377742  |
| Train/Policy_loss       | -0.078594305 |
| Train/Ratio             | 1.0042356    |
| Train/Return            | 0.38713732   |
| Train/V                 | 0.3126992    |
| Train/Value             | 0.3126992    |
| Train/control_penalty   | 0.86332166   |
| Train/policy_loss       | -0.078594305 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00975      |
------------------------------------------

 ---------------- Iteration 19 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 18           |
| Time/Actor_Time         | 0.0845       |
| Time/B_Format_Time      | 0.0729       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.29042345   |
| Train/Action_magnitu... | 1.1376784    |
| Train/Action_magnitude  | 0.8965655    |
| Train/Action_max        | 0.20979156   |
| Train/Action_std        | 0.570562     |
| Train/Entropy           | 0.78789455   |
| Train/Entropy_Loss      | -0.000788    |
| Train/Entropy_loss      | -0.000788    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.6483881   |
| Train/Loss              | -0.10039169  |
| Train/PolicyClip        | 0.0014544484 |
| Train/Policy_loss       | -0.10844765  |
| Train/Ratio             | 0.99983823   |
| Train/Return            | 0.39157337   |
| Train/V                 | 0.29021966   |
| Train/Value             | 0.29021966   |
| Train/control_penalty   | 0.88438463   |
| Train/policy_loss       | -0.10844765  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0115       |
------------------------------------------

 ---------------- Iteration 20 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 19           |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00259      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36238652   |
| Train/Action_magnitu... | 1.2416975    |
| Train/Action_magnitude  | 0.9867268    |
| Train/Action_max        | 0.3574483    |
| Train/Action_std        | 0.6069502    |
| Train/Entropy           | 0.8309405    |
| Train/Entropy_Loss      | -0.000831    |
| Train/Entropy_loss      | -0.000831    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7131716   |
| Train/Loss              | -0.056454685 |
| Train/PolicyClip        | 0.00133483   |
| Train/Policy_loss       | -0.06541817  |
| Train/Ratio             | 1.0011135    |
| Train/Return            | 0.51935047   |
| Train/V                 | 0.45941335   |
| Train/Value             | 0.45941335   |
| Train/control_penalty   | 0.9794424    |
| Train/policy_loss       | -0.06541817  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01275      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 21 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 20             |
| Time/Actor_Time         | 0.0853         |
| Time/B_Format_Time      | 0.0701         |
| Time/B_Original_Form... | 0.0707         |
| Time/Buffer             | 0.00243        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.32114214     |
| Train/Action_magnitu... | 1.166557       |
| Train/Action_magnitude  | 0.9314748      |
| Train/Action_max        | 0.24075165     |
| Train/Action_std        | 0.5924541      |
| Train/Entropy           | 0.8009993      |
| Train/Entropy_Loss      | -0.000801      |
| Train/Entropy_loss      | -0.000801      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -1.5570574     |
| Train/Loss              | 0.015696838    |
| Train/PolicyClip        | -0.00042277062 |
| Train/Policy_loss       | 0.0071156914   |
| Train/Ratio             | 0.98803675     |
| Train/Return            | 0.36794764     |
| Train/V                 | 0.37969294     |
| Train/Value             | 0.37969294     |
| Train/control_penalty   | 0.93821466     |
| Train/policy_loss       | 0.0071156914   |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.00625        |
--------------------------------------------

 ---------------- Iteration 22 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 21            |
| Time/Actor_Time         | 0.0883        |
| Time/B_Format_Time      | 0.0707        |
| Time/B_Original_Form... | 0.0727        |
| Time/Buffer             | 0.00236       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.43225414    |
| Train/Action_magnitu... | 1.4055961     |
| Train/Action_magnitude  | 1.1353159     |
| Train/Action_max        | 0.32720044    |
| Train/Action_std        | 0.6240793     |
| Train/Entropy           | 0.8055968     |
| Train/Entropy_Loss      | -0.000806     |
| Train/Entropy_loss      | -0.000806     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.6406703    |
| Train/Loss              | 0.03580548    |
| Train/PolicyClip        | 0.00059121154 |
| Train/Policy_loss       | 0.025646985   |
| Train/Ratio             | 0.98562366    |
| Train/Return            | 0.3848989     |
| Train/V                 | 0.41401732    |
| Train/Value             | 0.41401732    |
| Train/control_penalty   | 1.0964094     |
| Train/policy_loss       | 0.025646985   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0055        |
-------------------------------------------

 ---------------- Iteration 23 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 22           |
| Time/Actor_Time         | 0.0878       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00266      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.41798732   |
| Train/Action_magnitu... | 1.3970472    |
| Train/Action_magnitude  | 1.1172758    |
| Train/Action_max        | 0.3906017    |
| Train/Action_std        | 0.67182726   |
| Train/Entropy           | 0.87623453   |
| Train/Entropy_Loss      | -0.000876    |
| Train/Entropy_loss      | -0.000876    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.8058898   |
| Train/Loss              | 0.058189087  |
| Train/PolicyClip        | 0.0006354445 |
| Train/Policy_loss       | 0.047913276  |
| Train/Ratio             | 0.9858627    |
| Train/Return            | 0.3860717    |
| Train/V                 | 0.4371541    |
| Train/Value             | 0.4371541    |
| Train/control_penalty   | 1.1152045    |
| Train/policy_loss       | 0.047913276  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00575      |
------------------------------------------

 ---------------- Iteration 24 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 23            |
| Time/Actor_Time         | 0.0849        |
| Time/B_Format_Time      | 0.0716        |
| Time/B_Original_Form... | 0.0689        |
| Time/Buffer             | 0.00235       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.38198385    |
| Train/Action_magnitu... | 1.380935      |
| Train/Action_magnitude  | 1.1146576     |
| Train/Action_max        | 0.30053607    |
| Train/Action_std        | 0.6784891     |
| Train/Entropy           | 0.9007155     |
| Train/Entropy_Loss      | -0.000901     |
| Train/Entropy_loss      | -0.000901     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.8058897    |
| Train/Loss              | 0.06796669    |
| Train/PolicyClip        | 7.7437384e-05 |
| Train/Policy_loss       | 0.05792026    |
| Train/Ratio             | 0.98911124    |
| Train/Return            | 0.34070498    |
| Train/V                 | 0.40118933    |
| Train/Value             | 0.40118933    |
| Train/control_penalty   | 1.0947146     |
| Train/policy_loss       | 0.05792026    |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.00475       |
-------------------------------------------

 ---------------- Iteration 25 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 24            |
| Time/Actor_Time         | 0.0848        |
| Time/B_Format_Time      | 0.0708        |
| Time/B_Original_Form... | 0.0752        |
| Time/Buffer             | 0.00309       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.41450343    |
| Train/Action_magnitu... | 1.4354942     |
| Train/Action_magnitude  | 1.1710387     |
| Train/Action_max        | 0.13389644    |
| Train/Action_std        | 0.7139518     |
| Train/Entropy           | 0.93018883    |
| Train/Entropy_Loss      | -0.00093      |
| Train/Entropy_loss      | -0.00093      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.8565739    |
| Train/Loss              | 0.038321562   |
| Train/PolicyClip        | 0.00031171387 |
| Train/Policy_loss       | 0.028132433   |
| Train/Ratio             | 0.9918667     |
| Train/Return            | 0.33358222    |
| Train/V                 | 0.3648958     |
| Train/Value             | 0.3648958     |
| Train/control_penalty   | 1.1119318     |
| Train/policy_loss       | 0.028132433   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.005         |
-------------------------------------------

 ---------------- Iteration 26 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 25           |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3865519    |
| Train/Action_magnitu... | 1.293942     |
| Train/Action_magnitude  | 1.0456461    |
| Train/Action_max        | 0.19142011   |
| Train/Action_std        | 0.6370863    |
| Train/Entropy           | 0.8273308    |
| Train/Entropy_Loss      | -0.000827    |
| Train/Entropy_loss      | -0.000827    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7191542   |
| Train/Loss              | -0.030943207 |
| Train/PolicyClip        | 0.0009624642 |
| Train/Policy_loss       | -0.040283933 |
| Train/Ratio             | 0.9960816    |
| Train/Return            | 0.36350277   |
| Train/V                 | 0.32655      |
| Train/Value             | 0.32655      |
| Train/control_penalty   | 1.0168058    |
| Train/policy_loss       | -0.040283933 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00775      |
------------------------------------------

 ---------------- Iteration 27 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 26           |
| Time/Actor_Time         | 0.109        |
| Time/B_Format_Time      | 0.0777       |
| Time/B_Original_Form... | 0.0754       |
| Time/Buffer             | 0.00247      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.40387475   |
| Train/Action_magnitu... | 1.2155106    |
| Train/Action_magnitude  | 0.96235704   |
| Train/Action_max        | 0.23439063   |
| Train/Action_std        | 0.5459595    |
| Train/Entropy           | 0.7004983    |
| Train/Entropy_Loss      | -0.0007      |
| Train/Entropy_loss      | -0.0007      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.3960826   |
| Train/Loss              | -0.066305496 |
| Train/PolicyClip        | 0.0011186815 |
| Train/Policy_loss       | -0.075163834 |
| Train/Ratio             | 1.0049832    |
| Train/Return            | 0.4223982    |
| Train/V                 | 0.35159633   |
| Train/Value             | 0.35159633   |
| Train/control_penalty   | 0.95588326   |
| Train/policy_loss       | -0.075163834 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01025      |
------------------------------------------

 ---------------- Iteration 28 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 27           |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0811       |
| Time/B_Original_Form... | 0.0805       |
| Time/Buffer             | 0.00223      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.41829214   |
| Train/Action_magnitu... | 1.3010848    |
| Train/Action_magnitude  | 1.0416452    |
| Train/Action_max        | 0.3754216    |
| Train/Action_std        | 0.5968329    |
| Train/Entropy           | 0.7764234    |
| Train/Entropy_Loss      | -0.000776    |
| Train/Entropy_loss      | -0.000776    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.531454    |
| Train/Loss              | 0.0045932806 |
| Train/PolicyClip        | 0.0021140454 |
| Train/Policy_loss       | -0.004896989 |
| Train/Ratio             | 0.9951952    |
| Train/Return            | 0.41425556   |
| Train/V                 | 0.41331697   |
| Train/Value             | 0.41331697   |
| Train/control_penalty   | 1.0266694    |
| Train/policy_loss       | -0.004896989 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00575      |
------------------------------------------

 ---------------- Iteration 29 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 28          |
| Time/Actor_Time         | 0.0862      |
| Time/B_Format_Time      | 0.0749      |
| Time/B_Original_Form... | 0.0748      |
| Time/Buffer             | 0.0029      |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.41822127  |
| Train/Action_magnitu... | 1.3790175   |
| Train/Action_magnitude  | 1.1053517   |
| Train/Action_max        | 0.4371251   |
| Train/Action_std        | 0.6780969   |
| Train/Entropy           | 0.8763398   |
| Train/Entropy_Loss      | -0.000876   |
| Train/Entropy_loss      | -0.000876   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.7311618  |
| Train/Loss              | -0.03532935 |
| Train/PolicyClip        | 0.001685117 |
| Train/Policy_loss       | -0.04545428 |
| Train/Ratio             | 0.9987179   |
| Train/Return            | 0.58130664  |
| Train/V                 | 0.5413997   |
| Train/Value             | 0.5413997   |
| Train/control_penalty   | 1.100127    |
| Train/policy_loss       | -0.04545428 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0095      |
-----------------------------------------

 ---------------- Iteration 30 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 29           |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00235      |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.34129584   |
| Train/Action_magnitu... | 1.1902883    |
| Train/Action_magnitude  | 0.944977     |
| Train/Action_max        | 0.35000408   |
| Train/Action_std        | 0.6014999    |
| Train/Entropy           | 0.793137     |
| Train/Entropy_Loss      | -0.000793    |
| Train/Entropy_loss      | -0.000793    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.6085349   |
| Train/Loss              | 0.03523752   |
| Train/PolicyClip        | 0.0016013166 |
| Train/Policy_loss       | 0.026247837  |
| Train/Ratio             | 0.9917031    |
| Train/Return            | 0.5879311    |
| Train/V                 | 0.62202805   |
| Train/Value             | 0.62202805   |
| Train/control_penalty   | 0.9782823    |
| Train/policy_loss       | 0.026247837  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00575      |
------------------------------------------

 ---------------- Iteration 31 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 30           |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00516      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3702936    |
| Train/Action_magnitu... | 1.2868115    |
| Train/Action_magnitude  | 1.0220633    |
| Train/Action_max        | 0.37967166   |
| Train/Action_std        | 0.6323541    |
| Train/Entropy           | 0.8318474    |
| Train/Entropy_Loss      | -0.000832    |
| Train/Entropy_loss      | -0.000832    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.6847849   |
| Train/Loss              | 0.01464199   |
| Train/PolicyClip        | 0.0013334219 |
| Train/Policy_loss       | 0.005441467  |
| Train/Ratio             | 0.9889254    |
| Train/Return            | 0.47573313   |
| Train/V                 | 0.48669657   |
| Train/Value             | 0.48669657   |
| Train/control_penalty   | 1.0032371    |
| Train/policy_loss       | 0.005441467  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0065       |
------------------------------------------

 ---------------- Iteration 32 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 31           |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0731       |
| Time/Buffer             | 0.00318      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3938909    |
| Train/Action_magnitu... | 1.3432018    |
| Train/Action_magnitude  | 1.0744133    |
| Train/Action_max        | 0.4280372    |
| Train/Action_std        | 0.6672982    |
| Train/Entropy           | 0.861014     |
| Train/Entropy_Loss      | -0.000861    |
| Train/Entropy_loss      | -0.000861    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7504299   |
| Train/Loss              | -0.06956314  |
| Train/PolicyClip        | 0.0019859981 |
| Train/Policy_loss       | -0.07934594  |
| Train/Ratio             | 0.99993604   |
| Train/Return            | 0.6325995    |
| Train/V                 | 0.5597646    |
| Train/Value             | 0.5597646    |
| Train/control_penalty   | 1.0643811    |
| Train/policy_loss       | -0.07934594  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 33 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 32           |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0689       |
| Time/Buffer             | 0.00309      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3794398    |
| Train/Action_magnitu... | 1.2558745    |
| Train/Action_magnitude  | 0.9934356    |
| Train/Action_max        | 0.44071573   |
| Train/Action_std        | 0.59245163   |
| Train/Entropy           | 0.78843105   |
| Train/Entropy_Loss      | -0.000788    |
| Train/Entropy_loss      | -0.000788    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.6354824   |
| Train/Loss              | -0.07155798  |
| Train/PolicyClip        | 0.0013958869 |
| Train/Policy_loss       | -0.080494516 |
| Train/Ratio             | 1.0030668    |
| Train/Return            | 0.6831371    |
| Train/V                 | 0.6081558    |
| Train/Value             | 0.6081558    |
| Train/control_penalty   | 0.9724969    |
| Train/policy_loss       | -0.080494516 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01725      |
------------------------------------------

 ---------------- Iteration 34 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 33           |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.00242      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32532927   |
| Train/Action_magnitu... | 1.1691461    |
| Train/Action_magnitude  | 0.92495316   |
| Train/Action_max        | 0.40904766   |
| Train/Action_std        | 0.55698395   |
| Train/Entropy           | 0.7376305    |
| Train/Entropy_Loss      | -0.000738    |
| Train/Entropy_loss      | -0.000738    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5629302   |
| Train/Loss              | -0.08854807  |
| Train/PolicyClip        | 0.0025798627 |
| Train/Policy_loss       | -0.09683455  |
| Train/Ratio             | 1.0005283    |
| Train/Return            | 0.60246      |
| Train/V                 | 0.5111771    |
| Train/Value             | 0.5111771    |
| Train/control_penalty   | 0.90241086   |
| Train/policy_loss       | -0.09683455  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 35 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 34           |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00254      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35052922   |
| Train/Action_magnitu... | 1.2145771    |
| Train/Action_magnitude  | 0.96252525   |
| Train/Action_max        | 0.3812846    |
| Train/Action_std        | 0.5979861    |
| Train/Entropy           | 0.7842778    |
| Train/Entropy_Loss      | -0.000784    |
| Train/Entropy_loss      | -0.000784    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5817288   |
| Train/Loss              | -0.041974027 |
| Train/PolicyClip        | 0.003528744  |
| Train/Policy_loss       | -0.050753746 |
| Train/Ratio             | 1.0023055    |
| Train/Return            | 0.60415673   |
| Train/V                 | 0.56089896   |
| Train/Value             | 0.56089896   |
| Train/control_penalty   | 0.95639974   |
| Train/policy_loss       | -0.050753746 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0115       |
------------------------------------------

 ---------------- Iteration 36 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 35           |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00257      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31658924   |
| Train/Action_magnitu... | 1.1650097    |
| Train/Action_magnitude  | 0.9246442    |
| Train/Action_max        | 0.38304287   |
| Train/Action_std        | 0.5614095    |
| Train/Entropy           | 0.74453515   |
| Train/Entropy_Loss      | -0.000745    |
| Train/Entropy_loss      | -0.000745    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5651908   |
| Train/Loss              | 0.005285319  |
| Train/PolicyClip        | 0.0016484042 |
| Train/Policy_loss       | -0.002841163 |
| Train/Ratio             | 0.9997019    |
| Train/Return            | 0.47284657   |
| Train/V                 | 0.47541457   |
| Train/Value             | 0.47541457   |
| Train/control_penalty   | 0.8871017    |
| Train/policy_loss       | -0.002841163 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00675      |
------------------------------------------

 ---------------- Iteration 37 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 36            |
| Time/Actor_Time         | 0.0861        |
| Time/B_Format_Time      | 0.071         |
| Time/B_Original_Form... | 0.07          |
| Time/Buffer             | 0.00239       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31101933    |
| Train/Action_magnitu... | 1.1415558     |
| Train/Action_magnitude  | 0.9025661     |
| Train/Action_max        | 0.4197224     |
| Train/Action_std        | 0.5868976     |
| Train/Entropy           | 0.7960866     |
| Train/Entropy_Loss      | -0.000796     |
| Train/Entropy_loss      | -0.000796     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.6378996    |
| Train/Loss              | 0.0079399925  |
| Train/PolicyClip        | 0.0014756557  |
| Train/Policy_loss       | -0.0004091954 |
| Train/Ratio             | 1.0010247     |
| Train/Return            | 0.40309167    |
| Train/V                 | 0.40730312    |
| Train/Value             | 0.40730312    |
| Train/control_penalty   | 0.91452754    |
| Train/policy_loss       | -0.0004091954 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.006         |
-------------------------------------------

 ---------------- Iteration 38 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 37            |
| Time/Actor_Time         | 0.0853        |
| Time/B_Format_Time      | 0.072         |
| Time/B_Original_Form... | 0.0697        |
| Time/Buffer             | 0.00312       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31838384    |
| Train/Action_magnitu... | 1.1935574     |
| Train/Action_magnitude  | 0.9465326     |
| Train/Action_max        | 0.37934828    |
| Train/Action_std        | 0.584892      |
| Train/Entropy           | 0.78461057    |
| Train/Entropy_Loss      | -0.000785     |
| Train/Entropy_loss      | -0.000785     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.6765238    |
| Train/Loss              | 0.0070636575  |
| Train/PolicyClip        | 0.0013157618  |
| Train/Policy_loss       | -0.0013556279 |
| Train/Ratio             | 0.9945702     |
| Train/Return            | 0.4179054     |
| Train/V                 | 0.42218077    |
| Train/Value             | 0.42218077    |
| Train/control_penalty   | 0.9203896     |
| Train/policy_loss       | -0.0013556279 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.00675       |
-------------------------------------------

 ---------------- Iteration 39 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 38           |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0693       |
| Time/Buffer             | 0.00496      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.36179155   |
| Train/Action_magnitu... | 1.190448     |
| Train/Action_magnitude  | 0.9401543    |
| Train/Action_max        | 0.3625702    |
| Train/Action_std        | 0.53546786   |
| Train/Entropy           | 0.6658671    |
| Train/Entropy_Loss      | -0.000666    |
| Train/Entropy_loss      | -0.000666    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.4080613   |
| Train/Loss              | -0.017347721 |
| Train/PolicyClip        | 0.0036168376 |
| Train/Policy_loss       | -0.025831707 |
| Train/Ratio             | 1.0000832    |
| Train/Return            | 0.698143     |
| Train/V                 | 0.67864245   |
| Train/Value             | 0.67864245   |
| Train/control_penalty   | 0.91498536   |
| Train/policy_loss       | -0.025831707 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00925      |
------------------------------------------

 ---------------- Iteration 40 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 39           |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3895517    |
| Train/Action_magnitu... | 1.2410557    |
| Train/Action_magnitude  | 0.97541565   |
| Train/Action_max        | 0.37945935   |
| Train/Action_std        | 0.5481173    |
| Train/Entropy           | 0.6271818    |
| Train/Entropy_Loss      | -0.000627    |
| Train/Entropy_loss      | -0.000627    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.335742    |
| Train/Loss              | -0.07607501  |
| Train/PolicyClip        | 0.0028829875 |
| Train/Policy_loss       | -0.08498926  |
| Train/Ratio             | 1.0057197    |
| Train/Return            | 0.8925967    |
| Train/V                 | 0.81590086   |
| Train/Value             | 0.81590086   |
| Train/control_penalty   | 0.9541429    |
| Train/policy_loss       | -0.08498926  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01625      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 41 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 40           |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3950566    |
| Train/Action_magnitu... | 1.3581841    |
| Train/Action_magnitude  | 1.0659375    |
| Train/Action_max        | 0.40789104   |
| Train/Action_std        | 0.63056886   |
| Train/Entropy           | 0.78923774   |
| Train/Entropy_Loss      | -0.000789    |
| Train/Entropy_loss      | -0.000789    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5805451   |
| Train/Loss              | -0.003520145 |
| Train/PolicyClip        | 0.002710412  |
| Train/Policy_loss       | -0.013227593 |
| Train/Ratio             | 0.9968515    |
| Train/Return            | 1.0050864    |
| Train/V                 | 1.0016034    |
| Train/Value             | 1.0016034    |
| Train/control_penalty   | 1.0496687    |
| Train/policy_loss       | -0.013227593 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01725      |
------------------------------------------

 ---------------- Iteration 42 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 41           |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00263      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36213917   |
| Train/Action_magnitu... | 1.3445529    |
| Train/Action_magnitude  | 1.0614417    |
| Train/Action_max        | 0.42510673   |
| Train/Action_std        | 0.6811106    |
| Train/Entropy           | 0.8867132    |
| Train/Entropy_Loss      | -0.000887    |
| Train/Entropy_loss      | -0.000887    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7809744   |
| Train/Loss              | -0.044846483 |
| Train/PolicyClip        | 0.005324084  |
| Train/Policy_loss       | -0.054701317 |
| Train/Ratio             | 1.0058985    |
| Train/Return            | 0.9996421    |
| Train/V                 | 0.9505322    |
| Train/Value             | 0.9505322    |
| Train/control_penalty   | 1.0741547    |
| Train/policy_loss       | -0.054701317 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0195       |
------------------------------------------

 ---------------- Iteration 43 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 42           |
| Time/Actor_Time         | 0.0881       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00247      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33361766   |
| Train/Action_magnitu... | 1.3007976    |
| Train/Action_magnitude  | 1.0299362    |
| Train/Action_max        | 0.4209251    |
| Train/Action_std        | 0.66690344   |
| Train/Entropy           | 0.8849382    |
| Train/Entropy_Loss      | -0.000885    |
| Train/Entropy_loss      | -0.000885    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7928561   |
| Train/Loss              | -0.028802864 |
| Train/PolicyClip        | 0.006167137  |
| Train/Policy_loss       | -0.038084675 |
| Train/Ratio             | 0.9987078    |
| Train/Return            | 0.95609313   |
| Train/V                 | 0.9219485    |
| Train/Value             | 0.9219485    |
| Train/control_penalty   | 1.0166749    |
| Train/policy_loss       | -0.038084675 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01725      |
------------------------------------------

 ---------------- Iteration 44 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 43           |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.0734       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00411      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32689545   |
| Train/Action_magnitu... | 1.3069228    |
| Train/Action_magnitude  | 1.0298873    |
| Train/Action_max        | 0.4184884    |
| Train/Action_std        | 0.64302576   |
| Train/Entropy           | 0.85404617   |
| Train/Entropy_Loss      | -0.000854    |
| Train/Entropy_loss      | -0.000854    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.812274    |
| Train/Loss              | -0.06279909  |
| Train/PolicyClip        | 0.0051505314 |
| Train/Policy_loss       | -0.07187631  |
| Train/Ratio             | 1.0056694    |
| Train/Return            | 0.9550824    |
| Train/V                 | 0.88998276   |
| Train/Value             | 0.88998276   |
| Train/control_penalty   | 0.99312633   |
| Train/policy_loss       | -0.07187631  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01625      |
------------------------------------------

 ---------------- Iteration 45 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 44           |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.072        |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.4173979    |
| Train/Action_magnitu... | 1.3211163    |
| Train/Action_magnitude  | 1.0378159    |
| Train/Action_max        | 0.4017683    |
| Train/Action_std        | 0.58633864   |
| Train/Entropy           | 0.7354139    |
| Train/Entropy_Loss      | -0.000735    |
| Train/Entropy_loss      | -0.000735    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.648623    |
| Train/Loss              | -0.08724831  |
| Train/PolicyClip        | 0.0046531735 |
| Train/Policy_loss       | -0.09660878  |
| Train/Ratio             | 1.005443     |
| Train/Return            | 1.0677161    |
| Train/V                 | 0.973424     |
| Train/Value             | 0.973424     |
| Train/control_penalty   | 1.0095887    |
| Train/policy_loss       | -0.09660878  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 46 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 45           |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00269      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.41557258   |
| Train/Action_magnitu... | 1.3444312    |
| Train/Action_magnitude  | 1.0563047    |
| Train/Action_max        | 0.4461785    |
| Train/Action_std        | 0.651198     |
| Train/Entropy           | 0.8550159    |
| Train/Entropy_Loss      | -0.000855    |
| Train/Entropy_loss      | -0.000855    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.711323    |
| Train/Loss              | 0.024887763  |
| Train/PolicyClip        | 0.0035695718 |
| Train/Policy_loss       | 0.014880844  |
| Train/Ratio             | 0.9898308    |
| Train/Return            | 0.8506128    |
| Train/V                 | 0.8747659    |
| Train/Value             | 0.8747659    |
| Train/control_penalty   | 1.0861934    |
| Train/policy_loss       | 0.014880844  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01125      |
------------------------------------------

 ---------------- Iteration 47 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 46           |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.0706       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.0023       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.44951084   |
| Train/Action_magnitu... | 1.3344102    |
| Train/Action_magnitude  | 1.049907     |
| Train/Action_max        | 0.41276446   |
| Train/Action_std        | 0.635693     |
| Train/Entropy           | 0.8114275    |
| Train/Entropy_Loss      | -0.000811    |
| Train/Entropy_loss      | -0.000811    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.611677    |
| Train/Loss              | 0.03996323   |
| Train/PolicyClip        | 0.0018204248 |
| Train/Policy_loss       | 0.029837433  |
| Train/Ratio             | 0.9904461    |
| Train/Return            | 0.8630387    |
| Train/V                 | 0.9026615    |
| Train/Value             | 0.9026615    |
| Train/control_penalty   | 1.0937227    |
| Train/policy_loss       | 0.029837433  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0095       |
------------------------------------------

 ---------------- Iteration 48 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 47           |
| Time/Actor_Time         | 0.087        |
| Time/B_Format_Time      | 0.0697       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00227      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.47657678   |
| Train/Action_magnitu... | 1.3894675    |
| Train/Action_magnitude  | 1.0915304    |
| Train/Action_max        | 0.43320194   |
| Train/Action_std        | 0.5693947    |
| Train/Entropy           | 0.69007254   |
| Train/Entropy_Loss      | -0.00069     |
| Train/Entropy_loss      | -0.00069     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5646309   |
| Train/Loss              | 0.07848963   |
| Train/PolicyClip        | 0.0048985104 |
| Train/Policy_loss       | 0.068849064  |
| Train/Ratio             | 0.9837166    |
| Train/Return            | 0.82379895   |
| Train/V                 | 0.8932247    |
| Train/Value             | 0.8932247    |
| Train/control_penalty   | 1.0330641    |
| Train/policy_loss       | 0.068849064  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00775      |
------------------------------------------

 ---------------- Iteration 49 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 48           |
| Time/Actor_Time         | 0.0876       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00237      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.44006714   |
| Train/Action_magnitu... | 1.3035308    |
| Train/Action_magnitude  | 1.0247554    |
| Train/Action_max        | 0.4508191    |
| Train/Action_std        | 0.59296864   |
| Train/Entropy           | 0.7295507    |
| Train/Entropy_Loss      | -0.00073     |
| Train/Entropy_loss      | -0.00073     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.4633209   |
| Train/Loss              | 0.07608872   |
| Train/PolicyClip        | 0.0017293122 |
| Train/Policy_loss       | 0.066548586  |
| Train/Ratio             | 0.9853341    |
| Train/Return            | 0.8519255    |
| Train/V                 | 0.92468935   |
| Train/Value             | 0.92468935   |
| Train/control_penalty   | 1.026969     |
| Train/policy_loss       | 0.066548586  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0075       |
------------------------------------------

 ---------------- Iteration 50 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 49           |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00231      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.43966419   |
| Train/Action_magnitu... | 1.356647     |
| Train/Action_magnitude  | 1.0619402    |
| Train/Action_max        | 0.41351813   |
| Train/Action_std        | 0.6176128    |
| Train/Entropy           | 0.7725331    |
| Train/Entropy_Loss      | -0.000773    |
| Train/Entropy_loss      | -0.000773    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5651051   |
| Train/Loss              | 0.06596552   |
| Train/PolicyClip        | 0.0058701145 |
| Train/Policy_loss       | 0.056124732  |
| Train/Ratio             | 0.9937666    |
| Train/Return            | 0.9448234    |
| Train/V                 | 1.007829     |
| Train/Value             | 1.007829     |
| Train/control_penalty   | 1.0613322    |
| Train/policy_loss       | 0.056124732  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01         |
------------------------------------------

 ---------------- Iteration 51 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 50          |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0702      |
| Time/Buffer             | 0.00252     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.4460922   |
| Train/Action_magnitu... | 1.4205693   |
| Train/Action_magnitude  | 1.1161432   |
| Train/Action_max        | 0.43097153  |
| Train/Action_std        | 0.69092286  |
| Train/Entropy           | 0.8747892   |
| Train/Entropy_Loss      | -0.000875   |
| Train/Entropy_loss      | -0.000875   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.7427278  |
| Train/Loss              | 0.05162225  |
| Train/PolicyClip        | 0.004948359 |
| Train/Policy_loss       | 0.040978283 |
| Train/Ratio             | 0.9980628   |
| Train/Return            | 0.91826123  |
| Train/V                 | 0.96600324  |
| Train/Value             | 0.96600324  |
| Train/control_penalty   | 1.1518753   |
| Train/policy_loss       | 0.040978283 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0115      |
-----------------------------------------

 ---------------- Iteration 52 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 51           |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0699       |
| Time/B_Original_Form... | 0.0696       |
| Time/Buffer             | 0.00228      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.4387611    |
| Train/Action_magnitu... | 1.433091     |
| Train/Action_magnitude  | 1.1302345    |
| Train/Action_max        | 0.4240484    |
| Train/Action_std        | 0.6901133    |
| Train/Entropy           | 0.8451565    |
| Train/Entropy_Loss      | -0.000845    |
| Train/Entropy_loss      | -0.000845    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.7345248   |
| Train/Loss              | 0.01718764   |
| Train/PolicyClip        | 0.0016737928 |
| Train/Policy_loss       | 0.006708048  |
| Train/Ratio             | 0.988632     |
| Train/Return            | 0.9590081    |
| Train/V                 | 0.9728514    |
| Train/Value             | 0.9728514    |
| Train/control_penalty   | 1.1324749    |
| Train/policy_loss       | 0.006708048  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01275      |
------------------------------------------

 ---------------- Iteration 53 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 52           |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.00237      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.4511839    |
| Train/Action_magnitu... | 1.3872802    |
| Train/Action_magnitude  | 1.095854     |
| Train/Action_max        | 0.44737336   |
| Train/Action_std        | 0.6501848    |
| Train/Entropy           | 0.78858495   |
| Train/Entropy_Loss      | -0.000789    |
| Train/Entropy_loss      | -0.000789    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.6032932   |
| Train/Loss              | -0.018358752 |
| Train/PolicyClip        | 0.005103143  |
| Train/Policy_loss       | -0.02856119  |
| Train/Ratio             | 1.0017339    |
| Train/Return            | 0.9145581    |
| Train/V                 | 0.8932603    |
| Train/Value             | 0.8932603    |
| Train/control_penalty   | 1.0991021    |
| Train/policy_loss       | -0.02856119  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.016        |
------------------------------------------

 ---------------- Iteration 54 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 53           |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0696       |
| Time/Buffer             | 0.00433      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.42267543   |
| Train/Action_magnitu... | 1.3095818    |
| Train/Action_magnitude  | 1.0291741    |
| Train/Action_max        | 0.4069069    |
| Train/Action_std        | 0.60944337   |
| Train/Entropy           | 0.7204307    |
| Train/Entropy_Loss      | -0.00072     |
| Train/Entropy_loss      | -0.00072     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.3886136   |
| Train/Loss              | -0.012851057 |
| Train/PolicyClip        | 0.003761758  |
| Train/Policy_loss       | -0.022568507 |
| Train/Ratio             | 0.9866032    |
| Train/Return            | 0.9506818    |
| Train/V                 | 0.93452626   |
| Train/Value             | 0.93452626   |
| Train/control_penalty   | 1.0437881    |
| Train/policy_loss       | -0.022568507 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.017        |
------------------------------------------

 ---------------- Iteration 55 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 54            |
| Time/Actor_Time         | 0.0853        |
| Time/B_Format_Time      | 0.0714        |
| Time/B_Original_Form... | 0.0698        |
| Time/Buffer             | 0.00241       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.415599      |
| Train/Action_magnitu... | 1.316766      |
| Train/Action_magnitude  | 1.0397971     |
| Train/Action_max        | 0.4390247     |
| Train/Action_std        | 0.6313622     |
| Train/Entropy           | 0.8040074     |
| Train/Entropy_Loss      | -0.000804     |
| Train/Entropy_loss      | -0.000804     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.621986     |
| Train/Loss              | 0.008254893   |
| Train/PolicyClip        | 0.0034976355  |
| Train/Policy_loss       | -0.0013178277 |
| Train/Ratio             | 0.9902729     |
| Train/Return            | 0.8486352     |
| Train/V                 | 0.85530484    |
| Train/Value             | 0.85530484    |
| Train/control_penalty   | 1.0376729     |
| Train/policy_loss       | -0.0013178277 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01625       |
-------------------------------------------

 ---------------- Iteration 56 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 55           |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.43057743   |
| Train/Action_magnitu... | 1.2776905    |
| Train/Action_magnitude  | 1.00785      |
| Train/Action_max        | 0.41969988   |
| Train/Action_std        | 0.6092162    |
| Train/Entropy           | 0.7342534    |
| Train/Entropy_Loss      | -0.000734    |
| Train/Entropy_loss      | -0.000734    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.4164258   |
| Train/Loss              | -0.05576426  |
| Train/PolicyClip        | 0.0032339056 |
| Train/Policy_loss       | -0.06554847  |
| Train/Ratio             | 0.9912544    |
| Train/Return            | 1.004876     |
| Train/V                 | 0.94551957   |
| Train/Value             | 0.94551957   |
| Train/control_penalty   | 1.0518466    |
| Train/policy_loss       | -0.06554847  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 57 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 56           |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.0026       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.4070611    |
| Train/Action_magnitu... | 1.3354758    |
| Train/Action_magnitude  | 1.054832     |
| Train/Action_max        | 0.44930363   |
| Train/Action_std        | 0.6094181    |
| Train/Entropy           | 0.7684314    |
| Train/Entropy_Loss      | -0.000768    |
| Train/Entropy_loss      | -0.000768    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5969461   |
| Train/Loss              | -0.02173132  |
| Train/PolicyClip        | 0.0036309292 |
| Train/Policy_loss       | -0.031225033 |
| Train/Ratio             | 0.99417275   |
| Train/Return            | 0.73322475   |
| Train/V                 | 0.70978636   |
| Train/Value             | 0.70978636   |
| Train/control_penalty   | 1.0262142    |
| Train/policy_loss       | -0.031225033 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 58 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 57           |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0692       |
| Time/Buffer             | 0.00242      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.39289236   |
| Train/Action_magnitu... | 1.1909606    |
| Train/Action_magnitude  | 0.9415073    |
| Train/Action_max        | 0.38546214   |
| Train/Action_std        | 0.5502343    |
| Train/Entropy           | 0.6888322    |
| Train/Entropy_Loss      | -0.000689    |
| Train/Entropy_loss      | -0.000689    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.3061568   |
| Train/Loss              | 0.024536947  |
| Train/PolicyClip        | 0.0031900115 |
| Train/Policy_loss       | 0.015817411  |
| Train/Ratio             | 0.9864186    |
| Train/Return            | 0.5292533    |
| Train/V                 | 0.55137813   |
| Train/Value             | 0.55137813   |
| Train/control_penalty   | 0.94083685   |
| Train/policy_loss       | 0.015817411  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00825      |
------------------------------------------

 ---------------- Iteration 59 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 58           |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00189      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35471335   |
| Train/Action_magnitu... | 1.2431761    |
| Train/Action_magnitude  | 0.97860944   |
| Train/Action_max        | 0.37247232   |
| Train/Action_std        | 0.60536814   |
| Train/Entropy           | 0.81700295   |
| Train/Entropy_Loss      | -0.000817    |
| Train/Entropy_loss      | -0.000817    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.6511165   |
| Train/Loss              | 0.069060475  |
| Train/PolicyClip        | 0.0013646148 |
| Train/Policy_loss       | 0.060379006  |
| Train/Ratio             | 0.97483814   |
| Train/Return            | 0.33617944   |
| Train/V                 | 0.40159047   |
| Train/Value             | 0.40159047   |
| Train/control_penalty   | 0.94984764   |
| Train/policy_loss       | 0.060379006  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00175      |
------------------------------------------

 ---------------- Iteration 60 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 59          |
| Time/Actor_Time         | 0.0871      |
| Time/B_Format_Time      | 0.0702      |
| Time/B_Original_Form... | 0.0692      |
| Time/Buffer             | 0.00255     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31586358  |
| Train/Action_magnitu... | 1.1512241   |
| Train/Action_magnitude  | 0.9048662   |
| Train/Action_max        | 0.3049882   |
| Train/Action_std        | 0.56123143  |
| Train/Entropy           | 0.7763302   |
| Train/Entropy_Loss      | -0.000776   |
| Train/Entropy_loss      | -0.000776   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.6235932  |
| Train/Loss              | 0.01995692  |
| Train/PolicyClip        | 0.001964757 |
| Train/Policy_loss       | 0.012005597 |
| Train/Ratio             | 0.9898424   |
| Train/Return            | 0.37669536  |
| Train/V                 | 0.39419922  |
| Train/Value             | 0.39419922  |
| Train/control_penalty   | 0.87276536  |
| Train/policy_loss       | 0.012005597 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.003       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 61 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 60           |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0726       |
| Time/Buffer             | 0.00238      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3521092    |
| Train/Action_magnitu... | 1.1630943    |
| Train/Action_magnitude  | 0.9142186    |
| Train/Action_max        | 0.31742975   |
| Train/Action_std        | 0.5675164    |
| Train/Entropy           | 0.7781328    |
| Train/Entropy_Loss      | -0.000778    |
| Train/Entropy_loss      | -0.000778    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.607802    |
| Train/Loss              | 0.041742682  |
| Train/PolicyClip        | 0.0015365365 |
| Train/Policy_loss       | 0.03342898   |
| Train/Ratio             | 0.9881379    |
| Train/Return            | 0.48151636   |
| Train/V                 | 0.5243131    |
| Train/Value             | 0.5243131    |
| Train/control_penalty   | 0.9091836    |
| Train/policy_loss       | 0.03342898   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0055       |
------------------------------------------

 ---------------- Iteration 62 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 61           |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36119175   |
| Train/Action_magnitu... | 1.1424444    |
| Train/Action_magnitude  | 0.897787     |
| Train/Action_max        | 0.3538429    |
| Train/Action_std        | 0.55361986   |
| Train/Entropy           | 0.7626604    |
| Train/Entropy_Loss      | -0.000763    |
| Train/Entropy_loss      | -0.000763    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.5666324   |
| Train/Loss              | -0.13652484  |
| Train/PolicyClip        | 0.0048220414 |
| Train/Policy_loss       | -0.14476866  |
| Train/Ratio             | 1.0057718    |
| Train/Return            | 0.5555883    |
| Train/V                 | 0.4189597    |
| Train/Value             | 0.4189597    |
| Train/control_penalty   | 0.9006467    |
| Train/policy_loss       | -0.14476866  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0155       |
------------------------------------------

 ---------------- Iteration 63 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 62           |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0701       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.00525      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.36810845   |
| Train/Action_magnitu... | 1.1625769    |
| Train/Action_magnitude  | 0.9150977    |
| Train/Action_max        | 0.3755924    |
| Train/Action_std        | 0.53649884   |
| Train/Entropy           | 0.72744095   |
| Train/Entropy_Loss      | -0.000727    |
| Train/Entropy_loss      | -0.000727    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.4995384   |
| Train/Loss              | -0.09447731  |
| Train/PolicyClip        | 0.0020308723 |
| Train/Policy_loss       | -0.102617055 |
| Train/Ratio             | 1.0011818    |
| Train/Return            | 0.70992815   |
| Train/V                 | 0.613292     |
| Train/Value             | 0.613292     |
| Train/control_penalty   | 0.8867184    |
| Train/policy_loss       | -0.102617055 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01525      |
------------------------------------------

 ---------------- Iteration 64 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 63           |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.0729       |
| Time/Buffer             | 0.0025       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34444684   |
| Train/Action_magnitu... | 1.098585     |
| Train/Action_magnitude  | 0.86567146   |
| Train/Action_max        | 0.40988722   |
| Train/Action_std        | 0.54095143   |
| Train/Entropy           | 0.7383877    |
| Train/Entropy_Loss      | -0.000738    |
| Train/Entropy_loss      | -0.000738    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.4897134   |
| Train/Loss              | -0.14016677  |
| Train/PolicyClip        | 0.0052007083 |
| Train/Policy_loss       | -0.14821032  |
| Train/Ratio             | 0.99854034   |
| Train/Return            | 0.87791073   |
| Train/V                 | 0.7306861    |
| Train/Value             | 0.7306861    |
| Train/control_penalty   | 0.878192     |
| Train/policy_loss       | -0.14821032  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 65 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 64           |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00298      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3597975    |
| Train/Action_magnitu... | 1.06967      |
| Train/Action_magnitude  | 0.8402124    |
| Train/Action_max        | 0.32840738   |
| Train/Action_std        | 0.5104451    |
| Train/Entropy           | 0.6817961    |
| Train/Entropy_Loss      | -0.000682    |
| Train/Entropy_loss      | -0.000682    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.3832405   |
| Train/Loss              | -0.041410126 |
| Train/PolicyClip        | 0.0025696042 |
| Train/Policy_loss       | -0.049237058 |
| Train/Ratio             | 1.0012828    |
| Train/Return            | 0.7171476    |
| Train/V                 | 0.67525876   |
| Train/Value             | 0.67525876   |
| Train/control_penalty   | 0.8508728    |
| Train/policy_loss       | -0.049237058 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01325      |
------------------------------------------

 ---------------- Iteration 66 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 65           |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0698       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00213      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.37394148   |
| Train/Action_magnitu... | 1.0838838    |
| Train/Action_magnitude  | 0.8540649    |
| Train/Action_max        | 0.3321825    |
| Train/Action_std        | 0.47235614   |
| Train/Entropy           | 0.60444283   |
| Train/Entropy_Loss      | -0.000604    |
| Train/Entropy_loss      | -0.000604    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.2716045   |
| Train/Loss              | -0.13539729  |
| Train/PolicyClip        | 0.0027955484 |
| Train/Policy_loss       | -0.14314456  |
| Train/Ratio             | 1.0126988    |
| Train/Return            | 0.85065675   |
| Train/V                 | 0.71383494   |
| Train/Value             | 0.71383494   |
| Train/control_penalty   | 0.83517134   |
| Train/policy_loss       | -0.14314456  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 67 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 66          |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.0702      |
| Time/Buffer             | 0.00248     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.36975762  |
| Train/Action_magnitu... | 1.0832678   |
| Train/Action_magnitude  | 0.85489947  |
| Train/Action_max        | 0.32273012  |
| Train/Action_std        | 0.5199896   |
| Train/Entropy           | 0.69261026  |
| Train/Entropy_Loss      | -0.000693   |
| Train/Entropy_loss      | -0.000693   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.3612869  |
| Train/Loss              | -0.08982716 |
| Train/PolicyClip        | 0.006097364 |
| Train/Policy_loss       | -0.09796729 |
| Train/Ratio             | 1.0042256   |
| Train/Return            | 0.821788    |
| Train/V                 | 0.73168886  |
| Train/Value             | 0.73168886  |
| Train/control_penalty   | 0.88327473  |
| Train/policy_loss       | -0.09796729 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.018       |
-----------------------------------------

 ---------------- Iteration 68 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 67           |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.356948     |
| Train/Action_magnitu... | 1.0231044    |
| Train/Action_magnitude  | 0.809358     |
| Train/Action_max        | 0.29316932   |
| Train/Action_std        | 0.4539063    |
| Train/Entropy           | 0.5491005    |
| Train/Entropy_Loss      | -0.000549    |
| Train/Entropy_loss      | -0.000549    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1370943   |
| Train/Loss              | -0.05413359  |
| Train/PolicyClip        | 0.0031683214 |
| Train/Policy_loss       | -0.06175074  |
| Train/Ratio             | 1.0033168    |
| Train/Return            | 0.7954679    |
| Train/V                 | 0.74000555   |
| Train/Value             | 0.74000555   |
| Train/control_penalty   | 0.816625     |
| Train/policy_loss       | -0.06175074  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01875      |
------------------------------------------

 ---------------- Iteration 69 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 68           |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.00201      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33856678   |
| Train/Action_magnitu... | 0.9822161    |
| Train/Action_magnitude  | 0.77538157   |
| Train/Action_max        | 0.3949116    |
| Train/Action_std        | 0.42189342   |
| Train/Entropy           | 0.4692488    |
| Train/Entropy_Loss      | -0.000469    |
| Train/Entropy_loss      | -0.000469    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.992038    |
| Train/Loss              | -0.049703848 |
| Train/PolicyClip        | 0.0031740982 |
| Train/Policy_loss       | -0.05679146  |
| Train/Ratio             | 1.0054766    |
| Train/Return            | 0.86962306   |
| Train/V                 | 0.8198952    |
| Train/Value             | 0.8198952    |
| Train/control_penalty   | 0.75568604   |
| Train/policy_loss       | -0.05679146  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01575      |
------------------------------------------

 ---------------- Iteration 70 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 69           |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.3436362    |
| Train/Action_magnitu... | 0.93958557   |
| Train/Action_magnitude  | 0.7463811    |
| Train/Action_max        | 0.4517543    |
| Train/Action_std        | 0.36768115   |
| Train/Entropy           | 0.33551994   |
| Train/Entropy_Loss      | -0.000336    |
| Train/Entropy_loss      | -0.000336    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7714371   |
| Train/Loss              | 0.021935206  |
| Train/PolicyClip        | 0.0012070851 |
| Train/Policy_loss       | 0.014916485  |
| Train/Ratio             | 0.99313843   |
| Train/Return            | 1.1318789    |
| Train/V                 | 1.1524309    |
| Train/Value             | 1.1524309    |
| Train/control_penalty   | 0.73542416   |
| Train/policy_loss       | 0.014916485  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 71 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 70            |
| Time/Actor_Time         | 0.0854        |
| Time/B_Format_Time      | 0.0711        |
| Time/B_Original_Form... | 0.0696        |
| Time/Buffer             | 0.00265       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31755185    |
| Train/Action_magnitu... | 0.872803      |
| Train/Action_magnitude  | 0.69469553    |
| Train/Action_max        | 0.47237667    |
| Train/Action_std        | 0.36659       |
| Train/Entropy           | 0.35319588    |
| Train/Entropy_Loss      | -0.000353     |
| Train/Entropy_loss      | -0.000353     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6122074    |
| Train/Loss              | 0.10281894    |
| Train/PolicyClip        | -0.0038815183 |
| Train/Policy_loss       | 0.09619719    |
| Train/Ratio             | 0.99931407    |
| Train/Return            | 1.0115415     |
| Train/V                 | 1.1116729     |
| Train/Value             | 1.1116729     |
| Train/control_penalty   | 0.69749475    |
| Train/policy_loss       | 0.09619719    |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0125        |
-------------------------------------------

 ---------------- Iteration 72 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 71          |
| Time/Actor_Time         | 0.0859      |
| Time/B_Format_Time      | 0.07        |
| Time/B_Original_Form... | 0.0693      |
| Time/Buffer             | 0.00293     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.36141726  |
| Train/Action_magnitu... | 0.8795296   |
| Train/Action_magnitude  | 0.70284534  |
| Train/Action_max        | 0.44766456  |
| Train/Action_std        | 0.35758084  |
| Train/Entropy           | 0.31035006  |
| Train/Entropy_Loss      | -0.00031    |
| Train/Entropy_loss      | -0.00031    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.74607885 |
| Train/Loss              | 0.095533155 |
| Train/PolicyClip        | 0.002269946 |
| Train/Policy_loss       | 0.08825506  |
| Train/Ratio             | 0.99795943  |
| Train/Return            | 0.9603656   |
| Train/V                 | 1.037291    |
| Train/Value             | 1.037291    |
| Train/control_penalty   | 0.75884354  |
| Train/policy_loss       | 0.08825506  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.007       |
-----------------------------------------

 ---------------- Iteration 73 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 72            |
| Time/Actor_Time         | 0.0865        |
| Time/B_Format_Time      | 0.0725        |
| Time/B_Original_Form... | 0.0699        |
| Time/Buffer             | 0.00375       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3641122     |
| Train/Action_magnitu... | 0.9798378     |
| Train/Action_magnitude  | 0.78407717    |
| Train/Action_max        | 0.45151195    |
| Train/Action_std        | 0.36945593    |
| Train/Entropy           | 0.3122098     |
| Train/Entropy_Loss      | -0.000312     |
| Train/Entropy_loss      | -0.000312     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.67064834   |
| Train/Loss              | -0.009868405  |
| Train/PolicyClip        | 0.00049970346 |
| Train/Policy_loss       | -0.017171746  |
| Train/Ratio             | 1.0076932     |
| Train/Return            | 1.0148542     |
| Train/V                 | 1.0054866     |
| Train/Value             | 1.0054866     |
| Train/control_penalty   | 0.7615551     |
| Train/policy_loss       | -0.017171746  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01875       |
-------------------------------------------

 ---------------- Iteration 74 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 73           |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00321      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34989372   |
| Train/Action_magnitu... | 0.9744085    |
| Train/Action_magnitude  | 0.7691289    |
| Train/Action_max        | 0.34118864   |
| Train/Action_std        | 0.4168055    |
| Train/Entropy           | 0.4222351    |
| Train/Entropy_Loss      | -0.000422    |
| Train/Entropy_loss      | -0.000422    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.79126143  |
| Train/Loss              | 0.01749439   |
| Train/PolicyClip        | 0.0049802815 |
| Train/Policy_loss       | 0.010063945  |
| Train/Ratio             | 0.9923266    |
| Train/Return            | 0.72836906   |
| Train/V                 | 0.7421082    |
| Train/Value             | 0.7421082    |
| Train/control_penalty   | 0.78526795   |
| Train/policy_loss       | 0.010063945  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.013        |
------------------------------------------

 ---------------- Iteration 75 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 74           |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00318      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.38560873   |
| Train/Action_magnitu... | 1.0768684    |
| Train/Action_magnitude  | 0.8463866    |
| Train/Action_max        | 0.28419384   |
| Train/Action_std        | 0.44048658   |
| Train/Entropy           | 0.44381243   |
| Train/Entropy_Loss      | -0.000444    |
| Train/Entropy_loss      | -0.000444    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9490955   |
| Train/Loss              | -0.110164024 |
| Train/PolicyClip        | 0.006469885  |
| Train/Policy_loss       | -0.11820943  |
| Train/Ratio             | 1.0030053    |
| Train/Return            | 0.9074277    |
| Train/V                 | 0.7963897    |
| Train/Value             | 0.7963897    |
| Train/control_penalty   | 0.8489212    |
| Train/policy_loss       | -0.11820943  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 76 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 75           |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36132038   |
| Train/Action_magnitu... | 1.0440007    |
| Train/Action_magnitude  | 0.8224133    |
| Train/Action_max        | 0.33793226   |
| Train/Action_std        | 0.43494248   |
| Train/Entropy           | 0.43338907   |
| Train/Entropy_Loss      | -0.000433    |
| Train/Entropy_loss      | -0.000433    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.90068483  |
| Train/Loss              | -0.016501494 |
| Train/PolicyClip        | 0.0056970194 |
| Train/Policy_loss       | -0.024249235 |
| Train/Ratio             | 0.99592245   |
| Train/Return            | 0.6783557    |
| Train/V                 | 0.66023517   |
| Train/Value             | 0.66023517   |
| Train/control_penalty   | 0.8181131    |
| Train/policy_loss       | -0.024249235 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01525      |
------------------------------------------

 ---------------- Iteration 77 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 76           |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0706       |
| Time/B_Original_Form... | 0.0693       |
| Time/Buffer             | 0.00223      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.368848     |
| Train/Action_magnitu... | 1.0392247    |
| Train/Action_magnitude  | 0.8197484    |
| Train/Action_max        | 0.29848748   |
| Train/Action_std        | 0.43329912   |
| Train/Entropy           | 0.4324567    |
| Train/Entropy_Loss      | -0.000432    |
| Train/Entropy_loss      | -0.000432    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8669411   |
| Train/Loss              | -0.021390462 |
| Train/PolicyClip        | 0.003856853  |
| Train/Policy_loss       | -0.02922845  |
| Train/Ratio             | 0.9945071    |
| Train/Return            | 0.64137685   |
| Train/V                 | 0.62080044   |
| Train/Value             | 0.62080044   |
| Train/control_penalty   | 0.8270444    |
| Train/policy_loss       | -0.02922845  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01275      |
------------------------------------------

 ---------------- Iteration 78 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 77           |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00245      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3625335    |
| Train/Action_magnitu... | 1.0721636    |
| Train/Action_magnitude  | 0.8461184    |
| Train/Action_max        | 0.36879465   |
| Train/Action_std        | 0.43419108   |
| Train/Entropy           | 0.4229113    |
| Train/Entropy_Loss      | -0.000423    |
| Train/Entropy_loss      | -0.000423    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.95209634  |
| Train/Loss              | -0.05481457  |
| Train/PolicyClip        | 0.0022690017 |
| Train/Policy_loss       | -0.06279941  |
| Train/Ratio             | 1.0020869    |
| Train/Return            | 0.7156998    |
| Train/V                 | 0.65903294   |
| Train/Value             | 0.65903294   |
| Train/control_penalty   | 0.8407747    |
| Train/policy_loss       | -0.06279941  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 79 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 78          |
| Time/Actor_Time         | 0.0862      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00323     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.35629427  |
| Train/Action_magnitu... | 0.9938902   |
| Train/Action_magnitude  | 0.78443205  |
| Train/Action_max        | 0.3346445   |
| Train/Action_std        | 0.39294404  |
| Train/Entropy           | 0.31784236  |
| Train/Entropy_Loss      | -0.000318   |
| Train/Entropy_loss      | -0.000318   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.70574254 |
| Train/Loss              | -0.06517507 |
| Train/PolicyClip        | 0.009702223 |
| Train/Policy_loss       | -0.07272648 |
| Train/Ratio             | 1.0065831   |
| Train/Return            | 0.8984126   |
| Train/V                 | 0.82360387  |
| Train/Value             | 0.82360387  |
| Train/control_penalty   | 0.7869247   |
| Train/policy_loss       | -0.07272648 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02125     |
-----------------------------------------

 ---------------- Iteration 80 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 79           |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0699       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00256      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36714366   |
| Train/Action_magnitu... | 1.0223266    |
| Train/Action_magnitude  | 0.80989605   |
| Train/Action_max        | 0.32422525   |
| Train/Action_std        | 0.39366516   |
| Train/Entropy           | 0.33821908   |
| Train/Entropy_Loss      | -0.000338    |
| Train/Entropy_loss      | -0.000338    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.77015865  |
| Train/Loss              | -0.039620277 |
| Train/PolicyClip        | 0.0030294165 |
| Train/Policy_loss       | -0.04723152  |
| Train/Ratio             | 0.99799746   |
| Train/Return            | 0.8786803    |
| Train/V                 | 0.8377776    |
| Train/Value             | 0.8377776    |
| Train/control_penalty   | 0.7949463    |
| Train/policy_loss       | -0.04723152  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 81 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 80           |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3657205    |
| Train/Action_magnitu... | 1.0063109    |
| Train/Action_magnitude  | 0.79446614   |
| Train/Action_max        | 0.3072244    |
| Train/Action_std        | 0.41054988   |
| Train/Entropy           | 0.37995073   |
| Train/Entropy_Loss      | -0.00038     |
| Train/Entropy_loss      | -0.00038     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.79625857  |
| Train/Loss              | -0.038771942 |
| Train/PolicyClip        | 0.0038723887 |
| Train/Policy_loss       | -0.046345007 |
| Train/Ratio             | 1.0023266    |
| Train/Return            | 0.9811444    |
| Train/V                 | 0.943895     |
| Train/Value             | 0.943895     |
| Train/control_penalty   | 0.7953013    |
| Train/policy_loss       | -0.046345007 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 82 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 81           |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0688       |
| Time/Buffer             | 0.00303      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35816175   |
| Train/Action_magnitu... | 1.0005895    |
| Train/Action_magnitude  | 0.7884417    |
| Train/Action_max        | 0.29466712   |
| Train/Action_std        | 0.4103399    |
| Train/Entropy           | 0.38483015   |
| Train/Entropy_Loss      | -0.000385    |
| Train/Entropy_loss      | -0.000385    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7917661   |
| Train/Loss              | -0.031885147 |
| Train/PolicyClip        | 0.0017836989 |
| Train/Policy_loss       | -0.039424866 |
| Train/Ratio             | 1.0016439    |
| Train/Return            | 0.9823061    |
| Train/V                 | 0.95170236   |
| Train/Value             | 0.95170236   |
| Train/control_penalty   | 0.7924551    |
| Train/policy_loss       | -0.039424866 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.022        |
------------------------------------------

 ---------------- Iteration 83 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 82           |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.00215      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34523016   |
| Train/Action_magnitu... | 0.95883125   |
| Train/Action_magnitude  | 0.7583024    |
| Train/Action_max        | 0.22265658   |
| Train/Action_std        | 0.38057277   |
| Train/Entropy           | 0.32970765   |
| Train/Entropy_Loss      | -0.00033     |
| Train/Entropy_loss      | -0.00033     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7136793   |
| Train/Loss              | -0.028669804 |
| Train/PolicyClip        | 0.0043217014 |
| Train/Policy_loss       | -0.035925344 |
| Train/Ratio             | 1.0026193    |
| Train/Return            | 1.0189368    |
| Train/V                 | 0.9885761    |
| Train/Value             | 0.9885761    |
| Train/control_penalty   | 0.75852454   |
| Train/policy_loss       | -0.035925344 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01675      |
------------------------------------------

 ---------------- Iteration 84 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 83           |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00291      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.40910286   |
| Train/Action_magnitu... | 1.0533661    |
| Train/Action_magnitude  | 0.8346929    |
| Train/Action_max        | 0.2305738    |
| Train/Action_std        | 0.41703552   |
| Train/Entropy           | 0.38510582   |
| Train/Entropy_Loss      | -0.000385    |
| Train/Entropy_loss      | -0.000385    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7769287   |
| Train/Loss              | 0.0090879    |
| Train/PolicyClip        | 0.0066997767 |
| Train/Policy_loss       | 0.0009400477 |
| Train/Ratio             | 0.990739     |
| Train/Return            | 1.0482457    |
| Train/V                 | 1.0543602    |
| Train/Value             | 1.0543602    |
| Train/control_penalty   | 0.85329586   |
| Train/policy_loss       | 0.0009400477 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 85 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 84           |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0691       |
| Time/Buffer             | 0.00201      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34926736   |
| Train/Action_magnitu... | 0.9419771    |
| Train/Action_magnitude  | 0.7498979    |
| Train/Action_max        | 0.24599123   |
| Train/Action_std        | 0.4022631    |
| Train/Entropy           | 0.3861057    |
| Train/Entropy_Loss      | -0.000386    |
| Train/Entropy_loss      | -0.000386    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7550546   |
| Train/Loss              | 0.06128978   |
| Train/PolicyClip        | 0.0032390056 |
| Train/Policy_loss       | 0.05398496   |
| Train/Ratio             | 0.99158776   |
| Train/Return            | 0.8318196    |
| Train/V                 | 0.89345175   |
| Train/Value             | 0.89345175   |
| Train/control_penalty   | 0.7690925    |
| Train/policy_loss       | 0.05398496   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0095       |
------------------------------------------

 ---------------- Iteration 86 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 85           |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.00224      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32128876   |
| Train/Action_magnitu... | 0.8930898    |
| Train/Action_magnitude  | 0.7120717    |
| Train/Action_max        | 0.21455492   |
| Train/Action_std        | 0.37478498   |
| Train/Entropy           | 0.30234376   |
| Train/Entropy_Loss      | -0.000302    |
| Train/Entropy_loss      | -0.000302    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5805033   |
| Train/Loss              | 0.062054884  |
| Train/PolicyClip        | 0.0038866047 |
| Train/Policy_loss       | 0.05506952   |
| Train/Ratio             | 0.9938084    |
| Train/Return            | 1.0005825    |
| Train/V                 | 1.0635127    |
| Train/Value             | 1.0635127    |
| Train/control_penalty   | 0.72877085   |
| Train/policy_loss       | 0.05506952   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01325      |
------------------------------------------

 ---------------- Iteration 87 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 86            |
| Time/Actor_Time         | 0.0865        |
| Time/B_Format_Time      | 0.0695        |
| Time/B_Original_Form... | 0.0699        |
| Time/Buffer             | 0.00299       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.338478      |
| Train/Action_magnitu... | 0.9581072     |
| Train/Action_magnitude  | 0.7608478     |
| Train/Action_max        | 0.30057532    |
| Train/Action_std        | 0.39062423    |
| Train/Entropy           | 0.3163614     |
| Train/Entropy_Loss      | -0.000316     |
| Train/Entropy_loss      | -0.000316     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6392568    |
| Train/Loss              | 0.0054096486  |
| Train/PolicyClip        | 0.0048993346  |
| Train/Policy_loss       | -0.0019168293 |
| Train/Ratio             | 0.9968763     |
| Train/Return            | 1.0702343     |
| Train/V                 | 1.0758288     |
| Train/Value             | 1.0758288     |
| Train/control_penalty   | 0.76428396    |
| Train/policy_loss       | -0.0019168293 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0195        |
-------------------------------------------

 ---------------- Iteration 88 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 87           |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0694       |
| Time/B_Original_Form... | 0.0696       |
| Time/Buffer             | 0.00252      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3151109    |
| Train/Action_magnitu... | 0.9518623    |
| Train/Action_magnitude  | 0.753756     |
| Train/Action_max        | 0.28287393   |
| Train/Action_std        | 0.42636603   |
| Train/Entropy           | 0.4530095    |
| Train/Entropy_Loss      | -0.000453    |
| Train/Entropy_loss      | -0.000453    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.80810773  |
| Train/Loss              | 0.0095749535 |
| Train/PolicyClip        | 0.008878035  |
| Train/Policy_loss       | 0.0024081836 |
| Train/Ratio             | 0.9804611    |
| Train/Return            | 0.8838185    |
| Train/V                 | 0.8930718    |
| Train/Value             | 0.8930718    |
| Train/control_penalty   | 0.7619779    |
| Train/policy_loss       | 0.0024081836 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01325      |
------------------------------------------

 ---------------- Iteration 89 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 88           |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00223      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3243646    |
| Train/Action_magnitu... | 0.92724913   |
| Train/Action_magnitude  | 0.7362653    |
| Train/Action_max        | 0.21243097   |
| Train/Action_std        | 0.39505935   |
| Train/Entropy           | 0.37582234   |
| Train/Entropy_Loss      | -0.000376    |
| Train/Entropy_loss      | -0.000376    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8033751   |
| Train/Loss              | 0.0033799778 |
| Train/PolicyClip        | 0.0056594717 |
| Train/Policy_loss       | -0.003619504 |
| Train/Ratio             | 0.99687403   |
| Train/Return            | 0.9113691    |
| Train/V                 | 0.916422     |
| Train/Value             | 0.916422     |
| Train/control_penalty   | 0.7375304    |
| Train/policy_loss       | -0.003619504 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0155       |
------------------------------------------

 ---------------- Iteration 90 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 89          |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.0703      |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3461881   |
| Train/Action_magnitu... | 0.9968307   |
| Train/Action_magnitude  | 0.794623    |
| Train/Action_max        | 0.23368731  |
| Train/Action_std        | 0.4511268   |
| Train/Entropy           | 0.51307416  |
| Train/Entropy_Loss      | -0.000513   |
| Train/Entropy_loss      | -0.000513   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.99528855 |
| Train/Loss              | 0.04457456  |
| Train/PolicyClip        | 0.002645502 |
| Train/Policy_loss       | 0.037011746 |
| Train/Ratio             | 0.98984545  |
| Train/Return            | 0.7564775   |
| Train/V                 | 0.8023065   |
| Train/Value             | 0.8023065   |
| Train/control_penalty   | 0.8075885   |
| Train/policy_loss       | 0.037011746 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.009       |
-----------------------------------------

 ---------------- Iteration 91 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 90          |
| Time/Actor_Time         | 0.0861      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00279     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34709394  |
| Train/Action_magnitu... | 1.0162593   |
| Train/Action_magnitude  | 0.80387366  |
| Train/Action_max        | 0.2613618   |
| Train/Action_std        | 0.45105383  |
| Train/Entropy           | 0.52147454  |
| Train/Entropy_Loss      | -0.000521   |
| Train/Entropy_loss      | -0.000521   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.0523301  |
| Train/Loss              | 0.023499679 |
| Train/PolicyClip        | 0.005037594 |
| Train/Policy_loss       | 0.01605823  |
| Train/Ratio             | 0.98845273  |
| Train/Return            | 0.7585867   |
| Train/V                 | 0.7846587   |
| Train/Value             | 0.7846587   |
| Train/control_penalty   | 0.7962923   |
| Train/policy_loss       | 0.01605823  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.011       |
-----------------------------------------

 ---------------- Iteration 92 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 91           |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.00229      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3137617    |
| Train/Action_magnitu... | 0.9847621    |
| Train/Action_magnitude  | 0.7766821    |
| Train/Action_max        | 0.27179462   |
| Train/Action_std        | 0.45368418   |
| Train/Entropy           | 0.5352641    |
| Train/Entropy_Loss      | -0.000535    |
| Train/Entropy_loss      | -0.000535    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0867858   |
| Train/Loss              | -0.005718864 |
| Train/PolicyClip        | 0.0032014688 |
| Train/Policy_loss       | -0.012978926 |
| Train/Ratio             | 0.98579293   |
| Train/Return            | 0.6877093    |
| Train/V                 | 0.6840184    |
| Train/Value             | 0.6840184    |
| Train/control_penalty   | 0.7795326    |
| Train/policy_loss       | -0.012978926 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01175      |
------------------------------------------

 ---------------- Iteration 93 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 92          |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0711      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00278     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34868553  |
| Train/Action_magnitu... | 1.0305177   |
| Train/Action_magnitude  | 0.81757206  |
| Train/Action_max        | 0.2800584   |
| Train/Action_std        | 0.45498583  |
| Train/Entropy           | 0.54354644  |
| Train/Entropy_Loss      | -0.000544   |
| Train/Entropy_loss      | -0.000544   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1030885  |
| Train/Loss              | -0.0464575  |
| Train/PolicyClip        | 0.005878207 |
| Train/Policy_loss       | -0.05417103 |
| Train/Ratio             | 1.0000567   |
| Train/Return            | 0.68224114  |
| Train/V                 | 0.63738114  |
| Train/Value             | 0.63738114  |
| Train/control_penalty   | 0.8257078   |
| Train/policy_loss       | -0.05417103 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.014       |
-----------------------------------------

 ---------------- Iteration 94 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 93           |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00244      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3025849    |
| Train/Action_magnitu... | 0.95616394   |
| Train/Action_magnitude  | 0.7544479    |
| Train/Action_max        | 0.26997343   |
| Train/Action_std        | 0.43813214   |
| Train/Entropy           | 0.51790357   |
| Train/Entropy_Loss      | -0.000518    |
| Train/Entropy_loss      | -0.000518    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0838798   |
| Train/Loss              | -0.035200913 |
| Train/PolicyClip        | 0.002187362  |
| Train/Policy_loss       | -0.042197526 |
| Train/Ratio             | 0.9992402    |
| Train/Return            | 0.70823276   |
| Train/V                 | 0.6747875    |
| Train/Value             | 0.6747875    |
| Train/control_penalty   | 0.75145173   |
| Train/policy_loss       | -0.042197526 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01375      |
------------------------------------------

 ---------------- Iteration 95 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 94          |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0715      |
| Time/Buffer             | 0.00486     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34830356  |
| Train/Action_magnitu... | 0.9953059   |
| Train/Action_magnitude  | 0.7846011   |
| Train/Action_max        | 0.25589392  |
| Train/Action_std        | 0.43658477  |
| Train/Entropy           | 0.5115895   |
| Train/Entropy_Loss      | -0.000512   |
| Train/Entropy_loss      | -0.000512   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.0557901  |
| Train/Loss              | -0.04489865 |
| Train/PolicyClip        | 0.005250734 |
| Train/Policy_loss       | -0.0522547  |
| Train/Ratio             | 1.0022672   |
| Train/Return            | 0.72449464  |
| Train/V                 | 0.67958546  |
| Train/Value             | 0.67958546  |
| Train/control_penalty   | 0.7867637   |
| Train/policy_loss       | -0.0522547  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.015       |
-----------------------------------------

 ---------------- Iteration 96 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 95           |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0737       |
| Time/Buffer             | 0.00299      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36140966   |
| Train/Action_magnitu... | 1.0170277    |
| Train/Action_magnitude  | 0.80361205   |
| Train/Action_max        | 0.28861523   |
| Train/Action_std        | 0.4294078    |
| Train/Entropy           | 0.4892633    |
| Train/Entropy_Loss      | -0.000489    |
| Train/Entropy_loss      | -0.000489    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0654589   |
| Train/Loss              | -0.062268324 |
| Train/PolicyClip        | 0.0031874762 |
| Train/Policy_loss       | -0.06974063  |
| Train/Ratio             | 0.9952968    |
| Train/Return            | 0.7865585    |
| Train/V                 | 0.72648764   |
| Train/Value             | 0.72648764   |
| Train/control_penalty   | 0.79615736   |
| Train/policy_loss       | -0.06974063  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 97 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 96           |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00268      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3425169    |
| Train/Action_magnitu... | 0.9834834    |
| Train/Action_magnitude  | 0.7711045    |
| Train/Action_max        | 0.27040687   |
| Train/Action_std        | 0.4226781    |
| Train/Entropy           | 0.46126226   |
| Train/Entropy_Loss      | -0.000461    |
| Train/Entropy_loss      | -0.000461    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.935473    |
| Train/Loss              | -0.031629957 |
| Train/PolicyClip        | 0.0022802667 |
| Train/Policy_loss       | -0.038833015 |
| Train/Ratio             | 1.0008943    |
| Train/Return            | 0.7771663    |
| Train/V                 | 0.74592924   |
| Train/Value             | 0.74592924   |
| Train/control_penalty   | 0.76643175   |
| Train/policy_loss       | -0.038833015 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 98 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 97           |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.0029       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33855066   |
| Train/Action_magnitu... | 0.94757277   |
| Train/Action_magnitude  | 0.74708325   |
| Train/Action_max        | 0.2494061    |
| Train/Action_std        | 0.41061294   |
| Train/Entropy           | 0.43540245   |
| Train/Entropy_Loss      | -0.000435    |
| Train/Entropy_loss      | -0.000435    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9036058   |
| Train/Loss              | -0.11169298  |
| Train/PolicyClip        | 0.0056266626 |
| Train/Policy_loss       | -0.11897354  |
| Train/Ratio             | 0.9998466    |
| Train/Return            | 0.9505719    |
| Train/V                 | 0.83434784   |
| Train/Value             | 0.83434784   |
| Train/control_penalty   | 0.77159655   |
| Train/policy_loss       | -0.11897354  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 99 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 98           |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00481      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31899622   |
| Train/Action_magnitu... | 0.95182437   |
| Train/Action_magnitude  | 0.74771345   |
| Train/Action_max        | 0.23819926   |
| Train/Action_std        | 0.429465     |
| Train/Entropy           | 0.48453513   |
| Train/Entropy_Loss      | -0.000485    |
| Train/Entropy_loss      | -0.000485    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9662162   |
| Train/Loss              | 0.026309127  |
| Train/PolicyClip        | 0.0023366578 |
| Train/Policy_loss       | 0.019179635  |
| Train/Ratio             | 0.99014354   |
| Train/Return            | 0.7544334    |
| Train/V                 | 0.7825255    |
| Train/Value             | 0.7825255    |
| Train/control_penalty   | 0.76140285   |
| Train/policy_loss       | 0.019179635  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0125       |
------------------------------------------

 ---------------- Iteration 100 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 99           |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.0023       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32074225   |
| Train/Action_magnitu... | 0.93171173   |
| Train/Action_magnitude  | 0.73732924   |
| Train/Action_max        | 0.19585852   |
| Train/Action_std        | 0.39579493   |
| Train/Entropy           | 0.4194678    |
| Train/Entropy_Loss      | -0.000419    |
| Train/Entropy_loss      | -0.000419    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.88899     |
| Train/Loss              | -0.0691556   |
| Train/PolicyClip        | 0.0037212768 |
| Train/Policy_loss       | -0.07597765  |
| Train/Ratio             | 1.0051844    |
| Train/Return            | 0.7557974    |
| Train/V                 | 0.6883228    |
| Train/Value             | 0.6883228    |
| Train/control_penalty   | 0.7241517    |
| Train/policy_loss       | -0.07597765  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01825      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 101 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 100         |
| Time/Actor_Time         | 0.0851      |
| Time/B_Format_Time      | 0.0748      |
| Time/B_Original_Form... | 0.0727      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33066773  |
| Train/Action_magnitu... | 0.9236256   |
| Train/Action_magnitude  | 0.7274134   |
| Train/Action_max        | 0.1463322   |
| Train/Action_std        | 0.39535058  |
| Train/Entropy           | 0.4233899   |
| Train/Entropy_Loss      | -0.000423   |
| Train/Entropy_loss      | -0.000423   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.79034775 |
| Train/Loss              | -0.06357199 |
| Train/PolicyClip        | 0.002491713 |
| Train/Policy_loss       | -0.07053746 |
| Train/Ratio             | 0.9991999   |
| Train/Return            | 0.81180716  |
| Train/V                 | 0.7513363   |
| Train/Value             | 0.7513363   |
| Train/control_penalty   | 0.7388863   |
| Train/policy_loss       | -0.07053746 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 102 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 101          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0695       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3240643    |
| Train/Action_magnitu... | 0.9161351    |
| Train/Action_magnitude  | 0.7201395    |
| Train/Action_max        | 0.19063552   |
| Train/Action_std        | 0.36932752   |
| Train/Entropy           | 0.35054904   |
| Train/Entropy_Loss      | -0.000351    |
| Train/Entropy_loss      | -0.000351    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.72770107  |
| Train/Loss              | -0.045608237 |
| Train/PolicyClip        | 0.0074943136 |
| Train/Policy_loss       | -0.052326974 |
| Train/Ratio             | 1.0006586    |
| Train/Return            | 0.7662565    |
| Train/V                 | 0.7164775    |
| Train/Value             | 0.7164775    |
| Train/control_penalty   | 0.70692855   |
| Train/policy_loss       | -0.052326974 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------

 ---------------- Iteration 103 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 102         |
| Time/Actor_Time         | 0.0865      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.0717      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31604245  |
| Train/Action_magnitu... | 0.88477176  |
| Train/Action_magnitude  | 0.6931289   |
| Train/Action_max        | 0.21492416  |
| Train/Action_std        | 0.34260118  |
| Train/Entropy           | 0.27858037  |
| Train/Entropy_Loss      | -0.000279   |
| Train/Entropy_loss      | -0.000279   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6850931  |
| Train/Loss              | -0.14763953 |
| Train/PolicyClip        | 0.004874982 |
| Train/Policy_loss       | -0.15420637 |
| Train/Ratio             | 1.0004776   |
| Train/Return            | 0.8350658   |
| Train/V                 | 0.68346125  |
| Train/Value             | 0.68346125  |
| Train/control_penalty   | 0.6845415   |
| Train/policy_loss       | -0.15420637 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 104 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 103         |
| Time/Actor_Time         | 0.0861      |
| Time/B_Format_Time      | 0.0701      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00538     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.29577678  |
| Train/Action_magnitu... | 0.86353767  |
| Train/Action_magnitude  | 0.68008494  |
| Train/Action_max        | 0.19722436  |
| Train/Action_std        | 0.36606726  |
| Train/Entropy           | 0.3479588   |
| Train/Entropy_Loss      | -0.000348   |
| Train/Entropy_loss      | -0.000348   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.69031835 |
| Train/Loss              | -0.06983271 |
| Train/PolicyClip        | 0.004861811 |
| Train/Policy_loss       | -0.07631287 |
| Train/Ratio             | 1.0024673   |
| Train/Return            | 0.7447844   |
| Train/V                 | 0.67627054  |
| Train/Value             | 0.67627054  |
| Train/control_penalty   | 0.6828117   |
| Train/policy_loss       | -0.07631287 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0175      |
-----------------------------------------

 ---------------- Iteration 105 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 104          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.0025       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28272724   |
| Train/Action_magnitu... | 0.83841044   |
| Train/Action_magnitude  | 0.65878415   |
| Train/Action_max        | 0.20614782   |
| Train/Action_std        | 0.35723668   |
| Train/Entropy           | 0.3276965    |
| Train/Entropy_Loss      | -0.000328    |
| Train/Entropy_loss      | -0.000328    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7149414   |
| Train/Loss              | -0.08851801  |
| Train/PolicyClip        | 0.0058595557 |
| Train/Policy_loss       | -0.094831236 |
| Train/Ratio             | 0.99868524   |
| Train/Return            | 0.8703225    |
| Train/V                 | 0.78341675   |
| Train/Value             | 0.78341675   |
| Train/control_penalty   | 0.6640924    |
| Train/policy_loss       | -0.094831236 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 106 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 105          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00306      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31576484   |
| Train/Action_magnitu... | 0.88743937   |
| Train/Action_magnitude  | 0.70128393   |
| Train/Action_max        | 0.214136     |
| Train/Action_std        | 0.36574444   |
| Train/Entropy           | 0.34214008   |
| Train/Entropy_Loss      | -0.000342    |
| Train/Entropy_loss      | -0.000342    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6683941   |
| Train/Loss              | -0.026184076 |
| Train/PolicyClip        | 0.0047424175 |
| Train/Policy_loss       | -0.032861196 |
| Train/Ratio             | 1.0036321    |
| Train/Return            | 0.91748506   |
| Train/V                 | 0.89171445   |
| Train/Value             | 0.89171445   |
| Train/control_penalty   | 0.7019261    |
| Train/policy_loss       | -0.032861196 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01975      |
------------------------------------------

 ---------------- Iteration 107 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 106         |
| Time/Actor_Time         | 0.0868      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.0695      |
| Time/Buffer             | 0.00298     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.31300575  |
| Train/Action_magnitu... | 0.90558255  |
| Train/Action_magnitude  | 0.71626174  |
| Train/Action_max        | 0.19015622  |
| Train/Action_std        | 0.36186963  |
| Train/Entropy           | 0.33090916  |
| Train/Entropy_Loss      | -0.000331   |
| Train/Entropy_loss      | -0.000331   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.7668948  |
| Train/Loss              | -0.06692061 |
| Train/PolicyClip        | 0.008616161 |
| Train/Policy_loss       | -0.073611   |
| Train/Ratio             | 1.0025756   |
| Train/Return            | 0.9663633   |
| Train/V                 | 0.8939074   |
| Train/Value             | 0.8939074   |
| Train/control_penalty   | 0.70213026  |
| Train/policy_loss       | -0.073611   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 108 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 107          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0703       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32064325   |
| Train/Action_magnitu... | 0.9070891    |
| Train/Action_magnitude  | 0.7139037    |
| Train/Action_max        | 0.21372397   |
| Train/Action_std        | 0.35663214   |
| Train/Entropy           | 0.31632      |
| Train/Entropy_Loss      | -0.000316    |
| Train/Entropy_loss      | -0.000316    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6787835   |
| Train/Loss              | -0.04062989  |
| Train/PolicyClip        | 0.0026173498 |
| Train/Policy_loss       | -0.047358446 |
| Train/Ratio             | 1.0003421    |
| Train/Return            | 0.929439     |
| Train/V                 | 0.89155465   |
| Train/Value             | 0.89155465   |
| Train/control_penalty   | 0.7044878    |
| Train/policy_loss       | -0.047358446 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 109 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 108          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00236      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31757367   |
| Train/Action_magnitu... | 0.87376726   |
| Train/Action_magnitude  | 0.6879799    |
| Train/Action_max        | 0.22750622   |
| Train/Action_std        | 0.35154986   |
| Train/Entropy           | 0.30060157   |
| Train/Entropy_Loss      | -0.000301    |
| Train/Entropy_loss      | -0.000301    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5837256   |
| Train/Loss              | 0.013670659  |
| Train/PolicyClip        | 0.004730299  |
| Train/Policy_loss       | 0.0069327806 |
| Train/Ratio             | 0.9868652    |
| Train/Return            | 0.9498335    |
| Train/V                 | 0.96442366   |
| Train/Value             | 0.96442366   |
| Train/control_penalty   | 0.703848     |
| Train/policy_loss       | 0.0069327806 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.017        |
------------------------------------------

 ---------------- Iteration 110 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 109          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00246      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33160752   |
| Train/Action_magnitu... | 0.8900684    |
| Train/Action_magnitude  | 0.70134705   |
| Train/Action_max        | 0.22040832   |
| Train/Action_std        | 0.33796024   |
| Train/Entropy           | 0.26175606   |
| Train/Entropy_Loss      | -0.000262    |
| Train/Entropy_loss      | -0.000262    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5181085   |
| Train/Loss              | 0.023871187  |
| Train/PolicyClip        | 0.0035942937 |
| Train/Policy_loss       | 0.017140888  |
| Train/Ratio             | 0.99987876   |
| Train/Return            | 0.98000586   |
| Train/V                 | 1.0048671    |
| Train/Value             | 1.0048671    |
| Train/control_penalty   | 0.6992056    |
| Train/policy_loss       | 0.017140888  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01725      |
------------------------------------------

 ---------------- Iteration 111 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 110          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00238      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.37430114   |
| Train/Action_magnitu... | 0.9295232    |
| Train/Action_magnitude  | 0.73212004   |
| Train/Action_max        | 0.19927911   |
| Train/Action_std        | 0.315428     |
| Train/Entropy           | 0.18131511   |
| Train/Entropy_Loss      | -0.000181    |
| Train/Entropy_loss      | -0.000181    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.39409018  |
| Train/Loss              | -0.03708623  |
| Train/PolicyClip        | 0.003105819  |
| Train/Policy_loss       | -0.044327293 |
| Train/Ratio             | 0.9999475    |
| Train/Return            | 1.1733103    |
| Train/V                 | 1.1357291    |
| Train/Value             | 1.1357291    |
| Train/control_penalty   | 0.7422377    |
| Train/policy_loss       | -0.044327293 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 112 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 111          |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00251      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3563563    |
| Train/Action_magnitu... | 0.91629887   |
| Train/Action_magnitude  | 0.7191203    |
| Train/Action_max        | 0.19758594   |
| Train/Action_std        | 0.3301171    |
| Train/Entropy           | 0.21822341   |
| Train/Entropy_Loss      | -0.000218    |
| Train/Entropy_loss      | -0.000218    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4682621   |
| Train/Loss              | -0.038754493 |
| Train/PolicyClip        | 0.0064559635 |
| Train/Policy_loss       | -0.045776557 |
| Train/Ratio             | 1.001757     |
| Train/Return            | 1.0986769    |
| Train/V                 | 1.0526788    |
| Train/Value             | 1.0526788    |
| Train/control_penalty   | 0.72402865   |
| Train/policy_loss       | -0.045776557 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 113 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 112          |
| Time/Actor_Time         | 0.088        |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0696       |
| Time/Buffer             | 0.003        |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3491576    |
| Train/Action_magnitu... | 0.9340026    |
| Train/Action_magnitude  | 0.7356425    |
| Train/Action_max        | 0.23300539   |
| Train/Action_std        | 0.33291072   |
| Train/Entropy           | 0.21311255   |
| Train/Entropy_Loss      | -0.000213    |
| Train/Entropy_loss      | -0.000213    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.48054895  |
| Train/Loss              | -0.051657606 |
| Train/PolicyClip        | 0.004111302  |
| Train/Policy_loss       | -0.05859508  |
| Train/Ratio             | 1.0038234    |
| Train/Return            | 1.1089164    |
| Train/V                 | 1.0611843    |
| Train/Value             | 1.0611843    |
| Train/control_penalty   | 0.7150588    |
| Train/policy_loss       | -0.05859508  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------

 ---------------- Iteration 114 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 113          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00253      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3425195    |
| Train/Action_magnitu... | 0.87814724   |
| Train/Action_magnitude  | 0.69176495   |
| Train/Action_max        | 0.196932     |
| Train/Action_std        | 0.30074468   |
| Train/Entropy           | 0.123942874  |
| Train/Entropy_Loss      | -0.000124    |
| Train/Entropy_loss      | -0.000124    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.34967217  |
| Train/Loss              | -0.097185865 |
| Train/PolicyClip        | 0.0036148408 |
| Train/Policy_loss       | -0.103896834 |
| Train/Ratio             | 1.0116453    |
| Train/Return            | 1.2692723    |
| Train/V                 | 1.1735137    |
| Train/Value             | 1.1735137    |
| Train/control_penalty   | 0.68349105   |
| Train/policy_loss       | -0.103896834 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03125      |
------------------------------------------

 ---------------- Iteration 115 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 114          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00221      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30256808   |
| Train/Action_magnitu... | 0.8294366    |
| Train/Action_magnitude  | 0.65586007   |
| Train/Action_max        | 0.18269707   |
| Train/Action_std        | 0.31788507   |
| Train/Entropy           | 0.18893985   |
| Train/Entropy_Loss      | -0.000189    |
| Train/Entropy_loss      | -0.000189    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.42048112  |
| Train/Loss              | -0.092816144 |
| Train/PolicyClip        | 0.003276379  |
| Train/Policy_loss       | -0.09931529  |
| Train/Ratio             | 1.0048076    |
| Train/Return            | 1.0588694    |
| Train/V                 | 0.96721107   |
| Train/Value             | 0.96721107   |
| Train/control_penalty   | 0.6688092    |
| Train/policy_loss       | -0.09931529  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 116 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 115           |
| Time/Actor_Time         | 0.0862        |
| Time/B_Format_Time      | 0.0724        |
| Time/B_Original_Form... | 0.0718        |
| Time/Buffer             | 0.00242       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3034068     |
| Train/Action_magnitu... | 0.84112394    |
| Train/Action_magnitude  | 0.66340786    |
| Train/Action_max        | 0.15618597    |
| Train/Action_std        | 0.3424368     |
| Train/Entropy           | 0.2591093     |
| Train/Entropy_Loss      | -0.000259     |
| Train/Entropy_loss      | -0.000259     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.5037963    |
| Train/Loss              | -0.0047677215 |
| Train/PolicyClip        | 0.007062878   |
| Train/Policy_loss       | -0.011152679  |
| Train/Ratio             | 0.989679      |
| Train/Return            | 0.8912684     |
| Train/V                 | 0.8891196     |
| Train/Value             | 0.8891196     |
| Train/control_penalty   | 0.66440666    |
| Train/policy_loss       | -0.011152679  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01375       |
-------------------------------------------

 ---------------- Iteration 117 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 116         |
| Time/Actor_Time         | 0.0884      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.00233     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29639158  |
| Train/Action_magnitu... | 0.81952775  |
| Train/Action_magnitude  | 0.6465251   |
| Train/Action_max        | 0.17820095  |
| Train/Action_std        | 0.3062104   |
| Train/Entropy           | 0.1277496   |
| Train/Entropy_Loss      | -0.000128   |
| Train/Entropy_loss      | -0.000128   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.43342808 |
| Train/Loss              | -0.06489153 |
| Train/PolicyClip        | 0.007415925 |
| Train/Policy_loss       | -0.07101085 |
| Train/Ratio             | 1.0047698   |
| Train/Return            | 1.0906416   |
| Train/V                 | 1.0234811   |
| Train/Value             | 1.0234811   |
| Train/control_penalty   | 0.624707    |
| Train/policy_loss       | -0.07101085 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 118 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 117          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00283      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31821242   |
| Train/Action_magnitu... | 0.83370376   |
| Train/Action_magnitude  | 0.65692437   |
| Train/Action_max        | 0.14561956   |
| Train/Action_std        | 0.28367537   |
| Train/Entropy           | 0.059264686  |
| Train/Entropy_Loss      | -5.93e-05    |
| Train/Entropy_loss      | -5.93e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.25100133  |
| Train/Loss              | -0.07852431  |
| Train/PolicyClip        | 0.0056667062 |
| Train/Policy_loss       | -0.08489274  |
| Train/Ratio             | 0.99480885   |
| Train/Return            | 1.0578852    |
| Train/V                 | 0.9781117    |
| Train/Value             | 0.9781117    |
| Train/control_penalty   | 0.6427695    |
| Train/policy_loss       | -0.08489274  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 119 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 118          |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0692       |
| Time/Buffer             | 0.00286      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31413394   |
| Train/Action_magnitu... | 0.8121255    |
| Train/Action_magnitude  | 0.63740116   |
| Train/Action_max        | 0.12707752   |
| Train/Action_std        | 0.28666475   |
| Train/Entropy           | 0.062981285  |
| Train/Entropy_Loss      | -6.3e-05     |
| Train/Entropy_loss      | -6.3e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.18814358  |
| Train/Loss              | -0.10771676  |
| Train/PolicyClip        | 0.0059628366 |
| Train/Policy_loss       | -0.11398599  |
| Train/Ratio             | 0.9990965    |
| Train/Return            | 1.1313565    |
| Train/V                 | 1.024107     |
| Train/Value             | 1.024107     |
| Train/control_penalty   | 0.633221     |
| Train/policy_loss       | -0.11398599  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02825      |
------------------------------------------

 ---------------- Iteration 120 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 119           |
| Time/Actor_Time         | 0.0862        |
| Time/B_Format_Time      | 0.0718        |
| Time/B_Original_Form... | 0.0711        |
| Time/Buffer             | 0.00293       |
| Time/Critic_Time        | 7.15e-07      |
| Train/Action_abs_mean   | 0.31806284    |
| Train/Action_magnitu... | 0.8012902     |
| Train/Action_magnitude  | 0.63127255    |
| Train/Action_max        | 0.14668788    |
| Train/Action_std        | 0.27948385    |
| Train/Entropy           | 0.026050946   |
| Train/Entropy_Loss      | -2.61e-05     |
| Train/Entropy_loss      | -2.61e-05     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.009010957  |
| Train/Loss              | -0.0055648712 |
| Train/PolicyClip        | 0.0062459963  |
| Train/Policy_loss       | -0.011799617  |
| Train/Ratio             | 0.99866307    |
| Train/Return            | 1.1803579     |
| Train/V                 | 1.1716814     |
| Train/Value             | 1.1716814     |
| Train/control_penalty   | 0.6260797     |
| Train/policy_loss       | -0.011799617  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0235        |
-------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 121 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 120          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.0029       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35126233   |
| Train/Action_magnitu... | 0.8435231    |
| Train/Action_magnitude  | 0.66187835   |
| Train/Action_max        | 0.13900447   |
| Train/Action_std        | 0.2640889    |
| Train/Entropy           | -0.011484585 |
| Train/Entropy_Loss      | 1.15e-05     |
| Train/Entropy_loss      | 1.15e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.07212779  |
| Train/Loss              | -0.11604953  |
| Train/PolicyClip        | 0.0037694413 |
| Train/Policy_loss       | -0.122705914 |
| Train/Ratio             | 1.0137664    |
| Train/Return            | 1.3741679    |
| Train/V                 | 1.2539166    |
| Train/Value             | 1.2539166    |
| Train/control_penalty   | 0.66449046   |
| Train/policy_loss       | -0.122705914 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.036        |
------------------------------------------

 ---------------- Iteration 122 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 121          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00269      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3231496    |
| Train/Action_magnitu... | 0.81867963   |
| Train/Action_magnitude  | 0.641301     |
| Train/Action_max        | 0.1497446    |
| Train/Action_std        | 0.28942072   |
| Train/Entropy           | 0.0666821    |
| Train/Entropy_Loss      | -6.67e-05    |
| Train/Entropy_loss      | -6.67e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.12105579  |
| Train/Loss              | 0.0018550027 |
| Train/PolicyClip        | 0.0073102014 |
| Train/Policy_loss       | -0.004546411 |
| Train/Ratio             | 0.9950669    |
| Train/Return            | 1.1652415    |
| Train/V                 | 1.1652733    |
| Train/Value             | 1.1652733    |
| Train/control_penalty   | 0.6468096    |
| Train/policy_loss       | -0.004546411 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 123 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 122            |
| Time/Actor_Time         | 0.0866         |
| Time/B_Format_Time      | 0.0715         |
| Time/B_Original_Form... | 0.0701         |
| Time/Buffer             | 0.00344        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.3073514      |
| Train/Action_magnitu... | 0.8209737      |
| Train/Action_magnitude  | 0.6474078      |
| Train/Action_max        | 0.17460611     |
| Train/Action_std        | 0.2987261      |
| Train/Entropy           | 0.10296845     |
| Train/Entropy_Loss      | -0.000103      |
| Train/Entropy_loss      | -0.000103      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -0.18933584    |
| Train/Loss              | 0.006271231    |
| Train/PolicyClip        | 0.005818051    |
| Train/Policy_loss       | -0.00012551516 |
| Train/Ratio             | 1.0021119      |
| Train/Return            | 1.0677772      |
| Train/V                 | 1.0738074      |
| Train/Value             | 1.0738074      |
| Train/control_penalty   | 0.6499715      |
| Train/policy_loss       | -0.00012551516 |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.0215         |
--------------------------------------------

 ---------------- Iteration 124 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 123          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00242      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30824798   |
| Train/Action_magnitu... | 0.8211267    |
| Train/Action_magnitude  | 0.6469565    |
| Train/Action_max        | 0.19843853   |
| Train/Action_std        | 0.30828267   |
| Train/Entropy           | 0.15185374   |
| Train/Entropy_Loss      | -0.000152    |
| Train/Entropy_loss      | -0.000152    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.26445082  |
| Train/Loss              | 0.04306408   |
| Train/PolicyClip        | 0.0055792695 |
| Train/Policy_loss       | 0.0367428    |
| Train/Ratio             | 0.9832898    |
| Train/Return            | 0.8920623    |
| Train/V                 | 0.94045115   |
| Train/Value             | 0.94045115   |
| Train/control_penalty   | 0.64731336   |
| Train/policy_loss       | 0.0367428    |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0125       |
------------------------------------------

 ---------------- Iteration 125 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 124         |
| Time/Actor_Time         | 0.0872      |
| Time/B_Format_Time      | 0.0706      |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.00232     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.328802    |
| Train/Action_magnitu... | 0.8474229   |
| Train/Action_magnitude  | 0.6651347   |
| Train/Action_max        | 0.2279943   |
| Train/Action_std        | 0.32041633  |
| Train/Entropy           | 0.19663572  |
| Train/Entropy_Loss      | -0.000197   |
| Train/Entropy_loss      | -0.000197   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.39334145 |
| Train/Loss              | 0.04959171  |
| Train/PolicyClip        | 0.004793089 |
| Train/Policy_loss       | 0.04292023  |
| Train/Ratio             | 0.9852004   |
| Train/Return            | 0.8265793   |
| Train/V                 | 0.87707835  |
| Train/Value             | 0.87707835  |
| Train/control_penalty   | 0.68681127  |
| Train/policy_loss       | 0.04292023  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01125     |
-----------------------------------------

 ---------------- Iteration 126 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 125          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00258      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31952623   |
| Train/Action_magnitu... | 0.8598358    |
| Train/Action_magnitude  | 0.68246967   |
| Train/Action_max        | 0.20528232   |
| Train/Action_std        | 0.3235009    |
| Train/Entropy           | 0.20600039   |
| Train/Entropy_Loss      | -0.000206    |
| Train/Entropy_loss      | -0.000206    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4972653   |
| Train/Loss              | -0.002038659 |
| Train/PolicyClip        | 0.006267611  |
| Train/Policy_loss       | -0.008636125 |
| Train/Ratio             | 1.0023249    |
| Train/Return            | 0.7913277    |
| Train/V                 | 0.7889393    |
| Train/Value             | 0.7889393    |
| Train/control_penalty   | 0.68034667   |
| Train/policy_loss       | -0.008636125 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.014        |
------------------------------------------

 ---------------- Iteration 127 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 126           |
| Time/Actor_Time         | 0.086         |
| Time/B_Format_Time      | 0.0716        |
| Time/B_Original_Form... | 0.0723        |
| Time/Buffer             | 0.0027        |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.33924904    |
| Train/Action_magnitu... | 0.8699967     |
| Train/Action_magnitude  | 0.6879163     |
| Train/Action_max        | 0.20082462    |
| Train/Action_std        | 0.31381333    |
| Train/Entropy           | 0.18429269    |
| Train/Entropy_Loss      | -0.000184     |
| Train/Entropy_loss      | -0.000184     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.4342889    |
| Train/Loss              | -0.0040416345 |
| Train/PolicyClip        | 0.0057157995  |
| Train/Policy_loss       | -0.010777505  |
| Train/Ratio             | 0.9950894     |
| Train/Return            | 0.80452156    |
| Train/V                 | 0.8014565     |
| Train/Value             | 0.8014565     |
| Train/control_penalty   | 0.69201636    |
| Train/policy_loss       | -0.010777505  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0115        |
-------------------------------------------

 ---------------- Iteration 128 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 127          |
| Time/Actor_Time         | 0.087        |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00292      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35192123   |
| Train/Action_magnitu... | 0.886291     |
| Train/Action_magnitude  | 0.69878536   |
| Train/Action_max        | 0.2005939    |
| Train/Action_std        | 0.31242073   |
| Train/Entropy           | 0.17717843   |
| Train/Entropy_Loss      | -0.000177    |
| Train/Entropy_loss      | -0.000177    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3790784   |
| Train/Loss              | -0.06231851  |
| Train/PolicyClip        | 0.0064825397 |
| Train/Policy_loss       | -0.06913808  |
| Train/Ratio             | 1.0044786    |
| Train/Return            | 0.94589156   |
| Train/V                 | 0.88302016   |
| Train/Value             | 0.88302016   |
| Train/control_penalty   | 0.6996744    |
| Train/policy_loss       | -0.06913808  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 129 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 128          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.38757336   |
| Train/Action_magnitu... | 0.95207673   |
| Train/Action_magnitude  | 0.74986875   |
| Train/Action_max        | 0.13843776   |
| Train/Action_std        | 0.33092663   |
| Train/Entropy           | 0.22044206   |
| Train/Entropy_Loss      | -0.00022     |
| Train/Entropy_loss      | -0.00022     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.49235544  |
| Train/Loss              | -0.07601194  |
| Train/PolicyClip        | 0.0055687223 |
| Train/Policy_loss       | -0.08333303  |
| Train/Ratio             | 1.0102178    |
| Train/Return            | 0.98191386   |
| Train/V                 | 0.9053664    |
| Train/Value             | 0.9053664    |
| Train/control_penalty   | 0.7541527    |
| Train/policy_loss       | -0.08333303  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 130 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 129          |
| Time/Actor_Time         | 0.0872       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0694       |
| Time/Buffer             | 0.00253      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.38222867   |
| Train/Action_magnitu... | 0.92852      |
| Train/Action_magnitude  | 0.7311272    |
| Train/Action_max        | 0.07302348   |
| Train/Action_std        | 0.3100689    |
| Train/Entropy           | 0.15661119   |
| Train/Entropy_Loss      | -0.000157    |
| Train/Entropy_loss      | -0.000157    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.37879443  |
| Train/Loss              | -0.026076566 |
| Train/PolicyClip        | 0.0031008983 |
| Train/Policy_loss       | -0.03309008  |
| Train/Ratio             | 0.9978772    |
| Train/Return            | 1.0880027    |
| Train/V                 | 1.0639404    |
| Train/Value             | 1.0639404    |
| Train/control_penalty   | 0.7170127    |
| Train/policy_loss       | -0.03309008  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 131 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 130          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34245393   |
| Train/Action_magnitu... | 0.8681482    |
| Train/Action_magnitude  | 0.6837114    |
| Train/Action_max        | 0.15718801   |
| Train/Action_std        | 0.30044755   |
| Train/Entropy           | 0.13555083   |
| Train/Entropy_Loss      | -0.000136    |
| Train/Entropy_loss      | -0.000136    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.30308974  |
| Train/Loss              | -0.022890525 |
| Train/PolicyClip        | 0.0052210963 |
| Train/Policy_loss       | -0.029482787 |
| Train/Ratio             | 0.99834275   |
| Train/Return            | 1.015793     |
| Train/V                 | 0.9950405    |
| Train/Value             | 0.9950405    |
| Train/control_penalty   | 0.67278135   |
| Train/policy_loss       | -0.029482787 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02         |
------------------------------------------

 ---------------- Iteration 132 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 131          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.0711       |
| Time/Buffer             | 0.00263      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33969855   |
| Train/Action_magnitu... | 0.8867703    |
| Train/Action_magnitude  | 0.6966317    |
| Train/Action_max        | 0.15674517   |
| Train/Action_std        | 0.3085691    |
| Train/Entropy           | 0.15439722   |
| Train/Entropy_Loss      | -0.000154    |
| Train/Entropy_loss      | -0.000154    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.39727178  |
| Train/Loss              | -0.036219772 |
| Train/PolicyClip        | 0.005135663  |
| Train/Policy_loss       | -0.042840432 |
| Train/Ratio             | 0.9989267    |
| Train/Return            | 0.9379604    |
| Train/V                 | 0.9002657    |
| Train/Value             | 0.9002657    |
| Train/control_penalty   | 0.6775059    |
| Train/policy_loss       | -0.042840432 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 133 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 132         |
| Time/Actor_Time         | 0.0859      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00253     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33827922  |
| Train/Action_magnitu... | 0.88578963  |
| Train/Action_magnitude  | 0.69656324  |
| Train/Action_max        | 0.102848195 |
| Train/Action_std        | 0.3148112   |
| Train/Entropy           | 0.16363515  |
| Train/Entropy_Loss      | -0.000164   |
| Train/Entropy_loss      | -0.000164   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.34547246 |
| Train/Loss              | 0.07630209  |
| Train/PolicyClip        | 0.005497135 |
| Train/Policy_loss       | 0.0695386   |
| Train/Ratio             | 0.97856325  |
| Train/Return            | 0.97404903  |
| Train/V                 | 1.0524809   |
| Train/Value             | 1.0524809   |
| Train/control_penalty   | 0.6927126   |
| Train/policy_loss       | 0.0695386   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0165      |
-----------------------------------------

 ---------------- Iteration 134 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 133         |
| Time/Actor_Time         | 0.0868      |
| Time/B_Format_Time      | 0.0697      |
| Time/B_Original_Form... | 0.069       |
| Time/Buffer             | 0.00223     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33644778  |
| Train/Action_magnitu... | 0.8557917   |
| Train/Action_magnitude  | 0.673361    |
| Train/Action_max        | 0.1229016   |
| Train/Action_std        | 0.3121445   |
| Train/Entropy           | 0.16627063  |
| Train/Entropy_Loss      | -0.000166   |
| Train/Entropy_loss      | -0.000166   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.34341228 |
| Train/Loss              | 0.07602775  |
| Train/PolicyClip        | 0.004234408 |
| Train/Policy_loss       | 0.06942567  |
| Train/Ratio             | 0.98450977  |
| Train/Return            | 1.0989517   |
| Train/V                 | 1.1790701   |
| Train/Value             | 1.1790701   |
| Train/control_penalty   | 0.67683476  |
| Train/policy_loss       | 0.06942567  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 135 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 134          |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0692       |
| Time/Buffer             | 0.0035       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33910155   |
| Train/Action_magnitu... | 0.9030297    |
| Train/Action_magnitude  | 0.7098892    |
| Train/Action_max        | 0.13855554   |
| Train/Action_std        | 0.32900798   |
| Train/Entropy           | 0.20213552   |
| Train/Entropy_Loss      | -0.000202    |
| Train/Entropy_loss      | -0.000202    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.43777826  |
| Train/Loss              | 0.008578669  |
| Train/PolicyClip        | 0.0025160925 |
| Train/Policy_loss       | 0.0018622263 |
| Train/Ratio             | 0.99767715   |
| Train/Return            | 0.8763755    |
| Train/V                 | 0.88660866   |
| Train/Value             | 0.88660866   |
| Train/control_penalty   | 0.6918579    |
| Train/policy_loss       | 0.0018622263 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0165       |
------------------------------------------

 ---------------- Iteration 136 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 135          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.36040333   |
| Train/Action_magnitu... | 0.93323374   |
| Train/Action_magnitude  | 0.73463243   |
| Train/Action_max        | 0.15965882   |
| Train/Action_std        | 0.3495427    |
| Train/Entropy           | 0.26684576   |
| Train/Entropy_Loss      | -0.000267    |
| Train/Entropy_loss      | -0.000267    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5341255   |
| Train/Loss              | -0.01593037  |
| Train/PolicyClip        | 0.003820536  |
| Train/Policy_loss       | -0.023004547 |
| Train/Ratio             | 0.9910191    |
| Train/Return            | 0.77613676   |
| Train/V                 | 0.7620048    |
| Train/Value             | 0.7620048    |
| Train/control_penalty   | 0.7341023    |
| Train/policy_loss       | -0.023004547 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 137 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 136          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.0024       |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.36108702   |
| Train/Action_magnitu... | 0.9123782    |
| Train/Action_magnitude  | 0.72349614   |
| Train/Action_max        | 0.19564708   |
| Train/Action_std        | 0.3265961    |
| Train/Entropy           | 0.1919732    |
| Train/Entropy_Loss      | -0.000192    |
| Train/Entropy_loss      | -0.000192    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.43186918  |
| Train/Loss              | -0.048286952 |
| Train/PolicyClip        | 0.0034398302 |
| Train/Policy_loss       | -0.055436157 |
| Train/Ratio             | 1.0084444    |
| Train/Return            | 0.862844     |
| Train/V                 | 0.81375474   |
| Train/Value             | 0.81375474   |
| Train/control_penalty   | 0.73411745   |
| Train/policy_loss       | -0.055436157 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02         |
------------------------------------------

 ---------------- Iteration 138 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 137          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0688       |
| Time/Buffer             | 0.00543      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35072425   |
| Train/Action_magnitu... | 0.88035476   |
| Train/Action_magnitude  | 0.6939483    |
| Train/Action_max        | 0.19131826   |
| Train/Action_std        | 0.30059174   |
| Train/Entropy           | 0.11501627   |
| Train/Entropy_Loss      | -0.000115    |
| Train/Entropy_loss      | -0.000115    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.28751957  |
| Train/Loss              | -0.07797255  |
| Train/PolicyClip        | 0.0033571874 |
| Train/Policy_loss       | -0.08478351  |
| Train/Ratio             | 1.007664     |
| Train/Return            | 0.99615663   |
| Train/V                 | 0.9190818    |
| Train/Value             | 0.9190818    |
| Train/control_penalty   | 0.6925973    |
| Train/policy_loss       | -0.08478351  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 139 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 138          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00257      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32240108   |
| Train/Action_magnitu... | 0.8540466    |
| Train/Action_magnitude  | 0.67113864   |
| Train/Action_max        | 0.22217646   |
| Train/Action_std        | 0.32091013   |
| Train/Entropy           | 0.19083723   |
| Train/Entropy_Loss      | -0.000191    |
| Train/Entropy_loss      | -0.000191    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.42428544  |
| Train/Loss              | -0.037897207 |
| Train/PolicyClip        | 0.0031032627 |
| Train/Policy_loss       | -0.044565707 |
| Train/Ratio             | 0.9982432    |
| Train/Return            | 0.74361914   |
| Train/V                 | 0.70549244   |
| Train/Value             | 0.70549244   |
| Train/control_penalty   | 0.68593377   |
| Train/policy_loss       | -0.044565707 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0165       |
------------------------------------------

 ---------------- Iteration 140 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 139          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.069        |
| Time/Buffer             | 0.00317      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33953676   |
| Train/Action_magnitu... | 0.8648031    |
| Train/Action_magnitude  | 0.6814446    |
| Train/Action_max        | 0.20284946   |
| Train/Action_std        | 0.32565224   |
| Train/Entropy           | 0.2052248    |
| Train/Entropy_Loss      | -0.000205    |
| Train/Entropy_loss      | -0.000205    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.39791006  |
| Train/Loss              | -0.026444947 |
| Train/PolicyClip        | 0.0044720694 |
| Train/Policy_loss       | -0.033103496 |
| Train/Ratio             | 0.9964432    |
| Train/Return            | 0.6968068    |
| Train/V                 | 0.6689319    |
| Train/Value             | 0.6689319    |
| Train/control_penalty   | 0.68637747   |
| Train/policy_loss       | -0.033103496 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01525      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 141 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 140          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00254      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.31752327   |
| Train/Action_magnitu... | 0.8260401    |
| Train/Action_magnitude  | 0.64918804   |
| Train/Action_max        | 0.19875993   |
| Train/Action_std        | 0.3247171    |
| Train/Entropy           | 0.20692918   |
| Train/Entropy_Loss      | -0.000207    |
| Train/Entropy_loss      | -0.000207    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4056598   |
| Train/Loss              | -0.03153346  |
| Train/PolicyClip        | 0.0056448774 |
| Train/Policy_loss       | -0.03797251  |
| Train/Ratio             | 0.9985194    |
| Train/Return            | 0.6879956    |
| Train/V                 | 0.65323496   |
| Train/Value             | 0.65323496   |
| Train/control_penalty   | 0.66459763   |
| Train/policy_loss       | -0.03797251  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0145       |
------------------------------------------

 ---------------- Iteration 142 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 141          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3029493    |
| Train/Action_magnitu... | 0.809387     |
| Train/Action_magnitude  | 0.6396064    |
| Train/Action_max        | 0.18704996   |
| Train/Action_std        | 0.2870937    |
| Train/Entropy           | 0.073804475  |
| Train/Entropy_Loss      | -7.38e-05    |
| Train/Entropy_loss      | -7.38e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.23881339  |
| Train/Loss              | -0.07773998  |
| Train/PolicyClip        | 0.0030439277 |
| Train/Policy_loss       | -0.083961606 |
| Train/Ratio             | 1.0102566    |
| Train/Return            | 0.91123486   |
| Train/V                 | 0.8330947    |
| Train/Value             | 0.8330947    |
| Train/control_penalty   | 0.6295432    |
| Train/policy_loss       | -0.083961606 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 143 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 142          |
| Time/Actor_Time         | 0.0872       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0693       |
| Time/Buffer             | 0.00275      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28735253   |
| Train/Action_magnitu... | 0.7709952    |
| Train/Action_magnitude  | 0.6077746    |
| Train/Action_max        | 0.19710061   |
| Train/Action_std        | 0.28599346   |
| Train/Entropy           | 0.07972513   |
| Train/Entropy_Loss      | -7.97e-05    |
| Train/Entropy_loss      | -7.97e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.19942725  |
| Train/Loss              | -0.084763326 |
| Train/PolicyClip        | 0.0056928354 |
| Train/Policy_loss       | -0.09069197  |
| Train/Ratio             | 1.0039974    |
| Train/Return            | 0.8513033    |
| Train/V                 | 0.7663623    |
| Train/Value             | 0.7663623    |
| Train/control_penalty   | 0.6008374    |
| Train/policy_loss       | -0.09069197  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------

 ---------------- Iteration 144 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 143          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0701       |
| Time/B_Original_Form... | 0.0827       |
| Time/Buffer             | 0.0031       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28456378   |
| Train/Action_magnitu... | 0.78362405   |
| Train/Action_magnitude  | 0.61727667   |
| Train/Action_max        | 0.19578254   |
| Train/Action_std        | 0.29486102   |
| Train/Entropy           | 0.09673491   |
| Train/Entropy_Loss      | -9.67e-05    |
| Train/Entropy_loss      | -9.67e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.255177    |
| Train/Loss              | -0.20859803  |
| Train/PolicyClip        | 0.0043693506 |
| Train/Policy_loss       | -0.21472502  |
| Train/Ratio             | 1.0104319    |
| Train/Return            | 0.9788006    |
| Train/V                 | 0.77393514   |
| Train/Value             | 0.77393514   |
| Train/control_penalty   | 0.6223722    |
| Train/policy_loss       | -0.21472502  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0315       |
------------------------------------------

 ---------------- Iteration 145 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 144         |
| Time/Actor_Time         | 0.0865      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0694      |
| Time/Buffer             | 0.00259     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3023299   |
| Train/Action_magnitu... | 0.796452    |
| Train/Action_magnitude  | 0.6290045   |
| Train/Action_max        | 0.19332425  |
| Train/Action_std        | 0.27483097  |
| Train/Entropy           | 0.03645418  |
| Train/Entropy_Loss      | -3.65e-05   |
| Train/Entropy_loss      | -3.65e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.18103199 |
| Train/Loss              | -0.16364914 |
| Train/PolicyClip        | 0.003854455 |
| Train/Policy_loss       | -0.16978404 |
| Train/Ratio             | 1.0106804   |
| Train/Return            | 0.94418675  |
| Train/V                 | 0.78432775  |
| Train/Value             | 0.78432775  |
| Train/control_penalty   | 0.6171347   |
| Train/policy_loss       | -0.16978404 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 146 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 145          |
| Time/Actor_Time         | 0.0836       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00299      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30873564   |
| Train/Action_magnitu... | 0.7981385    |
| Train/Action_magnitude  | 0.6282389    |
| Train/Action_max        | 0.18204525   |
| Train/Action_std        | 0.26629415   |
| Train/Entropy           | 0.0071508256 |
| Train/Entropy_Loss      | -7.15e-06    |
| Train/Entropy_loss      | -7.15e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.075066015 |
| Train/Loss              | -0.092477076 |
| Train/PolicyClip        | 0.005683577  |
| Train/Policy_loss       | -0.098664306 |
| Train/Ratio             | 1.0067936    |
| Train/Return            | 1.0269383    |
| Train/V                 | 0.93835384   |
| Train/Value             | 0.93835384   |
| Train/control_penalty   | 0.6194381    |
| Train/policy_loss       | -0.098664306 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 147 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 146          |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00247      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30210656   |
| Train/Action_magnitu... | 0.745439     |
| Train/Action_magnitude  | 0.58902      |
| Train/Action_max        | 0.16803922   |
| Train/Action_std        | 0.25367656   |
| Train/Entropy           | -0.038593598 |
| Train/Entropy_Loss      | 3.86e-05     |
| Train/Entropy_loss      | 3.86e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.07048419   |
| Train/Loss              | -0.034977198 |
| Train/PolicyClip        | 0.0055270535 |
| Train/Policy_loss       | -0.041006133 |
| Train/Ratio             | 1.0003633    |
| Train/Return            | 1.0339537    |
| Train/V                 | 0.9980316    |
| Train/Value             | 0.9980316    |
| Train/control_penalty   | 0.59903395   |
| Train/policy_loss       | -0.041006133 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02         |
------------------------------------------

 ---------------- Iteration 148 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 147          |
| Time/Actor_Time         | 0.0871       |
| Time/B_Format_Time      | 0.0729       |
| Time/B_Original_Form... | 0.0693       |
| Time/Buffer             | 0.00255      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33650383   |
| Train/Action_magnitu... | 0.823833     |
| Train/Action_magnitude  | 0.6501621    |
| Train/Action_max        | 0.17322394   |
| Train/Action_std        | 0.25756893   |
| Train/Entropy           | -0.017450431 |
| Train/Entropy_Loss      | 1.75e-05     |
| Train/Entropy_loss      | 1.75e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.002999668  |
| Train/Loss              | -0.05175705  |
| Train/PolicyClip        | 0.0046123737 |
| Train/Policy_loss       | -0.05830491  |
| Train/Ratio             | 0.99221534   |
| Train/Return            | 1.0698866    |
| Train/V                 | 1.0201441    |
| Train/Value             | 1.0201441    |
| Train/control_penalty   | 0.6530411    |
| Train/policy_loss       | -0.05830491  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 149 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 148          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00349      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3313814    |
| Train/Action_magnitu... | 0.8092114    |
| Train/Action_magnitude  | 0.636667     |
| Train/Action_max        | 0.17195879   |
| Train/Action_std        | 0.26110542   |
| Train/Entropy           | 0.011955216  |
| Train/Entropy_Loss      | -1.2e-05     |
| Train/Entropy_loss      | -1.2e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.09473338  |
| Train/Loss              | -0.077835135 |
| Train/PolicyClip        | 0.0052410746 |
| Train/Policy_loss       | -0.084263824 |
| Train/Ratio             | 1.0037773    |
| Train/Return            | 1.0320286    |
| Train/V                 | 0.9576345    |
| Train/Value             | 0.9576345    |
| Train/control_penalty   | 0.64406455   |
| Train/policy_loss       | -0.084263824 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 150 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 149          |
| Time/Actor_Time         | 0.0884       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00231      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31561026   |
| Train/Action_magnitu... | 0.77329886   |
| Train/Action_magnitude  | 0.61071974   |
| Train/Action_max        | 0.18825236   |
| Train/Action_std        | 0.26690787   |
| Train/Entropy           | 0.01837796   |
| Train/Entropy_Loss      | -1.84e-05    |
| Train/Entropy_loss      | -1.84e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.037118595 |
| Train/Loss              | -0.07538771  |
| Train/PolicyClip        | 0.006813091  |
| Train/Policy_loss       | -0.08155586  |
| Train/Ratio             | 0.9990591    |
| Train/Return            | 1.0136143    |
| Train/V                 | 0.93758124   |
| Train/Value             | 0.93758124   |
| Train/control_penalty   | 0.6186529    |
| Train/policy_loss       | -0.08155586  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01975      |
------------------------------------------

 ---------------- Iteration 151 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 150          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0734       |
| Time/B_Original_Form... | 0.0743       |
| Time/Buffer             | 0.00301      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3282936    |
| Train/Action_magnitu... | 0.80376846   |
| Train/Action_magnitude  | 0.63586485   |
| Train/Action_max        | 0.1495217    |
| Train/Action_std        | 0.24513191   |
| Train/Entropy           | -0.066149674 |
| Train/Entropy_Loss      | 6.61e-05     |
| Train/Entropy_loss      | 6.61e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.046608284  |
| Train/Loss              | -0.108669005 |
| Train/PolicyClip        | 0.004377003  |
| Train/Policy_loss       | -0.1149481   |
| Train/Ratio             | 1.0043995    |
| Train/Return            | 1.0828664    |
| Train/V                 | 0.97719115   |
| Train/Value             | 0.97719115   |
| Train/control_penalty   | 0.621295     |
| Train/policy_loss       | -0.1149481   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 152 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 151          |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0692       |
| Time/Buffer             | 0.00205      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30678618   |
| Train/Action_magnitu... | 0.7676471    |
| Train/Action_magnitude  | 0.6088173    |
| Train/Action_max        | 0.15692914   |
| Train/Action_std        | 0.26785207   |
| Train/Entropy           | 0.022564285  |
| Train/Entropy_Loss      | -2.26e-05    |
| Train/Entropy_loss      | -2.26e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.024189832  |
| Train/Loss              | 0.082479194  |
| Train/PolicyClip        | 0.0037317916 |
| Train/Policy_loss       | 0.07625339   |
| Train/Ratio             | 0.9757913    |
| Train/Return            | 0.9769646    |
| Train/V                 | 1.0616425    |
| Train/Value             | 1.0616425    |
| Train/control_penalty   | 0.624837     |
| Train/policy_loss       | 0.07625339   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01         |
------------------------------------------

 ---------------- Iteration 153 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 152          |
| Time/Actor_Time         | 0.0919       |
| Time/B_Format_Time      | 0.0758       |
| Time/B_Original_Form... | 0.0737       |
| Time/Buffer             | 0.00339      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3262473    |
| Train/Action_magnitu... | 0.7934592    |
| Train/Action_magnitude  | 0.6292273    |
| Train/Action_max        | 0.18718392   |
| Train/Action_std        | 0.26007444   |
| Train/Entropy           | -0.025750278 |
| Train/Entropy_Loss      | 2.58e-05     |
| Train/Entropy_loss      | 2.58e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.015189855 |
| Train/Loss              | -0.030710563 |
| Train/PolicyClip        | 0.006493566  |
| Train/Policy_loss       | -0.037161253 |
| Train/Ratio             | 0.9996077    |
| Train/Return            | 1.0727928    |
| Train/V                 | 1.0407243    |
| Train/Value             | 1.0407243    |
| Train/control_penalty   | 0.64249414   |
| Train/policy_loss       | -0.037161253 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 154 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 153           |
| Time/Actor_Time         | 0.0854        |
| Time/B_Format_Time      | 0.0715        |
| Time/B_Original_Form... | 0.0692        |
| Time/Buffer             | 0.00244       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.34646675    |
| Train/Action_magnitu... | 0.84921294    |
| Train/Action_magnitude  | 0.67477       |
| Train/Action_max        | 0.18853898    |
| Train/Action_std        | 0.268459      |
| Train/Entropy           | 0.0058298605  |
| Train/Entropy_Loss      | -5.83e-06     |
| Train/Entropy_loss      | -5.83e-06     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.08209723   |
| Train/Loss              | -0.0072933408 |
| Train/PolicyClip        | 0.0048414655  |
| Train/Policy_loss       | -0.013952456  |
| Train/Ratio             | 0.98824465    |
| Train/Return            | 1.0274212     |
| Train/V                 | 1.0156394     |
| Train/Value             | 1.0156394     |
| Train/control_penalty   | 0.6664946     |
| Train/policy_loss       | -0.013952456  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01775       |
-------------------------------------------

 ---------------- Iteration 155 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 154          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00211      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31894526   |
| Train/Action_magnitu... | 0.78170437   |
| Train/Action_magnitude  | 0.61862963   |
| Train/Action_max        | 0.16840778   |
| Train/Action_std        | 0.25845918   |
| Train/Entropy           | -0.020752052 |
| Train/Entropy_Loss      | 2.08e-05     |
| Train/Entropy_loss      | 2.08e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.019291583  |
| Train/Loss              | 0.05980866   |
| Train/PolicyClip        | 0.0032403674 |
| Train/Policy_loss       | 0.05349723   |
| Train/Ratio             | 0.98187053   |
| Train/Return            | 0.8782342    |
| Train/V                 | 0.94026935   |
| Train/Value             | 0.94026935   |
| Train/control_penalty   | 0.62906766   |
| Train/policy_loss       | 0.05349723   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00925      |
------------------------------------------

 ---------------- Iteration 156 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 155          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00252      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2948976    |
| Train/Action_magnitu... | 0.7631337    |
| Train/Action_magnitude  | 0.59952784   |
| Train/Action_max        | 0.21408062   |
| Train/Action_std        | 0.25630918   |
| Train/Entropy           | -0.043086287 |
| Train/Entropy_Loss      | 4.31e-05     |
| Train/Entropy_loss      | 4.31e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.017659578  |
| Train/Loss              | 0.0034352748 |
| Train/PolicyClip        | 0.0061193197 |
| Train/Policy_loss       | -0.002574371 |
| Train/Ratio             | 0.9875802    |
| Train/Return            | 0.85611284   |
| Train/V                 | 0.8609851    |
| Train/Value             | 0.8609851    |
| Train/control_penalty   | 0.59665596   |
| Train/policy_loss       | -0.002574371 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0135       |
------------------------------------------

 ---------------- Iteration 157 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 156          |
| Time/Actor_Time         | 0.0882       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00314      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29713702   |
| Train/Action_magnitu... | 0.7542602    |
| Train/Action_magnitude  | 0.5960356    |
| Train/Action_max        | 0.2079469    |
| Train/Action_std        | 0.26528803   |
| Train/Entropy           | -0.005403919 |
| Train/Entropy_Loss      | 5.4e-06      |
| Train/Entropy_loss      | 5.4e-06      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.007431445  |
| Train/Loss              | -0.07506542  |
| Train/PolicyClip        | 0.0046050446 |
| Train/Policy_loss       | -0.081128486 |
| Train/Ratio             | 1.0019456    |
| Train/Return            | 0.9803531    |
| Train/V                 | 0.9064206    |
| Train/Value             | 0.9064206    |
| Train/control_penalty   | 0.6057661    |
| Train/policy_loss       | -0.081128486 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 158 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 157           |
| Time/Actor_Time         | 0.0858        |
| Time/B_Format_Time      | 0.0742        |
| Time/B_Original_Form... | 0.0718        |
| Time/Buffer             | 0.00311       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.33716267    |
| Train/Action_magnitu... | 0.8241744     |
| Train/Action_magnitude  | 0.6513132     |
| Train/Action_max        | 0.23311773    |
| Train/Action_std        | 0.27417794    |
| Train/Entropy           | 0.016780041   |
| Train/Entropy_Loss      | -1.68e-05     |
| Train/Entropy_loss      | -1.68e-05     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.08319157   |
| Train/Loss              | -0.0031818836 |
| Train/PolicyClip        | 0.0048105638  |
| Train/Policy_loss       | -0.009796603  |
| Train/Ratio             | 0.9940066     |
| Train/Return            | 0.8655826     |
| Train/V                 | 0.8625818     |
| Train/Value             | 0.8625818     |
| Train/control_penalty   | 0.6631499     |
| Train/policy_loss       | -0.009796603  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.016         |
-------------------------------------------

 ---------------- Iteration 159 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 158           |
| Time/Actor_Time         | 0.0849        |
| Time/B_Format_Time      | 0.0705        |
| Time/B_Original_Form... | 0.0691        |
| Time/Buffer             | 0.00296       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31678295    |
| Train/Action_magnitu... | 0.8035891     |
| Train/Action_magnitude  | 0.6373028     |
| Train/Action_max        | 0.22384773    |
| Train/Action_std        | 0.24906297    |
| Train/Entropy           | -0.07831975   |
| Train/Entropy_Loss      | 7.83e-05      |
| Train/Entropy_loss      | 7.83e-05      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.0016793051 |
| Train/Loss              | -0.09460775   |
| Train/PolicyClip        | 0.002507253   |
| Train/Policy_loss       | -0.10085514   |
| Train/Ratio             | 1.0007056     |
| Train/Return            | 0.99700624    |
| Train/V                 | 0.9033075     |
| Train/Value             | 0.9033075     |
| Train/control_penalty   | 0.6169071     |
| Train/policy_loss       | -0.10085514   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02225       |
-------------------------------------------

 ---------------- Iteration 160 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 159          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0693       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30866623   |
| Train/Action_magnitu... | 0.7747219    |
| Train/Action_magnitude  | 0.6170169    |
| Train/Action_max        | 0.23669705   |
| Train/Action_std        | 0.24087575   |
| Train/Entropy           | -0.10724165  |
| Train/Entropy_Loss      | 0.000107     |
| Train/Entropy_loss      | 0.000107     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.10908739   |
| Train/Loss              | -0.16199629  |
| Train/PolicyClip        | 0.0019016914 |
| Train/Policy_loss       | -0.16811141  |
| Train/Ratio             | 1.0073478    |
| Train/Return            | 1.1158494    |
| Train/V                 | 0.9567603    |
| Train/Value             | 0.9567603    |
| Train/control_penalty   | 0.60078824   |
| Train/policy_loss       | -0.16811141  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03         |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 161 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 160          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00316      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28930694   |
| Train/Action_magnitu... | 0.7213115    |
| Train/Action_magnitude  | 0.57357955   |
| Train/Action_max        | 0.22518602   |
| Train/Action_std        | 0.23122081   |
| Train/Entropy           | -0.1402781   |
| Train/Entropy_Loss      | 0.00014      |
| Train/Entropy_loss      | 0.00014      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.17869204   |
| Train/Loss              | -0.07443166  |
| Train/PolicyClip        | 0.0051126266 |
| Train/Policy_loss       | -0.08033186  |
| Train/Ratio             | 0.9975286    |
| Train/Return            | 1.2129103    |
| Train/V                 | 1.1296349    |
| Train/Value             | 1.1296349    |
| Train/control_penalty   | 0.5759925    |
| Train/policy_loss       | -0.08033186  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 162 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 161          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00245      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28908888   |
| Train/Action_magnitu... | 0.7119598    |
| Train/Action_magnitude  | 0.5639815    |
| Train/Action_max        | 0.23340842   |
| Train/Action_std        | 0.25210375   |
| Train/Entropy           | -0.05449103  |
| Train/Entropy_Loss      | 5.45e-05     |
| Train/Entropy_loss      | 5.45e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.14672      |
| Train/Loss              | 0.011828922  |
| Train/PolicyClip        | 0.0034026185 |
| Train/Policy_loss       | 0.006003766  |
| Train/Ratio             | 0.9869448    |
| Train/Return            | 1.237127     |
| Train/V                 | 1.2500707    |
| Train/Value             | 1.2500707    |
| Train/control_penalty   | 0.57706654   |
| Train/policy_loss       | 0.006003766  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 163 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 162          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0698       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00309      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30341133   |
| Train/Action_magnitu... | 0.7693822    |
| Train/Action_magnitude  | 0.6084367    |
| Train/Action_max        | 0.23495606   |
| Train/Action_std        | 0.2678019    |
| Train/Entropy           | 0.0042740745 |
| Train/Entropy_Loss      | -4.27e-06    |
| Train/Entropy_loss      | -4.27e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.020410046  |
| Train/Loss              | 0.025663672  |
| Train/PolicyClip        | 0.0069509554 |
| Train/Policy_loss       | 0.019471996  |
| Train/Ratio             | 0.9855781    |
| Train/Return            | 1.1238701    |
| Train/V                 | 1.1494259    |
| Train/Value             | 1.1494259    |
| Train/control_penalty   | 0.6195951    |
| Train/policy_loss       | 0.019471996  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01825      |
------------------------------------------

 ---------------- Iteration 164 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 163          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0693       |
| Time/Buffer             | 0.00275      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3380923    |
| Train/Action_magnitu... | 0.8472465    |
| Train/Action_magnitude  | 0.67134583   |
| Train/Action_max        | 0.25650278   |
| Train/Action_std        | 0.26446775   |
| Train/Entropy           | -0.020641675 |
| Train/Entropy_Loss      | 2.06e-05     |
| Train/Entropy_loss      | 2.06e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.0002626481 |
| Train/Loss              | -0.05090162  |
| Train/PolicyClip        | 0.0077647395 |
| Train/Policy_loss       | -0.05760601  |
| Train/Ratio             | 0.9924938    |
| Train/Return            | 1.2795191    |
| Train/V                 | 1.2323226    |
| Train/Value             | 1.2323226    |
| Train/control_penalty   | 0.6683748    |
| Train/policy_loss       | -0.05760601  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 165 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 164          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00242      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33595243   |
| Train/Action_magnitu... | 0.8329181    |
| Train/Action_magnitude  | 0.6595101    |
| Train/Action_max        | 0.23798347   |
| Train/Action_std        | 0.27329656   |
| Train/Entropy           | 0.036480628  |
| Train/Entropy_Loss      | -3.65e-05    |
| Train/Entropy_loss      | -3.65e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.06734819  |
| Train/Loss              | 0.060115125  |
| Train/PolicyClip        | 0.0067801382 |
| Train/Policy_loss       | 0.053565126  |
| Train/Ratio             | 0.99343175   |
| Train/Return            | 1.1008662    |
| Train/V                 | 1.1617892    |
| Train/Value             | 1.1617892    |
| Train/control_penalty   | 0.65864813   |
| Train/policy_loss       | 0.053565126  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.016        |
------------------------------------------

 ---------------- Iteration 166 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 165          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00258      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30654782   |
| Train/Action_magnitu... | 0.7935122    |
| Train/Action_magnitude  | 0.6285735    |
| Train/Action_max        | 0.24594617   |
| Train/Action_std        | 0.27449328   |
| Train/Entropy           | 0.023443328  |
| Train/Entropy_Loss      | -2.34e-05    |
| Train/Entropy_loss      | -2.34e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.103375435 |
| Train/Loss              | -0.062160287 |
| Train/PolicyClip        | 0.0044338657 |
| Train/Policy_loss       | -0.068429865 |
| Train/Ratio             | 1.0016859    |
| Train/Return            | 1.2468293    |
| Train/V                 | 1.1854956    |
| Train/Value             | 1.1854956    |
| Train/control_penalty   | 0.6293024    |
| Train/policy_loss       | -0.068429865 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 167 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 166           |
| Time/Actor_Time         | 0.0854        |
| Time/B_Format_Time      | 0.0718        |
| Time/B_Original_Form... | 0.0695        |
| Time/Buffer             | 0.00269       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.30881628    |
| Train/Action_magnitu... | 0.7647046     |
| Train/Action_magnitude  | 0.60785973    |
| Train/Action_max        | 0.25045428    |
| Train/Action_std        | 0.2574523     |
| Train/Entropy           | -0.04480826   |
| Train/Entropy_Loss      | 4.48e-05      |
| Train/Entropy_loss      | 4.48e-05      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.05867192    |
| Train/Loss              | 0.0018384852  |
| Train/PolicyClip        | 0.0042503728  |
| Train/Policy_loss       | -0.0044925767 |
| Train/Ratio             | 0.9891999     |
| Train/Return            | 1.2031057     |
| Train/V                 | 1.203124      |
| Train/Value             | 1.203124      |
| Train/control_penalty   | 0.6286254     |
| Train/policy_loss       | -0.0044925767 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02          |
-------------------------------------------

 ---------------- Iteration 168 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 167          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00317      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.330396     |
| Train/Action_magnitu... | 0.80689937   |
| Train/Action_magnitude  | 0.64374334   |
| Train/Action_max        | 0.2444356    |
| Train/Action_std        | 0.2858348    |
| Train/Entropy           | 0.06501162   |
| Train/Entropy_Loss      | -6.5e-05     |
| Train/Entropy_loss      | -6.5e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.03986766  |
| Train/Loss              | 0.11307025   |
| Train/PolicyClip        | 0.0054950793 |
| Train/Policy_loss       | 0.10651059   |
| Train/Ratio             | 0.9770071    |
| Train/Return            | 1.1006415    |
| Train/V                 | 1.2132279    |
| Train/Value             | 1.2132279    |
| Train/control_penalty   | 0.6624675    |
| Train/policy_loss       | 0.10651059   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0135       |
------------------------------------------

 ---------------- Iteration 169 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 168         |
| Time/Actor_Time         | 0.0855      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0699      |
| Time/Buffer             | 0.00243     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31792068  |
| Train/Action_magnitu... | 0.82399523  |
| Train/Action_magnitude  | 0.65671784  |
| Train/Action_max        | 0.24221511  |
| Train/Action_std        | 0.30365342  |
| Train/Entropy           | 0.1151876   |
| Train/Entropy_Loss      | -0.000115   |
| Train/Entropy_loss      | -0.000115   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.21993972 |
| Train/Loss              | 0.061960682 |
| Train/PolicyClip        | 0.006511876 |
| Train/Policy_loss       | 0.055454552 |
| Train/Ratio             | 0.9902837   |
| Train/Return            | 0.90819496  |
| Train/V                 | 0.97066265  |
| Train/Value             | 0.97066265  |
| Train/control_penalty   | 0.6621315   |
| Train/policy_loss       | 0.055454552 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01475     |
-----------------------------------------

 ---------------- Iteration 170 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 169          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.32329708   |
| Train/Action_magnitu... | 0.84261924   |
| Train/Action_magnitude  | 0.67228144   |
| Train/Action_max        | 0.2460355    |
| Train/Action_std        | 0.29506662   |
| Train/Entropy           | 0.08176456   |
| Train/Entropy_Loss      | -8.18e-05    |
| Train/Entropy_loss      | -8.18e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.16014019  |
| Train/Loss              | 0.04704297   |
| Train/PolicyClip        | 0.0075732293 |
| Train/Policy_loss       | 0.04037399   |
| Train/Ratio             | 0.99739546   |
| Train/Return            | 1.1178256    |
| Train/V                 | 1.1610619    |
| Train/Value             | 1.1610619    |
| Train/control_penalty   | 0.67507476   |
| Train/policy_loss       | 0.04037399   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 171 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 170         |
| Time/Actor_Time         | 0.087       |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.00233     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30602494  |
| Train/Action_magnitu... | 0.79607403  |
| Train/Action_magnitude  | 0.6298789   |
| Train/Action_max        | 0.23324507  |
| Train/Action_std        | 0.28618354  |
| Train/Entropy           | 0.05427404  |
| Train/Entropy_Loss      | -5.43e-05   |
| Train/Entropy_loss      | -5.43e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.1460447  |
| Train/Loss              | 0.030427027 |
| Train/PolicyClip        | 0.008212087 |
| Train/Policy_loss       | 0.024225395 |
| Train/Ratio             | 0.9878526   |
| Train/Return            | 1.0591154   |
| Train/V                 | 1.0872996   |
| Train/Value             | 1.0872996   |
| Train/control_penalty   | 0.62559056  |
| Train/policy_loss       | 0.024225395 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 172 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 171          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0697       |
| Time/B_Original_Form... | 0.0696       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.31329352   |
| Train/Action_magnitu... | 0.8114408    |
| Train/Action_magnitude  | 0.6431624    |
| Train/Action_max        | 0.22367477   |
| Train/Action_std        | 0.29084557   |
| Train/Entropy           | 0.06897829   |
| Train/Entropy_Loss      | -6.9e-05     |
| Train/Entropy_loss      | -6.9e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.12673575  |
| Train/Loss              | 0.033165712  |
| Train/PolicyClip        | 0.0063692215 |
| Train/Policy_loss       | 0.026763601  |
| Train/Ratio             | 0.9946258    |
| Train/Return            | 1.0802045    |
| Train/V                 | 1.1099939    |
| Train/Value             | 1.1099939    |
| Train/control_penalty   | 0.647109     |
| Train/policy_loss       | 0.026763601  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 173 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 172         |
| Time/Actor_Time         | 0.0855      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.0696      |
| Time/Buffer             | 0.00242     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30553254  |
| Train/Action_magnitu... | 0.82148606  |
| Train/Action_magnitude  | 0.651925    |
| Train/Action_max        | 0.20509398  |
| Train/Action_std        | 0.3019694   |
| Train/Entropy           | 0.12192825  |
| Train/Entropy_Loss      | -0.000122   |
| Train/Entropy_loss      | -0.000122   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.23321702 |
| Train/Loss              | 0.06870694  |
| Train/PolicyClip        | 0.00427963  |
| Train/Policy_loss       | 0.062345    |
| Train/Ratio             | 0.9823514   |
| Train/Return            | 0.9692031   |
| Train/V                 | 1.0406468   |
| Train/Value             | 1.0406468   |
| Train/control_penalty   | 0.64838654  |
| Train/policy_loss       | 0.062345    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01425     |
-----------------------------------------

 ---------------- Iteration 174 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 173         |
| Time/Actor_Time         | 0.0856      |
| Time/B_Format_Time      | 0.0719      |
| Time/B_Original_Form... | 0.0703      |
| Time/Buffer             | 0.00239     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3069424   |
| Train/Action_magnitu... | 0.8145126   |
| Train/Action_magnitude  | 0.6433981   |
| Train/Action_max        | 0.19275415  |
| Train/Action_std        | 0.3084935   |
| Train/Entropy           | 0.13240951  |
| Train/Entropy_Loss      | -0.000132   |
| Train/Entropy_loss      | -0.000132   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.23572892 |
| Train/Loss              | 0.047095273 |
| Train/PolicyClip        | 0.005059528 |
| Train/Policy_loss       | 0.040664136 |
| Train/Ratio             | 0.9924623   |
| Train/Return            | 1.0066208   |
| Train/V                 | 1.0506184   |
| Train/Value             | 1.0506184   |
| Train/control_penalty   | 0.65635437  |
| Train/policy_loss       | 0.040664136 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.017       |
-----------------------------------------

 ---------------- Iteration 175 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 174         |
| Time/Actor_Time         | 0.0852      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.00252     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.30112895  |
| Train/Action_magnitu... | 0.813845    |
| Train/Action_magnitude  | 0.6442886   |
| Train/Action_max        | 0.19544965  |
| Train/Action_std        | 0.32171103  |
| Train/Entropy           | 0.17409974  |
| Train/Entropy_Loss      | -0.000174   |
| Train/Entropy_loss      | -0.000174   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.370734   |
| Train/Loss              | 0.088397205 |
| Train/PolicyClip        | 0.007065085 |
| Train/Policy_loss       | 0.08196357  |
| Train/Ratio             | 0.98207533  |
| Train/Return            | 0.94013804  |
| Train/V                 | 1.0297917   |
| Train/Value             | 1.0297917   |
| Train/control_penalty   | 0.6607731   |
| Train/policy_loss       | 0.08196357  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01375     |
-----------------------------------------

 ---------------- Iteration 176 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 175          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0701       |
| Time/Buffer             | 0.0023       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29941425   |
| Train/Action_magnitu... | 0.80943865   |
| Train/Action_magnitude  | 0.6406803    |
| Train/Action_max        | 0.18642312   |
| Train/Action_std        | 0.3089548    |
| Train/Entropy           | 0.12565348   |
| Train/Entropy_Loss      | -0.000126    |
| Train/Entropy_loss      | -0.000126    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.32617792  |
| Train/Loss              | -0.017339382 |
| Train/PolicyClip        | 0.008084441  |
| Train/Policy_loss       | -0.02378199  |
| Train/Ratio             | 0.98602474   |
| Train/Return            | 0.9119015    |
| Train/V                 | 0.8959293    |
| Train/Value             | 0.8959293    |
| Train/control_penalty   | 0.65682614   |
| Train/policy_loss       | -0.02378199  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 177 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 176         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.0715      |
| Time/Buffer             | 0.00256     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2885919   |
| Train/Action_magnitu... | 0.76827806  |
| Train/Action_magnitude  | 0.6077454   |
| Train/Action_max        | 0.18190147  |
| Train/Action_std        | 0.29698122  |
| Train/Entropy           | 0.08663656  |
| Train/Entropy_Loss      | -8.66e-05   |
| Train/Entropy_loss      | -8.66e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.19078955 |
| Train/Loss              | 0.043094367 |
| Train/PolicyClip        | 0.004334136 |
| Train/Policy_loss       | 0.036933973 |
| Train/Ratio             | 0.98081213  |
| Train/Return            | 0.86136544  |
| Train/V                 | 0.9072929   |
| Train/Value             | 0.9072929   |
| Train/control_penalty   | 0.624703    |
| Train/policy_loss       | 0.036933973 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.013       |
-----------------------------------------

 ---------------- Iteration 178 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 177          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00233      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30073383   |
| Train/Action_magnitu... | 0.7977628    |
| Train/Action_magnitude  | 0.63444614   |
| Train/Action_max        | 0.18146439   |
| Train/Action_std        | 0.29840717   |
| Train/Entropy           | 0.08672307   |
| Train/Entropy_Loss      | -8.67e-05    |
| Train/Entropy_loss      | -8.67e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.20878607  |
| Train/Loss              | -0.033494562 |
| Train/PolicyClip        | 0.004729303  |
| Train/Policy_loss       | -0.039765354 |
| Train/Ratio             | 0.99662507   |
| Train/Return            | 1.003285     |
| Train/V                 | 0.97092634   |
| Train/Value             | 0.97092634   |
| Train/control_penalty   | 0.63575166   |
| Train/policy_loss       | -0.039765354 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 179 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 178          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.31010076   |
| Train/Action_magnitu... | 0.8002445    |
| Train/Action_magnitude  | 0.634471     |
| Train/Action_max        | 0.13248256   |
| Train/Action_std        | 0.28104174   |
| Train/Entropy           | 0.031670604  |
| Train/Entropy_Loss      | -3.17e-05    |
| Train/Entropy_loss      | -3.17e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.1444305   |
| Train/Loss              | -0.11395279  |
| Train/PolicyClip        | 0.0041344017 |
| Train/Policy_loss       | -0.120196015 |
| Train/Ratio             | 1.0069292    |
| Train/Return            | 1.1090599    |
| Train/V                 | 0.99566996   |
| Train/Value             | 0.99566996   |
| Train/control_penalty   | 0.6274893    |
| Train/policy_loss       | -0.120196015 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 180 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 179           |
| Time/Actor_Time         | 0.0861        |
| Time/B_Format_Time      | 0.0706        |
| Time/B_Original_Form... | 0.0701        |
| Time/Buffer             | 0.00316       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.32242605    |
| Train/Action_magnitu... | 0.81880206    |
| Train/Action_magnitude  | 0.65243816    |
| Train/Action_max        | 0.13354708    |
| Train/Action_std        | 0.27096638    |
| Train/Entropy           | -0.0018671913 |
| Train/Entropy_Loss      | 1.87e-06      |
| Train/Entropy_loss      | 1.87e-06      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.15322243   |
| Train/Loss              | -0.0813332    |
| Train/PolicyClip        | 0.0054514054  |
| Train/Policy_loss       | -0.0877852    |
| Train/Ratio             | 1.0007908     |
| Train/Return            | 1.0028867     |
| Train/V                 | 0.91788965    |
| Train/Value             | 0.91788965    |
| Train/control_penalty   | 0.6450132     |
| Train/policy_loss       | -0.0877852    |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02225       |
-------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 181 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 180          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33247393   |
| Train/Action_magnitu... | 0.8055391    |
| Train/Action_magnitude  | 0.6425963    |
| Train/Action_max        | 0.11000432   |
| Train/Action_std        | 0.24906431   |
| Train/Entropy           | -0.07758144  |
| Train/Entropy_Loss      | 7.76e-05     |
| Train/Entropy_loss      | 7.76e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.080379665  |
| Train/Loss              | -0.100340925 |
| Train/PolicyClip        | 0.0018346698 |
| Train/Policy_loss       | -0.10689699  |
| Train/Ratio             | 1.0011836    |
| Train/Return            | 1.2044163    |
| Train/V                 | 1.1047696    |
| Train/Value             | 1.1047696    |
| Train/control_penalty   | 0.647848     |
| Train/policy_loss       | -0.10689699  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 182 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 181          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00323      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33572227   |
| Train/Action_magnitu... | 0.82098466   |
| Train/Action_magnitude  | 0.6536591    |
| Train/Action_max        | 0.1659551    |
| Train/Action_std        | 0.2532016    |
| Train/Entropy           | -0.062799744 |
| Train/Entropy_Loss      | 6.28e-05     |
| Train/Entropy_loss      | 6.28e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.078029804  |
| Train/Loss              | -0.15394102  |
| Train/PolicyClip        | 0.0047444967 |
| Train/Policy_loss       | -0.16048852  |
| Train/Ratio             | 1.0062696    |
| Train/Return            | 1.4263127    |
| Train/V                 | 1.2739568    |
| Train/Value             | 1.2739568    |
| Train/control_penalty   | 0.64847064   |
| Train/policy_loss       | -0.16048852  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03775      |
------------------------------------------

 ---------------- Iteration 183 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 182          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0734       |
| Time/Buffer             | 0.00248      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3077075    |
| Train/Action_magnitu... | 0.77475953   |
| Train/Action_magnitude  | 0.61515003   |
| Train/Action_max        | 0.1860174    |
| Train/Action_std        | 0.26037976   |
| Train/Entropy           | -0.04587967  |
| Train/Entropy_Loss      | 4.59e-05     |
| Train/Entropy_loss      | 4.59e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.09904613   |
| Train/Loss              | -0.05785619  |
| Train/PolicyClip        | 0.005878444  |
| Train/Policy_loss       | -0.064034894 |
| Train/Ratio             | 1.0018408    |
| Train/Return            | 1.3071483    |
| Train/V                 | 1.2508056    |
| Train/Value             | 1.2508056    |
| Train/control_penalty   | 0.6132823    |
| Train/policy_loss       | -0.064034894 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 184 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 183          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00309      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2998241    |
| Train/Action_magnitu... | 0.7432319    |
| Train/Action_magnitude  | 0.59011996   |
| Train/Action_max        | 0.18202886   |
| Train/Action_std        | 0.24044418   |
| Train/Entropy           | -0.10599177  |
| Train/Entropy_Loss      | 0.000106     |
| Train/Entropy_loss      | 0.000106     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.19115938   |
| Train/Loss              | -0.07359375  |
| Train/PolicyClip        | 0.0045415903 |
| Train/Policy_loss       | -0.079567276 |
| Train/Ratio             | 1.0051583    |
| Train/Return            | 1.3985299    |
| Train/V                 | 1.3205701    |
| Train/Value             | 1.3205701    |
| Train/control_penalty   | 0.58675313   |
| Train/policy_loss       | -0.079567276 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0345       |
------------------------------------------

 ---------------- Iteration 185 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 184          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00298      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2544809    |
| Train/Action_magnitu... | 0.67872334   |
| Train/Action_magnitude  | 0.5390278    |
| Train/Action_max        | 0.16328204   |
| Train/Action_std        | 0.23870704   |
| Train/Entropy           | -0.11594021  |
| Train/Entropy_Loss      | 0.000116     |
| Train/Entropy_loss      | 0.000116     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.18669169   |
| Train/Loss              | -0.10177722  |
| Train/PolicyClip        | 0.0077409833 |
| Train/Policy_loss       | -0.10722701  |
| Train/Ratio             | 1.0026553    |
| Train/Return            | 1.3969755    |
| Train/V                 | 1.2947291    |
| Train/Value             | 1.2947291    |
| Train/control_penalty   | 0.5333859    |
| Train/policy_loss       | -0.10722701  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03225      |
------------------------------------------

 ---------------- Iteration 186 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 185          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28204015   |
| Train/Action_magnitu... | 0.6966247    |
| Train/Action_magnitude  | 0.55361277   |
| Train/Action_max        | 0.17089526   |
| Train/Action_std        | 0.24631916   |
| Train/Entropy           | -0.080516934 |
| Train/Entropy_Loss      | 8.05e-05     |
| Train/Entropy_loss      | 8.05e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.1213033    |
| Train/Loss              | -0.009788794 |
| Train/PolicyClip        | 0.007823286  |
| Train/Policy_loss       | -0.015498438 |
| Train/Ratio             | 0.9982423    |
| Train/Return            | 1.2767583    |
| Train/V                 | 1.264179     |
| Train/Value             | 1.264179     |
| Train/control_penalty   | 0.5629127    |
| Train/policy_loss       | -0.015498438 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02575      |
------------------------------------------

 ---------------- Iteration 187 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 186          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2819272    |
| Train/Action_magnitu... | 0.72131133   |
| Train/Action_magnitude  | 0.5735483    |
| Train/Action_max        | 0.16718467   |
| Train/Action_std        | 0.25873345   |
| Train/Entropy           | -0.031286296 |
| Train/Entropy_Loss      | 3.13e-05     |
| Train/Entropy_loss      | 3.13e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.07026898   |
| Train/Loss              | 0.05839339   |
| Train/PolicyClip        | 0.0029552418 |
| Train/Policy_loss       | 0.052595947  |
| Train/Ratio             | 0.995068     |
| Train/Return            | 1.2058973    |
| Train/V                 | 1.2640268    |
| Train/Value             | 1.2640268    |
| Train/control_penalty   | 0.5766158    |
| Train/policy_loss       | 0.052595947  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 188 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 187          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0736       |
| Time/Buffer             | 0.00228      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25223726   |
| Train/Action_magnitu... | 0.6709756    |
| Train/Action_magnitude  | 0.5332424    |
| Train/Action_max        | 0.1429991    |
| Train/Action_std        | 0.2535676    |
| Train/Entropy           | -0.04881087  |
| Train/Entropy_Loss      | 4.88e-05     |
| Train/Entropy_loss      | 4.88e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.08693846   |
| Train/Loss              | -0.016006002 |
| Train/PolicyClip        | 0.0064240075 |
| Train/Policy_loss       | -0.0213909   |
| Train/Ratio             | 1.0069185    |
| Train/Return            | 1.1459655    |
| Train/V                 | 1.1323001    |
| Train/Value             | 1.1323001    |
| Train/control_penalty   | 0.5336087    |
| Train/policy_loss       | -0.0213909   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 189 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 188          |
| Time/Actor_Time         | 0.0872       |
| Time/B_Format_Time      | 0.0735       |
| Time/B_Original_Form... | 0.0725       |
| Time/Buffer             | 0.00319      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25911447   |
| Train/Action_magnitu... | 0.6801267    |
| Train/Action_magnitude  | 0.5412736    |
| Train/Action_max        | 0.17643078   |
| Train/Action_std        | 0.24719362   |
| Train/Entropy           | -0.064058065 |
| Train/Entropy_Loss      | 6.41e-05     |
| Train/Entropy_loss      | 6.41e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.100578874  |
| Train/Loss              | -0.014067429 |
| Train/PolicyClip        | 0.006712955  |
| Train/Policy_loss       | -0.019490207 |
| Train/Ratio             | 0.99855256   |
| Train/Return            | 1.2711687    |
| Train/V                 | 1.2597405    |
| Train/Value             | 1.2597405    |
| Train/control_penalty   | 0.5358719    |
| Train/policy_loss       | -0.019490207 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 190 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 189         |
| Time/Actor_Time         | 0.0869      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0724      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2735084   |
| Train/Action_magnitu... | 0.70289385  |
| Train/Action_magnitude  | 0.5556548   |
| Train/Action_max        | 0.17559427  |
| Train/Action_std        | 0.2591996   |
| Train/Entropy           | -0.01698257 |
| Train/Entropy_Loss      | 1.7e-05     |
| Train/Entropy_loss      | 1.7e-05     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.02661475  |
| Train/Loss              | 0.030576462 |
| Train/PolicyClip        | 0.010205783 |
| Train/Policy_loss       | 0.024910038 |
| Train/Ratio             | 0.9866843   |
| Train/Return            | 1.221461    |
| Train/V                 | 1.251294    |
| Train/Value             | 1.251294    |
| Train/control_penalty   | 0.56494415  |
| Train/policy_loss       | 0.024910038 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02        |
-----------------------------------------

 ---------------- Iteration 191 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 190          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00249      |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.25394082   |
| Train/Action_magnitu... | 0.6737355    |
| Train/Action_magnitude  | 0.5328786    |
| Train/Action_max        | 0.15980503   |
| Train/Action_std        | 0.25817275   |
| Train/Entropy           | -0.025822842 |
| Train/Entropy_Loss      | 2.58e-05     |
| Train/Entropy_loss      | 2.58e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.050004397  |
| Train/Loss              | 0.06505327   |
| Train/PolicyClip        | 0.0030738334 |
| Train/Policy_loss       | 0.059597787  |
| Train/Ratio             | 0.98792523   |
| Train/Return            | 1.2218206    |
| Train/V                 | 1.2933596    |
| Train/Value             | 1.2933596    |
| Train/control_penalty   | 0.5429659    |
| Train/policy_loss       | 0.059597787  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 192 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 191          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00276      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2798719    |
| Train/Action_magnitu... | 0.7160634    |
| Train/Action_magnitude  | 0.5664789    |
| Train/Action_max        | 0.14790681   |
| Train/Action_std        | 0.26214942   |
| Train/Entropy           | -0.008670818 |
| Train/Entropy_Loss      | 8.67e-06     |
| Train/Entropy_loss      | 8.67e-06     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.020732002 |
| Train/Loss              | 0.009756306  |
| Train/PolicyClip        | 0.009021179  |
| Train/Policy_loss       | 0.0039747506 |
| Train/Ratio             | 1.000492     |
| Train/Return            | 1.336644     |
| Train/V                 | 1.3425924    |
| Train/Value             | 1.3425924    |
| Train/control_penalty   | 0.57728857   |
| Train/policy_loss       | 0.0039747506 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 193 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 192          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00513      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27933982   |
| Train/Action_magnitu... | 0.7385863    |
| Train/Action_magnitude  | 0.5850517    |
| Train/Action_max        | 0.17805423   |
| Train/Action_std        | 0.26227906   |
| Train/Entropy           | -0.004564449 |
| Train/Entropy_Loss      | 4.56e-06     |
| Train/Entropy_loss      | 4.56e-06     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.032562073 |
| Train/Loss              | 0.040182173  |
| Train/PolicyClip        | 0.0049680015 |
| Train/Policy_loss       | 0.034340356  |
| Train/Ratio             | 0.9921212    |
| Train/Return            | 1.2410834    |
| Train/V                 | 1.2855667    |
| Train/Value             | 1.2855667    |
| Train/control_penalty   | 0.58372533   |
| Train/policy_loss       | 0.034340356  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 194 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 193          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00312      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29589096   |
| Train/Action_magnitu... | 0.75325453   |
| Train/Action_magnitude  | 0.5970661    |
| Train/Action_max        | 0.16021186   |
| Train/Action_std        | 0.26581705   |
| Train/Entropy           | 0.015929641  |
| Train/Entropy_Loss      | -1.59e-05    |
| Train/Entropy_loss      | -1.59e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.031503517 |
| Train/Loss              | 0.020061228  |
| Train/PolicyClip        | 0.004676911  |
| Train/Policy_loss       | 0.014056134  |
| Train/Ratio             | 0.9914824    |
| Train/Return            | 1.1294987    |
| Train/V                 | 1.154408     |
| Train/Value             | 1.154408     |
| Train/control_penalty   | 0.60210246   |
| Train/policy_loss       | 0.014056134  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 195 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 194          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0729       |
| Time/Buffer             | 0.00322      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29672334   |
| Train/Action_magnitu... | 0.7552575    |
| Train/Action_magnitude  | 0.5965334    |
| Train/Action_max        | 0.15649147   |
| Train/Action_std        | 0.26542994   |
| Train/Entropy           | 0.013192337  |
| Train/Entropy_Loss      | -1.32e-05    |
| Train/Entropy_loss      | -1.32e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.07194974  |
| Train/Loss              | 0.01752292   |
| Train/PolicyClip        | 0.0067260833 |
| Train/Policy_loss       | 0.011518629  |
| Train/Ratio             | 1.0004233    |
| Train/Return            | 1.0694882    |
| Train/V                 | 1.0900866    |
| Train/Value             | 1.0900866    |
| Train/control_penalty   | 0.60174835   |
| Train/policy_loss       | 0.011518629  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 196 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 195           |
| Time/Actor_Time         | 0.0845        |
| Time/B_Format_Time      | 0.0702        |
| Time/B_Original_Form... | 0.0694        |
| Time/Buffer             | 0.00287       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31227708    |
| Train/Action_magnitu... | 0.7888615     |
| Train/Action_magnitude  | 0.6256389     |
| Train/Action_max        | 0.16141067    |
| Train/Action_std        | 0.26517934    |
| Train/Entropy           | -0.0044938964 |
| Train/Entropy_Loss      | 4.49e-06      |
| Train/Entropy_loss      | 4.49e-06      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.029082347  |
| Train/Loss              | -0.03300061   |
| Train/PolicyClip        | 0.00449171    |
| Train/Policy_loss       | -0.03918837   |
| Train/Ratio             | 0.9923851     |
| Train/Return            | 1.1974949     |
| Train/V                 | 1.1610662     |
| Train/Value             | 1.1610662     |
| Train/control_penalty   | 0.61832654    |
| Train/policy_loss       | -0.03918837   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02275       |
-------------------------------------------

 ---------------- Iteration 197 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 196          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0694       |
| Time/Buffer             | 0.00286      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30414152   |
| Train/Action_magnitu... | 0.7773797    |
| Train/Action_magnitude  | 0.61546475   |
| Train/Action_max        | 0.15168904   |
| Train/Action_std        | 0.273169     |
| Train/Entropy           | 0.032335907  |
| Train/Entropy_Loss      | -3.23e-05    |
| Train/Entropy_loss      | -3.23e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.058889795 |
| Train/Loss              | 0.0058499863 |
| Train/PolicyClip        | 0.005622372  |
| Train/Policy_loss       | -0.000206357 |
| Train/Ratio             | 0.9860302    |
| Train/Return            | 1.096343     |
| Train/V                 | 1.1061741    |
| Train/Value             | 1.1061741    |
| Train/control_penalty   | 0.60886794   |
| Train/policy_loss       | -0.000206357 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01925      |
------------------------------------------

 ---------------- Iteration 198 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 197          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00342      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31617284   |
| Train/Action_magnitu... | 0.7894967    |
| Train/Action_magnitude  | 0.6215356    |
| Train/Action_max        | 0.17042106   |
| Train/Action_std        | 0.2797831    |
| Train/Entropy           | 0.04426626   |
| Train/Entropy_Loss      | -4.43e-05    |
| Train/Entropy_loss      | -4.43e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.12749451  |
| Train/Loss              | -0.031169398 |
| Train/PolicyClip        | 0.006969286  |
| Train/Policy_loss       | -0.037448436 |
| Train/Ratio             | 0.9908439    |
| Train/Return            | 1.1944116    |
| Train/V                 | 1.1621561    |
| Train/Value             | 1.1621561    |
| Train/control_penalty   | 0.63233054   |
| Train/policy_loss       | -0.037448436 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02675      |
------------------------------------------

 ---------------- Iteration 199 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 198         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0694      |
| Time/Buffer             | 0.00337     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3228059   |
| Train/Action_magnitu... | 0.8164557   |
| Train/Action_magnitude  | 0.6446034   |
| Train/Action_max        | 0.17417221  |
| Train/Action_std        | 0.2723238   |
| Train/Entropy           | 0.009113727 |
| Train/Entropy_Loss      | -9.11e-06   |
| Train/Entropy_loss      | -9.11e-06   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.13361563 |
| Train/Loss              | -0.07481044 |
| Train/PolicyClip        | 0.005812505 |
| Train/Policy_loss       | -0.08108489 |
| Train/Ratio             | 1.0050749   |
| Train/Return            | 1.190424    |
| Train/V                 | 1.1190501   |
| Train/Value             | 1.1190501   |
| Train/control_penalty   | 0.6283569   |
| Train/policy_loss       | -0.08108489 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 200 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 199         |
| Time/Actor_Time         | 0.0861      |
| Time/B_Format_Time      | 0.0708      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.0142      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32489654  |
| Train/Action_magnitu... | 0.8169616   |
| Train/Action_magnitude  | 0.6450824   |
| Train/Action_max        | 0.2013537   |
| Train/Action_std        | 0.2821851   |
| Train/Entropy           | 0.051149894 |
| Train/Entropy_Loss      | -5.11e-05   |
| Train/Entropy_loss      | -5.11e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.069545   |
| Train/Loss              | 0.028884057 |
| Train/PolicyClip        | 0.003336169 |
| Train/Policy_loss       | 0.022488728 |
| Train/Ratio             | 0.9946348   |
| Train/Return            | 0.9740773   |
| Train/V                 | 1.0039766   |
| Train/Value             | 1.0039766   |
| Train/control_penalty   | 0.64464784  |
| Train/policy_loss       | 0.022488728 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01725     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 201 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 200          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.07         |
| Time/B_Original_Form... | 0.0694       |
| Time/Buffer             | 0.00493      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31172204   |
| Train/Action_magnitu... | 0.7834001    |
| Train/Action_magnitude  | 0.62112683   |
| Train/Action_max        | 0.175309     |
| Train/Action_std        | 0.2791555    |
| Train/Entropy           | 0.051575746  |
| Train/Entropy_Loss      | -5.16e-05    |
| Train/Entropy_loss      | -5.16e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.1534468   |
| Train/Loss              | 0.015653916  |
| Train/PolicyClip        | 0.004272628  |
| Train/Policy_loss       | 0.0094170505 |
| Train/Ratio             | 0.9968861    |
| Train/Return            | 1.0178475    |
| Train/V                 | 1.0345377    |
| Train/Value             | 1.0345377    |
| Train/control_penalty   | 0.62884414   |
| Train/policy_loss       | 0.0094170505 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 202 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 201         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0701      |
| Time/B_Original_Form... | 0.0721      |
| Time/Buffer             | 0.00296     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29452822  |
| Train/Action_magnitu... | 0.7541189   |
| Train/Action_magnitude  | 0.5971725   |
| Train/Action_max        | 0.20613235  |
| Train/Action_std        | 0.2668998   |
| Train/Entropy           | 0.010863087 |
| Train/Entropy_Loss      | -1.09e-05   |
| Train/Entropy_loss      | -1.09e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.06402667 |
| Train/Loss              | -0.10655933 |
| Train/PolicyClip        | 0.008931942 |
| Train/Policy_loss       | -0.11252652 |
| Train/Ratio             | 1.008011    |
| Train/Return            | 1.1948314   |
| Train/V                 | 1.0902281   |
| Train/Value             | 1.0902281   |
| Train/control_penalty   | 0.59780526  |
| Train/policy_loss       | -0.11252652 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 203 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 202          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00256      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30720124   |
| Train/Action_magnitu... | 0.7612547    |
| Train/Action_magnitude  | 0.60541385   |
| Train/Action_max        | 0.17087656   |
| Train/Action_std        | 0.26690218   |
| Train/Entropy           | 0.006883621  |
| Train/Entropy_Loss      | -6.88e-06    |
| Train/Entropy_loss      | -6.88e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.01895851  |
| Train/Loss              | 0.051910993  |
| Train/PolicyClip        | 0.0055007846 |
| Train/Policy_loss       | 0.04582501   |
| Train/Ratio             | 0.98748004   |
| Train/Return            | 1.0329834    |
| Train/V                 | 1.0886555    |
| Train/Value             | 1.0886555    |
| Train/control_penalty   | 0.60928696   |
| Train/policy_loss       | 0.04582501   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0145       |
------------------------------------------

 ---------------- Iteration 204 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 203         |
| Time/Actor_Time         | 0.0869      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00254     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30200434  |
| Train/Action_magnitu... | 0.77092934  |
| Train/Action_magnitude  | 0.61234754  |
| Train/Action_max        | 0.13431099  |
| Train/Action_std        | 0.26878566  |
| Train/Entropy           | 0.023552716 |
| Train/Entropy_Loss      | -2.36e-05   |
| Train/Entropy_loss      | -2.36e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.06619548 |
| Train/Loss              | 0.039342247 |
| Train/PolicyClip        | 0.006137573 |
| Train/Policy_loss       | 0.03327612  |
| Train/Ratio             | 0.9905723   |
| Train/Return            | 1.0712963   |
| Train/V                 | 1.1120042   |
| Train/Value             | 1.1120042   |
| Train/control_penalty   | 0.6089679   |
| Train/policy_loss       | 0.03327612  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 205 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 204          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0699       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.003        |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.28621724   |
| Train/Action_magnitu... | 0.75573564   |
| Train/Action_magnitude  | 0.6000941    |
| Train/Action_max        | 0.1914671    |
| Train/Action_std        | 0.2673378    |
| Train/Entropy           | 0.0059481277 |
| Train/Entropy_Loss      | -5.95e-06    |
| Train/Entropy_loss      | -5.95e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.09533149  |
| Train/Loss              | -0.06358474  |
| Train/PolicyClip        | 0.004006026  |
| Train/Policy_loss       | -0.069452204 |
| Train/Ratio             | 0.99649614   |
| Train/Return            | 1.1328174    |
| Train/V                 | 1.068489     |
| Train/Value             | 1.068489     |
| Train/control_penalty   | 0.5873409    |
| Train/policy_loss       | -0.069452204 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 206 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 205          |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00307      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29974434   |
| Train/Action_magnitu... | 0.74973476   |
| Train/Action_magnitude  | 0.59547967   |
| Train/Action_max        | 0.16554072   |
| Train/Action_std        | 0.26033485   |
| Train/Entropy           | -0.013579818 |
| Train/Entropy_Loss      | 1.36e-05     |
| Train/Entropy_loss      | 1.36e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.062297612 |
| Train/Loss              | 0.03325054   |
| Train/PolicyClip        | 0.0033624182 |
| Train/Policy_loss       | 0.027228279  |
| Train/Ratio             | 0.9906667    |
| Train/Return            | 0.96070695   |
| Train/V                 | 0.99547106   |
| Train/Value             | 0.99547106   |
| Train/control_penalty   | 0.60086805   |
| Train/policy_loss       | 0.027228279  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01425      |
------------------------------------------

 ---------------- Iteration 207 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 206          |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00203      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2900711    |
| Train/Action_magnitu... | 0.7615353    |
| Train/Action_magnitude  | 0.6044369    |
| Train/Action_max        | 0.15962033   |
| Train/Action_std        | 0.26899076   |
| Train/Entropy           | 0.0009581604 |
| Train/Entropy_Loss      | -9.58e-07    |
| Train/Entropy_loss      | -9.58e-07    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.04906658  |
| Train/Loss              | 0.06415368   |
| Train/PolicyClip        | 0.0032273107 |
| Train/Policy_loss       | 0.058199927  |
| Train/Ratio             | 0.9831354    |
| Train/Return            | 0.8865548    |
| Train/V                 | 0.9509084    |
| Train/Value             | 0.9509084    |
| Train/control_penalty   | 0.59547085   |
| Train/policy_loss       | 0.058199927  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01025      |
------------------------------------------

 ---------------- Iteration 208 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 207          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0726       |
| Time/Buffer             | 0.0058       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29236305   |
| Train/Action_magnitu... | 0.7668454    |
| Train/Action_magnitude  | 0.60724443   |
| Train/Action_max        | 0.20200557   |
| Train/Action_std        | 0.27078006   |
| Train/Entropy           | 0.007923183  |
| Train/Entropy_Loss      | -7.92e-06    |
| Train/Entropy_loss      | -7.92e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.066321336 |
| Train/Loss              | -0.036273986 |
| Train/PolicyClip        | 0.0060774265 |
| Train/Policy_loss       | -0.042229004 |
| Train/Ratio             | 1.002173     |
| Train/Return            | 1.0690827    |
| Train/V                 | 1.0351679    |
| Train/Value             | 1.0351679    |
| Train/control_penalty   | 0.59629434   |
| Train/policy_loss       | -0.042229004 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 209 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 208          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.0029       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26890638   |
| Train/Action_magnitu... | 0.70299566   |
| Train/Action_magnitude  | 0.5597085    |
| Train/Action_max        | 0.21581313   |
| Train/Action_std        | 0.26275134   |
| Train/Entropy           | -0.006464798 |
| Train/Entropy_Loss      | 6.46e-06     |
| Train/Entropy_loss      | 6.46e-06     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.010134606  |
| Train/Loss              | -0.02930857  |
| Train/PolicyClip        | 0.0063101416 |
| Train/Policy_loss       | -0.034817226 |
| Train/Ratio             | 0.99845016   |
| Train/Return            | 1.0463754    |
| Train/V                 | 1.0178841    |
| Train/Value             | 1.0178841    |
| Train/control_penalty   | 0.55021924   |
| Train/policy_loss       | -0.034817226 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01875      |
------------------------------------------

 ---------------- Iteration 210 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 209          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00236      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28668773   |
| Train/Action_magnitu... | 0.7236868    |
| Train/Action_magnitude  | 0.57432103   |
| Train/Action_max        | 0.19369636   |
| Train/Action_std        | 0.26645043   |
| Train/Entropy           | 0.009791245  |
| Train/Entropy_Loss      | -9.79e-06    |
| Train/Entropy_loss      | -9.79e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.05921657  |
| Train/Loss              | 0.026496448  |
| Train/PolicyClip        | 0.0059201736 |
| Train/Policy_loss       | 0.020672165  |
| Train/Ratio             | 0.9822113    |
| Train/Return            | 1.022159     |
| Train/V                 | 1.0475837    |
| Train/Value             | 1.0475837    |
| Train/control_penalty   | 0.58340746   |
| Train/policy_loss       | 0.020672165  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.016        |
------------------------------------------

 ---------------- Iteration 211 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 210          |
| Time/Actor_Time         | 0.088        |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2827146    |
| Train/Action_magnitu... | 0.71795666   |
| Train/Action_magnitude  | 0.5719869    |
| Train/Action_max        | 0.22586958   |
| Train/Action_std        | 0.25888994   |
| Train/Entropy           | -0.023142979 |
| Train/Entropy_Loss      | 2.31e-05     |
| Train/Entropy_loss      | 2.31e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.036365777  |
| Train/Loss              | -0.061423536 |
| Train/PolicyClip        | 0.007109553  |
| Train/Policy_loss       | -0.06717109  |
| Train/Ratio             | 1.0058795    |
| Train/Return            | 1.2288392    |
| Train/V                 | 1.1696821    |
| Train/Value             | 1.1696821    |
| Train/control_penalty   | 0.5724412    |
| Train/policy_loss       | -0.06717109  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 212 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 211          |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00264      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28094634   |
| Train/Action_magnitu... | 0.7397969    |
| Train/Action_magnitude  | 0.5860275    |
| Train/Action_max        | 0.21156146   |
| Train/Action_std        | 0.27427146   |
| Train/Entropy           | 0.03511662   |
| Train/Entropy_Loss      | -3.51e-05    |
| Train/Entropy_loss      | -3.51e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.0832642   |
| Train/Loss              | -0.032773275 |
| Train/PolicyClip        | 0.0045692753 |
| Train/Policy_loss       | -0.038543213 |
| Train/Ratio             | 0.99889684   |
| Train/Return            | 1.3179463    |
| Train/V                 | 1.2859432    |
| Train/Value             | 1.2859432    |
| Train/control_penalty   | 0.58050555   |
| Train/policy_loss       | -0.038543213 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 213 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 212          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0703       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00231      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26457283   |
| Train/Action_magnitu... | 0.6870165    |
| Train/Action_magnitude  | 0.54515016   |
| Train/Action_max        | 0.20709991   |
| Train/Action_std        | 0.26052454   |
| Train/Entropy           | -0.009055279 |
| Train/Entropy_Loss      | 9.06e-06     |
| Train/Entropy_loss      | 9.06e-06     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.045987852 |
| Train/Loss              | -0.06423932  |
| Train/PolicyClip        | 0.005104411  |
| Train/Policy_loss       | -0.06968866  |
| Train/Ratio             | 0.9978619    |
| Train/Return            | 1.3050593    |
| Train/V                 | 1.2413863    |
| Train/Value             | 1.2413863    |
| Train/control_penalty   | 0.54402864   |
| Train/policy_loss       | -0.06968866  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 214 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 213          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00285      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2836893    |
| Train/Action_magnitu... | 0.7410788    |
| Train/Action_magnitude  | 0.58555096   |
| Train/Action_max        | 0.19240636   |
| Train/Action_std        | 0.26749572   |
| Train/Entropy           | 0.020834293  |
| Train/Entropy_Loss      | -2.08e-05    |
| Train/Entropy_loss      | -2.08e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.07771374  |
| Train/Loss              | -0.013137502 |
| Train/PolicyClip        | 0.0045484994 |
| Train/Policy_loss       | -0.018847939 |
| Train/Ratio             | 0.9977827    |
| Train/Return            | 1.3995533    |
| Train/V                 | 1.3905048    |
| Train/Value             | 1.3905048    |
| Train/control_penalty   | 0.5731271    |
| Train/policy_loss       | -0.018847939 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02825      |
------------------------------------------

 ---------------- Iteration 215 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 214          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.27143764   |
| Train/Action_magnitu... | 0.69975024   |
| Train/Action_magnitude  | 0.5518841    |
| Train/Action_max        | 0.20485537   |
| Train/Action_std        | 0.25416496   |
| Train/Entropy           | -0.025667315 |
| Train/Entropy_Loss      | 2.57e-05     |
| Train/Entropy_loss      | 2.57e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.0021814422 |
| Train/Loss              | 0.043244593  |
| Train/PolicyClip        | 0.0067646075 |
| Train/Policy_loss       | 0.03773155   |
| Train/Ratio             | 0.9928969    |
| Train/Return            | 1.3132336    |
| Train/V                 | 1.3575479    |
| Train/Value             | 1.3575479    |
| Train/control_penalty   | 0.5487377    |
| Train/policy_loss       | 0.03773155   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 216 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 215          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00246      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28862172   |
| Train/Action_magnitu... | 0.7254667    |
| Train/Action_magnitude  | 0.5722321    |
| Train/Action_max        | 0.18603256   |
| Train/Action_std        | 0.25818756   |
| Train/Entropy           | 0.0048153093 |
| Train/Entropy_Loss      | -4.82e-06    |
| Train/Entropy_loss      | -4.82e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.02744163  |
| Train/Loss              | 0.012261715  |
| Train/PolicyClip        | 0.0059973914 |
| Train/Policy_loss       | 0.006562907  |
| Train/Ratio             | 0.99335307   |
| Train/Return            | 1.086637     |
| Train/V                 | 1.0986413    |
| Train/Value             | 1.0986413    |
| Train/control_penalty   | 0.5703623    |
| Train/policy_loss       | 0.006562907  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.019        |
------------------------------------------

 ---------------- Iteration 217 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 216          |
| Time/Actor_Time         | 0.092        |
| Time/B_Format_Time      | 0.078        |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00291      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30559832   |
| Train/Action_magnitu... | 0.7616519    |
| Train/Action_magnitude  | 0.60027176   |
| Train/Action_max        | 0.21237366   |
| Train/Action_std        | 0.25588012   |
| Train/Entropy           | -0.00775391  |
| Train/Entropy_Loss      | 7.75e-06     |
| Train/Entropy_loss      | 7.75e-06     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.042442087 |
| Train/Loss              | 0.023134882  |
| Train/PolicyClip        | 0.0036153642 |
| Train/Policy_loss       | 0.017176138  |
| Train/Ratio             | 0.9932743    |
| Train/Return            | 1.058636     |
| Train/V                 | 1.083597     |
| Train/Value             | 1.083597     |
| Train/control_penalty   | 0.595099     |
| Train/policy_loss       | 0.017176138  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 218 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 217           |
| Time/Actor_Time         | 0.0856        |
| Time/B_Format_Time      | 0.0737        |
| Time/B_Original_Form... | 0.0705        |
| Time/Buffer             | 0.00281       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.31409305    |
| Train/Action_magnitu... | 0.7557016     |
| Train/Action_magnitude  | 0.59975165    |
| Train/Action_max        | 0.19842611    |
| Train/Action_std        | 0.25666967    |
| Train/Entropy           | -0.0054949396 |
| Train/Entropy_Loss      | 5.49e-06      |
| Train/Entropy_loss      | 5.49e-06      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.014597156  |
| Train/Loss              | 0.03681855    |
| Train/PolicyClip        | 0.003659373   |
| Train/Policy_loss       | 0.030812087   |
| Train/Ratio             | 0.98761433    |
| Train/Return            | 0.97830856    |
| Train/V                 | 1.0159944     |
| Train/Value             | 1.0159944     |
| Train/control_penalty   | 0.60009694    |
| Train/policy_loss       | 0.030812087   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02          |
-------------------------------------------

 ---------------- Iteration 219 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 218          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.28742924   |
| Train/Action_magnitu... | 0.7155455    |
| Train/Action_magnitude  | 0.56539494   |
| Train/Action_max        | 0.18811962   |
| Train/Action_std        | 0.24666238   |
| Train/Entropy           | -0.05403592  |
| Train/Entropy_Loss      | 5.4e-05      |
| Train/Entropy_loss      | 5.4e-05      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.06791559   |
| Train/Loss              | -0.02009581  |
| Train/PolicyClip        | 0.0045024473 |
| Train/Policy_loss       | -0.02583676  |
| Train/Ratio             | 1.0054759    |
| Train/Return            | 0.9347798    |
| Train/V                 | 0.91459846   |
| Train/Value             | 0.91459846   |
| Train/control_penalty   | 0.56869155   |
| Train/policy_loss       | -0.02583676  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 220 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 219          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0706       |
| Time/B_Original_Form... | 0.0722       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25907314   |
| Train/Action_magnitu... | 0.68743056   |
| Train/Action_magnitude  | 0.5445259    |
| Train/Action_max        | 0.16552988   |
| Train/Action_std        | 0.24851969   |
| Train/Entropy           | -0.042067874 |
| Train/Entropy_Loss      | 4.21e-05     |
| Train/Entropy_loss      | 4.21e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.05783807   |
| Train/Loss              | 0.013635975  |
| Train/PolicyClip        | 0.007234562  |
| Train/Policy_loss       | 0.008248808  |
| Train/Ratio             | 0.99006176   |
| Train/Return            | 0.76070005   |
| Train/V                 | 0.7762338    |
| Train/Value             | 0.7762338    |
| Train/control_penalty   | 0.53450996   |
| Train/policy_loss       | 0.008248808  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01375      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 221 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 220          |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00321      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27430642   |
| Train/Action_magnitu... | 0.70140636   |
| Train/Action_magnitude  | 0.55439067   |
| Train/Action_max        | 0.17554466   |
| Train/Action_std        | 0.25553772   |
| Train/Entropy           | -0.010684912 |
| Train/Entropy_Loss      | 1.07e-05     |
| Train/Entropy_loss      | 1.07e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.0382676    |
| Train/Loss              | 0.06398671   |
| Train/PolicyClip        | 0.0022426713 |
| Train/Policy_loss       | 0.058355205  |
| Train/Ratio             | 0.9832327    |
| Train/Return            | 0.61266565   |
| Train/V                 | 0.6776849    |
| Train/Value             | 0.6776849    |
| Train/control_penalty   | 0.5620824    |
| Train/policy_loss       | 0.058355205  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.00775      |
------------------------------------------

 ---------------- Iteration 222 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 221          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0742       |
| Time/B_Original_Form... | 0.0734       |
| Time/Buffer             | 0.00377      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27505037   |
| Train/Action_magnitu... | 0.7094131    |
| Train/Action_magnitude  | 0.5589619    |
| Train/Action_max        | 0.1821459    |
| Train/Action_std        | 0.25473216   |
| Train/Entropy           | -0.021282723 |
| Train/Entropy_Loss      | 2.13e-05     |
| Train/Entropy_loss      | 2.13e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.010153858 |
| Train/Loss              | -0.052151274 |
| Train/PolicyClip        | 0.012260701  |
| Train/Policy_loss       | -0.05776131  |
| Train/Ratio             | 1.0106033    |
| Train/Return            | 0.80004525   |
| Train/V                 | 0.7370043    |
| Train/Value             | 0.7370043    |
| Train/control_penalty   | 0.5588755    |
| Train/policy_loss       | -0.05776131  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 223 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 222          |
| Time/Actor_Time         | 0.125        |
| Time/B_Format_Time      | 0.11         |
| Time/B_Original_Form... | 0.0966       |
| Time/Buffer             | 0.00577      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26813957   |
| Train/Action_magnitu... | 0.700292     |
| Train/Action_magnitude  | 0.5529404    |
| Train/Action_max        | 0.17903891   |
| Train/Action_std        | 0.2580585    |
| Train/Entropy           | -0.005257177 |
| Train/Entropy_Loss      | 5.26e-06     |
| Train/Entropy_loss      | 5.26e-06     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.0541102   |
| Train/Loss              | -0.05167895  |
| Train/PolicyClip        | 0.003638602  |
| Train/Policy_loss       | -0.057199188 |
| Train/Ratio             | 0.998422     |
| Train/Return            | 0.9394453    |
| Train/V                 | 0.89028233   |
| Train/Value             | 0.89028233   |
| Train/control_penalty   | 0.5514979    |
| Train/policy_loss       | -0.057199188 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 224 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 223          |
| Time/Actor_Time         | 0.0901       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0734       |
| Time/Buffer             | 0.00617      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26630417   |
| Train/Action_magnitu... | 0.6906472    |
| Train/Action_magnitude  | 0.54691815   |
| Train/Action_max        | 0.17103156   |
| Train/Action_std        | 0.24038069   |
| Train/Entropy           | -0.08327272  |
| Train/Entropy_Loss      | 8.33e-05     |
| Train/Entropy_loss      | 8.33e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.090271406  |
| Train/Loss              | -0.08107803  |
| Train/PolicyClip        | 0.0064668306 |
| Train/Policy_loss       | -0.08663197  |
| Train/Ratio             | 1.0009544    |
| Train/Return            | 0.9373623    |
| Train/V                 | 0.8570361    |
| Train/Value             | 0.8570361    |
| Train/control_penalty   | 0.54706645   |
| Train/policy_loss       | -0.08663197  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 225 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 224          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00332      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2583055    |
| Train/Action_magnitu... | 0.6829042    |
| Train/Action_magnitude  | 0.5410767    |
| Train/Action_max        | 0.16038603   |
| Train/Action_std        | 0.24203718   |
| Train/Entropy           | -0.069349654 |
| Train/Entropy_Loss      | 6.93e-05     |
| Train/Entropy_loss      | 6.93e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.0875163    |
| Train/Loss              | -0.05963636  |
| Train/PolicyClip        | 0.0053007053 |
| Train/Policy_loss       | -0.0650716   |
| Train/Ratio             | 0.9937484    |
| Train/Return            | 1.2405782    |
| Train/V                 | 1.1811265    |
| Train/Value             | 1.1811265    |
| Train/control_penalty   | 0.53658885   |
| Train/policy_loss       | -0.0650716   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 226 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 225          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00269      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2584673    |
| Train/Action_magnitu... | 0.69311386   |
| Train/Action_magnitude  | 0.5493374    |
| Train/Action_max        | 0.15539902   |
| Train/Action_std        | 0.2520215    |
| Train/Entropy           | -0.040330306 |
| Train/Entropy_Loss      | 4.03e-05     |
| Train/Entropy_loss      | 4.03e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.029238027  |
| Train/Loss              | 0.024423892  |
| Train/PolicyClip        | 0.005080871  |
| Train/Policy_loss       | 0.018899621  |
| Train/Ratio             | 0.9910239    |
| Train/Return            | 1.062307     |
| Train/V                 | 1.0894146    |
| Train/Value             | 1.0894146    |
| Train/control_penalty   | 0.5483941    |
| Train/policy_loss       | 0.018899621  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------

 ---------------- Iteration 227 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 226          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0754       |
| Time/Buffer             | 0.00311      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26875725   |
| Train/Action_magnitu... | 0.6856497    |
| Train/Action_magnitude  | 0.544741     |
| Train/Action_max        | 0.13592932   |
| Train/Action_std        | 0.22776057   |
| Train/Entropy           | -0.13131098  |
| Train/Entropy_Loss      | 0.000131     |
| Train/Entropy_loss      | 0.000131     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.15262371   |
| Train/Loss              | -0.03896058  |
| Train/PolicyClip        | 0.004597787  |
| Train/Policy_loss       | -0.044498112 |
| Train/Ratio             | 1.0013672    |
| Train/Return            | 1.0379745    |
| Train/V                 | 1.0007212    |
| Train/Value             | 1.0007212    |
| Train/control_penalty   | 0.5406221    |
| Train/policy_loss       | -0.044498112 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 228 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 227          |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.0036       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26197523   |
| Train/Action_magnitu... | 0.6638062    |
| Train/Action_magnitude  | 0.5273269    |
| Train/Action_max        | 0.16892703   |
| Train/Action_std        | 0.23459265   |
| Train/Entropy           | -0.10657833  |
| Train/Entropy_Loss      | 0.000107     |
| Train/Entropy_loss      | 0.000107     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.13912946   |
| Train/Loss              | -0.007945403 |
| Train/PolicyClip        | 0.006402848  |
| Train/Policy_loss       | -0.013469291 |
| Train/Ratio             | 0.9946624    |
| Train/Return            | 1.1258789    |
| Train/V                 | 1.1142278    |
| Train/Value             | 1.1142278    |
| Train/control_penalty   | 0.5417308    |
| Train/policy_loss       | -0.013469291 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 229 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 228          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00324      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.26004484   |
| Train/Action_magnitu... | 0.6663263    |
| Train/Action_magnitude  | 0.53037614   |
| Train/Action_max        | 0.15169896   |
| Train/Action_std        | 0.22566678   |
| Train/Entropy           | -0.15334576  |
| Train/Entropy_Loss      | 0.000153     |
| Train/Entropy_loss      | 0.000153     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.23337443   |
| Train/Loss              | -0.055156432 |
| Train/PolicyClip        | 0.0065320116 |
| Train/Policy_loss       | -0.06052776  |
| Train/Ratio             | 1.0001346    |
| Train/Return            | 1.0081949    |
| Train/V                 | 0.9509374    |
| Train/Value             | 0.9509374    |
| Train/control_penalty   | 0.5217983    |
| Train/policy_loss       | -0.06052776  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 230 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 229          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00307      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24367501   |
| Train/Action_magnitu... | 0.64191115   |
| Train/Action_magnitude  | 0.5106162    |
| Train/Action_max        | 0.14455949   |
| Train/Action_std        | 0.22283164   |
| Train/Entropy           | -0.16280286  |
| Train/Entropy_Loss      | 0.000163     |
| Train/Entropy_loss      | 0.000163     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.23564768   |
| Train/Loss              | -0.035668135 |
| Train/PolicyClip        | 0.0051550004 |
| Train/Policy_loss       | -0.040963333 |
| Train/Ratio             | 1.0086852    |
| Train/Return            | 1.0354054    |
| Train/V                 | 0.9827292    |
| Train/Value             | 0.9827292    |
| Train/control_penalty   | 0.5132395    |
| Train/policy_loss       | -0.040963333 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 231 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 230         |
| Time/Actor_Time         | 0.087       |
| Time/B_Format_Time      | 0.0735      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00217     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25265038  |
| Train/Action_magnitu... | 0.64995867  |
| Train/Action_magnitude  | 0.5173914   |
| Train/Action_max        | 0.15340249  |
| Train/Action_std        | 0.22382942  |
| Train/Entropy           | -0.15479185 |
| Train/Entropy_Loss      | 0.000155    |
| Train/Entropy_loss      | 0.000155    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.2111412   |
| Train/Loss              | 0.034111746 |
| Train/PolicyClip        | 0.0068709   |
| Train/Policy_loss       | 0.028771874 |
| Train/Ratio             | 0.99600023  |
| Train/Return            | 1.0755594   |
| Train/V                 | 1.1087632   |
| Train/Value             | 1.1087632   |
| Train/control_penalty   | 0.51850796  |
| Train/policy_loss       | 0.028771874 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01475     |
-----------------------------------------

 ---------------- Iteration 232 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 231         |
| Time/Actor_Time         | 0.0855      |
| Time/B_Format_Time      | 0.0745      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.0033      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.26370987  |
| Train/Action_magnitu... | 0.6653171   |
| Train/Action_magnitude  | 0.5281279   |
| Train/Action_max        | 0.13889651  |
| Train/Action_std        | 0.21448497  |
| Train/Entropy           | -0.19897604 |
| Train/Entropy_Loss      | 0.000199    |
| Train/Entropy_loss      | 0.000199    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.29619232  |
| Train/Loss              | 0.017290946 |
| Train/PolicyClip        | 0.007915056 |
| Train/Policy_loss       | 0.011872737 |
| Train/Ratio             | 0.98995006  |
| Train/Return            | 1.1966401   |
| Train/V                 | 1.2134981   |
| Train/Value             | 1.2134981   |
| Train/control_penalty   | 0.5219234   |
| Train/policy_loss       | 0.011872737 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 233 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 232         |
| Time/Actor_Time         | 0.085       |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.071       |
| Time/Buffer             | 0.00233     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.27624542  |
| Train/Action_magnitu... | 0.6820878   |
| Train/Action_magnitude  | 0.5401902   |
| Train/Action_max        | 0.14531903  |
| Train/Action_std        | 0.22028688  |
| Train/Entropy           | -0.17833368 |
| Train/Entropy_Loss      | 0.000178    |
| Train/Entropy_loss      | 0.000178    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.2869301   |
| Train/Loss              | 0.03168383  |
| Train/PolicyClip        | 0.006240222 |
| Train/Policy_loss       | 0.026147878 |
| Train/Ratio             | 0.997448    |
| Train/Return            | 1.0724145   |
| Train/V                 | 1.0997488   |
| Train/Value             | 1.0997488   |
| Train/control_penalty   | 0.53576165  |
| Train/policy_loss       | 0.026147878 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 234 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 233          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0726       |
| Time/Buffer             | 0.00229      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2610203    |
| Train/Action_magnitu... | 0.64494663   |
| Train/Action_magnitude  | 0.5112661    |
| Train/Action_max        | 0.13889395   |
| Train/Action_std        | 0.21590585   |
| Train/Entropy           | -0.19348241  |
| Train/Entropy_Loss      | 0.000193     |
| Train/Entropy_loss      | 0.000193     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.37189204   |
| Train/Loss              | 0.008230295  |
| Train/PolicyClip        | 0.0070528127 |
| Train/Policy_loss       | 0.002915888  |
| Train/Ratio             | 0.99632156   |
| Train/Return            | 0.98813486   |
| Train/V                 | 0.9961782    |
| Train/Value             | 0.9961782    |
| Train/control_penalty   | 0.5120924    |
| Train/policy_loss       | 0.002915888  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 235 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 234          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.25002193   |
| Train/Action_magnitu... | 0.63656837   |
| Train/Action_magnitude  | 0.50432736   |
| Train/Action_max        | 0.13149989   |
| Train/Action_std        | 0.21871255   |
| Train/Entropy           | -0.17952408  |
| Train/Entropy_Loss      | 0.00018      |
| Train/Entropy_loss      | 0.00018      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.37271088   |
| Train/Loss              | 0.012413622  |
| Train/PolicyClip        | 0.0068407366 |
| Train/Policy_loss       | 0.00717398   |
| Train/Ratio             | 0.988365     |
| Train/Return            | 0.9209023    |
| Train/V                 | 0.93238324   |
| Train/Value             | 0.93238324   |
| Train/control_penalty   | 0.5060118    |
| Train/policy_loss       | 0.00717398   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.016        |
------------------------------------------

 ---------------- Iteration 236 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 235          |
| Time/Actor_Time         | 0.0872       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0687       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.27116936   |
| Train/Action_magnitu... | 0.6794335    |
| Train/Action_magnitude  | 0.53659934   |
| Train/Action_max        | 0.1519559    |
| Train/Action_std        | 0.21923903   |
| Train/Entropy           | -0.19750072  |
| Train/Entropy_Loss      | 0.000198     |
| Train/Entropy_loss      | 0.000198     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.34197125   |
| Train/Loss              | -0.01993849  |
| Train/PolicyClip        | 0.0031717704 |
| Train/Policy_loss       | -0.025412956 |
| Train/Ratio             | 0.9988485    |
| Train/Return            | 1.1458085    |
| Train/V                 | 1.128688     |
| Train/Value             | 1.128688     |
| Train/control_penalty   | 0.52769667   |
| Train/policy_loss       | -0.025412956 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 237 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 236         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0732      |
| Time/Buffer             | 0.00263     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.26272613  |
| Train/Action_magnitu... | 0.654689    |
| Train/Action_magnitude  | 0.5178012   |
| Train/Action_max        | 0.18154143  |
| Train/Action_std        | 0.21208867  |
| Train/Entropy           | -0.22357929 |
| Train/Entropy_Loss      | 0.000224    |
| Train/Entropy_loss      | 0.000224    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.44993347  |
| Train/Loss              | 0.030436534 |
| Train/PolicyClip        | 0.003883193 |
| Train/Policy_loss       | 0.02513877  |
| Train/Ratio             | 0.9937357   |
| Train/Return            | 1.0713676   |
| Train/V                 | 1.1017616   |
| Train/Value             | 1.1017616   |
| Train/control_penalty   | 0.5074186   |
| Train/policy_loss       | 0.02513877  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 238 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 237          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00271      |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.25484973   |
| Train/Action_magnitu... | 0.6249119    |
| Train/Action_magnitude  | 0.49347803   |
| Train/Action_max        | 0.18859296   |
| Train/Action_std        | 0.20955347   |
| Train/Entropy           | -0.24109034  |
| Train/Entropy_Loss      | 0.000241     |
| Train/Entropy_loss      | 0.000241     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.45753133   |
| Train/Loss              | -0.008918766 |
| Train/PolicyClip        | 0.003813538  |
| Train/Policy_loss       | -0.014116415 |
| Train/Ratio             | 1.0022396    |
| Train/Return            | 0.9589548    |
| Train/V                 | 0.9509273    |
| Train/Value             | 0.9509273    |
| Train/control_penalty   | 0.49565586   |
| Train/policy_loss       | -0.014116415 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 239 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 238         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00255     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25437698  |
| Train/Action_magnitu... | 0.6337729   |
| Train/Action_magnitude  | 0.49938414  |
| Train/Action_max        | 0.1735574   |
| Train/Action_std        | 0.2066605   |
| Train/Entropy           | -0.25311303 |
| Train/Entropy_Loss      | 0.000253    |
| Train/Entropy_loss      | 0.000253    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.4785285   |
| Train/Loss              | 0.018537218 |
| Train/PolicyClip        | 0.004454255 |
| Train/Policy_loss       | 0.01322727  |
| Train/Ratio             | 0.99010277  |
| Train/Return            | 0.89851844  |
| Train/V                 | 0.9166949   |
| Train/Value             | 0.9166949   |
| Train/control_penalty   | 0.50568354  |
| Train/policy_loss       | 0.01322727  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.018       |
-----------------------------------------

 ---------------- Iteration 240 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 239          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00319      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25719917   |
| Train/Action_magnitu... | 0.63202435   |
| Train/Action_magnitude  | 0.49824747   |
| Train/Action_max        | 0.15541796   |
| Train/Action_std        | 0.2048635    |
| Train/Entropy           | -0.25766405  |
| Train/Entropy_Loss      | 0.000258     |
| Train/Entropy_loss      | 0.000258     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.46036005   |
| Train/Loss              | -0.017496055 |
| Train/PolicyClip        | 0.005118865  |
| Train/Policy_loss       | -0.022792682 |
| Train/Ratio             | 1.0067948    |
| Train/Return            | 1.2287899    |
| Train/V                 | 1.2146653    |
| Train/Value             | 1.2146653    |
| Train/control_penalty   | 0.5038964    |
| Train/policy_loss       | -0.022792682 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.027        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 241 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 240         |
| Time/Actor_Time         | 0.0855      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.0711      |
| Time/Buffer             | 0.00297     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.2487079   |
| Train/Action_magnitu... | 0.5963601   |
| Train/Action_magnitude  | 0.4721305   |
| Train/Action_max        | 0.1884115   |
| Train/Action_std        | 0.19662529  |
| Train/Entropy           | -0.30329338 |
| Train/Entropy_Loss      | 0.000303    |
| Train/Entropy_loss      | 0.000303    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5135775   |
| Train/Loss              | -0.03602269 |
| Train/PolicyClip        | 0.008564421 |
| Train/Policy_loss       | -0.04108164 |
| Train/Ratio             | 0.9992157   |
| Train/Return            | 1.1239367   |
| Train/V                 | 1.0812143   |
| Train/Value             | 1.0812143   |
| Train/control_penalty   | 0.4755656   |
| Train/policy_loss       | -0.04108164 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 242 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 241          |
| Time/Actor_Time         | 0.0881       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00315      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23861693   |
| Train/Action_magnitu... | 0.6006742    |
| Train/Action_magnitude  | 0.47662607   |
| Train/Action_max        | 0.15369597   |
| Train/Action_std        | 0.21315046   |
| Train/Entropy           | -0.22858618  |
| Train/Entropy_Loss      | 0.000229     |
| Train/Entropy_loss      | 0.000229     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.44636524   |
| Train/Loss              | 0.015054736  |
| Train/PolicyClip        | 0.005047205  |
| Train/Policy_loss       | 0.0099514965 |
| Train/Ratio             | 0.99689305   |
| Train/Return            | 1.2212517    |
| Train/V                 | 1.2377794    |
| Train/Value             | 1.2377794    |
| Train/control_penalty   | 0.4874653    |
| Train/policy_loss       | 0.0099514965 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 243 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 242         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00324     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.26505956  |
| Train/Action_magnitu... | 0.6465067   |
| Train/Action_magnitude  | 0.51061565  |
| Train/Action_max        | 0.17051737  |
| Train/Action_std        | 0.22298484  |
| Train/Entropy           | -0.20005298 |
| Train/Entropy_Loss      | 0.0002      |
| Train/Entropy_loss      | 0.0002      |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.3932965   |
| Train/Loss              | 0.02351195  |
| Train/PolicyClip        | 0.006417455 |
| Train/Policy_loss       | 0.018089658 |
| Train/Ratio             | 1.0043678   |
| Train/Return            | 1.095649    |
| Train/V                 | 1.1175202   |
| Train/Value             | 1.1175202   |
| Train/control_penalty   | 0.5222238   |
| Train/policy_loss       | 0.018089658 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 244 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 243          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.0035       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27081123   |
| Train/Action_magnitu... | 0.6664796    |
| Train/Action_magnitude  | 0.5264407    |
| Train/Action_max        | 0.13861611   |
| Train/Action_std        | 0.21397588   |
| Train/Entropy           | -0.23209877  |
| Train/Entropy_Loss      | 0.000232     |
| Train/Entropy_loss      | 0.000232     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.406489     |
| Train/Loss              | -0.048375268 |
| Train/PolicyClip        | 0.0054275854 |
| Train/Policy_loss       | -0.05386654  |
| Train/Ratio             | 1.0042741    |
| Train/Return            | 1.2021458    |
| Train/V                 | 1.1501653    |
| Train/Value             | 1.1501653    |
| Train/control_penalty   | 0.52591723   |
| Train/policy_loss       | -0.05386654  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 245 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 244          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0746       |
| Time/Buffer             | 0.00281      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2582345    |
| Train/Action_magnitu... | 0.65385556   |
| Train/Action_magnitude  | 0.5146702    |
| Train/Action_max        | 0.15022086   |
| Train/Action_std        | 0.22274691   |
| Train/Entropy           | -0.20922142  |
| Train/Entropy_Loss      | 0.000209     |
| Train/Entropy_loss      | 0.000209     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.35954842   |
| Train/Loss              | 0.014443281  |
| Train/PolicyClip        | 0.0073246784 |
| Train/Policy_loss       | 0.009102992  |
| Train/Ratio             | 1.0026753    |
| Train/Return            | 1.0164355    |
| Train/V                 | 1.0240746    |
| Train/Value             | 1.0240746    |
| Train/control_penalty   | 0.51310676   |
| Train/policy_loss       | 0.009102992  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 246 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 245         |
| Time/Actor_Time         | 0.0879      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00357     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25578     |
| Train/Action_magnitu... | 0.65146166  |
| Train/Action_magnitude  | 0.5138164   |
| Train/Action_max        | 0.15279227  |
| Train/Action_std        | 0.23529384  |
| Train/Entropy           | -0.15407227 |
| Train/Entropy_Loss      | 0.000154    |
| Train/Entropy_loss      | 0.000154    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.32548192  |
| Train/Loss              | 0.032450475 |
| Train/PolicyClip        | 0.008086451 |
| Train/Policy_loss       | 0.02716485  |
| Train/Ratio             | 0.9962124   |
| Train/Return            | 1.0141746   |
| Train/V                 | 1.0492871   |
| Train/Value             | 1.0492871   |
| Train/control_penalty   | 0.51315516  |
| Train/policy_loss       | 0.02716485  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 247 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 246          |
| Time/Actor_Time         | 0.0897       |
| Time/B_Format_Time      | 0.0757       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.0031       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27775908   |
| Train/Action_magnitu... | 0.69585854   |
| Train/Action_magnitude  | 0.5476922    |
| Train/Action_max        | 0.13620672   |
| Train/Action_std        | 0.24741009   |
| Train/Entropy           | -0.10191771  |
| Train/Entropy_Loss      | 0.000102     |
| Train/Entropy_loss      | 0.000102     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.18905772   |
| Train/Loss              | 0.073739685  |
| Train/PolicyClip        | 0.0058688326 |
| Train/Policy_loss       | 0.06810538   |
| Train/Ratio             | 0.9880335    |
| Train/Return            | 0.9721548    |
| Train/V                 | 1.0503509    |
| Train/Value             | 1.0503509    |
| Train/control_penalty   | 0.5532391    |
| Train/policy_loss       | 0.06810538   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01475      |
------------------------------------------

 ---------------- Iteration 248 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 247          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27965754   |
| Train/Action_magnitu... | 0.74019015   |
| Train/Action_magnitude  | 0.5825635    |
| Train/Action_max        | 0.136455     |
| Train/Action_std        | 0.27694216   |
| Train/Entropy           | -0.017415494 |
| Train/Entropy_Loss      | 1.74e-05     |
| Train/Entropy_loss      | 1.74e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.010311951 |
| Train/Loss              | -0.07874473  |
| Train/PolicyClip        | 0.005997102  |
| Train/Policy_loss       | -0.08458099  |
| Train/Ratio             | 1.0019906    |
| Train/Return            | 1.1359562    |
| Train/V                 | 1.063467     |
| Train/Value             | 1.063467     |
| Train/control_penalty   | 0.58188415   |
| Train/policy_loss       | -0.08458099  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 249 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 248         |
| Time/Actor_Time         | 0.0914      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0696      |
| Time/Buffer             | 0.00295     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25615478  |
| Train/Action_magnitu... | 0.6628462   |
| Train/Action_magnitude  | 0.5226316   |
| Train/Action_max        | 0.118507795 |
| Train/Action_std        | 0.2419805   |
| Train/Entropy           | -0.12263626 |
| Train/Entropy_Loss      | 0.000123    |
| Train/Entropy_loss      | 0.000123    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.22764495  |
| Train/Loss              | 0.009278399 |
| Train/PolicyClip        | 0.00866476  |
| Train/Policy_loss       | 0.003979444 |
| Train/Ratio             | 0.99150634  |
| Train/Return            | 1.1351422   |
| Train/V                 | 1.1506947   |
| Train/Value             | 1.1506947   |
| Train/control_penalty   | 0.5176319   |
| Train/policy_loss       | 0.003979444 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 250 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 249         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0743      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00296     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.26693615  |
| Train/Action_magnitu... | 0.66867924  |
| Train/Action_magnitude  | 0.52739394  |
| Train/Action_max        | 0.13482526  |
| Train/Action_std        | 0.24273005  |
| Train/Entropy           | -0.11724321 |
| Train/Entropy_Loss      | 0.000117    |
| Train/Entropy_loss      | 0.000117    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.22899783  |
| Train/Loss              | 0.0392308   |
| Train/PolicyClip        | 0.008451917 |
| Train/Policy_loss       | 0.033749253 |
| Train/Ratio             | 0.99285525  |
| Train/Return            | 1.0766703   |
| Train/V                 | 1.1217395   |
| Train/Value             | 1.1217395   |
| Train/control_penalty   | 0.53643084  |
| Train/policy_loss       | 0.033749253 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.017       |
-----------------------------------------

 ---------------- Iteration 251 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 250            |
| Time/Actor_Time         | 0.0859         |
| Time/B_Format_Time      | 0.0721         |
| Time/B_Original_Form... | 0.0723         |
| Time/Buffer             | 0.00276        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.2349268      |
| Train/Action_magnitu... | 0.61527514     |
| Train/Action_magnitude  | 0.48444393     |
| Train/Action_max        | 0.12962435     |
| Train/Action_std        | 0.21839337     |
| Train/Entropy           | -0.20775643    |
| Train/Entropy_Loss      | 0.000208       |
| Train/Entropy_loss      | 0.000208       |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | 0.38398677     |
| Train/Loss              | -0.00093412586 |
| Train/PolicyClip        | 0.008818388    |
| Train/Policy_loss       | -0.0059485636  |
| Train/Ratio             | 0.99115515     |
| Train/Return            | 1.1182196      |
| Train/V                 | 1.1209561      |
| Train/Value             | 1.1209561      |
| Train/control_penalty   | 0.48066813     |
| Train/policy_loss       | -0.0059485636  |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.02           |
--------------------------------------------

 ---------------- Iteration 252 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 251          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0751       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00273      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24441059   |
| Train/Action_magnitu... | 0.6241825    |
| Train/Action_magnitude  | 0.49199545   |
| Train/Action_max        | 0.14305094   |
| Train/Action_std        | 0.21312933   |
| Train/Entropy           | -0.24230762  |
| Train/Entropy_Loss      | 0.000242     |
| Train/Entropy_loss      | 0.000242     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.38500288   |
| Train/Loss              | -0.030814575 |
| Train/PolicyClip        | 0.009316243  |
| Train/Policy_loss       | -0.035893448 |
| Train/Ratio             | 0.995066     |
| Train/Return            | 1.2309955    |
| Train/V                 | 1.202561     |
| Train/Value             | 1.202561     |
| Train/control_penalty   | 0.4836565    |
| Train/policy_loss       | -0.035893448 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 253 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 252          |
| Time/Actor_Time         | 0.0871       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00276      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2633309    |
| Train/Action_magnitu... | 0.6458355    |
| Train/Action_magnitude  | 0.5083783    |
| Train/Action_max        | 0.12535113   |
| Train/Action_std        | 0.20592049   |
| Train/Entropy           | -0.26197845  |
| Train/Entropy_Loss      | 0.000262     |
| Train/Entropy_loss      | 0.000262     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.459648     |
| Train/Loss              | -0.019208407 |
| Train/PolicyClip        | 0.0080974605 |
| Train/Policy_loss       | -0.02448778  |
| Train/Ratio             | 0.9943458    |
| Train/Return            | 1.2948737    |
| Train/V                 | 1.2809402    |
| Train/Value             | 1.2809402    |
| Train/control_penalty   | 0.5017394    |
| Train/policy_loss       | -0.02448778  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 254 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 253          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00237      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23130202   |
| Train/Action_magnitu... | 0.59650123   |
| Train/Action_magnitude  | 0.46895823   |
| Train/Action_max        | 0.11984773   |
| Train/Action_std        | 0.21017352   |
| Train/Entropy           | -0.24487881  |
| Train/Entropy_Loss      | 0.000245     |
| Train/Entropy_loss      | 0.000245     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.48153728   |
| Train/Loss              | 0.022680806  |
| Train/PolicyClip        | 0.0059463545 |
| Train/Policy_loss       | 0.017716704  |
| Train/Ratio             | 0.998293     |
| Train/Return            | 1.2117281    |
| Train/V                 | 1.2322463    |
| Train/Value             | 1.2322463    |
| Train/control_penalty   | 0.47192243   |
| Train/policy_loss       | 0.017716704  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 255 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 254          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0763       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25097936   |
| Train/Action_magnitu... | 0.6172405    |
| Train/Action_magnitude  | 0.48109007   |
| Train/Action_max        | 0.11991134   |
| Train/Action_std        | 0.21166126   |
| Train/Entropy           | -0.23545113  |
| Train/Entropy_Loss      | 0.000235     |
| Train/Entropy_loss      | 0.000235     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.43525642   |
| Train/Loss              | -0.006802864 |
| Train/PolicyClip        | 0.0054584467 |
| Train/Policy_loss       | -0.011920771 |
| Train/Ratio             | 0.9996582    |
| Train/Return            | 1.2959207    |
| Train/V                 | 1.2894632    |
| Train/Value             | 1.2894632    |
| Train/control_penalty   | 0.48824555   |
| Train/policy_loss       | -0.011920771 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 256 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 255          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00303      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2296331    |
| Train/Action_magnitu... | 0.5791256    |
| Train/Action_magnitude  | 0.4549234    |
| Train/Action_max        | 0.115638845  |
| Train/Action_std        | 0.2030335    |
| Train/Entropy           | -0.27581924  |
| Train/Entropy_Loss      | 0.000276     |
| Train/Entropy_loss      | 0.000276     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5285605    |
| Train/Loss              | -0.01816366  |
| Train/PolicyClip        | 0.005774244  |
| Train/Policy_loss       | -0.023010394 |
| Train/Ratio             | 0.9944886    |
| Train/Return            | 1.3736359    |
| Train/V                 | 1.3593913    |
| Train/Value             | 1.3593913    |
| Train/control_penalty   | 0.45709142   |
| Train/policy_loss       | -0.023010394 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 257 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 256         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0704      |
| Time/Buffer             | 0.00283     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24511077  |
| Train/Action_magnitu... | 0.6129278   |
| Train/Action_magnitude  | 0.48234826  |
| Train/Action_max        | 0.12367186  |
| Train/Action_std        | 0.21650054  |
| Train/Entropy           | -0.22464998 |
| Train/Entropy_Loss      | 0.000225    |
| Train/Entropy_loss      | 0.000225    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.43272448  |
| Train/Loss              | 0.028819555 |
| Train/PolicyClip        | 0.008363915 |
| Train/Policy_loss       | 0.023707282 |
| Train/Ratio             | 0.98925054  |
| Train/Return            | 1.2553055   |
| Train/V                 | 1.2877901   |
| Train/Value             | 1.2877901   |
| Train/control_penalty   | 0.48876244  |
| Train/policy_loss       | 0.023707282 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 258 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 257         |
| Time/Actor_Time         | 0.0877      |
| Time/B_Format_Time      | 0.0735      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00235     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23806962  |
| Train/Action_magnitu... | 0.60206836  |
| Train/Action_magnitude  | 0.47284764  |
| Train/Action_max        | 0.129687    |
| Train/Action_std        | 0.20719972  |
| Train/Entropy           | -0.26764396 |
| Train/Entropy_Loss      | 0.000268    |
| Train/Entropy_loss      | 0.000268    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.54213566  |
| Train/Loss              | 0.041647717 |
| Train/PolicyClip        | 0.008315722 |
| Train/Policy_loss       | 0.03664612  |
| Train/Ratio             | 0.9916891   |
| Train/Return            | 1.2530012   |
| Train/V                 | 1.3006978   |
| Train/Value             | 1.3006978   |
| Train/control_penalty   | 0.4733953   |
| Train/policy_loss       | 0.03664612  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 259 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 258          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00229      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24347605   |
| Train/Action_magnitu... | 0.60564303   |
| Train/Action_magnitude  | 0.4768536    |
| Train/Action_max        | 0.15022036   |
| Train/Action_std        | 0.20109275   |
| Train/Entropy           | -0.29032153  |
| Train/Entropy_Loss      | 0.00029      |
| Train/Entropy_loss      | 0.00029      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.52601516   |
| Train/Loss              | -0.032777857 |
| Train/PolicyClip        | 0.0069430084 |
| Train/Policy_loss       | -0.037818678 |
| Train/Ratio             | 1.003875     |
| Train/Return            | 1.1120751    |
| Train/V                 | 1.0799223    |
| Train/Value             | 1.0799223    |
| Train/control_penalty   | 0.47504988   |
| Train/policy_loss       | -0.037818678 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 260 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 259          |
| Time/Actor_Time         | 0.0909       |
| Time/B_Format_Time      | 0.0733       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00265      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24802324   |
| Train/Action_magnitu... | 0.61280495   |
| Train/Action_magnitude  | 0.4806953    |
| Train/Action_max        | 0.13205159   |
| Train/Action_std        | 0.21330303   |
| Train/Entropy           | -0.2236091   |
| Train/Entropy_Loss      | 0.000224     |
| Train/Entropy_loss      | 0.000224     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.48144558   |
| Train/Loss              | 0.03886975   |
| Train/PolicyClip        | 0.0061217276 |
| Train/Policy_loss       | 0.0337297    |
| Train/Ratio             | 0.98915696   |
| Train/Return            | 1.1488243    |
| Train/V                 | 1.1921066    |
| Train/Value             | 1.1921066    |
| Train/control_penalty   | 0.49164405   |
| Train/policy_loss       | 0.0337297    |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 261 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 260         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.075       |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24103297  |
| Train/Action_magnitu... | 0.59626997  |
| Train/Action_magnitude  | 0.46960157  |
| Train/Action_max        | 0.19229452  |
| Train/Action_std        | 0.2028515   |
| Train/Entropy           | -0.29093587 |
| Train/Entropy_Loss      | 0.000291    |
| Train/Entropy_loss      | 0.000291    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5331763   |
| Train/Loss              | -0.07173065 |
| Train/PolicyClip        | 0.008690348 |
| Train/Policy_loss       | -0.0768042  |
| Train/Ratio             | 0.99902576  |
| Train/Return            | 1.0887413   |
| Train/V                 | 1.0213869   |
| Train/Value             | 1.0213869   |
| Train/control_penalty   | 0.47826082  |
| Train/policy_loss       | -0.0768042  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 262 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 261          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0744       |
| Time/B_Original_Form... | 0.0743       |
| Time/Buffer             | 0.00296      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.25198177   |
| Train/Action_magnitu... | 0.61646724   |
| Train/Action_magnitude  | 0.48706096   |
| Train/Action_max        | 0.16095652   |
| Train/Action_std        | 0.20487961   |
| Train/Entropy           | -0.27549112  |
| Train/Entropy_Loss      | 0.000275     |
| Train/Entropy_loss      | 0.000275     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.45559186   |
| Train/Loss              | -0.01665759  |
| Train/PolicyClip        | 0.00803046   |
| Train/Policy_loss       | -0.021784471 |
| Train/Ratio             | 1.0016472    |
| Train/Return            | 1.1499025    |
| Train/V                 | 1.1332539    |
| Train/Value             | 1.1332539    |
| Train/control_penalty   | 0.4851389    |
| Train/policy_loss       | -0.021784471 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 263 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 262          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00558      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26634863   |
| Train/Action_magnitu... | 0.64443123   |
| Train/Action_magnitude  | 0.505846     |
| Train/Action_max        | 0.1563913    |
| Train/Action_std        | 0.20248765   |
| Train/Entropy           | -0.2832495   |
| Train/Entropy_Loss      | 0.000283     |
| Train/Entropy_loss      | 0.000283     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5728799    |
| Train/Loss              | 0.053212512  |
| Train/PolicyClip        | 0.0071941274 |
| Train/Policy_loss       | 0.04789581   |
| Train/Ratio             | 0.9908595    |
| Train/Return            | 1.1494555    |
| Train/V                 | 1.2065148    |
| Train/Value             | 1.2065148    |
| Train/control_penalty   | 0.5033454    |
| Train/policy_loss       | 0.04789581   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 264 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 263          |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.0743       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.0029       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24469638   |
| Train/Action_magnitu... | 0.6013735    |
| Train/Action_magnitude  | 0.47409558   |
| Train/Action_max        | 0.16947992   |
| Train/Action_std        | 0.20879059   |
| Train/Entropy           | -0.25415158  |
| Train/Entropy_Loss      | 0.000254     |
| Train/Entropy_loss      | 0.000254     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5639218    |
| Train/Loss              | 0.018218622  |
| Train/PolicyClip        | 0.0058584125 |
| Train/Policy_loss       | 0.0130599765 |
| Train/Ratio             | 0.98365444   |
| Train/Return            | 0.9369051    |
| Train/V                 | 0.9524838    |
| Train/Value             | 0.9524838    |
| Train/control_penalty   | 0.49044946   |
| Train/policy_loss       | 0.0130599765 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0165       |
------------------------------------------

 ---------------- Iteration 265 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 264          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0764       |
| Time/B_Original_Form... | 0.0739       |
| Time/Buffer             | 0.00349      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26215783   |
| Train/Action_magnitu... | 0.66977954   |
| Train/Action_magnitude  | 0.52801555   |
| Train/Action_max        | 0.1787505    |
| Train/Action_std        | 0.21452998   |
| Train/Entropy           | -0.23236482  |
| Train/Entropy_Loss      | 0.000232     |
| Train/Entropy_loss      | 0.000232     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.3384239    |
| Train/Loss              | -0.017968021 |
| Train/PolicyClip        | 0.007343245  |
| Train/Policy_loss       | -0.023389325 |
| Train/Ratio             | 0.99700433   |
| Train/Return            | 1.0616411    |
| Train/V                 | 1.0406628    |
| Train/Value             | 1.0406628    |
| Train/control_penalty   | 0.5188938    |
| Train/policy_loss       | -0.023389325 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 266 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 265          |
| Time/Actor_Time         | 0.0872       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00278      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2595767    |
| Train/Action_magnitu... | 0.63484937   |
| Train/Action_magnitude  | 0.5018342    |
| Train/Action_max        | 0.14240417   |
| Train/Action_std        | 0.21210544   |
| Train/Entropy           | -0.23536749  |
| Train/Entropy_Loss      | 0.000235     |
| Train/Entropy_loss      | 0.000235     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.48322228   |
| Train/Loss              | -0.013020452 |
| Train/PolicyClip        | 0.007305037  |
| Train/Policy_loss       | -0.018413858 |
| Train/Ratio             | 0.9940462    |
| Train/Return            | 1.107321     |
| Train/V                 | 1.0981244    |
| Train/Value             | 1.0981244    |
| Train/control_penalty   | 0.5158039    |
| Train/policy_loss       | -0.018413858 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.022        |
------------------------------------------

 ---------------- Iteration 267 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 266          |
| Time/Actor_Time         | 0.0876       |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.248346     |
| Train/Action_magnitu... | 0.6224602    |
| Train/Action_magnitude  | 0.49163467   |
| Train/Action_max        | 0.14787352   |
| Train/Action_std        | 0.20193501   |
| Train/Entropy           | -0.27582824  |
| Train/Entropy_Loss      | 0.000276     |
| Train/Entropy_loss      | 0.000276     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.537733     |
| Train/Loss              | -0.033642232 |
| Train/PolicyClip        | 0.0077966605 |
| Train/Policy_loss       | -0.03883568  |
| Train/Ratio             | 0.9991416    |
| Train/Return            | 1.1063794    |
| Train/V                 | 1.0748969    |
| Train/Value             | 1.0748969    |
| Train/control_penalty   | 0.49176192   |
| Train/policy_loss       | -0.03883568  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 268 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 267          |
| Time/Actor_Time         | 0.0879       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00465      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.24364626   |
| Train/Action_magnitu... | 0.619666     |
| Train/Action_magnitude  | 0.49025494   |
| Train/Action_max        | 0.1293762    |
| Train/Action_std        | 0.2159725    |
| Train/Entropy           | -0.20800883  |
| Train/Entropy_Loss      | 0.000208     |
| Train/Entropy_loss      | 0.000208     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.44059595   |
| Train/Loss              | -0.048638143 |
| Train/PolicyClip        | 0.0077942973 |
| Train/Policy_loss       | -0.053781044 |
| Train/Ratio             | 0.9951005    |
| Train/Return            | 1.0837618    |
| Train/V                 | 1.0353318    |
| Train/Value             | 1.0353318    |
| Train/control_penalty   | 0.4934894    |
| Train/policy_loss       | -0.053781044 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 269 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 268          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0706       |
| Time/B_Original_Form... | 0.0734       |
| Time/Buffer             | 0.00264      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26973718   |
| Train/Action_magnitu... | 0.6586742    |
| Train/Action_magnitude  | 0.5198913    |
| Train/Action_max        | 0.15237378   |
| Train/Action_std        | 0.20259331   |
| Train/Entropy           | -0.27671915  |
| Train/Entropy_Loss      | 0.000277     |
| Train/Entropy_loss      | 0.000277     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.52244484   |
| Train/Loss              | -0.010129923 |
| Train/PolicyClip        | 0.0074457736 |
| Train/Policy_loss       | -0.015536197 |
| Train/Ratio             | 0.9920714    |
| Train/Return            | 1.1758609    |
| Train/V                 | 1.1683831    |
| Train/Value             | 1.1683831    |
| Train/control_penalty   | 0.5129555    |
| Train/policy_loss       | -0.015536197 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 270 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 269         |
| Time/Actor_Time         | 0.0874      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00408     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24509206  |
| Train/Action_magnitu... | 0.6123149   |
| Train/Action_magnitude  | 0.48513454  |
| Train/Action_max        | 0.18300508  |
| Train/Action_std        | 0.20048672  |
| Train/Entropy           | -0.28374988 |
| Train/Entropy_Loss      | 0.000284    |
| Train/Entropy_loss      | 0.000284    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.593334    |
| Train/Loss              | -0.04589894 |
| Train/PolicyClip        | 0.008701108 |
| Train/Policy_loss       | -0.05105002 |
| Train/Ratio             | 1.0029175   |
| Train/Return            | 1.0700036   |
| Train/V                 | 1.0281322   |
| Train/Value             | 1.0281322   |
| Train/control_penalty   | 0.48673254  |
| Train/policy_loss       | -0.05105002 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 271 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 270          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.0033       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2178692    |
| Train/Action_magnitu... | 0.5811358    |
| Train/Action_magnitude  | 0.45961198   |
| Train/Action_max        | 0.15264048   |
| Train/Action_std        | 0.19908138   |
| Train/Entropy           | -0.27748883  |
| Train/Entropy_Loss      | 0.000277     |
| Train/Entropy_loss      | 0.000277     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.50207853   |
| Train/Loss              | -0.055509705 |
| Train/PolicyClip        | 0.0048105544 |
| Train/Policy_loss       | -0.060288154 |
| Train/Ratio             | 0.9912646    |
| Train/Return            | 1.169282     |
| Train/V                 | 1.1108427    |
| Train/Value             | 1.1108427    |
| Train/control_penalty   | 0.45009583   |
| Train/policy_loss       | -0.060288154 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 272 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 271          |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0734       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00493      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23826762   |
| Train/Action_magnitu... | 0.5964761    |
| Train/Action_magnitude  | 0.46937525   |
| Train/Action_max        | 0.13973583   |
| Train/Action_std        | 0.21037845   |
| Train/Entropy           | -0.24754733  |
| Train/Entropy_Loss      | 0.000248     |
| Train/Entropy_loss      | 0.000248     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5318523    |
| Train/Loss              | -0.005798261 |
| Train/PolicyClip        | 0.012213689  |
| Train/Policy_loss       | -0.010870098 |
| Train/Ratio             | 1.00433      |
| Train/Return            | 1.2636465    |
| Train/V                 | 1.2543236    |
| Train/Value             | 1.2543236    |
| Train/control_penalty   | 0.48242897   |
| Train/policy_loss       | -0.010870098 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 273 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 272         |
| Time/Actor_Time         | 0.153       |
| Time/B_Format_Time      | 0.133       |
| Time/B_Original_Form... | 0.131       |
| Time/Buffer             | 0.00265     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.25254172  |
| Train/Action_magnitu... | 0.63910764  |
| Train/Action_magnitude  | 0.50238913  |
| Train/Action_max        | 0.1697639   |
| Train/Action_std        | 0.21283484  |
| Train/Entropy           | -0.23740938 |
| Train/Entropy_Loss      | 0.000237    |
| Train/Entropy_loss      | 0.000237    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.44149926  |
| Train/Loss              | 0.038053308 |
| Train/PolicyClip        | 0.004243871 |
| Train/Policy_loss       | 0.03283757  |
| Train/Ratio             | 0.9941483   |
| Train/Return            | 1.2369732   |
| Train/V                 | 1.2741797   |
| Train/Value             | 1.2741797   |
| Train/control_penalty   | 0.4978328   |
| Train/policy_loss       | 0.03283757  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 274 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 273          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0711       |
| Time/Buffer             | 0.00555      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.2174859    |
| Train/Action_magnitu... | 0.5694907    |
| Train/Action_magnitude  | 0.4515792    |
| Train/Action_max        | 0.18900982   |
| Train/Action_std        | 0.20716298   |
| Train/Entropy           | -0.2577589   |
| Train/Entropy_Loss      | 0.000258     |
| Train/Entropy_loss      | 0.000258     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.45780873   |
| Train/Loss              | -0.056625832 |
| Train/PolicyClip        | 0.007979369  |
| Train/Policy_loss       | -0.0613736   |
| Train/Ratio             | 1.0011584    |
| Train/Return            | 1.3018249    |
| Train/V                 | 1.2374341    |
| Train/Value             | 1.2374341    |
| Train/control_penalty   | 0.44900072   |
| Train/policy_loss       | -0.0613736   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 275 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 274         |
| Time/Actor_Time         | 0.0855      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.0713      |
| Time/Buffer             | 0.00225     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2472671   |
| Train/Action_magnitu... | 0.6236605   |
| Train/Action_magnitude  | 0.49176207  |
| Train/Action_max        | 0.12221582  |
| Train/Action_std        | 0.21378545  |
| Train/Entropy           | -0.23887995 |
| Train/Entropy_Loss      | 0.000239    |
| Train/Entropy_loss      | 0.000239    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.42115277  |
| Train/Loss              | 0.041798096 |
| Train/PolicyClip        | 0.005961361 |
| Train/Policy_loss       | 0.036720123 |
| Train/Ratio             | 0.9867609   |
| Train/Return            | 1.0108746   |
| Train/V                 | 1.0525368   |
| Train/Value             | 1.0525368   |
| Train/control_penalty   | 0.48390928  |
| Train/policy_loss       | 0.036720123 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 276 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 275          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00512      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23824483   |
| Train/Action_magnitu... | 0.6020457    |
| Train/Action_magnitude  | 0.47639978   |
| Train/Action_max        | 0.16110922   |
| Train/Action_std        | 0.20697638   |
| Train/Entropy           | -0.25842237  |
| Train/Entropy_Loss      | 0.000258     |
| Train/Entropy_loss      | 0.000258     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4994619    |
| Train/Loss              | -0.029289234 |
| Train/PolicyClip        | 0.005295445  |
| Train/Policy_loss       | -0.034258734 |
| Train/Ratio             | 1.0002825    |
| Train/Return            | 0.96887386   |
| Train/V                 | 0.94309986   |
| Train/Value             | 0.94309986   |
| Train/control_penalty   | 0.47110763   |
| Train/policy_loss       | -0.034258734 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 277 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 276          |
| Time/Actor_Time         | 0.0989       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.0026       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.21942182   |
| Train/Action_magnitu... | 0.56460124   |
| Train/Action_magnitude  | 0.44626963   |
| Train/Action_max        | 0.13006222   |
| Train/Action_std        | 0.20366019   |
| Train/Entropy           | -0.27103832  |
| Train/Entropy_Loss      | 0.000271     |
| Train/Entropy_loss      | 0.000271     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.46329853   |
| Train/Loss              | -0.09667762  |
| Train/PolicyClip        | 0.0063598747 |
| Train/Policy_loss       | -0.1013817   |
| Train/Ratio             | 1.0090857    |
| Train/Return            | 1.0009875    |
| Train/V                 | 0.90350765   |
| Train/Value             | 0.90350765   |
| Train/control_penalty   | 0.44330332   |
| Train/policy_loss       | -0.1013817   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 278 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 277          |
| Time/Actor_Time         | 0.091        |
| Time/B_Format_Time      | 0.0748       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00491      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2210218    |
| Train/Action_magnitu... | 0.56560206   |
| Train/Action_magnitude  | 0.4462285    |
| Train/Action_max        | 0.11643606   |
| Train/Action_std        | 0.20552191   |
| Train/Entropy           | -0.27654737  |
| Train/Entropy_Loss      | 0.000277     |
| Train/Entropy_loss      | 0.000277     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.57512033   |
| Train/Loss              | -0.019383632 |
| Train/PolicyClip        | 0.005637145  |
| Train/Policy_loss       | -0.024167549 |
| Train/Ratio             | 1.0016984    |
| Train/Return            | 1.0307726    |
| Train/V                 | 1.0126965    |
| Train/Value             | 1.0126965    |
| Train/control_penalty   | 0.45073715   |
| Train/policy_loss       | -0.024167549 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 279 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 278          |
| Time/Actor_Time         | 0.0871       |
| Time/B_Format_Time      | 0.0733       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00541      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23481384   |
| Train/Action_magnitu... | 0.58911574   |
| Train/Action_magnitude  | 0.46512395   |
| Train/Action_max        | 0.15056655   |
| Train/Action_std        | 0.19697075   |
| Train/Entropy           | -0.31991637  |
| Train/Entropy_Loss      | 0.00032      |
| Train/Entropy_loss      | 0.00032      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5315358    |
| Train/Loss              | -0.057899464 |
| Train/PolicyClip        | 0.010584289  |
| Train/Policy_loss       | -0.062843926 |
| Train/Ratio             | 1.0061382    |
| Train/Return            | 1.0552646    |
| Train/V                 | 0.99751604   |
| Train/Value             | 0.99751604   |
| Train/control_penalty   | 0.4624549    |
| Train/policy_loss       | -0.062843926 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 280 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 279          |
| Time/Actor_Time         | 0.147        |
| Time/B_Format_Time      | 0.127        |
| Time/B_Original_Form... | 0.127        |
| Time/Buffer             | 0.00254      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2298646    |
| Train/Action_magnitu... | 0.5791554    |
| Train/Action_magnitude  | 0.4569789    |
| Train/Action_max        | 0.14456359   |
| Train/Action_std        | 0.19125977   |
| Train/Entropy           | -0.35307086  |
| Train/Entropy_Loss      | 0.000353     |
| Train/Entropy_loss      | 0.000353     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6301322    |
| Train/Loss              | -0.017251268 |
| Train/PolicyClip        | 0.0052641667 |
| Train/Policy_loss       | -0.02213105  |
| Train/Ratio             | 0.99943274   |
| Train/Return            | 1.0629379    |
| Train/V                 | 1.0478038    |
| Train/Value             | 1.0478038    |
| Train/control_penalty   | 0.4526713    |
| Train/policy_loss       | -0.02213105  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0195       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 281 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 280          |
| Time/Actor_Time         | 0.147        |
| Time/B_Format_Time      | 0.129        |
| Time/B_Original_Form... | 0.127        |
| Time/Buffer             | 0.00338      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.25403544   |
| Train/Action_magnitu... | 0.62029845   |
| Train/Action_magnitude  | 0.49032202   |
| Train/Action_max        | 0.11865118   |
| Train/Action_std        | 0.1952686    |
| Train/Entropy           | -0.34141165  |
| Train/Entropy_Loss      | 0.000341     |
| Train/Entropy_loss      | 0.000341     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.641747     |
| Train/Loss              | -0.050639383 |
| Train/PolicyClip        | 0.0055483053 |
| Train/Policy_loss       | -0.055873282 |
| Train/Ratio             | 1.0010744    |
| Train/Return            | 1.2009068    |
| Train/V                 | 1.1522881    |
| Train/Value             | 1.1522881    |
| Train/control_penalty   | 0.48924854   |
| Train/policy_loss       | -0.055873282 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 282 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 281          |
| Time/Actor_Time         | 0.118        |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00276      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22656119   |
| Train/Action_magnitu... | 0.57359433   |
| Train/Action_magnitude  | 0.45417327   |
| Train/Action_max        | 0.13178524   |
| Train/Action_std        | 0.1987836    |
| Train/Entropy           | -0.31971502  |
| Train/Entropy_Loss      | 0.00032      |
| Train/Entropy_loss      | 0.00032      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6765182    |
| Train/Loss              | -0.02777528  |
| Train/PolicyClip        | 0.0054340437 |
| Train/Policy_loss       | -0.032688443 |
| Train/Ratio             | 1.000433     |
| Train/Return            | 1.0729427    |
| Train/V                 | 1.0438775    |
| Train/Value             | 1.0438775    |
| Train/control_penalty   | 0.45934474   |
| Train/policy_loss       | -0.032688443 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 283 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 282         |
| Time/Actor_Time         | 0.087       |
| Time/B_Format_Time      | 0.0841      |
| Time/B_Original_Form... | 0.13        |
| Time/Buffer             | 0.00531     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24841265  |
| Train/Action_magnitu... | 0.6172035   |
| Train/Action_magnitude  | 0.48608807  |
| Train/Action_max        | 0.12714687  |
| Train/Action_std        | 0.19218028  |
| Train/Entropy           | -0.35646534 |
| Train/Entropy_Loss      | 0.000356    |
| Train/Entropy_loss      | 0.000356    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.58912915  |
| Train/Loss              | 0.019978343 |
| Train/PolicyClip        | 0.011427507 |
| Train/Policy_loss       | 0.014898484 |
| Train/Ratio             | 0.9980603   |
| Train/Return            | 1.2230115   |
| Train/V                 | 1.2433583   |
| Train/Value             | 1.2433583   |
| Train/control_penalty   | 0.4723393   |
| Train/policy_loss       | 0.014898484 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 284 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 283          |
| Time/Actor_Time         | 0.11         |
| Time/B_Format_Time      | 0.0834       |
| Time/B_Original_Form... | 0.101        |
| Time/Buffer             | 0.00536      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25787082   |
| Train/Action_magnitu... | 0.61437523   |
| Train/Action_magnitude  | 0.48339596   |
| Train/Action_max        | 0.14631778   |
| Train/Action_std        | 0.17882991   |
| Train/Entropy           | -0.41804737  |
| Train/Entropy_Loss      | 0.000418     |
| Train/Entropy_loss      | 0.000418     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7675747    |
| Train/Loss              | -0.020661078 |
| Train/PolicyClip        | 0.0046489816 |
| Train/Policy_loss       | -0.02591921  |
| Train/Ratio             | 1.0049899    |
| Train/Return            | 1.280113     |
| Train/V                 | 1.2614545    |
| Train/Value             | 1.2614545    |
| Train/control_penalty   | 0.48400855   |
| Train/policy_loss       | -0.02591921  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 285 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 284          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0757       |
| Time/Buffer             | 0.0042       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24202918   |
| Train/Action_magnitu... | 0.59238756   |
| Train/Action_magnitude  | 0.466133     |
| Train/Action_max        | 0.1139416    |
| Train/Action_std        | 0.18383256   |
| Train/Entropy           | -0.41063052  |
| Train/Entropy_Loss      | 0.000411     |
| Train/Entropy_loss      | 0.000411     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7578889    |
| Train/Loss              | -0.04575304  |
| Train/PolicyClip        | 0.006953786  |
| Train/Policy_loss       | -0.050812073 |
| Train/Ratio             | 0.9885377    |
| Train/Return            | 1.2428979    |
| Train/V                 | 1.203633     |
| Train/Value             | 1.203633     |
| Train/control_penalty   | 0.46484026   |
| Train/policy_loss       | -0.050812073 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 286 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 285           |
| Time/Actor_Time         | 0.148         |
| Time/B_Format_Time      | 0.131         |
| Time/B_Original_Form... | 0.128         |
| Time/Buffer             | 0.00771       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.24689567    |
| Train/Action_magnitu... | 0.5748741     |
| Train/Action_magnitude  | 0.45236424    |
| Train/Action_max        | 0.10819189    |
| Train/Action_std        | 0.17408858    |
| Train/Entropy           | -0.45249215   |
| Train/Entropy_Loss      | 0.000452      |
| Train/Entropy_loss      | 0.000452      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.8775156     |
| Train/Loss              | 0.001195394   |
| Train/PolicyClip        | 0.010430817   |
| Train/Policy_loss       | -0.0038942352 |
| Train/Ratio             | 0.9822459     |
| Train/Return            | 1.2568648     |
| Train/V                 | 1.2642046     |
| Train/Value             | 1.2642046     |
| Train/control_penalty   | 0.4637137     |
| Train/policy_loss       | -0.0038942352 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02375       |
-------------------------------------------

 ---------------- Iteration 287 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 286          |
| Time/Actor_Time         | 0.148        |
| Time/B_Format_Time      | 0.129        |
| Time/B_Original_Form... | 0.128        |
| Time/Buffer             | 0.00638      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2616356    |
| Train/Action_magnitu... | 0.6171764    |
| Train/Action_magnitude  | 0.48658198   |
| Train/Action_max        | 0.12483134   |
| Train/Action_std        | 0.18792829   |
| Train/Entropy           | -0.38279417  |
| Train/Entropy_Loss      | 0.000383     |
| Train/Entropy_loss      | 0.000383     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7490209    |
| Train/Loss              | -0.019365655 |
| Train/PolicyClip        | 0.009123768  |
| Train/Policy_loss       | -0.02468454  |
| Train/Ratio             | 0.9999837    |
| Train/Return            | 1.230012     |
| Train/V                 | 1.2101755    |
| Train/Value             | 1.2101755    |
| Train/control_penalty   | 0.49360913   |
| Train/policy_loss       | -0.02468454  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------

 ---------------- Iteration 288 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 287         |
| Time/Actor_Time         | 0.121       |
| Time/B_Format_Time      | 0.117       |
| Time/B_Original_Form... | 0.101       |
| Time/Buffer             | 0.196       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25683212  |
| Train/Action_magnitu... | 0.6176008   |
| Train/Action_magnitude  | 0.48414764  |
| Train/Action_max        | 0.08971633  |
| Train/Action_std        | 0.18306549  |
| Train/Entropy           | -0.4115849  |
| Train/Entropy_Loss      | 0.000412    |
| Train/Entropy_loss      | 0.000412    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.81811506  |
| Train/Loss              | -0.03873582 |
| Train/PolicyClip        | 0.013926422 |
| Train/Policy_loss       | -0.04396539 |
| Train/Ratio             | 0.9985958   |
| Train/Return            | 1.2749428   |
| Train/V                 | 1.2333595   |
| Train/Value             | 1.2333595   |
| Train/control_penalty   | 0.4817987   |
| Train/policy_loss       | -0.04396539 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 289 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 288           |
| Time/Actor_Time         | 0.148         |
| Time/B_Format_Time      | 0.131         |
| Time/B_Original_Form... | 0.129         |
| Time/Buffer             | 0.00592       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.24641688    |
| Train/Action_magnitu... | 0.59220725    |
| Train/Action_magnitude  | 0.46571508    |
| Train/Action_max        | 0.109786786   |
| Train/Action_std        | 0.18394408    |
| Train/Entropy           | -0.4022669    |
| Train/Entropy_Loss      | 0.000402      |
| Train/Entropy_loss      | 0.000402      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.74541765    |
| Train/Loss              | 0.00086776563 |
| Train/PolicyClip        | 0.007257224   |
| Train/Policy_loss       | -0.0042422703 |
| Train/Ratio             | 0.98800284    |
| Train/Return            | 1.1565363     |
| Train/V                 | 1.1570764     |
| Train/Value             | 1.1570764     |
| Train/control_penalty   | 0.47077692    |
| Train/policy_loss       | -0.0042422703 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.026         |
-------------------------------------------

 ---------------- Iteration 290 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 289          |
| Time/Actor_Time         | 0.153        |
| Time/B_Format_Time      | 0.132        |
| Time/B_Original_Form... | 0.142        |
| Time/Buffer             | 0.00362      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23427382   |
| Train/Action_magnitu... | 0.5805118    |
| Train/Action_magnitude  | 0.4565585    |
| Train/Action_max        | 0.10314709   |
| Train/Action_std        | 0.18846883   |
| Train/Entropy           | -0.39190826  |
| Train/Entropy_Loss      | 0.000392     |
| Train/Entropy_loss      | 0.000392     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.77504164   |
| Train/Loss              | 0.014867712  |
| Train/PolicyClip        | 0.0050852434 |
| Train/Policy_loss       | 0.009842986  |
| Train/Ratio             | 0.9996812    |
| Train/Return            | 1.1419182    |
| Train/V                 | 1.1553192    |
| Train/Value             | 1.1553192    |
| Train/control_penalty   | 0.46328178   |
| Train/policy_loss       | 0.009842986  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 291 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 290          |
| Time/Actor_Time         | 0.107        |
| Time/B_Format_Time      | 0.0937       |
| Time/B_Original_Form... | 0.0944       |
| Time/Buffer             | 0.00438      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23623163   |
| Train/Action_magnitu... | 0.581348     |
| Train/Action_magnitude  | 0.45697773   |
| Train/Action_max        | 0.09266658   |
| Train/Action_std        | 0.18458264   |
| Train/Entropy           | -0.40023574  |
| Train/Entropy_Loss      | 0.0004       |
| Train/Entropy_loss      | 0.0004       |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7599397    |
| Train/Loss              | -0.012523461 |
| Train/PolicyClip        | 0.0043365117 |
| Train/Policy_loss       | -0.017461948 |
| Train/Ratio             | 0.9953152    |
| Train/Return            | 1.2599934    |
| Train/V                 | 1.247885     |
| Train/Value             | 1.247885     |
| Train/control_penalty   | 0.45382512   |
| Train/policy_loss       | -0.017461948 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 292 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 291          |
| Time/Actor_Time         | 0.102        |
| Time/B_Format_Time      | 0.131        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00627      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2279798    |
| Train/Action_magnitu... | 0.5821228    |
| Train/Action_magnitude  | 0.45820302   |
| Train/Action_max        | 0.105726376  |
| Train/Action_std        | 0.18763973   |
| Train/Entropy           | -0.38021776  |
| Train/Entropy_Loss      | 0.00038      |
| Train/Entropy_loss      | 0.00038      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.72628176   |
| Train/Loss              | 0.03282279   |
| Train/PolicyClip        | 0.0050978777 |
| Train/Policy_loss       | 0.027938705  |
| Train/Ratio             | 0.9987277    |
| Train/Return            | 1.1922805    |
| Train/V                 | 1.2272975    |
| Train/Value             | 1.2272975    |
| Train/control_penalty   | 0.45038673   |
| Train/policy_loss       | 0.027938705  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01975      |
------------------------------------------

 ---------------- Iteration 293 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 292          |
| Time/Actor_Time         | 0.22         |
| Time/B_Format_Time      | 0.152        |
| Time/B_Original_Form... | 0.137        |
| Time/Buffer             | 0.00588      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24253424   |
| Train/Action_magnitu... | 0.6020126    |
| Train/Action_magnitude  | 0.47389135   |
| Train/Action_max        | 0.10438444   |
| Train/Action_std        | 0.18846744   |
| Train/Entropy           | -0.36756366  |
| Train/Entropy_Loss      | 0.000368     |
| Train/Entropy_loss      | 0.000368     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6772766    |
| Train/Loss              | 0.023181407  |
| Train/PolicyClip        | 0.0049795303 |
| Train/Policy_loss       | 0.018168492  |
| Train/Ratio             | 0.99791366   |
| Train/Return            | 1.3259829    |
| Train/V                 | 1.349033     |
| Train/Value             | 1.349033     |
| Train/control_penalty   | 0.46453512   |
| Train/policy_loss       | 0.018168492  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 294 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 293          |
| Time/Actor_Time         | 0.151        |
| Time/B_Format_Time      | 0.134        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00542      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2309481    |
| Train/Action_magnitu... | 0.57891256   |
| Train/Action_magnitude  | 0.45657605   |
| Train/Action_max        | 0.12052121   |
| Train/Action_std        | 0.19109726   |
| Train/Entropy           | -0.3671817   |
| Train/Entropy_Loss      | 0.000367     |
| Train/Entropy_loss      | 0.000367     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7440133    |
| Train/Loss              | -0.017799195 |
| Train/PolicyClip        | 0.007557598  |
| Train/Policy_loss       | -0.022812605 |
| Train/Ratio             | 1.0003176    |
| Train/Return            | 1.1964307    |
| Train/V                 | 1.1837776    |
| Train/Value             | 1.1837776    |
| Train/control_penalty   | 0.46462286   |
| Train/policy_loss       | -0.022812605 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 295 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 294          |
| Time/Actor_Time         | 0.151        |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.136        |
| Time/Buffer             | 0.0037       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22299449   |
| Train/Action_magnitu... | 0.5645925    |
| Train/Action_magnitude  | 0.4453703    |
| Train/Action_max        | 0.12804306   |
| Train/Action_std        | 0.19093528   |
| Train/Entropy           | -0.35556445  |
| Train/Entropy_Loss      | 0.000356     |
| Train/Entropy_loss      | 0.000356     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7253173    |
| Train/Loss              | -0.025387313 |
| Train/PolicyClip        | 0.008755282  |
| Train/Policy_loss       | -0.030202864 |
| Train/Ratio             | 0.99689454   |
| Train/Return            | 1.1413164    |
| Train/V                 | 1.1171297    |
| Train/Value             | 1.1171297    |
| Train/control_penalty   | 0.44599858   |
| Train/policy_loss       | -0.030202864 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 296 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 295          |
| Time/Actor_Time         | 0.112        |
| Time/B_Format_Time      | 0.0893       |
| Time/B_Original_Form... | 0.107        |
| Time/Buffer             | 0.00438      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23731863   |
| Train/Action_magnitu... | 0.5846139    |
| Train/Action_magnitude  | 0.4597599    |
| Train/Action_max        | 0.1113281    |
| Train/Action_std        | 0.18994123   |
| Train/Entropy           | -0.37150878  |
| Train/Entropy_Loss      | 0.000372     |
| Train/Entropy_loss      | 0.000372     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7104414    |
| Train/Loss              | -0.020412682 |
| Train/PolicyClip        | 0.007539257  |
| Train/Policy_loss       | -0.025339855 |
| Train/Ratio             | 0.9892079    |
| Train/Return            | 1.1306555    |
| Train/V                 | 1.1113046    |
| Train/Value             | 1.1113046    |
| Train/control_penalty   | 0.45556644   |
| Train/policy_loss       | -0.025339855 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01825      |
------------------------------------------

 ---------------- Iteration 297 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 296          |
| Time/Actor_Time         | 0.151        |
| Time/B_Format_Time      | 0.136        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00507      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21110511   |
| Train/Action_magnitu... | 0.5470064    |
| Train/Action_magnitude  | 0.43017632   |
| Train/Action_max        | 0.14041178   |
| Train/Action_std        | 0.18963      |
| Train/Entropy           | -0.37503958  |
| Train/Entropy_Loss      | 0.000375     |
| Train/Entropy_loss      | 0.000375     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7257018    |
| Train/Loss              | -0.017458934 |
| Train/PolicyClip        | 0.008177309  |
| Train/Policy_loss       | -0.02212904  |
| Train/Ratio             | 0.99831086   |
| Train/Return            | 0.95926      |
| Train/V                 | 0.949188     |
| Train/Value             | 0.949188     |
| Train/control_penalty   | 0.42950666   |
| Train/policy_loss       | -0.02212904  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01725      |
------------------------------------------

 ---------------- Iteration 298 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 297          |
| Time/Actor_Time         | 0.15         |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.137        |
| Time/Buffer             | 0.00482      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22634552   |
| Train/Action_magnitu... | 0.5760517    |
| Train/Action_magnitude  | 0.4529975    |
| Train/Action_max        | 0.124010466  |
| Train/Action_std        | 0.19175105   |
| Train/Entropy           | -0.3758549   |
| Train/Entropy_Loss      | 0.000376     |
| Train/Entropy_loss      | 0.000376     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.71001256   |
| Train/Loss              | -0.047022086 |
| Train/PolicyClip        | 0.0048690587 |
| Train/Policy_loss       | -0.051858597 |
| Train/Ratio             | 1.0035031    |
| Train/Return            | 1.1679212    |
| Train/V                 | 1.1197124    |
| Train/Value             | 1.1197124    |
| Train/control_penalty   | 0.44606555   |
| Train/policy_loss       | -0.051858597 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 299 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 298         |
| Time/Actor_Time         | 0.153       |
| Time/B_Format_Time      | 0.137       |
| Time/B_Original_Form... | 0.133       |
| Time/Buffer             | 0.00309     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21993689  |
| Train/Action_magnitu... | 0.55497676  |
| Train/Action_magnitude  | 0.43879017  |
| Train/Action_max        | 0.14087504  |
| Train/Action_std        | 0.18461996  |
| Train/Entropy           | -0.40163025 |
| Train/Entropy_Loss      | 0.000402    |
| Train/Entropy_loss      | 0.000402    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.7390067   |
| Train/Loss              | -0.08231723 |
| Train/PolicyClip        | 0.007571591 |
| Train/Policy_loss       | -0.08707764 |
| Train/Ratio             | 1.0009098   |
| Train/Return            | 1.0868782   |
| Train/V                 | 1.005861    |
| Train/Value             | 1.005861    |
| Train/control_penalty   | 0.43587768  |
| Train/policy_loss       | -0.08707764 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 300 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 299          |
| Time/Actor_Time         | 0.149        |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.133        |
| Time/Buffer             | 0.00461      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23554322   |
| Train/Action_magnitu... | 0.5846094    |
| Train/Action_magnitude  | 0.46064353   |
| Train/Action_max        | 0.12033025   |
| Train/Action_std        | 0.18525355   |
| Train/Entropy           | -0.396871    |
| Train/Entropy_Loss      | 0.000397     |
| Train/Entropy_loss      | 0.000397     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.74721974   |
| Train/Loss              | -0.039270744 |
| Train/PolicyClip        | 0.009599105  |
| Train/Policy_loss       | -0.044246536 |
| Train/Ratio             | 0.9933438    |
| Train/Return            | 1.3267485    |
| Train/V                 | 1.2884395    |
| Train/Value             | 1.2884395    |
| Train/control_penalty   | 0.4578923    |
| Train/policy_loss       | -0.044246536 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 301 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 300          |
| Time/Actor_Time         | 0.149        |
| Time/B_Format_Time      | 0.134        |
| Time/B_Original_Form... | 0.132        |
| Time/Buffer             | 0.00528      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2212205    |
| Train/Action_magnitu... | 0.5638163    |
| Train/Action_magnitude  | 0.4462677    |
| Train/Action_max        | 0.1505763    |
| Train/Action_std        | 0.1803081    |
| Train/Entropy           | -0.4075907   |
| Train/Entropy_Loss      | 0.000408     |
| Train/Entropy_loss      | 0.000408     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7259497    |
| Train/Loss              | -0.02048895  |
| Train/PolicyClip        | 0.008698777  |
| Train/Policy_loss       | -0.025229232 |
| Train/Ratio             | 0.99891835   |
| Train/Return            | 1.3372548    |
| Train/V                 | 1.3200879    |
| Train/Value             | 1.3200879    |
| Train/control_penalty   | 0.4332691    |
| Train/policy_loss       | -0.025229232 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 302 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 301          |
| Time/Actor_Time         | 0.149        |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.135        |
| Time/Buffer             | 0.00548      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24067865   |
| Train/Action_magnitu... | 0.5914857    |
| Train/Action_magnitude  | 0.46587032   |
| Train/Action_max        | 0.113738835  |
| Train/Action_std        | 0.18808948   |
| Train/Entropy           | -0.3767239   |
| Train/Entropy_Loss      | 0.000377     |
| Train/Entropy_loss      | 0.000377     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7203257    |
| Train/Loss              | -0.025300035 |
| Train/PolicyClip        | 0.0061919806 |
| Train/Policy_loss       | -0.030302452 |
| Train/Ratio             | 1.0021567    |
| Train/Return            | 1.3527638    |
| Train/V                 | 1.3287717    |
| Train/Value             | 1.3287717    |
| Train/control_penalty   | 0.46256936   |
| Train/policy_loss       | -0.030302452 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 303 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 302          |
| Time/Actor_Time         | 0.149        |
| Time/B_Format_Time      | 0.137        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00602      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21036436   |
| Train/Action_magnitu... | 0.5306544    |
| Train/Action_magnitude  | 0.4195876    |
| Train/Action_max        | 0.14317314   |
| Train/Action_std        | 0.18107699   |
| Train/Entropy           | -0.41183996  |
| Train/Entropy_Loss      | 0.000412     |
| Train/Entropy_loss      | 0.000412     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7746841    |
| Train/Loss              | -0.033766948 |
| Train/PolicyClip        | 0.007245543  |
| Train/Policy_loss       | -0.038414564 |
| Train/Ratio             | 0.99650514   |
| Train/Return            | 1.2978257    |
| Train/V                 | 1.2679871    |
| Train/Value             | 1.2679871    |
| Train/control_penalty   | 0.4235776    |
| Train/policy_loss       | -0.038414564 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 304 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 303         |
| Time/Actor_Time         | 0.154       |
| Time/B_Format_Time      | 0.132       |
| Time/B_Original_Form... | 0.133       |
| Time/Buffer             | 0.00444     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22790852  |
| Train/Action_magnitu... | 0.560764    |
| Train/Action_magnitude  | 0.44312394  |
| Train/Action_max        | 0.107328825 |
| Train/Action_std        | 0.18439803  |
| Train/Entropy           | -0.4059317  |
| Train/Entropy_Loss      | 0.000406    |
| Train/Entropy_loss      | 0.000406    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.7946895   |
| Train/Loss              | 0.017551234 |
| Train/PolicyClip        | 0.009519361 |
| Train/Policy_loss       | 0.012680803 |
| Train/Ratio             | 1.000279    |
| Train/Return            | 1.211004    |
| Train/V                 | 1.2292566   |
| Train/Value             | 1.2292566   |
| Train/control_penalty   | 0.44644982  |
| Train/policy_loss       | 0.012680803 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.021       |
-----------------------------------------

 ---------------- Iteration 305 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 304          |
| Time/Actor_Time         | 0.157        |
| Time/B_Format_Time      | 0.139        |
| Time/B_Original_Form... | 0.136        |
| Time/Buffer             | 0.0052       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23010045   |
| Train/Action_magnitu... | 0.5640395    |
| Train/Action_magnitude  | 0.44569692   |
| Train/Action_max        | 0.110582545  |
| Train/Action_std        | 0.18401022   |
| Train/Entropy           | -0.402191    |
| Train/Entropy_Loss      | 0.000402     |
| Train/Entropy_loss      | 0.000402     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.76425683   |
| Train/Loss              | -0.07401194  |
| Train/PolicyClip        | 0.0061663184 |
| Train/Policy_loss       | -0.078880355 |
| Train/Ratio             | 0.9997095    |
| Train/Return            | 1.2113181    |
| Train/V                 | 1.1413128    |
| Train/Value             | 1.1413128    |
| Train/control_penalty   | 0.44662264   |
| Train/policy_loss       | -0.078880355 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 306 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 305          |
| Time/Actor_Time         | 0.158        |
| Time/B_Format_Time      | 0.14         |
| Time/B_Original_Form... | 0.138        |
| Time/Buffer             | 0.00594      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24022427   |
| Train/Action_magnitu... | 0.58122444   |
| Train/Action_magnitude  | 0.45768726   |
| Train/Action_max        | 0.10974871   |
| Train/Action_std        | 0.18058039   |
| Train/Entropy           | -0.42452815  |
| Train/Entropy_Loss      | 0.000425     |
| Train/Entropy_loss      | 0.000425     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.78656507   |
| Train/Loss              | -0.034853112 |
| Train/PolicyClip        | 0.0061923373 |
| Train/Policy_loss       | -0.039877724 |
| Train/Ratio             | 0.99906075   |
| Train/Return            | 1.3744241    |
| Train/V                 | 1.3413414    |
| Train/Value             | 1.3413414    |
| Train/control_penalty   | 0.4600085    |
| Train/policy_loss       | -0.039877724 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02975      |
------------------------------------------

 ---------------- Iteration 307 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 306         |
| Time/Actor_Time         | 0.155       |
| Time/B_Format_Time      | 0.136       |
| Time/B_Original_Form... | 0.138       |
| Time/Buffer             | 0.00507     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21092415  |
| Train/Action_magnitu... | 0.5300052   |
| Train/Action_magnitude  | 0.41933283  |
| Train/Action_max        | 0.16225334  |
| Train/Action_std        | 0.18087293  |
| Train/Entropy           | -0.42677173 |
| Train/Entropy_Loss      | 0.000427    |
| Train/Entropy_loss      | 0.000427    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.8368578   |
| Train/Loss              | -0.03552926 |
| Train/PolicyClip        | 0.011169456 |
| Train/Policy_loss       | -0.04016366 |
| Train/Ratio             | 0.9989977   |
| Train/Return            | 1.248088    |
| Train/V                 | 1.2146013   |
| Train/Value             | 1.2146013   |
| Train/control_penalty   | 0.4207626   |
| Train/policy_loss       | -0.04016366 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 308 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 307         |
| Time/Actor_Time         | 0.165       |
| Time/B_Format_Time      | 0.138       |
| Time/B_Original_Form... | 0.135       |
| Time/Buffer             | 0.00597     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22665259  |
| Train/Action_magnitu... | 0.5633777   |
| Train/Action_magnitude  | 0.44366488  |
| Train/Action_max        | 0.1277566   |
| Train/Action_std        | 0.18993616  |
| Train/Entropy           | -0.355445   |
| Train/Entropy_Loss      | 0.000355    |
| Train/Entropy_loss      | 0.000355    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.7171987   |
| Train/Loss              | 0.01319864  |
| Train/PolicyClip        | 0.008833878 |
| Train/Policy_loss       | 0.008374441 |
| Train/Ratio             | 0.99824226  |
| Train/Return            | 0.98682576  |
| Train/V                 | 1.0029869   |
| Train/Value             | 1.0029869   |
| Train/control_penalty   | 0.44687542  |
| Train/policy_loss       | 0.008374441 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 309 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 308          |
| Time/Actor_Time         | 0.169        |
| Time/B_Format_Time      | 0.134        |
| Time/B_Original_Form... | 0.135        |
| Time/Buffer             | 0.005        |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24137793   |
| Train/Action_magnitu... | 0.5925166    |
| Train/Action_magnitude  | 0.4668543    |
| Train/Action_max        | 0.13745992   |
| Train/Action_std        | 0.18935019   |
| Train/Entropy           | -0.3643985   |
| Train/Entropy_Loss      | 0.000364     |
| Train/Entropy_loss      | 0.000364     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.63031995   |
| Train/Loss              | -0.032329127 |
| Train/PolicyClip        | 0.010749238  |
| Train/Policy_loss       | -0.037306983 |
| Train/Ratio             | 0.99701244   |
| Train/Return            | 1.0884436    |
| Train/V                 | 1.0591778    |
| Train/Value             | 1.0591778    |
| Train/control_penalty   | 0.46134573   |
| Train/policy_loss       | -0.037306983 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 310 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 309          |
| Time/Actor_Time         | 0.154        |
| Time/B_Format_Time      | 0.147        |
| Time/B_Original_Form... | 0.14         |
| Time/Buffer             | 0.00513      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22998996   |
| Train/Action_magnitu... | 0.5754408    |
| Train/Action_magnitude  | 0.45424855   |
| Train/Action_max        | 0.13274612   |
| Train/Action_std        | 0.18980618   |
| Train/Entropy           | -0.36972454  |
| Train/Entropy_Loss      | 0.00037      |
| Train/Entropy_loss      | 0.00037      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7334984    |
| Train/Loss              | -0.006619384 |
| Train/PolicyClip        | 0.005803958  |
| Train/Policy_loss       | -0.011518353 |
| Train/Ratio             | 0.9924948    |
| Train/Return            | 1.0481571    |
| Train/V                 | 1.0417526    |
| Train/Value             | 1.0417526    |
| Train/control_penalty   | 0.4529244    |
| Train/policy_loss       | -0.011518353 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 311 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 310          |
| Time/Actor_Time         | 0.162        |
| Time/B_Format_Time      | 0.14         |
| Time/B_Original_Form... | 0.138        |
| Time/Buffer             | 0.00528      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22821009   |
| Train/Action_magnitu... | 0.5554474    |
| Train/Action_magnitude  | 0.4354225    |
| Train/Action_max        | 0.15645133   |
| Train/Action_std        | 0.18637176   |
| Train/Entropy           | -0.39359123  |
| Train/Entropy_Loss      | 0.000394     |
| Train/Entropy_loss      | 0.000394     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.74282867   |
| Train/Loss              | 0.06130255   |
| Train/PolicyClip        | 0.0073127183 |
| Train/Policy_loss       | 0.05643429   |
| Train/Ratio             | 0.9940204    |
| Train/Return            | 1.0084393    |
| Train/V                 | 1.0685011    |
| Train/Value             | 1.0685011    |
| Train/control_penalty   | 0.44746715   |
| Train/policy_loss       | 0.05643429   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.016        |
------------------------------------------

 ---------------- Iteration 312 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 311         |
| Time/Actor_Time         | 0.153       |
| Time/B_Format_Time      | 0.135       |
| Time/B_Original_Form... | 0.136       |
| Time/Buffer             | 0.00564     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23977701  |
| Train/Action_magnitu... | 0.6023932   |
| Train/Action_magnitude  | 0.4710721   |
| Train/Action_max        | 0.10800843  |
| Train/Action_std        | 0.19488633  |
| Train/Entropy           | -0.34387997 |
| Train/Entropy_Loss      | 0.000344    |
| Train/Entropy_loss      | 0.000344    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6872929   |
| Train/Loss              | 0.04113587  |
| Train/PolicyClip        | 0.006457763 |
| Train/Policy_loss       | 0.03605886  |
| Train/Ratio             | 0.9896496   |
| Train/Return            | 1.0140092   |
| Train/V                 | 1.0556456   |
| Train/Value             | 1.0556456   |
| Train/control_penalty   | 0.4733126   |
| Train/policy_loss       | 0.03605886  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01525     |
-----------------------------------------

 ---------------- Iteration 313 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 312         |
| Time/Actor_Time         | 0.149       |
| Time/B_Format_Time      | 0.134       |
| Time/B_Original_Form... | 0.13        |
| Time/Buffer             | 0.00342     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23914975  |
| Train/Action_magnitu... | 0.58646935  |
| Train/Action_magnitude  | 0.4592556   |
| Train/Action_max        | 0.11579813  |
| Train/Action_std        | 0.18706656  |
| Train/Entropy           | -0.3794427  |
| Train/Entropy_Loss      | 0.000379    |
| Train/Entropy_loss      | 0.000379    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.7220063   |
| Train/Loss              | 0.036920812 |
| Train/PolicyClip        | 0.004104271 |
| Train/Policy_loss       | 0.031967647 |
| Train/Ratio             | 0.9916208   |
| Train/Return            | 0.9611535   |
| Train/V                 | 0.9996567   |
| Train/Value             | 0.9996567   |
| Train/control_penalty   | 0.45737228  |
| Train/policy_loss       | 0.031967647 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0165      |
-----------------------------------------

 ---------------- Iteration 314 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 313         |
| Time/Actor_Time         | 0.153       |
| Time/B_Format_Time      | 0.141       |
| Time/B_Original_Form... | 0.143       |
| Time/Buffer             | 0.0051      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22750643  |
| Train/Action_magnitu... | 0.5827658   |
| Train/Action_magnitude  | 0.45567134  |
| Train/Action_max        | 0.13240771  |
| Train/Action_std        | 0.20288321  |
| Train/Entropy           | -0.28656125 |
| Train/Entropy_Loss      | 0.000287    |
| Train/Entropy_loss      | 0.000287    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6147653   |
| Train/Loss              | 0.060216196 |
| Train/PolicyClip        | 0.005553709 |
| Train/Policy_loss       | 0.055348523 |
| Train/Ratio             | 0.9795219   |
| Train/Return            | 0.9607108   |
| Train/V                 | 1.0197496   |
| Train/Value             | 1.0197496   |
| Train/control_penalty   | 0.4581111   |
| Train/policy_loss       | 0.055348523 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01475     |
-----------------------------------------

 ---------------- Iteration 315 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 314         |
| Time/Actor_Time         | 0.123       |
| Time/B_Format_Time      | 0.104       |
| Time/B_Original_Form... | 0.108       |
| Time/Buffer             | 0.00443     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2217601   |
| Train/Action_magnitu... | 0.5724036   |
| Train/Action_magnitude  | 0.44744927  |
| Train/Action_max        | 0.15467355  |
| Train/Action_std        | 0.22486626  |
| Train/Entropy           | -0.18073496 |
| Train/Entropy_Loss      | 0.000181    |
| Train/Entropy_loss      | 0.000181    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5315867   |
| Train/Loss              | 0.08227892  |
| Train/PolicyClip        | 0.00485331  |
| Train/Policy_loss       | 0.077356316 |
| Train/Ratio             | 0.963843    |
| Train/Return            | 0.7707663   |
| Train/V                 | 0.8564561   |
| Train/Value             | 0.8564561   |
| Train/control_penalty   | 0.47418723  |
| Train/policy_loss       | 0.077356316 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.009       |
-----------------------------------------

 ---------------- Iteration 316 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 315         |
| Time/Actor_Time         | 0.151       |
| Time/B_Format_Time      | 0.133       |
| Time/B_Original_Form... | 0.135       |
| Time/Buffer             | 0.00451     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23330079  |
| Train/Action_magnitu... | 0.59788644  |
| Train/Action_magnitude  | 0.46678665  |
| Train/Action_max        | 0.15397483  |
| Train/Action_std        | 0.22514834  |
| Train/Entropy           | -0.17989871 |
| Train/Entropy_Loss      | 0.00018     |
| Train/Entropy_loss      | 0.00018     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.39925036  |
| Train/Loss              | 0.03678015  |
| Train/PolicyClip        | 0.00514824  |
| Train/Policy_loss       | 0.03182665  |
| Train/Ratio             | 0.9856536   |
| Train/Return            | 0.83802366  |
| Train/V                 | 0.87550235  |
| Train/Value             | 0.87550235  |
| Train/control_penalty   | 0.47736016  |
| Train/policy_loss       | 0.03182665  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01425     |
-----------------------------------------

 ---------------- Iteration 317 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 316          |
| Time/Actor_Time         | 0.165        |
| Time/B_Format_Time      | 0.179        |
| Time/B_Original_Form... | 0.267        |
| Time/Buffer             | 0.0164       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2325406    |
| Train/Action_magnitu... | 0.61243165   |
| Train/Action_magnitude  | 0.4787688    |
| Train/Action_max        | 0.13936196   |
| Train/Action_std        | 0.22786321   |
| Train/Entropy           | -0.16579716  |
| Train/Entropy_Loss      | 0.000166     |
| Train/Entropy_loss      | 0.000166     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.34513438   |
| Train/Loss              | 0.03941058   |
| Train/PolicyClip        | 0.0029584893 |
| Train/Policy_loss       | 0.034417883  |
| Train/Ratio             | 0.98881626   |
| Train/Return            | 0.8563086    |
| Train/V                 | 0.89767945   |
| Train/Value             | 0.89767945   |
| Train/control_penalty   | 0.48268995   |
| Train/policy_loss       | 0.034417883  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0135       |
------------------------------------------

 ---------------- Iteration 318 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 317          |
| Time/Actor_Time         | 0.154        |
| Time/B_Format_Time      | 0.139        |
| Time/B_Original_Form... | 0.164        |
| Time/Buffer             | 0.00717      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.23633456   |
| Train/Action_magnitu... | 0.62020034   |
| Train/Action_magnitude  | 0.4851608    |
| Train/Action_max        | 0.16760305   |
| Train/Action_std        | 0.22658847   |
| Train/Entropy           | -0.18973166  |
| Train/Entropy_Loss      | 0.00019      |
| Train/Entropy_loss      | 0.00019      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.30281347   |
| Train/Loss              | -0.018000394 |
| Train/PolicyClip        | 0.00909229   |
| Train/Policy_loss       | -0.0230933   |
| Train/Ratio             | 1.0001887    |
| Train/Return            | 0.9293526    |
| Train/V                 | 0.9178399    |
| Train/Value             | 0.9178399    |
| Train/control_penalty   | 0.4903174    |
| Train/policy_loss       | -0.0230933   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0195       |
------------------------------------------

 ---------------- Iteration 319 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 318          |
| Time/Actor_Time         | 0.159        |
| Time/B_Format_Time      | 0.132        |
| Time/B_Original_Form... | 0.143        |
| Time/Buffer             | 0.00594      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22751066   |
| Train/Action_magnitu... | 0.60588384   |
| Train/Action_magnitude  | 0.47653288   |
| Train/Action_max        | 0.15139282   |
| Train/Action_std        | 0.21751687   |
| Train/Entropy           | -0.20359549  |
| Train/Entropy_Loss      | 0.000204     |
| Train/Entropy_loss      | 0.000204     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.35011157   |
| Train/Loss              | -0.063607074 |
| Train/PolicyClip        | 0.007882926  |
| Train/Policy_loss       | -0.0685083   |
| Train/Ratio             | 1.0026369    |
| Train/Return            | 0.8783621    |
| Train/V                 | 0.8169863    |
| Train/Value             | 0.8169863    |
| Train/control_penalty   | 0.46976292   |
| Train/policy_loss       | -0.0685083   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 320 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 319          |
| Time/Actor_Time         | 0.15         |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.138        |
| Time/Buffer             | 0.00566      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22442693   |
| Train/Action_magnitu... | 0.5870271    |
| Train/Action_magnitude  | 0.46155602   |
| Train/Action_max        | 0.14353012   |
| Train/Action_std        | 0.20811541   |
| Train/Entropy           | -0.2526003   |
| Train/Entropy_Loss      | 0.000253     |
| Train/Entropy_loss      | 0.000253     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.45114467   |
| Train/Loss              | -0.09887503  |
| Train/PolicyClip        | 0.0039841277 |
| Train/Policy_loss       | -0.1037293   |
| Train/Ratio             | 1.0056158    |
| Train/Return            | 1.1072124    |
| Train/V                 | 1.0107415    |
| Train/Value             | 1.0107415    |
| Train/control_penalty   | 0.4601675    |
| Train/policy_loss       | -0.1037293   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02875      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 321 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 320          |
| Time/Actor_Time         | 0.152        |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.136        |
| Time/Buffer             | 0.00568      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22545218   |
| Train/Action_magnitu... | 0.5830988    |
| Train/Action_magnitude  | 0.45992294   |
| Train/Action_max        | 0.14341578   |
| Train/Action_std        | 0.21245666   |
| Train/Entropy           | -0.22928022  |
| Train/Entropy_Loss      | 0.000229     |
| Train/Entropy_loss      | 0.000229     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4509047    |
| Train/Loss              | -0.048211362 |
| Train/PolicyClip        | 0.0024380693 |
| Train/Policy_loss       | -0.053023983 |
| Train/Ratio             | 1.000326     |
| Train/Return            | 1.2011806    |
| Train/V                 | 1.155941     |
| Train/Value             | 1.155941     |
| Train/control_penalty   | 0.4583341    |
| Train/policy_loss       | -0.053023983 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 322 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 321          |
| Time/Actor_Time         | 0.153        |
| Time/B_Format_Time      | 0.134        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00562      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21924688   |
| Train/Action_magnitu... | 0.5733591    |
| Train/Action_magnitude  | 0.45033398   |
| Train/Action_max        | 0.13481696   |
| Train/Action_std        | 0.2067743    |
| Train/Entropy           | -0.25859433  |
| Train/Entropy_Loss      | 0.000259     |
| Train/Entropy_loss      | 0.000259     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.48409742   |
| Train/Loss              | -0.067610525 |
| Train/PolicyClip        | 0.0038436656 |
| Train/Policy_loss       | -0.07233298  |
| Train/Ratio             | 1.0025706    |
| Train/Return            | 1.2190992    |
| Train/V                 | 1.1531036    |
| Train/Value             | 1.1531036    |
| Train/control_penalty   | 0.44638586   |
| Train/policy_loss       | -0.07233298  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.029        |
------------------------------------------

 ---------------- Iteration 323 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 322          |
| Time/Actor_Time         | 0.148        |
| Time/B_Format_Time      | 0.131        |
| Time/B_Original_Form... | 0.131        |
| Time/Buffer             | 0.0055       |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.22087382   |
| Train/Action_magnitu... | 0.5716832    |
| Train/Action_magnitude  | 0.45100978   |
| Train/Action_max        | 0.1532564    |
| Train/Action_std        | 0.21491523   |
| Train/Entropy           | -0.2206487   |
| Train/Entropy_Loss      | 0.000221     |
| Train/Entropy_loss      | 0.000221     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.43735832   |
| Train/Loss              | -0.06798537  |
| Train/PolicyClip        | 0.0059336997 |
| Train/Policy_loss       | -0.072720475 |
| Train/Ratio             | 1.0047129    |
| Train/Return            | 1.0666775    |
| Train/V                 | 0.9993596    |
| Train/Value             | 0.9993596    |
| Train/control_penalty   | 0.45144534   |
| Train/policy_loss       | -0.072720475 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 324 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 323          |
| Time/Actor_Time         | 0.152        |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00632      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2197747    |
| Train/Action_magnitu... | 0.5641728    |
| Train/Action_magnitude  | 0.44439116   |
| Train/Action_max        | 0.15508066   |
| Train/Action_std        | 0.20910503   |
| Train/Entropy           | -0.25340572  |
| Train/Entropy_Loss      | 0.000253     |
| Train/Entropy_loss      | 0.000253     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.48983118   |
| Train/Loss              | -0.032081235 |
| Train/PolicyClip        | 0.0065474706 |
| Train/Policy_loss       | -0.03683294  |
| Train/Ratio             | 1.0056791    |
| Train/Return            | 1.1230226    |
| Train/V                 | 1.087749     |
| Train/Value             | 1.087749     |
| Train/control_penalty   | 0.44982988   |
| Train/policy_loss       | -0.03683294  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 325 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 324          |
| Time/Actor_Time         | 0.151        |
| Time/B_Format_Time      | 0.135        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.0052       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2207803    |
| Train/Action_magnitu... | 0.5772264    |
| Train/Action_magnitude  | 0.45659053   |
| Train/Action_max        | 0.15927851   |
| Train/Action_std        | 0.2058517    |
| Train/Entropy           | -0.2736902   |
| Train/Entropy_Loss      | 0.000274     |
| Train/Entropy_loss      | 0.000274     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4887241    |
| Train/Loss              | 0.011838384  |
| Train/PolicyClip        | 0.007210725  |
| Train/Policy_loss       | 0.0070696827 |
| Train/Ratio             | 1.0021139    |
| Train/Return            | 1.1722337    |
| Train/V                 | 1.1868639    |
| Train/Value             | 1.1868639    |
| Train/control_penalty   | 0.44950113   |
| Train/policy_loss       | 0.0070696827 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 326 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 325         |
| Time/Actor_Time         | 0.153       |
| Time/B_Format_Time      | 0.137       |
| Time/B_Original_Form... | 0.135       |
| Time/Buffer             | 0.00505     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20663679  |
| Train/Action_magnitu... | 0.5437602   |
| Train/Action_magnitude  | 0.42922044  |
| Train/Action_max        | 0.15188372  |
| Train/Action_std        | 0.22450587  |
| Train/Entropy           | -0.17532247 |
| Train/Entropy_Loss      | 0.000175    |
| Train/Entropy_loss      | 0.000175    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.4182851   |
| Train/Loss              | 0.082658395 |
| Train/PolicyClip        | 0.004171684 |
| Train/Policy_loss       | 0.07799647  |
| Train/Ratio             | 0.98482144  |
| Train/Return            | 1.0016227   |
| Train/V                 | 1.0844734   |
| Train/Value             | 1.0844734   |
| Train/control_penalty   | 0.44866094  |
| Train/policy_loss       | 0.07799647  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01425     |
-----------------------------------------

 ---------------- Iteration 327 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 326         |
| Time/Actor_Time         | 0.124       |
| Time/B_Format_Time      | 0.117       |
| Time/B_Original_Form... | 0.112       |
| Time/Buffer             | 0.00404     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.20586759  |
| Train/Action_magnitu... | 0.55548215  |
| Train/Action_magnitude  | 0.43865204  |
| Train/Action_max        | 0.14214838  |
| Train/Action_std        | 0.20876117  |
| Train/Entropy           | -0.2592749  |
| Train/Entropy_Loss      | 0.000259    |
| Train/Entropy_loss      | 0.000259    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.43420884  |
| Train/Loss              | -0.07602809 |
| Train/PolicyClip        | 0.004207501 |
| Train/Policy_loss       | -0.08067706 |
| Train/Ratio             | 1.0078585   |
| Train/Return            | 1.0784192   |
| Train/V                 | 1.004156    |
| Train/Value             | 1.004156    |
| Train/control_penalty   | 0.43896922  |
| Train/policy_loss       | -0.08067706 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 328 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 327          |
| Time/Actor_Time         | 0.109        |
| Time/B_Format_Time      | 0.0964       |
| Time/B_Original_Form... | 0.0981       |
| Time/Buffer             | 0.00428      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20659427   |
| Train/Action_magnitu... | 0.55504906   |
| Train/Action_magnitude  | 0.4396406    |
| Train/Action_max        | 0.15242709   |
| Train/Action_std        | 0.2106569    |
| Train/Entropy           | -0.23823449  |
| Train/Entropy_Loss      | 0.000238     |
| Train/Entropy_loss      | 0.000238     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.47019383   |
| Train/Loss              | 0.007549747  |
| Train/PolicyClip        | 0.006681515  |
| Train/Policy_loss       | 0.0029952666 |
| Train/Ratio             | 0.9997536    |
| Train/Return            | 0.9781254    |
| Train/V                 | 0.9879617    |
| Train/Value             | 0.9879617    |
| Train/control_penalty   | 0.43162462   |
| Train/policy_loss       | 0.0029952666 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 329 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 328          |
| Time/Actor_Time         | 0.105        |
| Time/B_Format_Time      | 0.0918       |
| Time/B_Original_Form... | 0.0949       |
| Time/Buffer             | 0.00348      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2231927    |
| Train/Action_magnitu... | 0.5810631    |
| Train/Action_magnitude  | 0.45849413   |
| Train/Action_max        | 0.14518975   |
| Train/Action_std        | 0.21269579   |
| Train/Entropy           | -0.23144792  |
| Train/Entropy_Loss      | 0.000231     |
| Train/Entropy_loss      | 0.000231     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4113319    |
| Train/Loss              | -0.00488846  |
| Train/PolicyClip        | 0.0062563065 |
| Train/Policy_loss       | -0.009697739 |
| Train/Ratio             | 0.99905336   |
| Train/Return            | 1.006649     |
| Train/V                 | 1.0006421    |
| Train/Value             | 1.0006421    |
| Train/control_penalty   | 0.45778313   |
| Train/policy_loss       | -0.009697739 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 330 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 329          |
| Time/Actor_Time         | 0.1          |
| Time/B_Format_Time      | 0.0903       |
| Time/B_Original_Form... | 0.0919       |
| Time/Buffer             | 0.00396      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22239761   |
| Train/Action_magnitu... | 0.5702004    |
| Train/Action_magnitude  | 0.44946754   |
| Train/Action_max        | 0.15723398   |
| Train/Action_std        | 0.21492846   |
| Train/Entropy           | -0.22920457  |
| Train/Entropy_Loss      | 0.000229     |
| Train/Entropy_loss      | 0.000229     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.44979134   |
| Train/Loss              | 0.021574914  |
| Train/PolicyClip        | 0.0049714795 |
| Train/Policy_loss       | 0.016834956  |
| Train/Ratio             | 0.98987156   |
| Train/Return            | 1.0588716    |
| Train/V                 | 1.0863347    |
| Train/Value             | 1.0863347    |
| Train/control_penalty   | 0.45107552   |
| Train/policy_loss       | 0.016834956  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.019        |
------------------------------------------

 ---------------- Iteration 331 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 330          |
| Time/Actor_Time         | 0.101        |
| Time/B_Format_Time      | 0.09         |
| Time/B_Original_Form... | 0.0909       |
| Time/Buffer             | 0.00338      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21895826   |
| Train/Action_magnitu... | 0.5696526    |
| Train/Action_magnitude  | 0.44853467   |
| Train/Action_max        | 0.16243726   |
| Train/Action_std        | 0.20271844   |
| Train/Entropy           | -0.28604862  |
| Train/Entropy_Loss      | 0.000286     |
| Train/Entropy_loss      | 0.000286     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4314997    |
| Train/Loss              | -0.07393708  |
| Train/PolicyClip        | 0.0064638397 |
| Train/Policy_loss       | -0.07863861  |
| Train/Ratio             | 1.0014794    |
| Train/Return            | 1.0038924    |
| Train/V                 | 0.9296311    |
| Train/Value             | 0.9296311    |
| Train/control_penalty   | 0.44154793   |
| Train/policy_loss       | -0.07863861  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 332 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 331          |
| Time/Actor_Time         | 0.0979       |
| Time/B_Format_Time      | 0.0875       |
| Time/B_Original_Form... | 0.0871       |
| Time/Buffer             | 0.00374      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21267362   |
| Train/Action_magnitu... | 0.56034917   |
| Train/Action_magnitude  | 0.4402765    |
| Train/Action_max        | 0.14176393   |
| Train/Action_std        | 0.19696997   |
| Train/Entropy           | -0.3046347   |
| Train/Entropy_Loss      | 0.000305     |
| Train/Entropy_loss      | 0.000305     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.537524     |
| Train/Loss              | -0.033356305 |
| Train/PolicyClip        | 0.007080801  |
| Train/Policy_loss       | -0.037955686 |
| Train/Ratio             | 1.001752     |
| Train/Return            | 1.0281779    |
| Train/V                 | 0.99599314   |
| Train/Value             | 0.99599314   |
| Train/control_penalty   | 0.4294746    |
| Train/policy_loss       | -0.037955686 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 333 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 332         |
| Time/Actor_Time         | 0.0935      |
| Time/B_Format_Time      | 0.0821      |
| Time/B_Original_Form... | 0.0854      |
| Time/Buffer             | 0.00312     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.21907473  |
| Train/Action_magnitu... | 0.5694253   |
| Train/Action_magnitude  | 0.44947594  |
| Train/Action_max        | 0.19007719  |
| Train/Action_std        | 0.2092024   |
| Train/Entropy           | -0.2506352  |
| Train/Entropy_Loss      | 0.000251    |
| Train/Entropy_loss      | 0.000251    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.4560451   |
| Train/Loss              | -0.08387075 |
| Train/PolicyClip        | 0.006186277 |
| Train/Policy_loss       | -0.08862827 |
| Train/Ratio             | 1.0037231   |
| Train/Return            | 1.1351227   |
| Train/V                 | 1.0511816   |
| Train/Value             | 1.0511816   |
| Train/control_penalty   | 0.4506875   |
| Train/policy_loss       | -0.08862827 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 334 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 333           |
| Time/Actor_Time         | 0.0906        |
| Time/B_Format_Time      | 0.0809        |
| Time/B_Original_Form... | 0.0813        |
| Time/Buffer             | 0.00308       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.22062308    |
| Train/Action_magnitu... | 0.55101454    |
| Train/Action_magnitude  | 0.43365076    |
| Train/Action_max        | 0.15689066    |
| Train/Action_std        | 0.20814759    |
| Train/Entropy           | -0.25449222   |
| Train/Entropy_Loss      | 0.000254      |
| Train/Entropy_loss      | 0.000254      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.5230746     |
| Train/Loss              | 0.0046355785  |
| Train/PolicyClip        | 0.008027551   |
| Train/Policy_loss       | -6.331492e-05 |
| Train/Ratio             | 1.0000514     |
| Train/Return            | 1.1659197     |
| Train/V                 | 1.1742566     |
| Train/Value             | 1.1742566     |
| Train/control_penalty   | 0.44444016    |
| Train/policy_loss       | -6.331492e-05 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02175       |
-------------------------------------------

 ---------------- Iteration 335 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 334         |
| Time/Actor_Time         | 0.0926      |
| Time/B_Format_Time      | 0.0808      |
| Time/B_Original_Form... | 0.0869      |
| Time/Buffer             | 0.00316     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20713408  |
| Train/Action_magnitu... | 0.548045    |
| Train/Action_magnitude  | 0.43385988  |
| Train/Action_max        | 0.17735611  |
| Train/Action_std        | 0.20172311  |
| Train/Entropy           | -0.29919055 |
| Train/Entropy_Loss      | 0.000299    |
| Train/Entropy_loss      | 0.000299    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5198157   |
| Train/Loss              | -0.09227924 |
| Train/PolicyClip        | 0.005674075 |
| Train/Policy_loss       | -0.09685844 |
| Train/Ratio             | 1.005977    |
| Train/Return            | 1.1443487   |
| Train/V                 | 1.0547434   |
| Train/Value             | 1.0547434   |
| Train/control_penalty   | 0.42800108  |
| Train/policy_loss       | -0.09685844 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 336 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 335          |
| Time/Actor_Time         | 0.0875       |
| Time/B_Format_Time      | 0.0793       |
| Time/B_Original_Form... | 0.0789       |
| Time/Buffer             | 0.00317      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22791557   |
| Train/Action_magnitu... | 0.5654017    |
| Train/Action_magnitude  | 0.4422198    |
| Train/Action_max        | 0.14429834   |
| Train/Action_std        | 0.19190295   |
| Train/Entropy           | -0.3379362   |
| Train/Entropy_Loss      | 0.000338     |
| Train/Entropy_loss      | 0.000338     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.62887555   |
| Train/Loss              | 0.020572085  |
| Train/PolicyClip        | 0.0048661544 |
| Train/Policy_loss       | 0.015764127  |
| Train/Ratio             | 0.98085475   |
| Train/Return            | 1.1646286    |
| Train/V                 | 1.1876764    |
| Train/Value             | 1.1876764    |
| Train/control_penalty   | 0.44700217   |
| Train/policy_loss       | 0.015764127  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 337 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 336          |
| Time/Actor_Time         | 0.0977       |
| Time/B_Format_Time      | 0.0892       |
| Time/B_Original_Form... | 0.083        |
| Time/Buffer             | 0.00379      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21255362   |
| Train/Action_magnitu... | 0.53837556   |
| Train/Action_magnitude  | 0.42313483   |
| Train/Action_max        | 0.14970107   |
| Train/Action_std        | 0.18581513   |
| Train/Entropy           | -0.3676968   |
| Train/Entropy_Loss      | 0.000368     |
| Train/Entropy_loss      | 0.000368     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6677288    |
| Train/Loss              | -0.029168943 |
| Train/PolicyClip        | 0.0054462994 |
| Train/Policy_loss       | -0.033745654 |
| Train/Ratio             | 0.9995698    |
| Train/Return            | 1.1968647    |
| Train/V                 | 1.166249     |
| Train/Value             | 1.166249     |
| Train/control_penalty   | 0.42090142   |
| Train/policy_loss       | -0.033745654 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 338 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 337          |
| Time/Actor_Time         | 0.106        |
| Time/B_Format_Time      | 0.103        |
| Time/B_Original_Form... | 0.0972       |
| Time/Buffer             | 0.00318      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22022495   |
| Train/Action_magnitu... | 0.54201317   |
| Train/Action_magnitude  | 0.42736432   |
| Train/Action_max        | 0.1668432    |
| Train/Action_std        | 0.18851648   |
| Train/Entropy           | -0.3564808   |
| Train/Entropy_Loss      | 0.000356     |
| Train/Entropy_loss      | 0.000356     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6696937    |
| Train/Loss              | -0.07866284  |
| Train/PolicyClip        | 0.0064841006 |
| Train/Policy_loss       | -0.08329175  |
| Train/Ratio             | 1.0035403    |
| Train/Return            | 1.088968     |
| Train/V                 | 1.0105848    |
| Train/Value             | 1.0105848    |
| Train/control_penalty   | 0.42724207   |
| Train/policy_loss       | -0.08329175  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 339 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 338         |
| Time/Actor_Time         | 0.109       |
| Time/B_Format_Time      | 0.0997      |
| Time/B_Original_Form... | 0.12        |
| Time/Buffer             | 0.00337     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22268018  |
| Train/Action_magnitu... | 0.5405985   |
| Train/Action_magnitude  | 0.42513126  |
| Train/Action_max        | 0.18039705  |
| Train/Action_std        | 0.18426628  |
| Train/Entropy           | -0.38436943 |
| Train/Entropy_Loss      | 0.000384    |
| Train/Entropy_loss      | 0.000384    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6938204   |
| Train/Loss              | 0.030073833 |
| Train/PolicyClip        | 0.00945367  |
| Train/Policy_loss       | 0.02541476  |
| Train/Ratio             | 0.99239177  |
| Train/Return            | 1.1510547   |
| Train/V                 | 1.1791608   |
| Train/Value             | 1.1791608   |
| Train/control_penalty   | 0.42747033  |
| Train/policy_loss       | 0.02541476  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 340 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 339          |
| Time/Actor_Time         | 0.0912       |
| Time/B_Format_Time      | 0.0806       |
| Time/B_Original_Form... | 0.0775       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22354428   |
| Train/Action_magnitu... | 0.56064075   |
| Train/Action_magnitude  | 0.4421815    |
| Train/Action_max        | 0.17147379   |
| Train/Action_std        | 0.19505104   |
| Train/Entropy           | -0.33169323  |
| Train/Entropy_Loss      | 0.000332     |
| Train/Entropy_loss      | 0.000332     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6610888    |
| Train/Loss              | -0.033990327 |
| Train/PolicyClip        | 0.007851924  |
| Train/Policy_loss       | -0.038756866 |
| Train/Ratio             | 1.0029385    |
| Train/Return            | 1.215656     |
| Train/V                 | 1.1841327    |
| Train/Value             | 1.1841327    |
| Train/control_penalty   | 0.44348451   |
| Train/policy_loss       | -0.038756866 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 341 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 340           |
| Time/Actor_Time         | 0.0889        |
| Time/B_Format_Time      | 0.0778        |
| Time/B_Original_Form... | 0.0815        |
| Time/Buffer             | 0.00369       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.22248194    |
| Train/Action_magnitu... | 0.5748948     |
| Train/Action_magnitude  | 0.45350954    |
| Train/Action_max        | 0.16209489    |
| Train/Action_std        | 0.20576268    |
| Train/Entropy           | -0.2757832    |
| Train/Entropy_Loss      | 0.000276      |
| Train/Entropy_loss      | 0.000276      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.56135666    |
| Train/Loss              | 0.00047810562 |
| Train/PolicyClip        | 0.010642327   |
| Train/Policy_loss       | -0.0043373494 |
| Train/Ratio             | 0.9903595     |
| Train/Return            | 1.2399361     |
| Train/V                 | 1.2419041     |
| Train/Value             | 1.2419041     |
| Train/control_penalty   | 0.4539672     |
| Train/policy_loss       | -0.0043373494 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02575       |
-------------------------------------------

 ---------------- Iteration 342 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 341           |
| Time/Actor_Time         | 0.0849        |
| Time/B_Format_Time      | 0.0716        |
| Time/B_Original_Form... | 0.0765        |
| Time/Buffer             | 0.00449       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.21498251    |
| Train/Action_magnitu... | 0.54528797    |
| Train/Action_magnitude  | 0.42905214    |
| Train/Action_max        | 0.18956251    |
| Train/Action_std        | 0.19352642    |
| Train/Entropy           | -0.3337265    |
| Train/Entropy_Loss      | 0.000334      |
| Train/Entropy_loss      | 0.000334      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.6241765     |
| Train/Loss              | 0.0026739915  |
| Train/PolicyClip        | 0.010204943   |
| Train/Policy_loss       | -0.0019454441 |
| Train/Ratio             | 1.0046728     |
| Train/Return            | 1.1255609     |
| Train/V                 | 1.1326948     |
| Train/Value             | 1.1326948     |
| Train/control_penalty   | 0.4285709     |
| Train/policy_loss       | -0.0019454441 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02025       |
-------------------------------------------

 ---------------- Iteration 343 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 342          |
| Time/Actor_Time         | 0.0917       |
| Time/B_Format_Time      | 0.0808       |
| Time/B_Original_Form... | 0.079        |
| Time/Buffer             | 0.00344      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.22202574   |
| Train/Action_magnitu... | 0.54417306   |
| Train/Action_magnitude  | 0.4293624    |
| Train/Action_max        | 0.20134743   |
| Train/Action_std        | 0.18456696   |
| Train/Entropy           | -0.4018241   |
| Train/Entropy_Loss      | 0.000402     |
| Train/Entropy_loss      | 0.000402     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7069249    |
| Train/Loss              | -0.07699343  |
| Train/PolicyClip        | 0.0032693574 |
| Train/Policy_loss       | -0.08171809  |
| Train/Ratio             | 1.0027637    |
| Train/Return            | 1.3643494    |
| Train/V                 | 1.2881706    |
| Train/Value             | 1.2881706    |
| Train/control_penalty   | 0.43228334   |
| Train/policy_loss       | -0.08171809  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03125      |
------------------------------------------

 ---------------- Iteration 344 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 343          |
| Time/Actor_Time         | 0.0901       |
| Time/B_Format_Time      | 0.0739       |
| Time/B_Original_Form... | 0.0773       |
| Time/Buffer             | 0.00333      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2243253    |
| Train/Action_magnitu... | 0.5598226    |
| Train/Action_magnitude  | 0.44081312   |
| Train/Action_max        | 0.17680478   |
| Train/Action_std        | 0.18427864   |
| Train/Entropy           | -0.42276344  |
| Train/Entropy_Loss      | 0.000423     |
| Train/Entropy_loss      | 0.000423     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.70363253   |
| Train/Loss              | -0.040781587 |
| Train/PolicyClip        | 0.010045085  |
| Train/Policy_loss       | -0.045631874 |
| Train/Ratio             | 0.998532     |
| Train/Return            | 1.3795732    |
| Train/V                 | 1.3332703    |
| Train/Value             | 1.3332703    |
| Train/control_penalty   | 0.44275224   |
| Train/policy_loss       | -0.045631874 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 345 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 344           |
| Time/Actor_Time         | 0.0843        |
| Time/B_Format_Time      | 0.073         |
| Time/B_Original_Form... | 0.0755        |
| Time/Buffer             | 0.00366       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.21784894    |
| Train/Action_magnitu... | 0.5562979     |
| Train/Action_magnitude  | 0.43881595    |
| Train/Action_max        | 0.19402428    |
| Train/Action_std        | 0.18795894    |
| Train/Entropy           | -0.41285002   |
| Train/Entropy_Loss      | 0.000413      |
| Train/Entropy_loss      | 0.000413      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.7678966     |
| Train/Loss              | 0.003674696   |
| Train/PolicyClip        | 0.009862775   |
| Train/Policy_loss       | -0.0011136216 |
| Train/Ratio             | 0.9991273     |
| Train/Return            | 1.255744      |
| Train/V                 | 1.258726      |
| Train/Value             | 1.258726      |
| Train/control_penalty   | 0.43754676    |
| Train/policy_loss       | -0.0011136216 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0255        |
-------------------------------------------

 ---------------- Iteration 346 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 345          |
| Time/Actor_Time         | 0.0929       |
| Time/B_Format_Time      | 0.075        |
| Time/B_Original_Form... | 0.0758       |
| Time/Buffer             | 0.00299      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21840973   |
| Train/Action_magnitu... | 0.56553274   |
| Train/Action_magnitude  | 0.44537872   |
| Train/Action_max        | 0.16972542   |
| Train/Action_std        | 0.18710233   |
| Train/Entropy           | -0.41557235  |
| Train/Entropy_Loss      | 0.000416     |
| Train/Entropy_loss      | 0.000416     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7453165    |
| Train/Loss              | -0.047133382 |
| Train/PolicyClip        | 0.0076469183 |
| Train/Policy_loss       | -0.05197907  |
| Train/Ratio             | 0.9959384    |
| Train/Return            | 1.3700914    |
| Train/V                 | 1.3274057    |
| Train/Value             | 1.3274057    |
| Train/control_penalty   | 0.44301152   |
| Train/policy_loss       | -0.05197907  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 347 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 346          |
| Time/Actor_Time         | 0.0845       |
| Time/B_Format_Time      | 0.0762       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00293      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22534013   |
| Train/Action_magnitu... | 0.56704175   |
| Train/Action_magnitude  | 0.4450809    |
| Train/Action_max        | 0.20236982   |
| Train/Action_std        | 0.18254583   |
| Train/Entropy           | -0.45202824  |
| Train/Entropy_Loss      | 0.000452     |
| Train/Entropy_loss      | 0.000452     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.8279523    |
| Train/Loss              | 0.0014989045 |
| Train/PolicyClip        | 0.007745685  |
| Train/Policy_loss       | -0.003381517 |
| Train/Ratio             | 0.98945904   |
| Train/Return            | 1.4317578    |
| Train/V                 | 1.4306108    |
| Train/Value             | 1.4306108    |
| Train/control_penalty   | 0.44283932   |
| Train/policy_loss       | -0.003381517 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03175      |
------------------------------------------

 ---------------- Iteration 348 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 347          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0747       |
| Time/B_Original_Form... | 0.078        |
| Time/Buffer             | 0.00302      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21073525   |
| Train/Action_magnitu... | 0.5519415    |
| Train/Action_magnitude  | 0.43450463   |
| Train/Action_max        | 0.19224446   |
| Train/Action_std        | 0.19045937   |
| Train/Entropy           | -0.4003061   |
| Train/Entropy_Loss      | 0.0004       |
| Train/Entropy_loss      | 0.0004       |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.82213163   |
| Train/Loss              | 0.046883207  |
| Train/PolicyClip        | 0.0046482794 |
| Train/Policy_loss       | 0.04224144   |
| Train/Ratio             | 0.9855856    |
| Train/Return            | 1.3846997    |
| Train/V                 | 1.43136      |
| Train/Value             | 1.43136      |
| Train/control_penalty   | 0.42414615   |
| Train/policy_loss       | 0.04224144   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 349 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 348          |
| Time/Actor_Time         | 0.0901       |
| Time/B_Format_Time      | 0.0786       |
| Time/B_Original_Form... | 0.0766       |
| Time/Buffer             | 0.00384      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23674136   |
| Train/Action_magnitu... | 0.58056736   |
| Train/Action_magnitude  | 0.45629847   |
| Train/Action_max        | 0.20871916   |
| Train/Action_std        | 0.19057427   |
| Train/Entropy           | -0.39372912  |
| Train/Entropy_Loss      | 0.000394     |
| Train/Entropy_loss      | 0.000394     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7273565    |
| Train/Loss              | -0.028564135 |
| Train/PolicyClip        | 0.008241617  |
| Train/Policy_loss       | -0.033506274 |
| Train/Ratio             | 0.9993838    |
| Train/Return            | 1.3613147    |
| Train/V                 | 1.3349441    |
| Train/Value             | 1.3349441    |
| Train/control_penalty   | 0.45484105   |
| Train/policy_loss       | -0.033506274 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03425      |
------------------------------------------

 ---------------- Iteration 350 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 349           |
| Time/Actor_Time         | 0.0858        |
| Time/B_Format_Time      | 0.0726        |
| Time/B_Original_Form... | 0.0739        |
| Time/Buffer             | 0.0029        |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.21682304    |
| Train/Action_magnitu... | 0.53049195    |
| Train/Action_magnitude  | 0.41816276    |
| Train/Action_max        | 0.1677917     |
| Train/Action_std        | 0.18952218    |
| Train/Entropy           | -0.39638415   |
| Train/Entropy_Loss      | 0.000396      |
| Train/Entropy_loss      | 0.000396      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.8164956     |
| Train/Loss              | 0.00023864722 |
| Train/PolicyClip        | 0.005570637   |
| Train/Policy_loss       | -0.00442581   |
| Train/Ratio             | 0.99618566    |
| Train/Return            | 1.2471099     |
| Train/V                 | 1.2499222     |
| Train/Value             | 1.2499222     |
| Train/control_penalty   | 0.4268073     |
| Train/policy_loss       | -0.00442581   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.024         |
-------------------------------------------

 ---------------- Iteration 351 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 350         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0727      |
| Time/Buffer             | 0.00285     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21321031  |
| Train/Action_magnitu... | 0.54527473  |
| Train/Action_magnitude  | 0.43206158  |
| Train/Action_max        | 0.17224142  |
| Train/Action_std        | 0.1865242   |
| Train/Entropy           | -0.40972894 |
| Train/Entropy_Loss      | 0.00041     |
| Train/Entropy_loss      | 0.00041     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.72868276  |
| Train/Loss              | 0.03175104  |
| Train/PolicyClip        | 0.010183611 |
| Train/Policy_loss       | 0.02713566  |
| Train/Ratio             | 0.98532885  |
| Train/Return            | 1.28026     |
| Train/V                 | 1.309355    |
| Train/Value             | 1.309355    |
| Train/control_penalty   | 0.420565    |
| Train/policy_loss       | 0.02713566  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 352 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 351          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.0756       |
| Time/B_Original_Form... | 0.078        |
| Time/Buffer             | 0.00295      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21797244   |
| Train/Action_magnitu... | 0.5377175    |
| Train/Action_magnitude  | 0.42656755   |
| Train/Action_max        | 0.15613636   |
| Train/Action_std        | 0.19289435   |
| Train/Entropy           | -0.36178178  |
| Train/Entropy_Loss      | 0.000362     |
| Train/Entropy_loss      | 0.000362     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6687799    |
| Train/Loss              | 0.049015302  |
| Train/PolicyClip        | 0.0131976595 |
| Train/Policy_loss       | 0.044308368  |
| Train/Ratio             | 0.97521377   |
| Train/Return            | 1.2911317    |
| Train/V                 | 1.3253638    |
| Train/Value             | 1.3253638    |
| Train/control_penalty   | 0.43451536   |
| Train/policy_loss       | 0.044308368  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 353 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 352          |
| Time/Actor_Time         | 0.0846       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0757       |
| Time/Buffer             | 0.00261      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21921204   |
| Train/Action_magnitu... | 0.5526038    |
| Train/Action_magnitude  | 0.4346181    |
| Train/Action_max        | 0.16745079   |
| Train/Action_std        | 0.18890765   |
| Train/Entropy           | -0.39670184  |
| Train/Entropy_Loss      | 0.000397     |
| Train/Entropy_loss      | 0.000397     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.70387137   |
| Train/Loss              | -0.061397415 |
| Train/PolicyClip        | 0.007042014  |
| Train/Policy_loss       | -0.0660623   |
| Train/Ratio             | 0.99572706   |
| Train/Return            | 1.3352346    |
| Train/V                 | 1.2766467    |
| Train/Value             | 1.2766467    |
| Train/control_penalty   | 0.42681888   |
| Train/policy_loss       | -0.0660623   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03         |
------------------------------------------

 ---------------- Iteration 354 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 353          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0767       |
| Time/Buffer             | 0.00334      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22138156   |
| Train/Action_magnitu... | 0.5602515    |
| Train/Action_magnitude  | 0.44185844   |
| Train/Action_max        | 0.17040892   |
| Train/Action_std        | 0.1997011    |
| Train/Entropy           | -0.34000763  |
| Train/Entropy_Loss      | 0.00034      |
| Train/Entropy_loss      | 0.00034      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6909276    |
| Train/Loss              | 0.03341266   |
| Train/PolicyClip        | 0.0071563576 |
| Train/Policy_loss       | 0.028573638  |
| Train/Ratio             | 0.9942018    |
| Train/Return            | 1.2080117    |
| Train/V                 | 1.2334555    |
| Train/Value             | 1.2334555    |
| Train/control_penalty   | 0.4499015    |
| Train/policy_loss       | 0.028573638  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 355 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 354           |
| Time/Actor_Time         | 0.085         |
| Time/B_Format_Time      | 0.0712        |
| Time/B_Original_Form... | 0.0738        |
| Time/Buffer             | 0.00259       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.20988992    |
| Train/Action_magnitu... | 0.52836156    |
| Train/Action_magnitude  | 0.4183026     |
| Train/Action_max        | 0.16893156    |
| Train/Action_std        | 0.2190723     |
| Train/Entropy           | -0.256009     |
| Train/Entropy_Loss      | 0.000256      |
| Train/Entropy_loss      | 0.000256      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.5438656     |
| Train/Loss              | 0.00055693556 |
| Train/PolicyClip        | 0.010972267   |
| Train/Policy_loss       | -0.004090315  |
| Train/Ratio             | 0.99217224    |
| Train/Return            | 1.2038777     |
| Train/V                 | 1.204179      |
| Train/Value             | 1.204179      |
| Train/control_penalty   | 0.4391242     |
| Train/policy_loss       | -0.004090315  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.023         |
-------------------------------------------

 ---------------- Iteration 356 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 355          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0736       |
| Time/B_Original_Form... | 0.0738       |
| Time/Buffer             | 0.00285      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21288054   |
| Train/Action_magnitu... | 0.53621197   |
| Train/Action_magnitude  | 0.42433912   |
| Train/Action_max        | 0.18031715   |
| Train/Action_std        | 0.20610957   |
| Train/Entropy           | -0.28630865  |
| Train/Entropy_Loss      | 0.000286     |
| Train/Entropy_loss      | 0.000286     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6534616    |
| Train/Loss              | 0.046911247  |
| Train/PolicyClip        | 0.0058566164 |
| Train/Policy_loss       | 0.042284854  |
| Train/Ratio             | 0.9932531    |
| Train/Return            | 1.3760682    |
| Train/V                 | 1.4267635    |
| Train/Value             | 1.4267635    |
| Train/control_penalty   | 0.43400845   |
| Train/policy_loss       | 0.042284854  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02575      |
------------------------------------------

 ---------------- Iteration 357 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 356         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00246     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.19881803  |
| Train/Action_magnitu... | 0.53289485  |
| Train/Action_magnitude  | 0.42024595  |
| Train/Action_max        | 0.16621192  |
| Train/Action_std        | 0.20810387  |
| Train/Entropy           | -0.27226746 |
| Train/Entropy_Loss      | 0.000272    |
| Train/Entropy_loss      | 0.000272    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.54589146  |
| Train/Loss              | 0.030457452 |
| Train/PolicyClip        | 0.007090213 |
| Train/Policy_loss       | 0.02592161  |
| Train/Ratio             | 0.99679786  |
| Train/Return            | 1.246272    |
| Train/V                 | 1.2788502   |
| Train/Value             | 1.2788502   |
| Train/control_penalty   | 0.42635766  |
| Train/policy_loss       | 0.02592161  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 358 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 357          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0736       |
| Time/Buffer             | 0.00256      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20194061   |
| Train/Action_magnitu... | 0.5369501    |
| Train/Action_magnitude  | 0.42387912   |
| Train/Action_max        | 0.16846982   |
| Train/Action_std        | 0.21381956   |
| Train/Entropy           | -0.26691112  |
| Train/Entropy_Loss      | 0.000267     |
| Train/Entropy_loss      | 0.000267     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.47816223   |
| Train/Loss              | -0.021172144 |
| Train/PolicyClip        | 0.0054052696 |
| Train/Policy_loss       | -0.025813073 |
| Train/Ratio             | 0.99616617   |
| Train/Return            | 1.1983467    |
| Train/V                 | 1.1797537    |
| Train/Value             | 1.1797537    |
| Train/control_penalty   | 0.4374019    |
| Train/policy_loss       | -0.025813073 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 359 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 358           |
| Time/Actor_Time         | 0.0847        |
| Time/B_Format_Time      | 0.071         |
| Time/B_Original_Form... | 0.0763        |
| Time/Buffer             | 0.00327       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.22374077    |
| Train/Action_magnitu... | 0.57635385    |
| Train/Action_magnitude  | 0.45462963    |
| Train/Action_max        | 0.19118822    |
| Train/Action_std        | 0.21588849    |
| Train/Entropy           | -0.26938823   |
| Train/Entropy_Loss      | 0.000269      |
| Train/Entropy_loss      | 0.000269      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.5462727     |
| Train/Loss              | -0.0070006396 |
| Train/PolicyClip        | 0.006760047   |
| Train/Policy_loss       | -0.011865143  |
| Train/Ratio             | 1.0004731     |
| Train/Return            | 1.2856569     |
| Train/V                 | 1.2804513     |
| Train/Value             | 1.2804513     |
| Train/control_penalty   | 0.45951155    |
| Train/policy_loss       | -0.011865143  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0245        |
-------------------------------------------

 ---------------- Iteration 360 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 359          |
| Time/Actor_Time         | 0.089        |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0748       |
| Time/Buffer             | 0.0024       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22090715   |
| Train/Action_magnitu... | 0.57817215   |
| Train/Action_magnitude  | 0.45441753   |
| Train/Action_max        | 0.18440573   |
| Train/Action_std        | 0.21874163   |
| Train/Entropy           | -0.24014603  |
| Train/Entropy_Loss      | 0.00024      |
| Train/Entropy_loss      | 0.00024      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.51139945   |
| Train/Loss              | 0.008130128  |
| Train/PolicyClip        | 0.0054347427 |
| Train/Policy_loss       | 0.0033903164 |
| Train/Ratio             | 0.99399656   |
| Train/Return            | 1.2054591    |
| Train/V                 | 1.2140487    |
| Train/Value             | 1.2140487    |
| Train/control_penalty   | 0.44996655   |
| Train/policy_loss       | 0.0033903164 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 361 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 360          |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0751       |
| Time/B_Original_Form... | 0.0732       |
| Time/Buffer             | 0.00286      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2150864    |
| Train/Action_magnitu... | 0.5723098    |
| Train/Action_magnitude  | 0.45228234   |
| Train/Action_max        | 0.18890902   |
| Train/Action_std        | 0.21593088   |
| Train/Entropy           | -0.28734636  |
| Train/Entropy_Loss      | 0.000287     |
| Train/Entropy_loss      | 0.000287     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.49708396   |
| Train/Loss              | -0.006939016 |
| Train/PolicyClip        | 0.0077620414 |
| Train/Policy_loss       | -0.011794107 |
| Train/Ratio             | 0.9943718    |
| Train/Return            | 1.3307968    |
| Train/V                 | 1.3245009    |
| Train/Value             | 1.3245009    |
| Train/control_penalty   | 0.45677447   |
| Train/policy_loss       | -0.011794107 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 362 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 361          |
| Time/Actor_Time         | 0.0878       |
| Time/B_Format_Time      | 0.0786       |
| Time/B_Original_Form... | 0.075        |
| Time/Buffer             | 0.00251      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2095852    |
| Train/Action_magnitu... | 0.56840396   |
| Train/Action_magnitude  | 0.45009357   |
| Train/Action_max        | 0.17469217   |
| Train/Action_std        | 0.21534155   |
| Train/Entropy           | -0.27333796  |
| Train/Entropy_Loss      | 0.000273     |
| Train/Entropy_loss      | 0.000273     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5127142    |
| Train/Loss              | -0.012497047 |
| Train/PolicyClip        | 0.0051575243 |
| Train/Policy_loss       | -0.017222222 |
| Train/Ratio             | 0.9968971    |
| Train/Return            | 1.3094068    |
| Train/V                 | 1.2980585    |
| Train/Value             | 1.2980585    |
| Train/control_penalty   | 0.44518378   |
| Train/policy_loss       | -0.017222222 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 363 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 362          |
| Time/Actor_Time         | 0.151        |
| Time/B_Format_Time      | 0.132        |
| Time/B_Original_Form... | 0.135        |
| Time/Buffer             | 0.00603      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21606638   |
| Train/Action_magnitu... | 0.57567537   |
| Train/Action_magnitude  | 0.4509366    |
| Train/Action_max        | 0.18134779   |
| Train/Action_std        | 0.21747941   |
| Train/Entropy           | -0.27317944  |
| Train/Entropy_Loss      | 0.000273     |
| Train/Entropy_loss      | 0.000273     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5152124    |
| Train/Loss              | -0.030358747 |
| Train/PolicyClip        | 0.005908796  |
| Train/Policy_loss       | -0.035168905 |
| Train/Ratio             | 0.99939704   |
| Train/Return            | 1.2010628    |
| Train/V                 | 1.1731223    |
| Train/Value             | 1.1731223    |
| Train/control_penalty   | 0.453698     |
| Train/policy_loss       | -0.035168905 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 364 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 363          |
| Time/Actor_Time         | 0.147        |
| Time/B_Format_Time      | 0.153        |
| Time/B_Original_Form... | 0.134        |
| Time/Buffer             | 0.00716      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22933847   |
| Train/Action_magnitu... | 0.6019258    |
| Train/Action_magnitude  | 0.47298872   |
| Train/Action_max        | 0.18918079   |
| Train/Action_std        | 0.22542866   |
| Train/Entropy           | -0.25072733  |
| Train/Entropy_Loss      | 0.000251     |
| Train/Entropy_loss      | 0.000251     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.50384814   |
| Train/Loss              | 0.026240442  |
| Train/PolicyClip        | 0.0060309223 |
| Train/Policy_loss       | 0.021135686  |
| Train/Ratio             | 0.99498975   |
| Train/Return            | 1.0946714    |
| Train/V                 | 1.1219176    |
| Train/Value             | 1.1219176    |
| Train/control_penalty   | 0.485403     |
| Train/policy_loss       | 0.021135686  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01825      |
------------------------------------------

 ---------------- Iteration 365 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 364          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0997       |
| Time/B_Original_Form... | 0.0732       |
| Time/Buffer             | 0.00456      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22562018   |
| Train/Action_magnitu... | 0.59437543   |
| Train/Action_magnitude  | 0.46483725   |
| Train/Action_max        | 0.1922709    |
| Train/Action_std        | 0.21221109   |
| Train/Entropy           | -0.30527753  |
| Train/Entropy_Loss      | 0.000305     |
| Train/Entropy_loss      | 0.000305     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.50890845   |
| Train/Loss              | 0.0065950854 |
| Train/PolicyClip        | 0.011802353  |
| Train/Policy_loss       | 0.0016501225 |
| Train/Ratio             | 0.9930555    |
| Train/Return            | 1.083844     |
| Train/V                 | 1.0871383    |
| Train/Value             | 1.0871383    |
| Train/control_penalty   | 0.46396858   |
| Train/policy_loss       | 0.0016501225 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02         |
------------------------------------------

 ---------------- Iteration 366 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 365          |
| Time/Actor_Time         | 0.0889       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0743       |
| Time/Buffer             | 0.00291      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21627386   |
| Train/Action_magnitu... | 0.5613874    |
| Train/Action_magnitude  | 0.44035712   |
| Train/Action_max        | 0.15447924   |
| Train/Action_std        | 0.1992321    |
| Train/Entropy           | -0.35092106  |
| Train/Entropy_Loss      | 0.000351     |
| Train/Entropy_loss      | 0.000351     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6601776    |
| Train/Loss              | -0.08386525  |
| Train/PolicyClip        | 0.0057190806 |
| Train/Policy_loss       | -0.08865779  |
| Train/Ratio             | 1.0035691    |
| Train/Return            | 1.1471372    |
| Train/V                 | 1.0663294    |
| Train/Value             | 1.0663294    |
| Train/control_penalty   | 0.44416183   |
| Train/policy_loss       | -0.08865779  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02725      |
------------------------------------------

 ---------------- Iteration 367 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 366          |
| Time/Actor_Time         | 0.0924       |
| Time/B_Format_Time      | 0.0756       |
| Time/B_Original_Form... | 0.0766       |
| Time/Buffer             | 0.00293      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20864336   |
| Train/Action_magnitu... | 0.55479914   |
| Train/Action_magnitude  | 0.43536994   |
| Train/Action_max        | 0.18467213   |
| Train/Action_std        | 0.20016009   |
| Train/Entropy           | -0.33975193  |
| Train/Entropy_Loss      | 0.00034      |
| Train/Entropy_loss      | 0.00034      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5690642    |
| Train/Loss              | -0.014704961 |
| Train/PolicyClip        | 0.0059222793 |
| Train/Policy_loss       | -0.0193546   |
| Train/Ratio             | 0.9923611    |
| Train/Return            | 1.1788708    |
| Train/V                 | 1.1652874    |
| Train/Value             | 1.1652874    |
| Train/control_penalty   | 0.43098867   |
| Train/policy_loss       | -0.0193546   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 368 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 367          |
| Time/Actor_Time         | 0.0975       |
| Time/B_Format_Time      | 0.0946       |
| Time/B_Original_Form... | 0.0826       |
| Time/Buffer             | 0.00237      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21755417   |
| Train/Action_magnitu... | 0.55641323   |
| Train/Action_magnitude  | 0.4363429    |
| Train/Action_max        | 0.18258008   |
| Train/Action_std        | 0.2017315    |
| Train/Entropy           | -0.32535163  |
| Train/Entropy_Loss      | 0.000325     |
| Train/Entropy_loss      | 0.000325     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6282212    |
| Train/Loss              | 0.03758017   |
| Train/PolicyClip        | 0.0028158063 |
| Train/Policy_loss       | 0.032851752  |
| Train/Ratio             | 0.9888138    |
| Train/Return            | 1.0170656    |
| Train/V                 | 1.0562855    |
| Train/Value             | 1.0562855    |
| Train/control_penalty   | 0.4403065    |
| Train/policy_loss       | 0.032851752  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0145       |
------------------------------------------

 ---------------- Iteration 369 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 368         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2218456   |
| Train/Action_magnitu... | 0.568159    |
| Train/Action_magnitude  | 0.44426504  |
| Train/Action_max        | 0.14228015  |
| Train/Action_std        | 0.20079538  |
| Train/Entropy           | -0.33580923 |
| Train/Entropy_Loss      | 0.000336    |
| Train/Entropy_loss      | 0.000336    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6187335   |
| Train/Loss              | 0.025029406 |
| Train/PolicyClip        | 0.005880286 |
| Train/Policy_loss       | 0.020245206 |
| Train/Ratio             | 0.9881759   |
| Train/Return            | 1.0453186   |
| Train/V                 | 1.0730017   |
| Train/Value             | 1.0730017   |
| Train/control_penalty   | 0.44483912  |
| Train/policy_loss       | 0.020245206 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.017       |
-----------------------------------------

 ---------------- Iteration 370 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 369          |
| Time/Actor_Time         | 0.0838       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20680872   |
| Train/Action_magnitu... | 0.54754025   |
| Train/Action_magnitude  | 0.43150035   |
| Train/Action_max        | 0.1777409    |
| Train/Action_std        | 0.2067901    |
| Train/Entropy           | -0.3213498   |
| Train/Entropy_Loss      | 0.000321     |
| Train/Entropy_loss      | 0.000321     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6407368    |
| Train/Loss              | -0.047993973 |
| Train/PolicyClip        | 0.0071633724 |
| Train/Policy_loss       | -0.052637566 |
| Train/Ratio             | 0.9972173    |
| Train/Return            | 0.953812     |
| Train/V                 | 0.90750444   |
| Train/Value             | 0.90750444   |
| Train/control_penalty   | 0.43222433   |
| Train/policy_loss       | -0.052637566 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01925      |
------------------------------------------

 ---------------- Iteration 371 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 370          |
| Time/Actor_Time         | 0.0884       |
| Time/B_Format_Time      | 0.0748       |
| Time/B_Original_Form... | 0.0732       |
| Time/Buffer             | 0.00251      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.2170528    |
| Train/Action_magnitu... | 0.5711264    |
| Train/Action_magnitude  | 0.4454358    |
| Train/Action_max        | 0.17564829   |
| Train/Action_std        | 0.21696739   |
| Train/Entropy           | -0.27207983  |
| Train/Entropy_Loss      | 0.000272     |
| Train/Entropy_loss      | 0.000272     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5780891    |
| Train/Loss              | 0.063678585  |
| Train/PolicyClip        | 0.0051411637 |
| Train/Policy_loss       | 0.058914915  |
| Train/Ratio             | 0.9797505    |
| Train/Return            | 0.8688682    |
| Train/V                 | 0.9323974    |
| Train/Value             | 0.9323974    |
| Train/control_penalty   | 0.4491592    |
| Train/policy_loss       | 0.058914915  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.011        |
------------------------------------------

 ---------------- Iteration 372 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 371          |
| Time/Actor_Time         | 0.0834       |
| Time/B_Format_Time      | 0.0752       |
| Time/B_Original_Form... | 0.0754       |
| Time/Buffer             | 0.00296      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20320599   |
| Train/Action_magnitu... | 0.55597544   |
| Train/Action_magnitude  | 0.43763292   |
| Train/Action_max        | 0.17424212   |
| Train/Action_std        | 0.19842407   |
| Train/Entropy           | -0.36794716  |
| Train/Entropy_Loss      | 0.000368     |
| Train/Entropy_loss      | 0.000368     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.60912293   |
| Train/Loss              | -0.135704    |
| Train/PolicyClip        | 0.0032316807 |
| Train/Policy_loss       | -0.140297    |
| Train/Ratio             | 1.0063331    |
| Train/Return            | 1.0352052    |
| Train/V                 | 0.9027337    |
| Train/Value             | 0.9027337    |
| Train/control_penalty   | 0.42250523   |
| Train/policy_loss       | -0.140297    |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 373 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 372           |
| Time/Actor_Time         | 0.0879        |
| Time/B_Format_Time      | 0.0713        |
| Time/B_Original_Form... | 0.0748        |
| Time/Buffer             | 0.00264       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.20343734    |
| Train/Action_magnitu... | 0.53013843    |
| Train/Action_magnitude  | 0.4181668     |
| Train/Action_max        | 0.15397258    |
| Train/Action_std        | 0.1971802     |
| Train/Entropy           | -0.3487127    |
| Train/Entropy_Loss      | 0.000349      |
| Train/Entropy_loss      | 0.000349      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.70267564    |
| Train/Loss              | -0.0059602372 |
| Train/PolicyClip        | 0.005606189   |
| Train/Policy_loss       | -0.0105514685 |
| Train/Ratio             | 0.98832387    |
| Train/Return            | 1.0344844     |
| Train/V                 | 1.0329696     |
| Train/Value             | 1.0329696     |
| Train/control_penalty   | 0.42425188    |
| Train/policy_loss       | -0.0105514685 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01975       |
-------------------------------------------

 ---------------- Iteration 374 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 373         |
| Time/Actor_Time         | 0.0871      |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0747      |
| Time/Buffer             | 0.00221     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20578037  |
| Train/Action_magnitu... | 0.5487961   |
| Train/Action_magnitude  | 0.43199646  |
| Train/Action_max        | 0.20344295  |
| Train/Action_std        | 0.20865624  |
| Train/Entropy           | -0.3144089  |
| Train/Entropy_Loss      | 0.000314    |
| Train/Entropy_loss      | 0.000314    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5739542   |
| Train/Loss              | -0.07119333 |
| Train/PolicyClip        | 0.009194492 |
| Train/Policy_loss       | -0.07584712 |
| Train/Ratio             | 1.0059849   |
| Train/Return            | 1.1072365   |
| Train/V                 | 1.0366334   |
| Train/Value             | 1.0366334   |
| Train/control_penalty   | 0.43393824  |
| Train/policy_loss       | -0.07584712 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 375 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 374          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0733       |
| Time/Buffer             | 0.00321      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22878414   |
| Train/Action_magnitu... | 0.58383906   |
| Train/Action_magnitude  | 0.4607661    |
| Train/Action_max        | 0.1711475    |
| Train/Action_std        | 0.20379716   |
| Train/Entropy           | -0.3222399   |
| Train/Entropy_Loss      | 0.000322     |
| Train/Entropy_loss      | 0.000322     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.635834     |
| Train/Loss              | -0.040748946 |
| Train/PolicyClip        | 0.0077519054 |
| Train/Policy_loss       | -0.045689024 |
| Train/Ratio             | 1.0006057    |
| Train/Return            | 1.1670586    |
| Train/V                 | 1.1290305    |
| Train/Value             | 1.1290305    |
| Train/control_penalty   | 0.46178344   |
| Train/policy_loss       | -0.045689024 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 376 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 375          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0729       |
| Time/Buffer             | 0.00286      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21088083   |
| Train/Action_magnitu... | 0.5559423    |
| Train/Action_magnitude  | 0.4393697    |
| Train/Action_max        | 0.1579053    |
| Train/Action_std        | 0.1932105    |
| Train/Entropy           | -0.37106556  |
| Train/Entropy_Loss      | 0.000371     |
| Train/Entropy_loss      | 0.000371     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.67034966   |
| Train/Loss              | -0.057903092 |
| Train/PolicyClip        | 0.008230846  |
| Train/Policy_loss       | -0.06266564  |
| Train/Ratio             | 0.99872655   |
| Train/Return            | 1.1208777    |
| Train/V                 | 1.0567676    |
| Train/Value             | 1.0567676    |
| Train/control_penalty   | 0.4391484    |
| Train/policy_loss       | -0.06266564  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 377 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 376          |
| Time/Actor_Time         | 0.0881       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0772       |
| Time/Buffer             | 0.00256      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.20448303   |
| Train/Action_magnitu... | 0.52839744   |
| Train/Action_magnitude  | 0.42053497   |
| Train/Action_max        | 0.1513865    |
| Train/Action_std        | 0.1952211    |
| Train/Entropy           | -0.3615238   |
| Train/Entropy_Loss      | 0.000362     |
| Train/Entropy_loss      | 0.000362     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6824942    |
| Train/Loss              | 0.0060091107 |
| Train/PolicyClip        | 0.0067400853 |
| Train/Policy_loss       | 0.0013441134 |
| Train/Ratio             | 0.9837813    |
| Train/Return            | 1.0077596    |
| Train/V                 | 1.011986     |
| Train/Value             | 1.011986     |
| Train/control_penalty   | 0.43034735   |
| Train/policy_loss       | 0.0013441134 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01625      |
------------------------------------------

 ---------------- Iteration 378 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 377         |
| Time/Actor_Time         | 0.0878      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.0757      |
| Time/Buffer             | 0.00366     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2097113   |
| Train/Action_magnitu... | 0.541998    |
| Train/Action_magnitude  | 0.42960238  |
| Train/Action_max        | 0.14813332  |
| Train/Action_std        | 0.19332242  |
| Train/Entropy           | -0.36186197 |
| Train/Entropy_Loss      | 0.000362    |
| Train/Entropy_loss      | 0.000362    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.65878844  |
| Train/Loss              | -0.04683194 |
| Train/PolicyClip        | 0.007885082 |
| Train/Policy_loss       | -0.05155517 |
| Train/Ratio             | 1.00372     |
| Train/Return            | 1.1778286   |
| Train/V                 | 1.1320778   |
| Train/Value             | 1.1320778   |
| Train/control_penalty   | 0.43613696  |
| Train/policy_loss       | -0.05155517 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 379 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 378         |
| Time/Actor_Time         | 0.084       |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.0743      |
| Time/Buffer             | 0.00259     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.20939834  |
| Train/Action_magnitu... | 0.54169446  |
| Train/Action_magnitude  | 0.42998236  |
| Train/Action_max        | 0.12922291  |
| Train/Action_std        | 0.19032753  |
| Train/Entropy           | -0.36940008 |
| Train/Entropy_Loss      | 0.000369    |
| Train/Entropy_loss      | 0.000369    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6797025   |
| Train/Loss              | -0.04089824 |
| Train/PolicyClip        | 0.006668867 |
| Train/Policy_loss       | -0.04556618 |
| Train/Ratio             | 0.99678373  |
| Train/Return            | 1.2037894   |
| Train/V                 | 1.1629363   |
| Train/Value             | 1.1629363   |
| Train/control_penalty   | 0.42985368  |
| Train/policy_loss       | -0.04556618 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 380 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 379         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0739      |
| Time/Buffer             | 0.00276     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.21414673  |
| Train/Action_magnitu... | 0.54110694  |
| Train/Action_magnitude  | 0.42838344  |
| Train/Action_max        | 0.14569074  |
| Train/Action_std        | 0.18670015  |
| Train/Entropy           | -0.37738103 |
| Train/Entropy_Loss      | 0.000377    |
| Train/Entropy_loss      | 0.000377    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.64733124  |
| Train/Loss              | -0.03451863 |
| Train/PolicyClip        | 0.008290527 |
| Train/Policy_loss       | -0.03916957 |
| Train/Ratio             | 1.0003487   |
| Train/Return            | 1.0925689   |
| Train/V                 | 1.0532774   |
| Train/Value             | 1.0532774   |
| Train/control_penalty   | 0.427356    |
| Train/policy_loss       | -0.03916957 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 381 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 380         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0743      |
| Time/B_Original_Form... | 0.076       |
| Time/Buffer             | 0.0026      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2208958   |
| Train/Action_magnitu... | 0.5545359   |
| Train/Action_magnitude  | 0.4384872   |
| Train/Action_max        | 0.15673177  |
| Train/Action_std        | 0.19297133  |
| Train/Entropy           | -0.36191624 |
| Train/Entropy_Loss      | 0.000362    |
| Train/Entropy_loss      | 0.000362    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.69817466  |
| Train/Loss              | 0.024765003 |
| Train/PolicyClip        | 0.007111004 |
| Train/Policy_loss       | 0.019990314 |
| Train/Ratio             | 0.9929978   |
| Train/Return            | 1.0794957   |
| Train/V                 | 1.1061311   |
| Train/Value             | 1.1061311   |
| Train/control_penalty   | 0.44127738  |
| Train/policy_loss       | 0.019990314 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0175      |
-----------------------------------------

 ---------------- Iteration 382 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 381          |
| Time/Actor_Time         | 0.0897       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00451      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.21562962   |
| Train/Action_magnitu... | 0.54854554   |
| Train/Action_magnitude  | 0.4297587    |
| Train/Action_max        | 0.1392424    |
| Train/Action_std        | 0.18745568   |
| Train/Entropy           | -0.38690963  |
| Train/Entropy_Loss      | 0.000387     |
| Train/Entropy_loss      | 0.000387     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.72633576   |
| Train/Loss              | 0.0014874057 |
| Train/PolicyClip        | 0.006598612  |
| Train/Policy_loss       | -0.003182154 |
| Train/Ratio             | 1.0045981    |
| Train/Return            | 1.1578972    |
| Train/V                 | 1.1609926    |
| Train/Value             | 1.1609926    |
| Train/control_penalty   | 0.428265     |
| Train/policy_loss       | -0.003182154 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 383 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 382         |
| Time/Actor_Time         | 0.0844      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00251     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20934887  |
| Train/Action_magnitu... | 0.5437703   |
| Train/Action_magnitude  | 0.42630783  |
| Train/Action_max        | 0.16076331  |
| Train/Action_std        | 0.20132722  |
| Train/Entropy           | -0.30977827 |
| Train/Entropy_Loss      | 0.00031     |
| Train/Entropy_loss      | 0.00031     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.61741734  |
| Train/Loss              | 0.060072638 |
| Train/PolicyClip        | 0.009119279 |
| Train/Policy_loss       | 0.05548435  |
| Train/Ratio             | 0.980159    |
| Train/Return            | 1.0199845   |
| Train/V                 | 1.0852503   |
| Train/Value             | 1.0852503   |
| Train/control_penalty   | 0.4278512   |
| Train/policy_loss       | 0.05548435  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0165      |
-----------------------------------------

 ---------------- Iteration 384 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 383          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0747       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22234516   |
| Train/Action_magnitu... | 0.5664481    |
| Train/Action_magnitude  | 0.4440032    |
| Train/Action_max        | 0.13934219   |
| Train/Action_std        | 0.19077653   |
| Train/Entropy           | -0.38174444  |
| Train/Entropy_Loss      | 0.000382     |
| Train/Entropy_loss      | 0.000382     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6870099    |
| Train/Loss              | -0.021256614 |
| Train/PolicyClip        | 0.007346526  |
| Train/Policy_loss       | -0.026083216 |
| Train/Ratio             | 0.9993464    |
| Train/Return            | 1.3629857    |
| Train/V                 | 1.3456384    |
| Train/Value             | 1.3456384    |
| Train/control_penalty   | 0.44448572   |
| Train/policy_loss       | -0.026083216 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.029        |
------------------------------------------

 ---------------- Iteration 385 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 384          |
| Time/Actor_Time         | 0.0901       |
| Time/B_Format_Time      | 0.0743       |
| Time/B_Original_Form... | 0.0761       |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21606687   |
| Train/Action_magnitu... | 0.56879836   |
| Train/Action_magnitude  | 0.44564274   |
| Train/Action_max        | 0.16564822   |
| Train/Action_std        | 0.20002301   |
| Train/Entropy           | -0.34160256  |
| Train/Entropy_Loss      | 0.000342     |
| Train/Entropy_loss      | 0.000342     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6334564    |
| Train/Loss              | -0.021071581 |
| Train/PolicyClip        | 0.007419206  |
| Train/Policy_loss       | -0.02582335  |
| Train/Ratio             | 0.9933922    |
| Train/Return            | 1.233919     |
| Train/V                 | 1.214853     |
| Train/Value             | 1.214853     |
| Train/control_penalty   | 0.44101664   |
| Train/policy_loss       | -0.02582335  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 386 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 385          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0806       |
| Time/Buffer             | 0.00243      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22546986   |
| Train/Action_magnitu... | 0.5596761    |
| Train/Action_magnitude  | 0.43652618   |
| Train/Action_max        | 0.15736718   |
| Train/Action_std        | 0.18764514   |
| Train/Entropy           | -0.3931734   |
| Train/Entropy_Loss      | 0.000393     |
| Train/Entropy_loss      | 0.000393     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.7385221    |
| Train/Loss              | 0.012478143  |
| Train/PolicyClip        | 0.006122774  |
| Train/Policy_loss       | 0.0076588914 |
| Train/Ratio             | 0.9926228    |
| Train/Return            | 1.30099      |
| Train/V                 | 1.3158839    |
| Train/Value             | 1.3158839    |
| Train/control_penalty   | 0.44260782   |
| Train/policy_loss       | 0.0076588914 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 387 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 386         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.078       |
| Time/Buffer             | 0.0026      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21594952  |
| Train/Action_magnitu... | 0.55019706  |
| Train/Action_magnitude  | 0.42928296  |
| Train/Action_max        | 0.14864972  |
| Train/Action_std        | 0.18914786  |
| Train/Entropy           | -0.3816416  |
| Train/Entropy_Loss      | 0.000382    |
| Train/Entropy_loss      | 0.000382    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.75090575  |
| Train/Loss              | 0.04610987  |
| Train/PolicyClip        | 0.004969414 |
| Train/Policy_loss       | 0.041413337 |
| Train/Ratio             | 0.98373747  |
| Train/Return            | 1.1053553   |
| Train/V                 | 1.1573415   |
| Train/Value             | 1.1573415   |
| Train/control_penalty   | 0.43148905  |
| Train/policy_loss       | 0.041413337 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 388 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 387         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.0703      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21796788  |
| Train/Action_magnitu... | 0.55237794  |
| Train/Action_magnitude  | 0.43401662  |
| Train/Action_max        | 0.13016507  |
| Train/Action_std        | 0.1941773   |
| Train/Entropy           | -0.36169586 |
| Train/Entropy_Loss      | 0.000362    |
| Train/Entropy_loss      | 0.000362    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.7490406   |
| Train/Loss              | 0.064404644 |
| Train/PolicyClip        | 0.011041707 |
| Train/Policy_loss       | 0.05961718  |
| Train/Ratio             | 0.9803597   |
| Train/Return            | 1.2005188   |
| Train/V                 | 1.2674809   |
| Train/Value             | 1.2674809   |
| Train/control_penalty   | 0.4425765   |
| Train/policy_loss       | 0.05961718  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.018       |
-----------------------------------------

 ---------------- Iteration 389 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 388          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0734       |
| Time/B_Original_Form... | 0.0739       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21600495   |
| Train/Action_magnitu... | 0.5665623    |
| Train/Action_magnitude  | 0.44295266   |
| Train/Action_max        | 0.1467917    |
| Train/Action_std        | 0.19566575   |
| Train/Entropy           | -0.36325067  |
| Train/Entropy_Loss      | 0.000363     |
| Train/Entropy_loss      | 0.000363     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.65750307   |
| Train/Loss              | -0.06483202  |
| Train/PolicyClip        | 0.0055519473 |
| Train/Policy_loss       | -0.06954294  |
| Train/Ratio             | 1.000843     |
| Train/Return            | 1.3153595    |
| Train/V                 | 1.2527243    |
| Train/Value             | 1.2527243    |
| Train/control_penalty   | 0.43476647   |
| Train/policy_loss       | -0.06954294  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.029        |
------------------------------------------

 ---------------- Iteration 390 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 389          |
| Time/Actor_Time         | 0.0888       |
| Time/B_Format_Time      | 0.0752       |
| Time/B_Original_Form... | 0.0711       |
| Time/Buffer             | 0.00393      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20705868   |
| Train/Action_magnitu... | 0.5433226    |
| Train/Action_magnitude  | 0.4274773    |
| Train/Action_max        | 0.13247162   |
| Train/Action_std        | 0.1866764    |
| Train/Entropy           | -0.40441012  |
| Train/Entropy_Loss      | 0.000404     |
| Train/Entropy_loss      | 0.000404     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.75323415   |
| Train/Loss              | -0.024312692 |
| Train/PolicyClip        | 0.0065551824 |
| Train/Policy_loss       | -0.028921634 |
| Train/Ratio             | 0.9976757    |
| Train/Return            | 0.9616404    |
| Train/V                 | 0.93905705   |
| Train/Value             | 0.93905705   |
| Train/control_penalty   | 0.42045322   |
| Train/policy_loss       | -0.028921634 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 391 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 390         |
| Time/Actor_Time         | 0.0875      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0726      |
| Time/Buffer             | 0.00294     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20911041  |
| Train/Action_magnitu... | 0.54716593  |
| Train/Action_magnitude  | 0.42916086  |
| Train/Action_max        | 0.15217282  |
| Train/Action_std        | 0.1957883   |
| Train/Entropy           | -0.34688765 |
| Train/Entropy_Loss      | 0.000347    |
| Train/Entropy_loss      | 0.000347    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6516922   |
| Train/Loss              | 0.006508834 |
| Train/PolicyClip        | 0.00507054  |
| Train/Policy_loss       | 0.001955033 |
| Train/Ratio             | 0.99157184  |
| Train/Return            | 1.0137738   |
| Train/V                 | 1.0208235   |
| Train/Value             | 1.0208235   |
| Train/control_penalty   | 0.42069137  |
| Train/policy_loss       | 0.001955033 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 392 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 391         |
| Time/Actor_Time         | 0.0911      |
| Time/B_Format_Time      | 0.0749      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00253     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21931933  |
| Train/Action_magnitu... | 0.55104023  |
| Train/Action_magnitude  | 0.4313122   |
| Train/Action_max        | 0.11939276  |
| Train/Action_std        | 0.18241964  |
| Train/Entropy           | -0.41276357 |
| Train/Entropy_Loss      | 0.000413    |
| Train/Entropy_loss      | 0.000413    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.716441    |
| Train/Loss              | 0.03223972  |
| Train/PolicyClip        | 0.012015325 |
| Train/Policy_loss       | 0.027525716 |
| Train/Ratio             | 0.9871241   |
| Train/Return            | 1.025573    |
| Train/V                 | 1.0549434   |
| Train/Value             | 1.0549434   |
| Train/control_penalty   | 0.4301239   |
| Train/policy_loss       | 0.027525716 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 393 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 392         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0696      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00224     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21237619  |
| Train/Action_magnitu... | 0.55613256  |
| Train/Action_magnitude  | 0.4364829   |
| Train/Action_max        | 0.15622604  |
| Train/Action_std        | 0.19849059  |
| Train/Entropy           | -0.33623144 |
| Train/Entropy_Loss      | 0.000336    |
| Train/Entropy_loss      | 0.000336    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5887462   |
| Train/Loss              | 0.014578667 |
| Train/PolicyClip        | 0.007497134 |
| Train/Policy_loss       | 0.009891929 |
| Train/Ratio             | 0.9987458   |
| Train/Return            | 1.0557567   |
| Train/V                 | 1.0698158   |
| Train/Value             | 1.0698158   |
| Train/control_penalty   | 0.43505064  |
| Train/policy_loss       | 0.009891929 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 394 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 393          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0739       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.0026       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21128549   |
| Train/Action_magnitu... | 0.5428591    |
| Train/Action_magnitude  | 0.42587036   |
| Train/Action_max        | 0.13573532   |
| Train/Action_std        | 0.19562097   |
| Train/Entropy           | -0.34744146  |
| Train/Entropy_Loss      | 0.000347     |
| Train/Entropy_loss      | 0.000347     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.66294557   |
| Train/Loss              | 0.0004839953 |
| Train/PolicyClip        | 0.007216459  |
| Train/Policy_loss       | -0.00424885  |
| Train/Ratio             | 0.9941463    |
| Train/Return            | 1.0514655    |
| Train/V                 | 1.0541228    |
| Train/Value             | 1.0541228    |
| Train/control_penalty   | 0.4385404    |
| Train/policy_loss       | -0.00424885  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 395 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 394          |
| Time/Actor_Time         | 0.0834       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0734       |
| Time/Buffer             | 0.00275      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20721087   |
| Train/Action_magnitu... | 0.56579494   |
| Train/Action_magnitude  | 0.4431174    |
| Train/Action_max        | 0.17488836   |
| Train/Action_std        | 0.2087194    |
| Train/Entropy           | -0.2974334   |
| Train/Entropy_Loss      | 0.000297     |
| Train/Entropy_loss      | 0.000297     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5663026    |
| Train/Loss              | -0.013314757 |
| Train/PolicyClip        | 0.004754878  |
| Train/Policy_loss       | -0.017998066 |
| Train/Ratio             | 0.99729884   |
| Train/Return            | 0.98816884   |
| Train/V                 | 0.976285     |
| Train/Value             | 0.976285     |
| Train/control_penalty   | 0.43858767   |
| Train/policy_loss       | -0.017998066 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 396 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 395          |
| Time/Actor_Time         | 0.0839       |
| Time/B_Format_Time      | 0.0793       |
| Time/B_Original_Form... | 0.0739       |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21950686   |
| Train/Action_magnitu... | 0.59122044   |
| Train/Action_magnitude  | 0.4646496    |
| Train/Action_max        | 0.21355176   |
| Train/Action_std        | 0.2234095    |
| Train/Entropy           | -0.26756313  |
| Train/Entropy_Loss      | 0.000268     |
| Train/Entropy_loss      | 0.000268     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.46279827   |
| Train/Loss              | -0.030697275 |
| Train/PolicyClip        | 0.0072210045 |
| Train/Policy_loss       | -0.035585977 |
| Train/Ratio             | 0.9939305    |
| Train/Return            | 1.0499775    |
| Train/V                 | 1.019027     |
| Train/Value             | 1.019027     |
| Train/control_penalty   | 0.46211413   |
| Train/policy_loss       | -0.035585977 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 397 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 396          |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0737       |
| Time/Buffer             | 0.00303      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22225153   |
| Train/Action_magnitu... | 0.5833741    |
| Train/Action_magnitude  | 0.45819458   |
| Train/Action_max        | 0.21472846   |
| Train/Action_std        | 0.21299411   |
| Train/Entropy           | -0.30058917  |
| Train/Entropy_Loss      | 0.000301     |
| Train/Entropy_loss      | 0.000301     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.54266936   |
| Train/Loss              | -0.06172007  |
| Train/PolicyClip        | 0.0070472728 |
| Train/Policy_loss       | -0.06662038  |
| Train/Ratio             | 1.0059096    |
| Train/Return            | 1.1304166    |
| Train/V                 | 1.0747546    |
| Train/Value             | 1.0747546    |
| Train/control_penalty   | 0.45997253   |
| Train/policy_loss       | -0.06662038  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 398 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 397          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0755       |
| Time/B_Original_Form... | 0.0733       |
| Time/Buffer             | 0.00229      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21959285   |
| Train/Action_magnitu... | 0.5644542    |
| Train/Action_magnitude  | 0.44492006   |
| Train/Action_max        | 0.17176232   |
| Train/Action_std        | 0.20386547   |
| Train/Entropy           | -0.32635388  |
| Train/Entropy_Loss      | 0.000326     |
| Train/Entropy_loss      | 0.000326     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.63817966   |
| Train/Loss              | -0.007746844 |
| Train/PolicyClip        | 0.0096369255 |
| Train/Policy_loss       | -0.012607847 |
| Train/Ratio             | 0.9874754    |
| Train/Return            | 1.0017647    |
| Train/V                 | 1.0003492    |
| Train/Value             | 1.0003492    |
| Train/control_penalty   | 0.45346493   |
| Train/policy_loss       | -0.012607847 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 399 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 398          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00316      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22155751   |
| Train/Action_magnitu... | 0.5819798    |
| Train/Action_magnitude  | 0.45575222   |
| Train/Action_max        | 0.18635358   |
| Train/Action_std        | 0.21647213   |
| Train/Entropy           | -0.2704594   |
| Train/Entropy_Loss      | 0.00027      |
| Train/Entropy_loss      | 0.00027      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.55708224   |
| Train/Loss              | -0.027246367 |
| Train/PolicyClip        | 0.00998932   |
| Train/Policy_loss       | -0.03210462  |
| Train/Ratio             | 1.0015955    |
| Train/Return            | 1.0794945    |
| Train/V                 | 1.0543438    |
| Train/Value             | 1.0543438    |
| Train/control_penalty   | 0.4587791    |
| Train/policy_loss       | -0.03210462  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 400 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 399          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.08         |
| Time/B_Original_Form... | 0.0805       |
| Time/Buffer             | 0.0135       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21504255   |
| Train/Action_magnitu... | 0.5816519    |
| Train/Action_magnitude  | 0.45740283   |
| Train/Action_max        | 0.2157651    |
| Train/Action_std        | 0.22713694   |
| Train/Entropy           | -0.24223147  |
| Train/Entropy_Loss      | 0.000242     |
| Train/Entropy_loss      | 0.000242     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.44300234   |
| Train/Loss              | -0.05663656  |
| Train/PolicyClip        | 0.0076675415 |
| Train/Policy_loss       | -0.061545238 |
| Train/Ratio             | 0.99733645   |
| Train/Return            | 1.026174     |
| Train/V                 | 0.9717372    |
| Train/Value             | 0.9717372    |
| Train/control_penalty   | 0.46664447   |
| Train/policy_loss       | -0.061545238 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 401 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 400          |
| Time/Actor_Time         | 0.0893       |
| Time/B_Format_Time      | 0.0903       |
| Time/B_Original_Form... | 0.0733       |
| Time/Buffer             | 0.00276      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20915078   |
| Train/Action_magnitu... | 0.5658668    |
| Train/Action_magnitude  | 0.4468679    |
| Train/Action_max        | 0.19816472   |
| Train/Action_std        | 0.22019316   |
| Train/Entropy           | -0.25553304  |
| Train/Entropy_Loss      | 0.000256     |
| Train/Entropy_loss      | 0.000256     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5215403    |
| Train/Loss              | -0.01258056  |
| Train/PolicyClip        | 0.0096872365 |
| Train/Policy_loss       | -0.017337328 |
| Train/Ratio             | 1.0061318    |
| Train/Return            | 1.0988017    |
| Train/V                 | 1.0896585    |
| Train/Value             | 1.0896585    |
| Train/control_penalty   | 0.45012358   |
| Train/policy_loss       | -0.017337328 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 402 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 401         |
| Time/Actor_Time         | 0.16        |
| Time/B_Format_Time      | 0.135       |
| Time/B_Original_Form... | 0.158       |
| Time/Buffer             | 0.00427     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21973658  |
| Train/Action_magnitu... | 0.6026433   |
| Train/Action_magnitude  | 0.47419885  |
| Train/Action_max        | 0.21853814  |
| Train/Action_std        | 0.22473785  |
| Train/Entropy           | -0.251939   |
| Train/Entropy_Loss      | 0.000252    |
| Train/Entropy_loss      | 0.000252    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.47842932  |
| Train/Loss              | 0.013305006 |
| Train/PolicyClip        | 0.011280435 |
| Train/Policy_loss       | 0.008436839 |
| Train/Ratio             | 0.99026793  |
| Train/Return            | 1.117505    |
| Train/V                 | 1.1372405   |
| Train/Value             | 1.1372405   |
| Train/control_penalty   | 0.4616228   |
| Train/policy_loss       | 0.008436839 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 403 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 402          |
| Time/Actor_Time         | 0.0885       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0771       |
| Time/Buffer             | 0.0034       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24500005   |
| Train/Action_magnitu... | 0.5835899    |
| Train/Action_magnitude  | 0.45846936   |
| Train/Action_max        | 0.16422455   |
| Train/Action_std        | 0.19391868   |
| Train/Entropy           | -0.36366805  |
| Train/Entropy_Loss      | 0.000364     |
| Train/Entropy_loss      | 0.000364     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6325283    |
| Train/Loss              | -0.08041734  |
| Train/PolicyClip        | 0.0131112235 |
| Train/Policy_loss       | -0.085496165 |
| Train/Ratio             | 1.0097694    |
| Train/Return            | 1.3314241    |
| Train/V                 | 1.2469803    |
| Train/Value             | 1.2469803    |
| Train/control_penalty   | 0.4715152    |
| Train/policy_loss       | -0.085496165 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.032        |
------------------------------------------

 ---------------- Iteration 404 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 403          |
| Time/Actor_Time         | 0.0891       |
| Time/B_Format_Time      | 0.0757       |
| Time/B_Original_Form... | 0.0767       |
| Time/Buffer             | 0.00235      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23109737   |
| Train/Action_magnitu... | 0.5917688    |
| Train/Action_magnitude  | 0.46587372   |
| Train/Action_max        | 0.17957816   |
| Train/Action_std        | 0.21700084   |
| Train/Entropy           | -0.28321612  |
| Train/Entropy_Loss      | 0.000283     |
| Train/Entropy_loss      | 0.000283     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5197742    |
| Train/Loss              | -0.016119797 |
| Train/PolicyClip        | 0.012898361  |
| Train/Policy_loss       | -0.021110993 |
| Train/Ratio             | 0.97917354   |
| Train/Return            | 1.278355     |
| Train/V                 | 1.2665586    |
| Train/Value             | 1.2665586    |
| Train/control_penalty   | 0.47079805   |
| Train/policy_loss       | -0.021110993 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 405 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 404         |
| Time/Actor_Time         | 0.088       |
| Time/B_Format_Time      | 0.0815      |
| Time/B_Original_Form... | 0.0807      |
| Time/Buffer             | 0.00254     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.22484162  |
| Train/Action_magnitu... | 0.5870113   |
| Train/Action_magnitude  | 0.4624627   |
| Train/Action_max        | 0.20263079  |
| Train/Action_std        | 0.22117084  |
| Train/Entropy           | -0.2574284  |
| Train/Entropy_Loss      | 0.000257    |
| Train/Entropy_loss      | 0.000257    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5479763   |
| Train/Loss              | 0.072058186 |
| Train/PolicyClip        | 0.008164549 |
| Train/Policy_loss       | 0.067093134 |
| Train/Ratio             | 0.9829104   |
| Train/Return            | 1.0555921   |
| Train/V                 | 1.1315256   |
| Train/Value             | 1.1315256   |
| Train/control_penalty   | 0.47076252  |
| Train/policy_loss       | 0.067093134 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01275     |
-----------------------------------------

 ---------------- Iteration 406 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 405          |
| Time/Actor_Time         | 0.0871       |
| Time/B_Format_Time      | 0.103        |
| Time/B_Original_Form... | 0.0726       |
| Time/Buffer             | 0.00313      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23326296   |
| Train/Action_magnitu... | 0.58750886   |
| Train/Action_magnitude  | 0.46238324   |
| Train/Action_max        | 0.21129826   |
| Train/Action_std        | 0.20317319   |
| Train/Entropy           | -0.32160652  |
| Train/Entropy_Loss      | 0.000322     |
| Train/Entropy_loss      | 0.000322     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5855256    |
| Train/Loss              | -0.008928325 |
| Train/PolicyClip        | 0.009122627  |
| Train/Policy_loss       | -0.013828683 |
| Train/Ratio             | 0.988923     |
| Train/Return            | 1.2881662    |
| Train/V                 | 1.2793628    |
| Train/Value             | 1.2793628    |
| Train/control_penalty   | 0.4578752    |
| Train/policy_loss       | -0.013828683 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 407 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 406         |
| Time/Actor_Time         | 0.0876      |
| Time/B_Format_Time      | 0.0735      |
| Time/B_Original_Form... | 0.0727      |
| Time/Buffer             | 0.00439     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23178253  |
| Train/Action_magnitu... | 0.5916748   |
| Train/Action_magnitude  | 0.46701708  |
| Train/Action_max        | 0.21426299  |
| Train/Action_std        | 0.20264895  |
| Train/Entropy           | -0.33742642 |
| Train/Entropy_Loss      | 0.000337    |
| Train/Entropy_loss      | 0.000337    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.55849326  |
| Train/Loss              | -0.06518487 |
| Train/PolicyClip        | 0.013159351 |
| Train/Policy_loss       | -0.07015114 |
| Train/Ratio             | 1.0061581   |
| Train/Return            | 1.3091499   |
| Train/V                 | 1.2469475   |
| Train/Value             | 1.2469475   |
| Train/control_penalty   | 0.46288466  |
| Train/policy_loss       | -0.07015114 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0285      |
-----------------------------------------

 ---------------- Iteration 408 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 407         |
| Time/Actor_Time         | 0.0881      |
| Time/B_Format_Time      | 0.0762      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00408     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2404684   |
| Train/Action_magnitu... | 0.6084694   |
| Train/Action_magnitude  | 0.47899887  |
| Train/Action_max        | 0.20505789  |
| Train/Action_std        | 0.195635    |
| Train/Entropy           | -0.37963063 |
| Train/Entropy_Loss      | 0.00038     |
| Train/Entropy_loss      | 0.00038     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6859243   |
| Train/Loss              | -0.06937791 |
| Train/PolicyClip        | 0.006529032 |
| Train/Policy_loss       | -0.07450281 |
| Train/Ratio             | 1.001971    |
| Train/Return            | 1.3598762   |
| Train/V                 | 1.2950156   |
| Train/Value             | 1.2950156   |
| Train/control_penalty   | 0.47452724  |
| Train/policy_loss       | -0.07450281 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------

 ---------------- Iteration 409 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 408         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00247     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2364252   |
| Train/Action_magnitu... | 0.5914398   |
| Train/Action_magnitude  | 0.46497172  |
| Train/Action_max        | 0.18870442  |
| Train/Action_std        | 0.19918412  |
| Train/Entropy           | -0.35723892 |
| Train/Entropy_Loss      | 0.000357    |
| Train/Entropy_loss      | 0.000357    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6634132   |
| Train/Loss              | -0.04414508 |
| Train/PolicyClip        | 0.005404255 |
| Train/Policy_loss       | -0.04919162 |
| Train/Ratio             | 1.0064644   |
| Train/Return            | 1.266483    |
| Train/V                 | 1.2275522   |
| Train/Value             | 1.2275522   |
| Train/control_penalty   | 0.46892998  |
| Train/policy_loss       | -0.04919162 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 410 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 409         |
| Time/Actor_Time         | 0.0891      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0743      |
| Time/Buffer             | 0.00433     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24196556  |
| Train/Action_magnitu... | 0.60422623  |
| Train/Action_magnitude  | 0.47276366  |
| Train/Action_max        | 0.16764873  |
| Train/Action_std        | 0.19325995  |
| Train/Entropy           | -0.3953483  |
| Train/Entropy_Loss      | 0.000395    |
| Train/Entropy_loss      | 0.000395    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.84564114  |
| Train/Loss              | 0.03263123  |
| Train/PolicyClip        | 0.008718828 |
| Train/Policy_loss       | 0.027524801 |
| Train/Ratio             | 0.9850931   |
| Train/Return            | 1.5651698   |
| Train/V                 | 1.6018488   |
| Train/Value             | 1.6018488   |
| Train/control_penalty   | 0.471108    |
| Train/policy_loss       | 0.027524801 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 411 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 410         |
| Time/Actor_Time         | 0.0912      |
| Time/B_Format_Time      | 0.0754      |
| Time/B_Original_Form... | 0.0773      |
| Time/Buffer             | 0.00249     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23035364  |
| Train/Action_magnitu... | 0.5919835   |
| Train/Action_magnitude  | 0.4659465   |
| Train/Action_max        | 0.19277507  |
| Train/Action_std        | 0.21166962  |
| Train/Entropy           | -0.31326354 |
| Train/Entropy_Loss      | 0.000313    |
| Train/Entropy_loss      | 0.000313    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.6258977   |
| Train/Loss              | 0.062443912 |
| Train/PolicyClip        | 0.00830352  |
| Train/Policy_loss       | 0.05739201  |
| Train/Ratio             | 0.987369    |
| Train/Return            | 1.3292254   |
| Train/V                 | 1.3966849   |
| Train/Value             | 1.3966849   |
| Train/control_penalty   | 0.47386405  |
| Train/policy_loss       | 0.05739201  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 412 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 411          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0736       |
| Time/Buffer             | 0.00281      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.22487934   |
| Train/Action_magnitu... | 0.57828146   |
| Train/Action_magnitude  | 0.4551289    |
| Train/Action_max        | 0.19977473   |
| Train/Action_std        | 0.2103021    |
| Train/Entropy           | -0.30814287  |
| Train/Entropy_Loss      | 0.000308     |
| Train/Entropy_loss      | 0.000308     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.63818043   |
| Train/Loss              | -0.0145921   |
| Train/PolicyClip        | 0.016058115  |
| Train/Policy_loss       | -0.019550437 |
| Train/Ratio             | 1.0029347    |
| Train/Return            | 1.3197327    |
| Train/V                 | 1.3057815    |
| Train/Value             | 1.3057815    |
| Train/control_penalty   | 0.46501943   |
| Train/policy_loss       | -0.019550437 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 413 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 412          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0724       |
| Time/Buffer             | 0.00678      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22632222   |
| Train/Action_magnitu... | 0.5914409    |
| Train/Action_magnitude  | 0.46483245   |
| Train/Action_max        | 0.19602264   |
| Train/Action_std        | 0.22187307   |
| Train/Entropy           | -0.24897696  |
| Train/Entropy_Loss      | 0.000249     |
| Train/Entropy_loss      | 0.000249     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5253848    |
| Train/Loss              | 0.117008545  |
| Train/PolicyClip        | 0.0065863184 |
| Train/Policy_loss       | 0.111998074  |
| Train/Ratio             | 0.97343427   |
| Train/Return            | 1.1568751    |
| Train/V                 | 1.2817605    |
| Train/Value             | 1.2817605    |
| Train/control_penalty   | 0.47614968   |
| Train/policy_loss       | 0.111998074  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0155       |
------------------------------------------

 ---------------- Iteration 414 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 413          |
| Time/Actor_Time         | 0.0846       |
| Time/B_Format_Time      | 0.0749       |
| Time/B_Original_Form... | 0.0762       |
| Time/Buffer             | 0.00303      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.231871     |
| Train/Action_magnitu... | 0.59237593   |
| Train/Action_magnitude  | 0.4645946    |
| Train/Action_max        | 0.19222331   |
| Train/Action_std        | 0.20104787   |
| Train/Entropy           | -0.34686047  |
| Train/Entropy_Loss      | 0.000347     |
| Train/Entropy_loss      | 0.000347     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6449861    |
| Train/Loss              | -0.044476505 |
| Train/PolicyClip        | 0.010337636  |
| Train/Policy_loss       | -0.049471475 |
| Train/Ratio             | 1.0087689    |
| Train/Return            | 1.404485     |
| Train/V                 | 1.3649116    |
| Train/Value             | 1.3649116    |
| Train/control_penalty   | 0.46481097   |
| Train/policy_loss       | -0.049471475 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03175      |
------------------------------------------

 ---------------- Iteration 415 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 414          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0703       |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22204804   |
| Train/Action_magnitu... | 0.58722574   |
| Train/Action_magnitude  | 0.46015796   |
| Train/Action_max        | 0.17795265   |
| Train/Action_std        | 0.21952298   |
| Train/Entropy           | -0.26147807  |
| Train/Entropy_Loss      | 0.000261     |
| Train/Entropy_loss      | 0.000261     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.54261154   |
| Train/Loss              | 0.017810779  |
| Train/PolicyClip        | 0.007130513  |
| Train/Policy_loss       | 0.0129170185 |
| Train/Ratio             | 0.9899364    |
| Train/Return            | 1.117872     |
| Train/V                 | 1.1363612    |
| Train/Value             | 1.1363612    |
| Train/control_penalty   | 0.4632282    |
| Train/policy_loss       | 0.0129170185 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 416 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 415          |
| Time/Actor_Time         | 0.1          |
| Time/B_Format_Time      | 0.0733       |
| Time/B_Original_Form... | 0.0726       |
| Time/Buffer             | 0.00287      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22378376   |
| Train/Action_magnitu... | 0.60289747   |
| Train/Action_magnitude  | 0.47439817   |
| Train/Action_max        | 0.20228694   |
| Train/Action_std        | 0.2213524    |
| Train/Entropy           | -0.24802835  |
| Train/Entropy_Loss      | 0.000248     |
| Train/Entropy_loss      | 0.000248     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.45331958   |
| Train/Loss              | -0.00628778  |
| Train/PolicyClip        | 0.0052432274 |
| Train/Policy_loss       | -0.011252878 |
| Train/Ratio             | 1.0023313    |
| Train/Return            | 1.074647     |
| Train/V                 | 1.0652859    |
| Train/Value             | 1.0652859    |
| Train/control_penalty   | 0.47170705   |
| Train/policy_loss       | -0.011252878 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 417 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 416          |
| Time/Actor_Time         | 0.0871       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.22363661   |
| Train/Action_magnitu... | 0.57857287   |
| Train/Action_magnitude  | 0.45463815   |
| Train/Action_max        | 0.1964468    |
| Train/Action_std        | 0.20749173   |
| Train/Entropy           | -0.3116944   |
| Train/Entropy_Loss      | 0.000312     |
| Train/Entropy_loss      | 0.000312     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.569022     |
| Train/Loss              | 0.009785211  |
| Train/PolicyClip        | 0.01146461   |
| Train/Policy_loss       | 0.0049271793 |
| Train/Ratio             | 0.979944     |
| Train/Return            | 1.0146999    |
| Train/V                 | 1.0296191    |
| Train/Value             | 1.0296191    |
| Train/control_penalty   | 0.45463374   |
| Train/policy_loss       | 0.0049271793 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 418 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 417          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00302      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23410733   |
| Train/Action_magnitu... | 0.60300875   |
| Train/Action_magnitude  | 0.47323233   |
| Train/Action_max        | 0.20566584   |
| Train/Action_std        | 0.2157603    |
| Train/Entropy           | -0.26961544  |
| Train/Entropy_Loss      | 0.00027      |
| Train/Entropy_loss      | 0.00027      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.49722785   |
| Train/Loss              | -0.027265657 |
| Train/PolicyClip        | 0.008446728  |
| Train/Policy_loss       | -0.032297704 |
| Train/Ratio             | 0.99456906   |
| Train/Return            | 0.96675646   |
| Train/V                 | 0.9445954    |
| Train/Value             | 0.9445954    |
| Train/control_penalty   | 0.4762433    |
| Train/policy_loss       | -0.032297704 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 419 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 418          |
| Time/Actor_Time         | 0.0879       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0742       |
| Time/Buffer             | 0.00293      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21479449   |
| Train/Action_magnitu... | 0.5829951    |
| Train/Action_magnitude  | 0.46049082   |
| Train/Action_max        | 0.2074445    |
| Train/Action_std        | 0.22796682   |
| Train/Entropy           | -0.21645626  |
| Train/Entropy_Loss      | 0.000216     |
| Train/Entropy_loss      | 0.000216     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4731354    |
| Train/Loss              | 0.010141119  |
| Train/PolicyClip        | 0.0060995244 |
| Train/Policy_loss       | 0.005279888  |
| Train/Ratio             | 0.9933079    |
| Train/Return            | 0.8135636    |
| Train/V                 | 0.8260204    |
| Train/Value             | 0.8260204    |
| Train/control_penalty   | 0.4644776    |
| Train/policy_loss       | 0.005279888  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.017        |
------------------------------------------

 ---------------- Iteration 420 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 419           |
| Time/Actor_Time         | 0.0846        |
| Time/B_Format_Time      | 0.073         |
| Time/B_Original_Form... | 0.0747        |
| Time/Buffer             | 0.00249       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.22231591    |
| Train/Action_magnitu... | 0.6050309     |
| Train/Action_magnitude  | 0.47434214    |
| Train/Action_max        | 0.20794295    |
| Train/Action_std        | 0.22444381    |
| Train/Entropy           | -0.22138698   |
| Train/Entropy_Loss      | 0.000221      |
| Train/Entropy_loss      | 0.000221      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.39541212    |
| Train/Loss              | -0.0062314197 |
| Train/PolicyClip        | 0.009144125   |
| Train/Policy_loss       | -0.011146972  |
| Train/Ratio             | 0.9942294     |
| Train/Return            | 0.75909853    |
| Train/V                 | 0.7590825     |
| Train/Value             | 0.7590825     |
| Train/control_penalty   | 0.4694166     |
| Train/policy_loss       | -0.011146972  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0135        |
-------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 421 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 420           |
| Time/Actor_Time         | 0.0861        |
| Time/B_Format_Time      | 0.0768        |
| Time/B_Original_Form... | 0.123         |
| Time/Buffer             | 0.00354       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.23024751    |
| Train/Action_magnitu... | 0.5948199     |
| Train/Action_magnitude  | 0.4694493     |
| Train/Action_max        | 0.18531522    |
| Train/Action_std        | 0.21137021    |
| Train/Entropy           | -0.28367168   |
| Train/Entropy_Loss      | 0.000284      |
| Train/Entropy_loss      | 0.000284      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.47354138    |
| Train/Loss              | 0.0048212027  |
| Train/PolicyClip        | 0.010336442   |
| Train/Policy_loss       | -0.0001284461 |
| Train/Ratio             | 0.9840572     |
| Train/Return            | 0.85325825    |
| Train/V                 | 0.860821      |
| Train/Value             | 0.860821      |
| Train/control_penalty   | 0.46659777    |
| Train/policy_loss       | -0.0001284461 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01675       |
-------------------------------------------

 ---------------- Iteration 422 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 421         |
| Time/Actor_Time         | 0.0872      |
| Time/B_Format_Time      | 0.07        |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00312     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21703202  |
| Train/Action_magnitu... | 0.5699648   |
| Train/Action_magnitude  | 0.44794396  |
| Train/Action_max        | 0.18970108  |
| Train/Action_std        | 0.21100014  |
| Train/Entropy           | -0.2990557  |
| Train/Entropy_Loss      | 0.000299    |
| Train/Entropy_loss      | 0.000299    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5655293   |
| Train/Loss              | 0.02133903  |
| Train/PolicyClip        | 0.00932582  |
| Train/Policy_loss       | 0.016550407 |
| Train/Ratio             | 0.98868275  |
| Train/Return            | 0.8260058   |
| Train/V                 | 0.8489439   |
| Train/Value             | 0.8489439   |
| Train/control_penalty   | 0.4489568   |
| Train/policy_loss       | 0.016550407 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.015       |
-----------------------------------------

 ---------------- Iteration 423 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 422          |
| Time/Actor_Time         | 0.0879       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0758       |
| Time/Buffer             | 0.00261      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22893961   |
| Train/Action_magnitu... | 0.5962126    |
| Train/Action_magnitude  | 0.46744788   |
| Train/Action_max        | 0.19206817   |
| Train/Action_std        | 0.21336485   |
| Train/Entropy           | -0.2685058   |
| Train/Entropy_Loss      | 0.000269     |
| Train/Entropy_loss      | 0.000269     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4607328    |
| Train/Loss              | -0.022156615 |
| Train/PolicyClip        | 0.012203751  |
| Train/Policy_loss       | -0.02709329  |
| Train/Ratio             | 0.9874744    |
| Train/Return            | 0.91490626   |
| Train/V                 | 0.8954776    |
| Train/Value             | 0.8954776    |
| Train/control_penalty   | 0.4668169    |
| Train/policy_loss       | -0.02709329  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 424 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 423         |
| Time/Actor_Time         | 0.0848      |
| Time/B_Format_Time      | 0.0707      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00251     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22503069  |
| Train/Action_magnitu... | 0.5812751   |
| Train/Action_magnitude  | 0.45587596  |
| Train/Action_max        | 0.18056898  |
| Train/Action_std        | 0.20409648  |
| Train/Entropy           | -0.32993442 |
| Train/Entropy_Loss      | 0.00033     |
| Train/Entropy_loss      | 0.00033     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5239184   |
| Train/Loss              | -0.0441721  |
| Train/PolicyClip        | 0.009076179 |
| Train/Policy_loss       | -0.04903505 |
| Train/Ratio             | 1.0089406   |
| Train/Return            | 0.9775925   |
| Train/V                 | 0.9272709   |
| Train/Value             | 0.9272709   |
| Train/control_penalty   | 0.45330167  |
| Train/policy_loss       | -0.04903505 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02125     |
-----------------------------------------

 ---------------- Iteration 425 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 424          |
| Time/Actor_Time         | 0.0907       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.2146608    |
| Train/Action_magnitu... | 0.56074315   |
| Train/Action_magnitude  | 0.4415566    |
| Train/Action_max        | 0.19726561   |
| Train/Action_std        | 0.1964963    |
| Train/Entropy           | -0.353443    |
| Train/Entropy_Loss      | 0.000353     |
| Train/Entropy_loss      | 0.000353     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.64001817   |
| Train/Loss              | -0.085101634 |
| Train/PolicyClip        | 0.01053541   |
| Train/Policy_loss       | -0.089894846 |
| Train/Ratio             | 1.0052915    |
| Train/Return            | 0.86656713   |
| Train/V                 | 0.77310276   |
| Train/Value             | 0.77310276   |
| Train/control_penalty   | 0.4439772    |
| Train/policy_loss       | -0.089894846 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 426 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 425         |
| Time/Actor_Time         | 0.0878      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22213428  |
| Train/Action_magnitu... | 0.56142026  |
| Train/Action_magnitude  | 0.44340783  |
| Train/Action_max        | 0.19981186  |
| Train/Action_std        | 0.19737658  |
| Train/Entropy           | -0.3535819  |
| Train/Entropy_Loss      | 0.000354    |
| Train/Entropy_loss      | 0.000354    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.65146106  |
| Train/Loss              | -0.08480002 |
| Train/PolicyClip        | 0.007594065 |
| Train/Policy_loss       | -0.08961912 |
| Train/Ratio             | 1.0112938   |
| Train/Return            | 0.9608185   |
| Train/V                 | 0.87576276  |
| Train/Value             | 0.87576276  |
| Train/control_penalty   | 0.4465522   |
| Train/policy_loss       | -0.08961912 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 427 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 426          |
| Time/Actor_Time         | 0.0891       |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00293      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23225425   |
| Train/Action_magnitu... | 0.5774844    |
| Train/Action_magnitude  | 0.45683846   |
| Train/Action_max        | 0.20544548   |
| Train/Action_std        | 0.19763155   |
| Train/Entropy           | -0.35229883  |
| Train/Entropy_Loss      | 0.000352     |
| Train/Entropy_loss      | 0.000352     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6743574    |
| Train/Loss              | -0.03514172  |
| Train/PolicyClip        | 0.005775666  |
| Train/Policy_loss       | -0.040096518 |
| Train/Ratio             | 0.9989447    |
| Train/Return            | 0.887159     |
| Train/V                 | 0.8506096    |
| Train/Value             | 0.8506096    |
| Train/control_penalty   | 0.46025002   |
| Train/policy_loss       | -0.040096518 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 428 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 427          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0725       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22182098   |
| Train/Action_magnitu... | 0.555651     |
| Train/Action_magnitude  | 0.43861884   |
| Train/Action_max        | 0.21539162   |
| Train/Action_std        | 0.18829082   |
| Train/Entropy           | -0.3980633   |
| Train/Entropy_Loss      | 0.000398     |
| Train/Entropy_loss      | 0.000398     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.75878114   |
| Train/Loss              | -0.052093193 |
| Train/PolicyClip        | 0.008331052  |
| Train/Policy_loss       | -0.05686515  |
| Train/Ratio             | 0.9953744    |
| Train/Return            | 0.99041086   |
| Train/V                 | 0.94217783   |
| Train/Value             | 0.94217783   |
| Train/control_penalty   | 0.43738934   |
| Train/policy_loss       | -0.05686515  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 429 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 428          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0736       |
| Time/B_Original_Form... | 0.0759       |
| Time/Buffer             | 0.00339      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23195422   |
| Train/Action_magnitu... | 0.57313025   |
| Train/Action_magnitude  | 0.4489734    |
| Train/Action_max        | 0.19856982   |
| Train/Action_std        | 0.18679807   |
| Train/Entropy           | -0.4084472   |
| Train/Entropy_Loss      | 0.000408     |
| Train/Entropy_loss      | 0.000408     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.71549034   |
| Train/Loss              | -0.11677541  |
| Train/PolicyClip        | 0.010339356  |
| Train/Policy_loss       | -0.121699676 |
| Train/Ratio             | 1.0145358    |
| Train/Return            | 1.2851276    |
| Train/V                 | 1.1722865    |
| Train/Value             | 1.1722865    |
| Train/control_penalty   | 0.45158157   |
| Train/policy_loss       | -0.121699676 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0325       |
------------------------------------------

 ---------------- Iteration 430 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 429          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0745       |
| Time/Buffer             | 0.00216      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22111875   |
| Train/Action_magnitu... | 0.5448338    |
| Train/Action_magnitude  | 0.42854345   |
| Train/Action_max        | 0.20446579   |
| Train/Action_std        | 0.1916027    |
| Train/Entropy           | -0.3707584   |
| Train/Entropy_Loss      | 0.000371     |
| Train/Entropy_loss      | 0.000371     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.767953     |
| Train/Loss              | -0.017611604 |
| Train/PolicyClip        | 0.0074764597 |
| Train/Policy_loss       | -0.02235727  |
| Train/Ratio             | 0.9945252    |
| Train/Return            | 1.0075555    |
| Train/V                 | 0.99234444   |
| Train/Value             | 0.99234444   |
| Train/control_penalty   | 0.43749088   |
| Train/policy_loss       | -0.02235727  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------

 ---------------- Iteration 431 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 430          |
| Time/Actor_Time         | 0.117        |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0739       |
| Time/Buffer             | 0.00283      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2171184    |
| Train/Action_magnitu... | 0.5278766    |
| Train/Action_magnitude  | 0.41638854   |
| Train/Action_max        | 0.2086324    |
| Train/Action_std        | 0.18625468   |
| Train/Entropy           | -0.39474356  |
| Train/Entropy_Loss      | 0.000395     |
| Train/Entropy_loss      | 0.000395     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.82580996   |
| Train/Loss              | -0.05711785  |
| Train/PolicyClip        | 0.0061611678 |
| Train/Policy_loss       | -0.061789293 |
| Train/Ratio             | 1.0046352    |
| Train/Return            | 1.2940843    |
| Train/V                 | 1.2245246    |
| Train/Value             | 1.2245246    |
| Train/control_penalty   | 0.4276702    |
| Train/policy_loss       | -0.061789293 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03025      |
------------------------------------------

 ---------------- Iteration 432 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 431         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0784      |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21579017  |
| Train/Action_magnitu... | 0.5426919   |
| Train/Action_magnitude  | 0.42809266  |
| Train/Action_max        | 0.20517498  |
| Train/Action_std        | 0.19098105  |
| Train/Entropy           | -0.3535777  |
| Train/Entropy_Loss      | 0.000354    |
| Train/Entropy_loss      | 0.000354    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.693172    |
| Train/Loss              | -0.06587272 |
| Train/PolicyClip        | 0.007974758 |
| Train/Policy_loss       | -0.07054956 |
| Train/Ratio             | 1.0031582   |
| Train/Return            | 1.0456294   |
| Train/V                 | 0.9848699   |
| Train/Value             | 0.9848699   |
| Train/control_penalty   | 0.4323265   |
| Train/policy_loss       | -0.07054956 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 433 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 432          |
| Time/Actor_Time         | 0.0908       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0761       |
| Time/Buffer             | 0.00311      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22919169   |
| Train/Action_magnitu... | 0.5716688    |
| Train/Action_magnitude  | 0.44984645   |
| Train/Action_max        | 0.17442651   |
| Train/Action_std        | 0.19773968   |
| Train/Entropy           | -0.3344495   |
| Train/Entropy_Loss      | 0.000334     |
| Train/Entropy_loss      | 0.000334     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.61535215   |
| Train/Loss              | -0.032629814 |
| Train/PolicyClip        | 0.013837575  |
| Train/Policy_loss       | -0.037452612 |
| Train/Ratio             | 1.00761      |
| Train/Return            | 1.1457173    |
| Train/V                 | 1.1151155    |
| Train/Value             | 1.1151155    |
| Train/control_penalty   | 0.44883478   |
| Train/policy_loss       | -0.037452612 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 434 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 433          |
| Time/Actor_Time         | 0.0977       |
| Time/B_Format_Time      | 0.077        |
| Time/B_Original_Form... | 0.0789       |
| Time/Buffer             | 0.00322      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2260466    |
| Train/Action_magnitu... | 0.564766     |
| Train/Action_magnitude  | 0.44503906   |
| Train/Action_max        | 0.18478917   |
| Train/Action_std        | 0.20320398   |
| Train/Entropy           | -0.33000666  |
| Train/Entropy_Loss      | 0.00033      |
| Train/Entropy_loss      | 0.00033      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.63321084   |
| Train/Loss              | -0.028844494 |
| Train/PolicyClip        | 0.009807428  |
| Train/Policy_loss       | -0.03368842  |
| Train/Ratio             | 0.99536324   |
| Train/Return            | 1.2322961    |
| Train/V                 | 1.2061206    |
| Train/Value             | 1.2061206    |
| Train/control_penalty   | 0.45139197   |
| Train/policy_loss       | -0.03368842  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02675      |
------------------------------------------

 ---------------- Iteration 435 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 434          |
| Time/Actor_Time         | 0.0934       |
| Time/B_Format_Time      | 0.0807       |
| Time/B_Original_Form... | 0.0836       |
| Time/Buffer             | 0.00321      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22613256   |
| Train/Action_magnitu... | 0.56898403   |
| Train/Action_magnitude  | 0.44740048   |
| Train/Action_max        | 0.21651532   |
| Train/Action_std        | 0.20277473   |
| Train/Entropy           | -0.309052    |
| Train/Entropy_Loss      | 0.000309     |
| Train/Entropy_loss      | 0.000309     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5880435    |
| Train/Loss              | -0.032443568 |
| Train/PolicyClip        | 0.010237826  |
| Train/Policy_loss       | -0.037226465 |
| Train/Ratio             | 1.0066783    |
| Train/Return            | 1.181792     |
| Train/V                 | 1.1459697    |
| Train/Value             | 1.1459697    |
| Train/control_penalty   | 0.44738433   |
| Train/policy_loss       | -0.037226465 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 436 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 435           |
| Time/Actor_Time         | 0.0897        |
| Time/B_Format_Time      | 0.0732        |
| Time/B_Original_Form... | 0.0739        |
| Time/Buffer             | 0.00425       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.21950515    |
| Train/Action_magnitu... | 0.55841166    |
| Train/Action_magnitude  | 0.43973276    |
| Train/Action_max        | 0.20978215    |
| Train/Action_std        | 0.20336275    |
| Train/Entropy           | -0.31303346   |
| Train/Entropy_Loss      | 0.000313      |
| Train/Entropy_loss      | 0.000313      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.59127295    |
| Train/Loss              | 0.00066531356 |
| Train/PolicyClip        | 0.009689489   |
| Train/Policy_loss       | -0.004068583  |
| Train/Ratio             | 0.98660433    |
| Train/Return            | 1.2267826     |
| Train/V                 | 1.2298747     |
| Train/Value             | 1.2298747     |
| Train/control_penalty   | 0.4420863     |
| Train/policy_loss       | -0.004068583  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02425       |
-------------------------------------------

 ---------------- Iteration 437 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 436         |
| Time/Actor_Time         | 0.0864      |
| Time/B_Format_Time      | 0.0778      |
| Time/B_Original_Form... | 0.0783      |
| Time/Buffer             | 0.00238     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22450879  |
| Train/Action_magnitu... | 0.5807638   |
| Train/Action_magnitude  | 0.45650184  |
| Train/Action_max        | 0.19135685  |
| Train/Action_std        | 0.21594772  |
| Train/Entropy           | -0.2429701  |
| Train/Entropy_Loss      | 0.000243    |
| Train/Entropy_loss      | 0.000243    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.47847655  |
| Train/Loss              | 0.031507432 |
| Train/PolicyClip        | 0.006261157 |
| Train/Policy_loss       | 0.026674904 |
| Train/Ratio             | 0.9868265   |
| Train/Return            | 0.8347691   |
| Train/V                 | 0.87021416  |
| Train/Value             | 0.87021416  |
| Train/control_penalty   | 0.45895562  |
| Train/policy_loss       | 0.026674904 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0135      |
-----------------------------------------

 ---------------- Iteration 438 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 437          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0765       |
| Time/Buffer             | 0.00308      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.231498     |
| Train/Action_magnitu... | 0.56532484   |
| Train/Action_magnitude  | 0.44511816   |
| Train/Action_max        | 0.17806461   |
| Train/Action_std        | 0.1943321    |
| Train/Entropy           | -0.35626996  |
| Train/Entropy_Loss      | 0.000356     |
| Train/Entropy_loss      | 0.000356     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6571988    |
| Train/Loss              | -0.036941428 |
| Train/PolicyClip        | 0.009262287  |
| Train/Policy_loss       | -0.041786596 |
| Train/Ratio             | 1.0027273    |
| Train/Return            | 1.2153467    |
| Train/V                 | 1.1732738    |
| Train/Value             | 1.1732738    |
| Train/control_penalty   | 0.44889003   |
| Train/policy_loss       | -0.041786596 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 439 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 438          |
| Time/Actor_Time         | 0.0929       |
| Time/B_Format_Time      | 0.0738       |
| Time/B_Original_Form... | 0.074        |
| Time/Buffer             | 0.00271      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22290547   |
| Train/Action_magnitu... | 0.56250024   |
| Train/Action_magnitude  | 0.44319642   |
| Train/Action_max        | 0.20108047   |
| Train/Action_std        | 0.19746268   |
| Train/Entropy           | -0.31632435  |
| Train/Entropy_Loss      | 0.000316     |
| Train/Entropy_loss      | 0.000316     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.54432106   |
| Train/Loss              | -0.03780923  |
| Train/PolicyClip        | 0.0062253783 |
| Train/Policy_loss       | -0.042574607 |
| Train/Ratio             | 1.0018457    |
| Train/Return            | 1.0579313    |
| Train/V                 | 1.0207658    |
| Train/Value             | 1.0207658    |
| Train/control_penalty   | 0.44490528   |
| Train/policy_loss       | -0.042574607 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 440 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 439         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00264     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24280642  |
| Train/Action_magnitu... | 0.60822546  |
| Train/Action_magnitude  | 0.47801715  |
| Train/Action_max        | 0.20772007  |
| Train/Action_std        | 0.2188191   |
| Train/Entropy           | -0.22215933 |
| Train/Entropy_Loss      | 0.000222    |
| Train/Entropy_loss      | 0.000222    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.4570959   |
| Train/Loss              | 0.039872188 |
| Train/PolicyClip        | 0.017353382 |
| Train/Policy_loss       | 0.034809485 |
| Train/Ratio             | 0.98600596  |
| Train/Return            | 1.1076071   |
| Train/V                 | 1.1496176   |
| Train/Value             | 1.1496176   |
| Train/control_penalty   | 0.48405448  |
| Train/policy_loss       | 0.034809485 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 441 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 440          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0806       |
| Time/B_Original_Form... | 0.0792       |
| Time/Buffer             | 0.00329      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2152838    |
| Train/Action_magnitu... | 0.56177175   |
| Train/Action_magnitude  | 0.4433567    |
| Train/Action_max        | 0.1900888    |
| Train/Action_std        | 0.20094897   |
| Train/Entropy           | -0.3023353   |
| Train/Entropy_Loss      | 0.000302     |
| Train/Entropy_loss      | 0.000302     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.5486649    |
| Train/Loss              | 0.01647998   |
| Train/PolicyClip        | 0.0062856474 |
| Train/Policy_loss       | 0.011826292  |
| Train/Ratio             | 0.9944375    |
| Train/Return            | 1.2181492    |
| Train/V                 | 1.2427347    |
| Train/Value             | 1.2427347    |
| Train/control_penalty   | 0.43513533   |
| Train/policy_loss       | 0.011826292  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 442 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 441         |
| Time/Actor_Time         | 0.0856      |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.075       |
| Time/Buffer             | 0.00559     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22193515  |
| Train/Action_magnitu... | 0.5582973   |
| Train/Action_magnitude  | 0.43833143  |
| Train/Action_max        | 0.20184794  |
| Train/Action_std        | 0.20703912  |
| Train/Entropy           | -0.25447455 |
| Train/Entropy_Loss      | 0.000254    |
| Train/Entropy_loss      | 0.000254    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.52389014  |
| Train/Loss              | 0.03249583  |
| Train/PolicyClip        | 0.006596307 |
| Train/Policy_loss       | 0.027780294 |
| Train/Ratio             | 0.9810548   |
| Train/Return            | 1.290905    |
| Train/V                 | 1.3257737   |
| Train/Value             | 1.3257737   |
| Train/control_penalty   | 0.44610608  |
| Train/policy_loss       | 0.027780294 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 443 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 442          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0758       |
| Time/Buffer             | 0.00315      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23916373   |
| Train/Action_magnitu... | 0.61108226   |
| Train/Action_magnitude  | 0.48094258   |
| Train/Action_max        | 0.2045138    |
| Train/Action_std        | 0.21758038   |
| Train/Entropy           | -0.21955256  |
| Train/Entropy_Loss      | 0.00022      |
| Train/Entropy_loss      | 0.00022      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4355162    |
| Train/Loss              | -0.017889991 |
| Train/PolicyClip        | 0.008712069  |
| Train/Policy_loss       | -0.022921333 |
| Train/Ratio             | 0.99609053   |
| Train/Return            | 1.2152746    |
| Train/V                 | 1.1997346    |
| Train/Value             | 1.1997346    |
| Train/control_penalty   | 0.48117906   |
| Train/policy_loss       | -0.022921333 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0295       |
------------------------------------------

 ---------------- Iteration 444 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 443           |
| Time/Actor_Time         | 0.0848        |
| Time/B_Format_Time      | 0.0745        |
| Time/B_Original_Form... | 0.0787        |
| Time/Buffer             | 0.00284       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.21932445    |
| Train/Action_magnitu... | 0.574201      |
| Train/Action_magnitude  | 0.45154372    |
| Train/Action_max        | 0.19121565    |
| Train/Action_std        | 0.21143307    |
| Train/Entropy           | -0.25175676   |
| Train/Entropy_Loss      | 0.000252      |
| Train/Entropy_loss      | 0.000252      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.41676053    |
| Train/Loss              | -0.0066811475 |
| Train/PolicyClip        | 0.012983601   |
| Train/Policy_loss       | -0.011420132  |
| Train/Ratio             | 0.9932881     |
| Train/Return            | 1.0566596     |
| Train/V                 | 1.048056      |
| Train/Value             | 1.048056      |
| Train/control_penalty   | 0.44872275    |
| Train/policy_loss       | -0.011420132  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02275       |
-------------------------------------------

 ---------------- Iteration 445 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 444          |
| Time/Actor_Time         | 0.0898       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.075        |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22467934   |
| Train/Action_magnitu... | 0.5759078    |
| Train/Action_magnitude  | 0.45388126   |
| Train/Action_max        | 0.20772362   |
| Train/Action_std        | 0.20904742   |
| Train/Entropy           | -0.24235238  |
| Train/Entropy_Loss      | 0.000242     |
| Train/Entropy_loss      | 0.000242     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.42372647   |
| Train/Loss              | -0.016020939 |
| Train/PolicyClip        | 0.0097006615 |
| Train/Policy_loss       | -0.020808524 |
| Train/Ratio             | 0.9933302    |
| Train/Return            | 1.2128848    |
| Train/V                 | 1.2039641    |
| Train/Value             | 1.2039641    |
| Train/control_penalty   | 0.45452318   |
| Train/policy_loss       | -0.020808524 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 446 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 445         |
| Time/Actor_Time         | 0.0862      |
| Time/B_Format_Time      | 0.08        |
| Time/B_Original_Form... | 0.0778      |
| Time/Buffer             | 0.00299     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2143117   |
| Train/Action_magnitu... | 0.5673675   |
| Train/Action_magnitude  | 0.44619554  |
| Train/Action_max        | 0.18880633  |
| Train/Action_std        | 0.20528974  |
| Train/Entropy           | -0.2588089  |
| Train/Entropy_Loss      | 0.000259    |
| Train/Entropy_loss      | 0.000259    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.47076786  |
| Train/Loss              | -0.06584336 |
| Train/PolicyClip        | 0.009748679 |
| Train/Policy_loss       | -0.07053461 |
| Train/Ratio             | 1.00668     |
| Train/Return            | 1.3070946   |
| Train/V                 | 1.2483634   |
| Train/Value             | 1.2483634   |
| Train/control_penalty   | 0.44324395  |
| Train/policy_loss       | -0.07053461 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 447 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 446          |
| Time/Actor_Time         | 0.0882       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0748       |
| Time/Buffer             | 0.00349      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21387787   |
| Train/Action_magnitu... | 0.5579948    |
| Train/Action_magnitude  | 0.44017962   |
| Train/Action_max        | 0.20376922   |
| Train/Action_std        | 0.20412393   |
| Train/Entropy           | -0.29375246  |
| Train/Entropy_Loss      | 0.000294     |
| Train/Entropy_loss      | 0.000294     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.50706565   |
| Train/Loss              | -0.036455117 |
| Train/PolicyClip        | 0.009438426  |
| Train/Policy_loss       | -0.04116934  |
| Train/Ratio             | 0.99763644   |
| Train/Return            | 1.2789222    |
| Train/V                 | 1.2425369    |
| Train/Value             | 1.2425369    |
| Train/control_penalty   | 0.4420471    |
| Train/policy_loss       | -0.04116934  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0305       |
------------------------------------------

 ---------------- Iteration 448 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 447          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0757       |
| Time/B_Original_Form... | 0.0775       |
| Time/Buffer             | 0.00363      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20709066   |
| Train/Action_magnitu... | 0.5493046    |
| Train/Action_magnitude  | 0.43295392   |
| Train/Action_max        | 0.21620491   |
| Train/Action_std        | 0.21207878   |
| Train/Entropy           | -0.23874661  |
| Train/Entropy_Loss      | 0.000239     |
| Train/Entropy_loss      | 0.000239     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4429041    |
| Train/Loss              | -0.013750995 |
| Train/PolicyClip        | 0.010128109  |
| Train/Policy_loss       | -0.018319584 |
| Train/Ratio             | 0.9925617    |
| Train/Return            | 1.3323348    |
| Train/V                 | 1.3205318    |
| Train/Value             | 1.3205318    |
| Train/control_penalty   | 0.43298438   |
| Train/policy_loss       | -0.018319584 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 449 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 448          |
| Time/Actor_Time         | 0.0933       |
| Time/B_Format_Time      | 0.0795       |
| Time/B_Original_Form... | 0.0815       |
| Time/Buffer             | 0.0031       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.212866     |
| Train/Action_magnitu... | 0.53509885   |
| Train/Action_magnitude  | 0.42270902   |
| Train/Action_max        | 0.2244321    |
| Train/Action_std        | 0.19608344   |
| Train/Entropy           | -0.32224247  |
| Train/Entropy_Loss      | 0.000322     |
| Train/Entropy_loss      | 0.000322     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.6180583    |
| Train/Loss              | -0.055109542 |
| Train/PolicyClip        | 0.005761092  |
| Train/Policy_loss       | -0.05974415  |
| Train/Ratio             | 1.0004818    |
| Train/Return            | 1.4200176    |
| Train/V                 | 1.3678428    |
| Train/Value             | 1.3678428    |
| Train/control_penalty   | 0.43123668   |
| Train/policy_loss       | -0.05974415  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03375      |
------------------------------------------

 ---------------- Iteration 450 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 449           |
| Time/Actor_Time         | 0.0893        |
| Time/B_Format_Time      | 0.071         |
| Time/B_Original_Form... | 0.0813        |
| Time/Buffer             | 0.00337       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.21033078    |
| Train/Action_magnitu... | 0.54940194    |
| Train/Action_magnitude  | 0.43308437    |
| Train/Action_max        | 0.21739843    |
| Train/Action_std        | 0.20189682    |
| Train/Entropy           | -0.28797075   |
| Train/Entropy_Loss      | 0.000288      |
| Train/Entropy_loss      | 0.000288      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.49809104    |
| Train/Loss              | -0.0032478068 |
| Train/PolicyClip        | 0.006673022   |
| Train/Policy_loss       | -0.007855198  |
| Train/Ratio             | 0.9969967     |
| Train/Return            | 1.2307243     |
| Train/V                 | 1.2308164     |
| Train/Value             | 1.2308164     |
| Train/control_penalty   | 0.4319421     |
| Train/policy_loss       | -0.007855198  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.027         |
-------------------------------------------

 ---------------- Iteration 451 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 450         |
| Time/Actor_Time         | 0.091       |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.0803      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.19064358  |
| Train/Action_magnitu... | 0.52282083  |
| Train/Action_magnitude  | 0.4131567   |
| Train/Action_max        | 0.20410512  |
| Train/Action_std        | 0.2101894   |
| Train/Entropy           | -0.2491118  |
| Train/Entropy_Loss      | 0.000249    |
| Train/Entropy_loss      | 0.000249    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.51715636  |
| Train/Loss              | 0.013004084 |
| Train/PolicyClip        | 0.007225904 |
| Train/Policy_loss       | 0.008618207 |
| Train/Ratio             | 0.9924992   |
| Train/Return            | 1.2114733   |
| Train/V                 | 1.226723    |
| Train/Value             | 1.226723    |
| Train/control_penalty   | 0.41367656  |
| Train/policy_loss       | 0.008618207 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 452 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 451          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0738       |
| Time/Buffer             | 0.00327      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20628795   |
| Train/Action_magnitu... | 0.5495321    |
| Train/Action_magnitude  | 0.43243268   |
| Train/Action_max        | 0.2112417    |
| Train/Action_std        | 0.20754842   |
| Train/Entropy           | -0.26132116  |
| Train/Entropy_Loss      | 0.000261     |
| Train/Entropy_loss      | 0.000261     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4783256    |
| Train/Loss              | -0.005898803 |
| Train/PolicyClip        | 0.006997108  |
| Train/Policy_loss       | -0.010500167 |
| Train/Ratio             | 0.991438     |
| Train/Return            | 1.1775044    |
| Train/V                 | 1.1769584    |
| Train/Value             | 1.1769584    |
| Train/control_penalty   | 0.43400428   |
| Train/policy_loss       | -0.010500167 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 453 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 452            |
| Time/Actor_Time         | 0.0875         |
| Time/B_Format_Time      | 0.0723         |
| Time/B_Original_Form... | 0.0765         |
| Time/Buffer             | 0.00307        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.20950426     |
| Train/Action_magnitu... | 0.5407983      |
| Train/Action_magnitude  | 0.4262162      |
| Train/Action_max        | 0.18859698     |
| Train/Action_std        | 0.20057154     |
| Train/Entropy           | -0.29739928    |
| Train/Entropy_Loss      | 0.000297       |
| Train/Entropy_loss      | 0.000297       |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | 0.5157393      |
| Train/Loss              | -0.00014325697 |
| Train/PolicyClip        | 0.0051061823   |
| Train/Policy_loss       | -0.004692274   |
| Train/Ratio             | 0.99973434     |
| Train/Return            | 1.3448305      |
| Train/V                 | 1.3491496      |
| Train/Value             | 1.3491496      |
| Train/control_penalty   | 0.4251618      |
| Train/policy_loss       | -0.004692274   |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.02625        |
--------------------------------------------

 ---------------- Iteration 454 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 453         |
| Time/Actor_Time         | 0.0865      |
| Time/B_Format_Time      | 0.076       |
| Time/B_Original_Form... | 0.0772      |
| Time/Buffer             | 0.00433     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.19937095  |
| Train/Action_magnitu... | 0.5303723   |
| Train/Action_magnitude  | 0.4168188   |
| Train/Action_max        | 0.18963297  |
| Train/Action_std        | 0.20174928  |
| Train/Entropy           | -0.28172442 |
| Train/Entropy_Loss      | 0.000282    |
| Train/Entropy_loss      | 0.000282    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.5091599   |
| Train/Loss              | -0.03132151 |
| Train/PolicyClip        | 0.010426742 |
| Train/Policy_loss       | -0.03580927 |
| Train/Ratio             | 0.99767524  |
| Train/Return            | 1.465979    |
| Train/V                 | 1.4366511   |
| Train/Value             | 1.4366511   |
| Train/control_penalty   | 0.4206035   |
| Train/policy_loss       | -0.03580927 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.032       |
-----------------------------------------

 ---------------- Iteration 455 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 454           |
| Time/Actor_Time         | 0.0861        |
| Time/B_Format_Time      | 0.0756        |
| Time/B_Original_Form... | 0.0722        |
| Time/Buffer             | 0.00297       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.20890598    |
| Train/Action_magnitu... | 0.544357      |
| Train/Action_magnitude  | 0.42821413    |
| Train/Action_max        | 0.1957428     |
| Train/Action_std        | 0.21005335    |
| Train/Entropy           | -0.23838644   |
| Train/Entropy_Loss      | 0.000238      |
| Train/Entropy_loss      | 0.000238      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.4598839     |
| Train/Loss              | -0.0024708528 |
| Train/PolicyClip        | 0.010077965   |
| Train/Policy_loss       | -0.0070455004 |
| Train/Ratio             | 1.0004325     |
| Train/Return            | 1.1809313     |
| Train/V                 | 1.172906      |
| Train/Value             | 1.172906      |
| Train/control_penalty   | 0.43362615    |
| Train/policy_loss       | -0.0070455004 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02325       |
-------------------------------------------

 ---------------- Iteration 456 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 455          |
| Time/Actor_Time         | 0.0846       |
| Time/B_Format_Time      | 0.0749       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00306      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21670313   |
| Train/Action_magnitu... | 0.5610302    |
| Train/Action_magnitude  | 0.44189566   |
| Train/Action_max        | 0.21053946   |
| Train/Action_std        | 0.21357717   |
| Train/Entropy           | -0.23101932  |
| Train/Entropy_Loss      | 0.000231     |
| Train/Entropy_loss      | 0.000231     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.39553273   |
| Train/Loss              | 0.07801702   |
| Train/PolicyClip        | 0.0099217305 |
| Train/Policy_loss       | 0.07329719   |
| Train/Ratio             | 0.9789913    |
| Train/Return            | 1.1029103    |
| Train/V                 | 1.185616     |
| Train/Value             | 1.185616     |
| Train/control_penalty   | 0.44888124   |
| Train/policy_loss       | 0.07329719   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.017        |
------------------------------------------

 ---------------- Iteration 457 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 456         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.0745      |
| Time/B_Original_Form... | 0.0828      |
| Time/Buffer             | 0.00256     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.19884917  |
| Train/Action_magnitu... | 0.54334366  |
| Train/Action_magnitude  | 0.43015727  |
| Train/Action_max        | 0.21584636  |
| Train/Action_std        | 0.2247512   |
| Train/Entropy           | -0.16887799 |
| Train/Entropy_Loss      | 0.000169    |
| Train/Entropy_loss      | 0.000169    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.39488146  |
| Train/Loss              | 0.046529382 |
| Train/PolicyClip        | 0.005079715 |
| Train/Policy_loss       | 0.04194675  |
| Train/Ratio             | 0.9911419   |
| Train/Return            | 1.0626674   |
| Train/V                 | 1.1152478   |
| Train/Value             | 1.1152478   |
| Train/control_penalty   | 0.4413755   |
| Train/policy_loss       | 0.04194675  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 458 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 457          |
| Time/Actor_Time         | 0.0906       |
| Time/B_Format_Time      | 0.079        |
| Time/B_Original_Form... | 0.076        |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20843577   |
| Train/Action_magnitu... | 0.5581666    |
| Train/Action_magnitude  | 0.44085032   |
| Train/Action_max        | 0.19032171   |
| Train/Action_std        | 0.21642004   |
| Train/Entropy           | -0.19033085  |
| Train/Entropy_Loss      | 0.00019      |
| Train/Entropy_loss      | 0.00019      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.41016835   |
| Train/Loss              | -0.017571703 |
| Train/PolicyClip        | 0.008182161  |
| Train/Policy_loss       | -0.022189584 |
| Train/Ratio             | 0.9968536    |
| Train/Return            | 0.9566297    |
| Train/V                 | 0.94285333   |
| Train/Value             | 0.94285333   |
| Train/control_penalty   | 0.44275498   |
| Train/policy_loss       | -0.022189584 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01975      |
------------------------------------------

 ---------------- Iteration 459 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 458          |
| Time/Actor_Time         | 0.0882       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0735       |
| Time/Buffer             | 0.00303      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21952495   |
| Train/Action_magnitu... | 0.59245545   |
| Train/Action_magnitude  | 0.46819487   |
| Train/Action_max        | 0.21271798   |
| Train/Action_std        | 0.22335291   |
| Train/Entropy           | -0.19149788  |
| Train/Entropy_Loss      | 0.000191     |
| Train/Entropy_loss      | 0.000191     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.32433107   |
| Train/Loss              | -0.0380543   |
| Train/PolicyClip        | 0.0048535587 |
| Train/Policy_loss       | -0.042849127 |
| Train/Ratio             | 0.996776     |
| Train/Return            | 0.93790567   |
| Train/V                 | 0.9051327    |
| Train/Value             | 0.9051327    |
| Train/control_penalty   | 0.4603329    |
| Train/policy_loss       | -0.042849127 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 460 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 459         |
| Time/Actor_Time         | 0.0937      |
| Time/B_Format_Time      | 0.0751      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00268     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22121151  |
| Train/Action_magnitu... | 0.604019    |
| Train/Action_magnitude  | 0.47487673  |
| Train/Action_max        | 0.18523775  |
| Train/Action_std        | 0.22777429  |
| Train/Entropy           | -0.16196048 |
| Train/Entropy_Loss      | 0.000162    |
| Train/Entropy_loss      | 0.000162    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.20281798  |
| Train/Loss              | -0.06271848 |
| Train/PolicyClip        | 0.00975339  |
| Train/Policy_loss       | -0.06749867 |
| Train/Ratio             | 1.0024699   |
| Train/Return            | 1.003513    |
| Train/V                 | 0.94484043  |
| Train/Value             | 0.94484043  |
| Train/control_penalty   | 0.46182305  |
| Train/policy_loss       | -0.06749867 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 461 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 460         |
| Time/Actor_Time         | 0.0912      |
| Time/B_Format_Time      | 0.0708      |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.003       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2100142   |
| Train/Action_magnitu... | 0.59398156  |
| Train/Action_magnitude  | 0.46976402  |
| Train/Action_max        | 0.21864723  |
| Train/Action_std        | 0.24780421  |
| Train/Entropy           | -0.09553885 |
| Train/Entropy_Loss      | 9.55e-05    |
| Train/Entropy_loss      | 9.55e-05    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.18574917  |
| Train/Loss              | 0.009521263 |
| Train/PolicyClip        | 0.009911652 |
| Train/Policy_loss       | 0.004717083 |
| Train/Ratio             | 0.98995405  |
| Train/Return            | 0.8523407   |
| Train/V                 | 0.86752576  |
| Train/Value             | 0.86752576  |
| Train/control_penalty   | 0.47086403  |
| Train/policy_loss       | 0.004717083 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01575     |
-----------------------------------------

 ---------------- Iteration 462 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 461          |
| Time/Actor_Time         | 0.0911       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0765       |
| Time/Buffer             | 0.00259      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22837095   |
| Train/Action_magnitu... | 0.5901106    |
| Train/Action_magnitude  | 0.4660525    |
| Train/Action_max        | 0.20786816   |
| Train/Action_std        | 0.23181      |
| Train/Entropy           | -0.16787393  |
| Train/Entropy_Loss      | 0.000168     |
| Train/Entropy_loss      | 0.000168     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.33218995   |
| Train/Loss              | -0.026954968 |
| Train/PolicyClip        | 0.006146464  |
| Train/Policy_loss       | -0.031912908 |
| Train/Ratio             | 0.989617     |
| Train/Return            | 1.033198     |
| Train/V                 | 1.0119028    |
| Train/Value             | 1.0119028    |
| Train/control_penalty   | 0.4790069    |
| Train/policy_loss       | -0.031912908 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 463 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 462         |
| Time/Actor_Time         | 0.0912      |
| Time/B_Format_Time      | 0.0724      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21333389  |
| Train/Action_magnitu... | 0.5912214   |
| Train/Action_magnitude  | 0.46482876  |
| Train/Action_max        | 0.20508991  |
| Train/Action_std        | 0.2504064   |
| Train/Entropy           | -0.08290388 |
| Train/Entropy_Loss      | 8.29e-05    |
| Train/Entropy_loss      | 8.29e-05    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.18217629  |
| Train/Loss              | 0.05854719  |
| Train/PolicyClip        | 0.008952571 |
| Train/Policy_loss       | 0.053667005 |
| Train/Ratio             | 0.97797835  |
| Train/Return            | 0.9293995   |
| Train/V                 | 0.99540526  |
| Train/Value             | 0.99540526  |
| Train/control_penalty   | 0.47972816  |
| Train/policy_loss       | 0.053667005 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.012       |
-----------------------------------------

 ---------------- Iteration 464 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 463          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0775       |
| Time/Buffer             | 0.00321      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21402317   |
| Train/Action_magnitu... | 0.60748476   |
| Train/Action_magnitude  | 0.48211265   |
| Train/Action_max        | 0.21123807   |
| Train/Action_std        | 0.26897937   |
| Train/Entropy           | -0.018762473 |
| Train/Entropy_Loss      | 1.88e-05     |
| Train/Entropy_loss      | 1.88e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.02972102   |
| Train/Loss              | -0.025017273 |
| Train/PolicyClip        | 0.00639774   |
| Train/Policy_loss       | -0.030030103 |
| Train/Ratio             | 0.9960651    |
| Train/Return            | 0.787982     |
| Train/V                 | 0.7677949    |
| Train/Value             | 0.7677949    |
| Train/control_penalty   | 0.49940678   |
| Train/policy_loss       | -0.030030103 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 465 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 464         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.0767      |
| Time/B_Original_Form... | 0.0775      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23096734  |
| Train/Action_magnitu... | 0.6232259   |
| Train/Action_magnitude  | 0.4932259   |
| Train/Action_max        | 0.19451387  |
| Train/Action_std        | 0.25678676  |
| Train/Entropy           | -0.07780408 |
| Train/Entropy_Loss      | 7.78e-05    |
| Train/Entropy_loss      | 7.78e-05    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.10378979  |
| Train/Loss              | 0.026376069 |
| Train/PolicyClip        | 0.005471009 |
| Train/Policy_loss       | 0.021342948 |
| Train/Ratio             | 0.99053216  |
| Train/Return            | 0.7947625   |
| Train/V                 | 0.82351816  |
| Train/Value             | 0.82351816  |
| Train/control_penalty   | 0.49553162  |
| Train/policy_loss       | 0.021342948 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.013       |
-----------------------------------------

 ---------------- Iteration 466 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 465          |
| Time/Actor_Time         | 0.0878       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00305      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22199276   |
| Train/Action_magnitu... | 0.6338362    |
| Train/Action_magnitude  | 0.50217885   |
| Train/Action_max        | 0.19714591   |
| Train/Action_std        | 0.25753838   |
| Train/Entropy           | -0.07163272  |
| Train/Entropy_Loss      | 7.16e-05     |
| Train/Entropy_loss      | 7.16e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.05324175   |
| Train/Loss              | -0.06645225  |
| Train/PolicyClip        | 0.0051743733 |
| Train/Policy_loss       | -0.071484074 |
| Train/Ratio             | 1.0029019    |
| Train/Return            | 0.8509836    |
| Train/V                 | 0.7866842    |
| Train/Value             | 0.7866842    |
| Train/control_penalty   | 0.4960193    |
| Train/policy_loss       | -0.071484074 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01875      |
------------------------------------------

 ---------------- Iteration 467 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 466          |
| Time/Actor_Time         | 0.088        |
| Time/B_Format_Time      | 0.0754       |
| Time/B_Original_Form... | 0.0751       |
| Time/Buffer             | 0.00291      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2215953    |
| Train/Action_magnitu... | 0.6050151    |
| Train/Action_magnitude  | 0.47988075   |
| Train/Action_max        | 0.18637614   |
| Train/Action_std        | 0.24670924   |
| Train/Entropy           | -0.124483764 |
| Train/Entropy_Loss      | 0.000124     |
| Train/Entropy_loss      | 0.000124     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.25723022   |
| Train/Loss              | -0.09905568  |
| Train/PolicyClip        | 0.006304079  |
| Train/Policy_loss       | -0.10394915  |
| Train/Ratio             | 1.006438     |
| Train/Return            | 1.107942     |
| Train/V                 | 1.0090537    |
| Train/Value             | 1.0090537    |
| Train/control_penalty   | 0.47689924   |
| Train/policy_loss       | -0.10394915  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02775      |
------------------------------------------

 ---------------- Iteration 468 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 467          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21308847   |
| Train/Action_magnitu... | 0.5803393    |
| Train/Action_magnitude  | 0.4588181    |
| Train/Action_max        | 0.18055224   |
| Train/Action_std        | 0.22988236   |
| Train/Entropy           | -0.18313487  |
| Train/Entropy_Loss      | 0.000183     |
| Train/Entropy_loss      | 0.000183     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.29527888   |
| Train/Loss              | -0.084640004 |
| Train/PolicyClip        | 0.007364966  |
| Train/Policy_loss       | -0.0894463   |
| Train/Ratio             | 1.0063597    |
| Train/Return            | 1.1907024    |
| Train/V                 | 1.1114849    |
| Train/Value             | 1.1114849    |
| Train/control_penalty   | 0.46231586   |
| Train/policy_loss       | -0.0894463   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 469 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 468          |
| Time/Actor_Time         | 0.087        |
| Time/B_Format_Time      | 0.076        |
| Time/B_Original_Form... | 0.0743       |
| Time/Buffer             | 0.00232      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2168103    |
| Train/Action_magnitu... | 0.59733284   |
| Train/Action_magnitude  | 0.47005314   |
| Train/Action_max        | 0.15812603   |
| Train/Action_std        | 0.23363358   |
| Train/Entropy           | -0.17886518  |
| Train/Entropy_Loss      | 0.000179     |
| Train/Entropy_loss      | 0.000179     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.32960236   |
| Train/Loss              | -0.012439021 |
| Train/PolicyClip        | 0.0073075886 |
| Train/Policy_loss       | -0.017285645 |
| Train/Ratio             | 1.0020431    |
| Train/Return            | 1.2236835    |
| Train/V                 | 1.2157648    |
| Train/Value             | 1.2157648    |
| Train/control_penalty   | 0.46677583   |
| Train/policy_loss       | -0.017285645 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 470 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 469          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0695       |
| Time/B_Original_Form... | 0.077        |
| Time/Buffer             | 0.00314      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23297495   |
| Train/Action_magnitu... | 0.6200918    |
| Train/Action_magnitude  | 0.48944184   |
| Train/Action_max        | 0.18357684   |
| Train/Action_std        | 0.236253     |
| Train/Entropy           | -0.1802622   |
| Train/Entropy_Loss      | 0.00018      |
| Train/Entropy_loss      | 0.00018      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.3067097    |
| Train/Loss              | -0.021303503 |
| Train/PolicyClip        | 0.0064700157 |
| Train/Policy_loss       | -0.026390726 |
| Train/Ratio             | 1.0033722    |
| Train/Return            | 1.1958749    |
| Train/V                 | 1.1762397    |
| Train/Value             | 1.1762397    |
| Train/control_penalty   | 0.49069595   |
| Train/policy_loss       | -0.026390726 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 471 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 470         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0789      |
| Time/Buffer             | 0.00284     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22633825  |
| Train/Action_magnitu... | 0.60282254  |
| Train/Action_magnitude  | 0.47495294  |
| Train/Action_max        | 0.17896876  |
| Train/Action_std        | 0.22843443  |
| Train/Entropy           | -0.22941549 |
| Train/Entropy_Loss      | 0.000229    |
| Train/Entropy_loss      | 0.000229    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.41352624  |
| Train/Loss              | -0.04845917 |
| Train/PolicyClip        | 0.007950257 |
| Train/Policy_loss       | -0.05340683 |
| Train/Ratio             | 1.0001106   |
| Train/Return            | 1.3315783   |
| Train/V                 | 1.2890917   |
| Train/Value             | 1.2890917   |
| Train/control_penalty   | 0.47182485  |
| Train/policy_loss       | -0.05340683 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 472 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 471          |
| Time/Actor_Time         | 0.0939       |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.0804       |
| Time/Buffer             | 0.00266      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20867656   |
| Train/Action_magnitu... | 0.581569     |
| Train/Action_magnitude  | 0.46053594   |
| Train/Action_max        | 0.20789935   |
| Train/Action_std        | 0.2271586    |
| Train/Entropy           | -0.2303525   |
| Train/Entropy_Loss      | 0.00023      |
| Train/Entropy_loss      | 0.00023      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.39720014   |
| Train/Loss              | -0.05704522  |
| Train/PolicyClip        | 0.0066752085 |
| Train/Policy_loss       | -0.06188073  |
| Train/Ratio             | 0.99956936   |
| Train/Return            | 1.2773694    |
| Train/V                 | 1.2260158    |
| Train/Value             | 1.2260158    |
| Train/control_penalty   | 0.46051544   |
| Train/policy_loss       | -0.06188073  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02575      |
------------------------------------------

 ---------------- Iteration 473 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 472          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0724       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21713752   |
| Train/Action_magnitu... | 0.5886062    |
| Train/Action_magnitude  | 0.4656416    |
| Train/Action_max        | 0.18452477   |
| Train/Action_std        | 0.22903271   |
| Train/Entropy           | -0.20827131  |
| Train/Entropy_Loss      | 0.000208     |
| Train/Entropy_loss      | 0.000208     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.40313634   |
| Train/Loss              | -0.06057843  |
| Train/PolicyClip        | 0.0056277835 |
| Train/Policy_loss       | -0.06541532  |
| Train/Ratio             | 0.9968817    |
| Train/Return            | 1.3261192    |
| Train/V                 | 1.2718239    |
| Train/Value             | 1.2718239    |
| Train/control_penalty   | 0.4628623    |
| Train/policy_loss       | -0.06541532  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 474 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 473          |
| Time/Actor_Time         | 0.0888       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0748       |
| Time/Buffer             | 0.00315      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22260341   |
| Train/Action_magnitu... | 0.5782375    |
| Train/Action_magnitude  | 0.45496008   |
| Train/Action_max        | 0.18451989   |
| Train/Action_std        | 0.21484624   |
| Train/Entropy           | -0.26476058  |
| Train/Entropy_Loss      | 0.000265     |
| Train/Entropy_loss      | 0.000265     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4942838    |
| Train/Loss              | -0.028330974 |
| Train/PolicyClip        | 0.006445738  |
| Train/Policy_loss       | -0.03314317  |
| Train/Ratio             | 0.99181813   |
| Train/Return            | 1.4088805    |
| Train/V                 | 1.3878465    |
| Train/Value             | 1.3878465    |
| Train/control_penalty   | 0.45474365   |
| Train/policy_loss       | -0.03314317  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02725      |
------------------------------------------

 ---------------- Iteration 475 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 474          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0739       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.21556228   |
| Train/Action_magnitu... | 0.5906209    |
| Train/Action_magnitude  | 0.46686026   |
| Train/Action_max        | 0.1887906    |
| Train/Action_std        | 0.22253592   |
| Train/Entropy           | -0.21379355  |
| Train/Entropy_Loss      | 0.000214     |
| Train/Entropy_loss      | 0.000214     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.37387946   |
| Train/Loss              | -0.04304543  |
| Train/PolicyClip        | 0.008505001  |
| Train/Policy_loss       | -0.047827575 |
| Train/Ratio             | 1.0075054    |
| Train/Return            | 1.4021972    |
| Train/V                 | 1.3656034    |
| Train/Value             | 1.3656034    |
| Train/control_penalty   | 0.45683485   |
| Train/policy_loss       | -0.047827575 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 476 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 475          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0702       |
| Time/B_Original_Form... | 0.0758       |
| Time/Buffer             | 0.00321      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23117341   |
| Train/Action_magnitu... | 0.588402     |
| Train/Action_magnitude  | 0.46641088   |
| Train/Action_max        | 0.20920771   |
| Train/Action_std        | 0.21427658   |
| Train/Entropy           | -0.25842083  |
| Train/Entropy_Loss      | 0.000258     |
| Train/Entropy_loss      | 0.000258     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.4251274    |
| Train/Loss              | -0.011445637 |
| Train/PolicyClip        | 0.008396884  |
| Train/Policy_loss       | -0.016376708 |
| Train/Ratio             | 1.0027583    |
| Train/Return            | 1.3990093    |
| Train/V                 | 1.393284     |
| Train/Value             | 1.393284     |
| Train/control_penalty   | 0.46726492   |
| Train/policy_loss       | -0.016376708 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 477 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 476         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0749      |
| Time/Buffer             | 0.00311     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21356668  |
| Train/Action_magnitu... | 0.5634417   |
| Train/Action_magnitude  | 0.44360107  |
| Train/Action_max        | 0.15873483  |
| Train/Action_std        | 0.21689035  |
| Train/Entropy           | -0.23768982 |
| Train/Entropy_Loss      | 0.000238    |
| Train/Entropy_loss      | 0.000238    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.44386417  |
| Train/Loss              | 0.015035259 |
| Train/PolicyClip        | 0.008099809 |
| Train/Policy_loss       | 0.010288769 |
| Train/Ratio             | 0.9870205   |
| Train/Return            | 1.3885593   |
| Train/V                 | 1.4087965   |
| Train/Value             | 1.4087965   |
| Train/control_penalty   | 0.45087987  |
| Train/policy_loss       | 0.010288769 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 478 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 477          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0752       |
| Time/Buffer             | 0.00272      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21146472   |
| Train/Action_magnitu... | 0.568294     |
| Train/Action_magnitude  | 0.44874078   |
| Train/Action_max        | 0.19948778   |
| Train/Action_std        | 0.22694044   |
| Train/Entropy           | -0.19521643  |
| Train/Entropy_Loss      | 0.000195     |
| Train/Entropy_loss      | 0.000195     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.3329615    |
| Train/Loss              | -0.0334955   |
| Train/PolicyClip        | 0.017328402  |
| Train/Policy_loss       | -0.038162842 |
| Train/Ratio             | 1.0075573    |
| Train/Return            | 1.4239464    |
| Train/V                 | 1.3783561    |
| Train/Value             | 1.3783561    |
| Train/control_penalty   | 0.44721234   |
| Train/policy_loss       | -0.038162842 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02825      |
------------------------------------------

 ---------------- Iteration 479 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 478          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00311      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23639461   |
| Train/Action_magnitu... | 0.6098904    |
| Train/Action_magnitude  | 0.47964576   |
| Train/Action_max        | 0.20866357   |
| Train/Action_std        | 0.22729912   |
| Train/Entropy           | -0.19598976  |
| Train/Entropy_Loss      | 0.000196     |
| Train/Entropy_loss      | 0.000196     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.39802966   |
| Train/Loss              | 0.048570804  |
| Train/PolicyClip        | 0.0056168037 |
| Train/Policy_loss       | 0.043604266  |
| Train/Ratio             | 0.9852344    |
| Train/Return            | 1.4614322    |
| Train/V                 | 1.5174844    |
| Train/Value             | 1.5174844    |
| Train/control_penalty   | 0.4770548    |
| Train/policy_loss       | 0.043604266  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 480 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 479          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00252      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22059067   |
| Train/Action_magnitu... | 0.5754835    |
| Train/Action_magnitude  | 0.45303112   |
| Train/Action_max        | 0.21492143   |
| Train/Action_std        | 0.21766211   |
| Train/Entropy           | -0.2244942   |
| Train/Entropy_Loss      | 0.000224     |
| Train/Entropy_loss      | 0.000224     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.42328846   |
| Train/Loss              | 0.06646197   |
| Train/PolicyClip        | 0.0072590886 |
| Train/Policy_loss       | 0.06174324   |
| Train/Ratio             | 0.98304164   |
| Train/Return            | 1.3207192    |
| Train/V                 | 1.3926941    |
| Train/Value             | 1.3926941    |
| Train/control_penalty   | 0.44942394   |
| Train/policy_loss       | 0.06174324   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02         |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 481 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 480           |
| Time/Actor_Time         | 0.0896        |
| Time/B_Format_Time      | 0.0702        |
| Time/B_Original_Form... | 0.0729        |
| Time/Buffer             | 0.0022        |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.2241672     |
| Train/Action_magnitu... | 0.57883227    |
| Train/Action_magnitude  | 0.45660874    |
| Train/Action_max        | 0.22552957    |
| Train/Action_std        | 0.22174935    |
| Train/Entropy           | -0.19391274   |
| Train/Entropy_Loss      | 0.000194      |
| Train/Entropy_loss      | 0.000194      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.34300587    |
| Train/Loss              | 0.0027555048  |
| Train/PolicyClip        | 0.009729335   |
| Train/Policy_loss       | -0.0020341044 |
| Train/Ratio             | 0.99560577    |
| Train/Return            | 1.1977593     |
| Train/V                 | 1.2075803     |
| Train/Value             | 1.2075803     |
| Train/control_penalty   | 0.45956966    |
| Train/policy_loss       | -0.0020341044 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02075       |
-------------------------------------------

 ---------------- Iteration 482 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 481         |
| Time/Actor_Time         | 0.0913      |
| Time/B_Format_Time      | 0.0702      |
| Time/B_Original_Form... | 0.0758      |
| Time/Buffer             | 0.00287     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21635543  |
| Train/Action_magnitu... | 0.5716384   |
| Train/Action_magnitude  | 0.45013276  |
| Train/Action_max        | 0.22824189  |
| Train/Action_std        | 0.21625882  |
| Train/Entropy           | -0.22286521 |
| Train/Entropy_Loss      | 0.000223    |
| Train/Entropy_loss      | 0.000223    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.4294414   |
| Train/Loss              | 0.03474899  |
| Train/PolicyClip        | 0.00988619  |
| Train/Policy_loss       | 0.030024016 |
| Train/Ratio             | 0.9893667   |
| Train/Return            | 1.1274446   |
| Train/V                 | 1.1686108   |
| Train/Value             | 1.1686108   |
| Train/control_penalty   | 0.4502108   |
| Train/policy_loss       | 0.030024016 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 483 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 482         |
| Time/Actor_Time         | 0.0876      |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22270623  |
| Train/Action_magnitu... | 0.5887953   |
| Train/Action_magnitude  | 0.4643673   |
| Train/Action_max        | 0.24828257  |
| Train/Action_std        | 0.22582357  |
| Train/Entropy           | -0.18818793 |
| Train/Entropy_Loss      | 0.000188    |
| Train/Entropy_loss      | 0.000188    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.31456402  |
| Train/Loss              | 0.022513801 |
| Train/PolicyClip        | 0.011067616 |
| Train/Policy_loss       | 0.017739646 |
| Train/Ratio             | 0.99289024  |
| Train/Return            | 1.3196204   |
| Train/V                 | 1.3468481   |
| Train/Value             | 1.3468481   |
| Train/control_penalty   | 0.45859662  |
| Train/policy_loss       | 0.017739646 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 484 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 483           |
| Time/Actor_Time         | 0.0855        |
| Time/B_Format_Time      | 0.0774        |
| Time/B_Original_Form... | 0.0786        |
| Time/Buffer             | 0.00253       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.21458814    |
| Train/Action_magnitu... | 0.5676553     |
| Train/Action_magnitude  | 0.44749376    |
| Train/Action_max        | 0.23798446    |
| Train/Action_std        | 0.23784363    |
| Train/Entropy           | -0.12828909   |
| Train/Entropy_Loss      | 0.000128      |
| Train/Entropy_loss      | 0.000128      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 0.28359607    |
| Train/Loss              | 0.0038967365  |
| Train/PolicyClip        | 0.009312657   |
| Train/Policy_loss       | -0.0007672366 |
| Train/Ratio             | 0.9854923     |
| Train/Return            | 1.0642097     |
| Train/V                 | 1.0744377     |
| Train/Value             | 1.0744377     |
| Train/control_penalty   | 0.4535684     |
| Train/policy_loss       | -0.0007672366 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02          |
-------------------------------------------

 ---------------- Iteration 485 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 484         |
| Time/Actor_Time         | 0.0864      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0758      |
| Time/Buffer             | 0.00243     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23115821  |
| Train/Action_magnitu... | 0.59544843  |
| Train/Action_magnitude  | 0.4691297   |
| Train/Action_max        | 0.23748489  |
| Train/Action_std        | 0.23900338  |
| Train/Entropy           | -0.13045968 |
| Train/Entropy_Loss      | 0.00013     |
| Train/Entropy_loss      | 0.00013     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.2464555   |
| Train/Loss              | 0.037269063 |
| Train/PolicyClip        | 0.007365215 |
| Train/Policy_loss       | 0.032441303 |
| Train/Ratio             | 0.97950685  |
| Train/Return            | 1.0421752   |
| Train/V                 | 1.0870676   |
| Train/Value             | 1.0870676   |
| Train/control_penalty   | 0.46973014  |
| Train/policy_loss       | 0.032441303 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01475     |
-----------------------------------------

 ---------------- Iteration 486 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 485          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0706       |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00296      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2286004    |
| Train/Action_magnitu... | 0.60028523   |
| Train/Action_magnitude  | 0.47378132   |
| Train/Action_max        | 0.280817     |
| Train/Action_std        | 0.24135378   |
| Train/Entropy           | -0.11694188  |
| Train/Entropy_Loss      | 0.000117     |
| Train/Entropy_loss      | 0.000117     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.25747153   |
| Train/Loss              | -0.06915754  |
| Train/PolicyClip        | 0.0034375468 |
| Train/Policy_loss       | -0.07410712  |
| Train/Ratio             | 0.99885863   |
| Train/Return            | 1.0947144    |
| Train/V                 | 1.0335513    |
| Train/Value             | 1.0335513    |
| Train/control_penalty   | 0.48326296   |
| Train/policy_loss       | -0.07410712  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 487 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 486         |
| Time/Actor_Time         | 0.102       |
| Time/B_Format_Time      | 0.0896      |
| Time/B_Original_Form... | 0.083       |
| Time/Buffer             | 0.00313     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22660066  |
| Train/Action_magnitu... | 0.5933401   |
| Train/Action_magnitude  | 0.46634227  |
| Train/Action_max        | 0.24456492  |
| Train/Action_std        | 0.22463714  |
| Train/Entropy           | -0.1863251  |
| Train/Entropy_Loss      | 0.000186    |
| Train/Entropy_loss      | 0.000186    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.26728404  |
| Train/Loss              | -0.08433372 |
| Train/PolicyClip        | 0.008727791 |
| Train/Policy_loss       | -0.08907051 |
| Train/Ratio             | 1.0078162   |
| Train/Return            | 1.07999     |
| Train/V                 | 0.9988394   |
| Train/Value             | 0.9988394   |
| Train/control_penalty   | 0.45504662  |
| Train/policy_loss       | -0.08907051 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 488 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 487          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0743       |
| Time/B_Original_Form... | 0.0753       |
| Time/Buffer             | 0.00272      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22174865   |
| Train/Action_magnitu... | 0.5664415    |
| Train/Action_magnitude  | 0.44741997   |
| Train/Action_max        | 0.2308997    |
| Train/Action_std        | 0.22890401   |
| Train/Entropy           | -0.17094788  |
| Train/Entropy_Loss      | 0.000171     |
| Train/Entropy_loss      | 0.000171     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.3299275    |
| Train/Loss              | -0.021773376 |
| Train/PolicyClip        | 0.010112418  |
| Train/Policy_loss       | -0.02643437  |
| Train/Ratio             | 0.98734313   |
| Train/Return            | 0.87428254   |
| Train/V                 | 0.85994923   |
| Train/Value             | 0.85994923   |
| Train/control_penalty   | 0.44900462   |
| Train/policy_loss       | -0.02643437  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01675      |
------------------------------------------

 ---------------- Iteration 489 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 488          |
| Time/Actor_Time         | 0.0844       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0752       |
| Time/Buffer             | 0.00472      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21098015   |
| Train/Action_magnitu... | 0.5707572    |
| Train/Action_magnitude  | 0.4489091    |
| Train/Action_max        | 0.22936547   |
| Train/Action_std        | 0.23775104   |
| Train/Entropy           | -0.1459434   |
| Train/Entropy_Loss      | 0.000146     |
| Train/Entropy_loss      | 0.000146     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.2685591    |
| Train/Loss              | -0.05568489  |
| Train/PolicyClip        | 0.008955434  |
| Train/Policy_loss       | -0.060381275 |
| Train/Ratio             | 1.000421     |
| Train/Return            | 0.8822249    |
| Train/V                 | 0.83065796   |
| Train/Value             | 0.83065796   |
| Train/control_penalty   | 0.45504418   |
| Train/policy_loss       | -0.060381275 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 490 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 489          |
| Time/Actor_Time         | 0.0826       |
| Time/B_Format_Time      | 0.0742       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00312      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.22401349   |
| Train/Action_magnitu... | 0.5934331    |
| Train/Action_magnitude  | 0.4656347    |
| Train/Action_max        | 0.22423118   |
| Train/Action_std        | 0.23551813   |
| Train/Entropy           | -0.14418094  |
| Train/Entropy_Loss      | 0.000144     |
| Train/Entropy_loss      | 0.000144     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.29106113   |
| Train/Loss              | -0.0338737   |
| Train/PolicyClip        | 0.009650342  |
| Train/Policy_loss       | -0.038529556 |
| Train/Ratio             | 1.0053602    |
| Train/Return            | 1.0713767    |
| Train/V                 | 1.0427159    |
| Train/Value             | 1.0427159    |
| Train/control_penalty   | 0.45116773   |
| Train/policy_loss       | -0.038529556 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 491 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 490          |
| Time/Actor_Time         | 0.0845       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00247      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2163824    |
| Train/Action_magnitu... | 0.5606308    |
| Train/Action_magnitude  | 0.44126934   |
| Train/Action_max        | 0.20711835   |
| Train/Action_std        | 0.22902171   |
| Train/Entropy           | -0.19428779  |
| Train/Entropy_Loss      | 0.000194     |
| Train/Entropy_loss      | 0.000194     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.3276765    |
| Train/Loss              | -0.035380155 |
| Train/PolicyClip        | 0.006391827  |
| Train/Policy_loss       | -0.04013122  |
| Train/Ratio             | 0.997656     |
| Train/Return            | 1.0367115    |
| Train/V                 | 1.0064636    |
| Train/Value             | 1.0064636    |
| Train/control_penalty   | 0.45567742   |
| Train/policy_loss       | -0.04013122  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 492 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 491         |
| Time/Actor_Time         | 0.0851      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0709      |
| Time/Buffer             | 0.00365     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.21947296  |
| Train/Action_magnitu... | 0.5753653   |
| Train/Action_magnitude  | 0.45481715  |
| Train/Action_max        | 0.22815078  |
| Train/Action_std        | 0.22219273  |
| Train/Entropy           | -0.19906098 |
| Train/Entropy_Loss      | 0.000199    |
| Train/Entropy_loss      | 0.000199    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.26917756  |
| Train/Loss              | -0.07746897 |
| Train/PolicyClip        | 0.010698377 |
| Train/Policy_loss       | -0.0821785  |
| Train/Ratio             | 1.0069549   |
| Train/Return            | 1.3069589   |
| Train/V                 | 1.2344066   |
| Train/Value             | 1.2344066   |
| Train/control_penalty   | 0.4510472   |
| Train/policy_loss       | -0.0821785  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 493 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 492          |
| Time/Actor_Time         | 0.0839       |
| Time/B_Format_Time      | 0.0704       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00243      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22185989   |
| Train/Action_magnitu... | 0.59670526   |
| Train/Action_magnitude  | 0.4732974    |
| Train/Action_max        | 0.25377014   |
| Train/Action_std        | 0.23628421   |
| Train/Entropy           | -0.14688098  |
| Train/Entropy_Loss      | 0.000147     |
| Train/Entropy_loss      | 0.000147     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.1874357    |
| Train/Loss              | -0.026414827 |
| Train/PolicyClip        | 0.008835268  |
| Train/Policy_loss       | -0.031208972 |
| Train/Ratio             | 1.0058715    |
| Train/Return            | 1.0951087    |
| Train/V                 | 1.0743139    |
| Train/Value             | 1.0743139    |
| Train/control_penalty   | 0.46472642   |
| Train/policy_loss       | -0.031208972 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 494 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 493         |
| Time/Actor_Time         | 0.0841      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00276     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20744781  |
| Train/Action_magnitu... | 0.5576065   |
| Train/Action_magnitude  | 0.44130197  |
| Train/Action_max        | 0.21073514  |
| Train/Action_std        | 0.21333064  |
| Train/Entropy           | -0.23872735 |
| Train/Entropy_Loss      | 0.000239    |
| Train/Entropy_loss      | 0.000239    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.34305263  |
| Train/Loss              | -0.08056762 |
| Train/PolicyClip        | 0.005702758 |
| Train/Policy_loss       | -0.08505613 |
| Train/Ratio             | 0.99939454  |
| Train/Return            | 1.2349519   |
| Train/V                 | 1.1559304   |
| Train/Value             | 1.1559304   |
| Train/control_penalty   | 0.42497897  |
| Train/policy_loss       | -0.08505613 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 495 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 494         |
| Time/Actor_Time         | 0.084       |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0738      |
| Time/Buffer             | 0.00311     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.20566656  |
| Train/Action_magnitu... | 0.5600006   |
| Train/Action_magnitude  | 0.4425659   |
| Train/Action_max        | 0.2239011   |
| Train/Action_std        | 0.2261862   |
| Train/Entropy           | -0.18668155 |
| Train/Entropy_Loss      | 0.000187    |
| Train/Entropy_loss      | 0.000187    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.30599603  |
| Train/Loss              | 0.053268157 |
| Train/PolicyClip        | 0.007746999 |
| Train/Policy_loss       | 0.048673294 |
| Train/Ratio             | 0.9822876   |
| Train/Return            | 1.1698816   |
| Train/V                 | 1.2315527   |
| Train/Value             | 1.2315527   |
| Train/control_penalty   | 0.4408182   |
| Train/policy_loss       | 0.048673294 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0165      |
-----------------------------------------

 ---------------- Iteration 496 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 495            |
| Time/Actor_Time         | 0.0851         |
| Time/B_Format_Time      | 0.0729         |
| Time/B_Original_Form... | 0.0723         |
| Time/Buffer             | 0.00249        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.22023435     |
| Train/Action_magnitu... | 0.57826906     |
| Train/Action_magnitude  | 0.45932034     |
| Train/Action_max        | 0.22285575     |
| Train/Action_std        | 0.25390485     |
| Train/Entropy           | -0.08080431    |
| Train/Entropy_Loss      | 8.08e-05       |
| Train/Entropy_loss      | 8.08e-05       |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | 0.2090055      |
| Train/Loss              | -0.00010894239 |
| Train/PolicyClip        | 0.01048033     |
| Train/Policy_loss       | -0.004963836   |
| Train/Ratio             | 0.9888174      |
| Train/Return            | 1.087651       |
| Train/V                 | 1.0912583      |
| Train/Value             | 1.0912583      |
| Train/control_penalty   | 0.47740898     |
| Train/policy_loss       | -0.004963836   |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.01875        |
--------------------------------------------

 ---------------- Iteration 497 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 496          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00244      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.21624938   |
| Train/Action_magnitu... | 0.58363986   |
| Train/Action_magnitude  | 0.4626578    |
| Train/Action_max        | 0.22916853   |
| Train/Action_std        | 0.24498637   |
| Train/Entropy           | -0.10765874  |
| Train/Entropy_Loss      | 0.000108     |
| Train/Entropy_loss      | 0.000108     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.17518042   |
| Train/Loss              | -0.007029472 |
| Train/PolicyClip        | 0.005133941  |
| Train/Policy_loss       | -0.011779231 |
| Train/Ratio             | 0.998585     |
| Train/Return            | 1.0712358    |
| Train/V                 | 1.0657529    |
| Train/Value             | 1.0657529    |
| Train/control_penalty   | 0.46421003   |
| Train/policy_loss       | -0.011779231 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.019        |
------------------------------------------

 ---------------- Iteration 498 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 497         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0719      |
| Time/B_Original_Form... | 0.0697      |
| Time/Buffer             | 0.00284     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.21556371  |
| Train/Action_magnitu... | 0.5895925   |
| Train/Action_magnitude  | 0.46537587  |
| Train/Action_max        | 0.22065698  |
| Train/Action_std        | 0.2552122   |
| Train/Entropy           | -0.06093762 |
| Train/Entropy_Loss      | 6.09e-05    |
| Train/Entropy_loss      | 6.09e-05    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.103982724 |
| Train/Loss              | 0.019680941 |
| Train/PolicyClip        | 0.007310349 |
| Train/Policy_loss       | 0.014869214 |
| Train/Ratio             | 0.989416    |
| Train/Return            | 0.9227677   |
| Train/V                 | 0.94857556  |
| Train/Value             | 0.94857556  |
| Train/control_penalty   | 0.47507897  |
| Train/policy_loss       | 0.014869214 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.015       |
-----------------------------------------

 ---------------- Iteration 499 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 498          |
| Time/Actor_Time         | 0.0844       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00243      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21424192   |
| Train/Action_magnitu... | 0.59791565   |
| Train/Action_magnitude  | 0.47779712   |
| Train/Action_max        | 0.26019642   |
| Train/Action_std        | 0.2473652    |
| Train/Entropy           | -0.11209852  |
| Train/Entropy_Loss      | 0.000112     |
| Train/Entropy_loss      | 0.000112     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.13123426   |
| Train/Loss              | -0.058270074 |
| Train/PolicyClip        | 0.005823291  |
| Train/Policy_loss       | -0.06308037  |
| Train/Ratio             | 1.004719     |
| Train/Return            | 1.135408     |
| Train/V                 | 1.0830886    |
| Train/Value             | 1.0830886    |
| Train/control_penalty   | 0.46981928   |
| Train/policy_loss       | -0.06308037  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 500 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 499          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0703       |
| Time/B_Original_Form... | 0.072        |
| Time/Buffer             | 0.00232      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22079101   |
| Train/Action_magnitu... | 0.6023984    |
| Train/Action_magnitude  | 0.4754687    |
| Train/Action_max        | 0.19537325   |
| Train/Action_std        | 0.24704713   |
| Train/Entropy           | -0.09881844  |
| Train/Entropy_Loss      | 9.88e-05     |
| Train/Entropy_loss      | 9.88e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.16043326   |
| Train/Loss              | -0.03381432  |
| Train/PolicyClip        | 0.0069426647 |
| Train/Policy_loss       | -0.0385585   |
| Train/Ratio             | 0.987621     |
| Train/Return            | 1.0303586    |
| Train/V                 | 1.0038296    |
| Train/Value             | 1.0038296    |
| Train/control_penalty   | 0.4645366    |
| Train/policy_loss       | -0.0385585   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01925      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 501 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 500          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.076        |
| Time/Buffer             | 0.00243      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.20069738   |
| Train/Action_magnitu... | 0.55140203   |
| Train/Action_magnitude  | 0.43753037   |
| Train/Action_max        | 0.18603025   |
| Train/Action_std        | 0.23161854   |
| Train/Entropy           | -0.164591    |
| Train/Entropy_Loss      | 0.000165     |
| Train/Entropy_loss      | 0.000165     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.29678974   |
| Train/Loss              | -0.073140845 |
| Train/PolicyClip        | 0.0072020544 |
| Train/Policy_loss       | -0.07768784  |
| Train/Ratio             | 1.0014058    |
| Train/Return            | 1.1347454    |
| Train/V                 | 1.0662297    |
| Train/Value             | 1.0662297    |
| Train/control_penalty   | 0.43824032   |
| Train/policy_loss       | -0.07768784  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 502 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 501          |
| Time/Actor_Time         | 0.0826       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00249      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.19967772   |
| Train/Action_magnitu... | 0.5612969    |
| Train/Action_magnitude  | 0.44508174   |
| Train/Action_max        | 0.1777691    |
| Train/Action_std        | 0.24034084   |
| Train/Entropy           | -0.14273341  |
| Train/Entropy_Loss      | 0.000143     |
| Train/Entropy_loss      | 0.000143     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.2804322    |
| Train/Loss              | -0.036813904 |
| Train/PolicyClip        | 0.0065042432 |
| Train/Policy_loss       | -0.041380063 |
| Train/Ratio             | 0.99958336   |
| Train/Return            | 1.1172998    |
| Train/V                 | 1.0840132    |
| Train/Value             | 1.0840132    |
| Train/control_penalty   | 0.4423424    |
| Train/policy_loss       | -0.041380063 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 503 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 502          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00299      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2117307    |
| Train/Action_magnitu... | 0.5826499    |
| Train/Action_magnitude  | 0.4599396    |
| Train/Action_max        | 0.20102507   |
| Train/Action_std        | 0.24066934   |
| Train/Entropy           | -0.14326315  |
| Train/Entropy_Loss      | 0.000143     |
| Train/Entropy_loss      | 0.000143     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.20276596   |
| Train/Loss              | -0.07871024  |
| Train/PolicyClip        | 0.008113196  |
| Train/Policy_loss       | -0.083454914 |
| Train/Ratio             | 1.0094227    |
| Train/Return            | 1.0956365    |
| Train/V                 | 1.0231233    |
| Train/Value             | 1.0231233    |
| Train/control_penalty   | 0.4601413    |
| Train/policy_loss       | -0.083454914 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 504 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 503          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00318      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2126552    |
| Train/Action_magnitu... | 0.5825432    |
| Train/Action_magnitude  | 0.45880887   |
| Train/Action_max        | 0.22012435   |
| Train/Action_std        | 0.25244504   |
| Train/Entropy           | -0.09399189  |
| Train/Entropy_Loss      | 9.4e-05      |
| Train/Entropy_loss      | 9.4e-05      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.21189208   |
| Train/Loss              | -0.02491474  |
| Train/PolicyClip        | 0.009052663  |
| Train/Policy_loss       | -0.029643461 |
| Train/Ratio             | 0.99370384   |
| Train/Return            | 1.0041611    |
| Train/V                 | 0.98439884   |
| Train/Value             | 0.98439884   |
| Train/control_penalty   | 0.4634729    |
| Train/policy_loss       | -0.029643461 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 505 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 504          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00259      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21611822   |
| Train/Action_magnitu... | 0.60961413   |
| Train/Action_magnitude  | 0.48217115   |
| Train/Action_max        | 0.20224449   |
| Train/Action_std        | 0.24935505   |
| Train/Entropy           | -0.108245425 |
| Train/Entropy_Loss      | 0.000108     |
| Train/Entropy_loss      | 0.000108     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.16217011   |
| Train/Loss              | 0.006239198  |
| Train/PolicyClip        | 0.0068802843 |
| Train/Policy_loss       | 0.0013610614 |
| Train/Ratio             | 0.99148524   |
| Train/Return            | 1.0056986    |
| Train/V                 | 1.015904     |
| Train/Value             | 1.015904     |
| Train/control_penalty   | 0.47698915   |
| Train/policy_loss       | 0.0013610614 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01875      |
------------------------------------------

 ---------------- Iteration 506 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 505         |
| Time/Actor_Time         | 0.0852      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0715      |
| Time/Buffer             | 0.0025      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22856146  |
| Train/Action_magnitu... | 0.6051918   |
| Train/Action_magnitude  | 0.477984    |
| Train/Action_max        | 0.20568779  |
| Train/Action_std        | 0.23507343  |
| Train/Entropy           | -0.16626894 |
| Train/Entropy_Loss      | 0.000166    |
| Train/Entropy_loss      | 0.000166    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 0.23017985  |
| Train/Loss              | -0.07610426 |
| Train/PolicyClip        | 0.008996472 |
| Train/Policy_loss       | -0.08100326 |
| Train/Ratio             | 0.99862397  |
| Train/Return            | 1.1331959   |
| Train/V                 | 1.0617732   |
| Train/Value             | 1.0617732   |
| Train/control_penalty   | 0.4732737   |
| Train/policy_loss       | -0.08100326 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 507 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 506          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00242      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.22910367   |
| Train/Action_magnitu... | 0.60210645   |
| Train/Action_magnitude  | 0.4785412    |
| Train/Action_max        | 0.24705014   |
| Train/Action_std        | 0.2458991    |
| Train/Entropy           | -0.118378945 |
| Train/Entropy_Loss      | 0.000118     |
| Train/Entropy_loss      | 0.000118     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.24220447   |
| Train/Loss              | -0.032551166 |
| Train/PolicyClip        | 0.007339035  |
| Train/Policy_loss       | -0.037566505 |
| Train/Ratio             | 0.9932424    |
| Train/Return            | 1.190306     |
| Train/V                 | 1.1632338    |
| Train/Value             | 1.1632338    |
| Train/control_penalty   | 0.489696     |
| Train/policy_loss       | -0.037566505 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 508 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 507          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00282      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21497792   |
| Train/Action_magnitu... | 0.58544225   |
| Train/Action_magnitude  | 0.4615376    |
| Train/Action_max        | 0.23760009   |
| Train/Action_std        | 0.23592907   |
| Train/Entropy           | -0.16121586  |
| Train/Entropy_Loss      | 0.000161     |
| Train/Entropy_loss      | 0.000161     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.33017913   |
| Train/Loss              | -0.032519195 |
| Train/PolicyClip        | 0.007912446  |
| Train/Policy_loss       | -0.037341643 |
| Train/Ratio             | 0.98914146   |
| Train/Return            | 1.2178504    |
| Train/V                 | 1.1921371    |
| Train/Value             | 1.1921371    |
| Train/control_penalty   | 0.4661232    |
| Train/policy_loss       | -0.037341643 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 509 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 508          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.0737       |
| Time/Buffer             | 0.00253      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.21191643   |
| Train/Action_magnitu... | 0.56796885   |
| Train/Action_magnitude  | 0.45278278   |
| Train/Action_max        | 0.25570655   |
| Train/Action_std        | 0.23983127   |
| Train/Entropy           | -0.14434089  |
| Train/Entropy_Loss      | 0.000144     |
| Train/Entropy_loss      | 0.000144     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.27087006   |
| Train/Loss              | -0.057847824 |
| Train/PolicyClip        | 0.006535359  |
| Train/Policy_loss       | -0.06263338  |
| Train/Ratio             | 0.99526876   |
| Train/Return            | 1.1236467    |
| Train/V                 | 1.0664475    |
| Train/Value             | 1.0664475    |
| Train/control_penalty   | 0.46412176   |
| Train/policy_loss       | -0.06263338  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 510 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 509          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00306      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2128985    |
| Train/Action_magnitu... | 0.5855044    |
| Train/Action_magnitude  | 0.4625376    |
| Train/Action_max        | 0.2038979    |
| Train/Action_std        | 0.24474402   |
| Train/Entropy           | -0.12968428  |
| Train/Entropy_Loss      | 0.00013      |
| Train/Entropy_loss      | 0.00013      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.27419868   |
| Train/Loss              | -0.027598683 |
| Train/PolicyClip        | 0.009084368  |
| Train/Policy_loss       | -0.032420192 |
| Train/Ratio             | 1.0003059    |
| Train/Return            | 1.2259119    |
| Train/V                 | 1.2031081    |
| Train/Value             | 1.2031081    |
| Train/control_penalty   | 0.46918234   |
| Train/policy_loss       | -0.032420192 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 511 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 510          |
| Time/Actor_Time         | 0.0844       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00283      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23027246   |
| Train/Action_magnitu... | 0.6227988    |
| Train/Action_magnitude  | 0.49518573   |
| Train/Action_max        | 0.21840334   |
| Train/Action_std        | 0.24733108   |
| Train/Entropy           | -0.12656379  |
| Train/Entropy_Loss      | 0.000127     |
| Train/Entropy_loss      | 0.000127     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.15267994   |
| Train/Loss              | -0.05576986  |
| Train/PolicyClip        | 0.004760873  |
| Train/Policy_loss       | -0.060947374 |
| Train/Ratio             | 0.99563223   |
| Train/Return            | 1.3407592    |
| Train/V                 | 1.2922304    |
| Train/Value             | 1.2922304    |
| Train/control_penalty   | 0.5050949    |
| Train/policy_loss       | -0.060947374 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0325       |
------------------------------------------

 ---------------- Iteration 512 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 511          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23976572   |
| Train/Action_magnitu... | 0.6326724    |
| Train/Action_magnitude  | 0.5018165    |
| Train/Action_max        | 0.20781678   |
| Train/Action_std        | 0.2638392    |
| Train/Entropy           | -0.050470077 |
| Train/Entropy_Loss      | 5.05e-05     |
| Train/Entropy_loss      | 5.05e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.10652245   |
| Train/Loss              | -0.008952443 |
| Train/PolicyClip        | 0.0045407014 |
| Train/Policy_loss       | -0.014259131 |
| Train/Ratio             | 0.97967273   |
| Train/Return            | 1.0421716    |
| Train/V                 | 1.038184     |
| Train/Value             | 1.038184     |
| Train/control_penalty   | 0.52562183   |
| Train/policy_loss       | -0.014259131 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 513 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 512          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00301      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.23354273   |
| Train/Action_magnitu... | 0.6306086    |
| Train/Action_magnitude  | 0.5011599    |
| Train/Action_max        | 0.19121598   |
| Train/Action_std        | 0.26155964   |
| Train/Entropy           | -0.057399545 |
| Train/Entropy_Loss      | 5.74e-05     |
| Train/Entropy_loss      | 5.74e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.11938864   |
| Train/Loss              | 0.037653893  |
| Train/PolicyClip        | 0.007630245  |
| Train/Policy_loss       | 0.03262383   |
| Train/Ratio             | 0.9862721    |
| Train/Return            | 1.2808901    |
| Train/V                 | 1.3209819    |
| Train/Value             | 1.3209819    |
| Train/control_penalty   | 0.49726617   |
| Train/policy_loss       | 0.03262383   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 514 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 513          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00522      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24236225   |
| Train/Action_magnitu... | 0.6511468    |
| Train/Action_magnitude  | 0.5145133    |
| Train/Action_max        | 0.22430511   |
| Train/Action_std        | 0.24849369   |
| Train/Entropy           | -0.11509243  |
| Train/Entropy_Loss      | 0.000115     |
| Train/Entropy_loss      | 0.000115     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.1303418    |
| Train/Loss              | -0.04059918  |
| Train/PolicyClip        | 0.013117526  |
| Train/Policy_loss       | -0.045762334 |
| Train/Ratio             | 0.9929297    |
| Train/Return            | 1.3276141    |
| Train/V                 | 1.2935852    |
| Train/Value             | 1.2935852    |
| Train/control_penalty   | 0.5048063    |
| Train/policy_loss       | -0.045762334 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0345       |
------------------------------------------

 ---------------- Iteration 515 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 514          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.21068598   |
| Train/Action_magnitu... | 0.6279746    |
| Train/Action_magnitude  | 0.49675798   |
| Train/Action_max        | 0.22477609   |
| Train/Action_std        | 0.27091566   |
| Train/Entropy           | -0.014446023 |
| Train/Entropy_Loss      | 1.44e-05     |
| Train/Entropy_loss      | 1.44e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.022087658 |
| Train/Loss              | 0.030761763  |
| Train/PolicyClip        | 0.0044288016 |
| Train/Policy_loss       | 0.025832     |
| Train/Ratio             | 0.9849615    |
| Train/Return            | 0.8644762    |
| Train/V                 | 0.90128523   |
| Train/Value             | 0.90128523   |
| Train/control_penalty   | 0.49153173   |
| Train/policy_loss       | 0.025832     |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0145       |
------------------------------------------

 ---------------- Iteration 516 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 515           |
| Time/Actor_Time         | 0.0859        |
| Time/B_Format_Time      | 0.0726        |
| Time/B_Original_Form... | 0.0715        |
| Time/Buffer             | 0.00295       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.24067767    |
| Train/Action_magnitu... | 0.6577099     |
| Train/Action_magnitude  | 0.5192128     |
| Train/Action_max        | 0.1914308     |
| Train/Action_std        | 0.27293417    |
| Train/Entropy           | -0.0144593315 |
| Train/Entropy_Loss      | 1.45e-05      |
| Train/Entropy_loss      | 1.45e-05      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.0029310875 |
| Train/Loss              | 0.016842822   |
| Train/PolicyClip        | 0.008106798   |
| Train/Policy_loss       | 0.011567087   |
| Train/Ratio             | 0.99138236    |
| Train/Return            | 1.1789374     |
| Train/V                 | 1.1937654     |
| Train/Value             | 1.1937654     |
| Train/control_penalty   | 0.52612746    |
| Train/policy_loss       | 0.011567087   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01875       |
-------------------------------------------

 ---------------- Iteration 517 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 516           |
| Time/Actor_Time         | 0.0853        |
| Time/B_Format_Time      | 0.0744        |
| Time/B_Original_Form... | 0.0728        |
| Time/Buffer             | 0.00275       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.2573022     |
| Train/Action_magnitu... | 0.70918125    |
| Train/Action_magnitude  | 0.5602283     |
| Train/Action_max        | 0.2126731     |
| Train/Action_std        | 0.28030592    |
| Train/Entropy           | 0.017664734   |
| Train/Entropy_Loss      | -1.77e-05     |
| Train/Entropy_loss      | -1.77e-05     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.18047635   |
| Train/Loss              | 0.002470528   |
| Train/PolicyClip        | 0.008044414   |
| Train/Policy_loss       | -0.0029614773 |
| Train/Ratio             | 0.99436355    |
| Train/Return            | 1.2788969     |
| Train/V                 | 1.2861207     |
| Train/Value             | 1.2861207     |
| Train/control_penalty   | 0.544967      |
| Train/policy_loss       | -0.0029614773 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.026         |
-------------------------------------------

 ---------------- Iteration 518 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 517          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00213      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.25113267   |
| Train/Action_magnitu... | 0.6969054    |
| Train/Action_magnitude  | 0.5497949    |
| Train/Action_max        | 0.20848148   |
| Train/Action_std        | 0.2966983    |
| Train/Entropy           | 0.07156329   |
| Train/Entropy_Loss      | -7.16e-05    |
| Train/Entropy_loss      | -7.16e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.13186954  |
| Train/Loss              | 0.04847022   |
| Train/PolicyClip        | 0.0042505683 |
| Train/Policy_loss       | 0.042922724  |
| Train/Ratio             | 0.9774629    |
| Train/Return            | 1.103015     |
| Train/V                 | 1.1588216    |
| Train/Value             | 1.1588216    |
| Train/control_penalty   | 0.56190586   |
| Train/policy_loss       | 0.042922724  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 519 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 518          |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.0032       |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.24899033   |
| Train/Action_magnitu... | 0.70317274   |
| Train/Action_magnitude  | 0.5586963    |
| Train/Action_max        | 0.22593038   |
| Train/Action_std        | 0.29191482   |
| Train/Entropy           | 0.0632293    |
| Train/Entropy_Loss      | -6.32e-05    |
| Train/Entropy_loss      | -6.32e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.183279    |
| Train/Loss              | -0.015857529 |
| Train/PolicyClip        | 0.00701289   |
| Train/Policy_loss       | -0.021358244 |
| Train/Ratio             | 0.9932802    |
| Train/Return            | 1.1119331    |
| Train/V                 | 1.1005692    |
| Train/Value             | 1.1005692    |
| Train/control_penalty   | 0.55639446   |
| Train/policy_loss       | -0.021358244 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 520 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 519         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.0721      |
| Time/Buffer             | 0.00255     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25279078  |
| Train/Action_magnitu... | 0.72626144  |
| Train/Action_magnitude  | 0.5735351   |
| Train/Action_max        | 0.2166779   |
| Train/Action_std        | 0.30929145  |
| Train/Entropy           | 0.115733884 |
| Train/Entropy_Loss      | -0.000116   |
| Train/Entropy_loss      | -0.000116   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.24659249 |
| Train/Loss              | 0.013763548 |
| Train/PolicyClip        | 0.009100456 |
| Train/Policy_loss       | 0.008172517 |
| Train/Ratio             | 0.99514747  |
| Train/Return            | 1.3125366   |
| Train/V                 | 1.3326762   |
| Train/Value             | 1.3326762   |
| Train/control_penalty   | 0.5706765   |
| Train/policy_loss       | 0.008172517 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 521 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 520          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.0024       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24612834   |
| Train/Action_magnitu... | 0.7001493    |
| Train/Action_magnitude  | 0.55251724   |
| Train/Action_max        | 0.23713262   |
| Train/Action_std        | 0.29404846   |
| Train/Entropy           | 0.05880559   |
| Train/Entropy_Loss      | -5.88e-05    |
| Train/Entropy_loss      | -5.88e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.16752288  |
| Train/Loss              | -0.03370914  |
| Train/PolicyClip        | 0.0070328657 |
| Train/Policy_loss       | -0.039194126 |
| Train/Ratio             | 0.9933939    |
| Train/Return            | 1.1376566    |
| Train/V                 | 1.109785     |
| Train/Value             | 1.109785     |
| Train/control_penalty   | 0.5543796    |
| Train/policy_loss       | -0.039194126 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 522 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 521          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0739       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00494      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24216169   |
| Train/Action_magnitu... | 0.69454765   |
| Train/Action_magnitude  | 0.5529211    |
| Train/Action_max        | 0.2584833    |
| Train/Action_std        | 0.28527802   |
| Train/Entropy           | 0.014693693  |
| Train/Entropy_Loss      | -1.47e-05    |
| Train/Entropy_loss      | -1.47e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.10900587  |
| Train/Loss              | -0.009389874 |
| Train/PolicyClip        | 0.0076108347 |
| Train/Policy_loss       | -0.014855229 |
| Train/Ratio             | 1.0020689    |
| Train/Return            | 1.2052109    |
| Train/V                 | 1.2014885    |
| Train/Value             | 1.2014885    |
| Train/control_penalty   | 0.54800487   |
| Train/policy_loss       | -0.014855229 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------

 ---------------- Iteration 523 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 522          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.0026       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25374833   |
| Train/Action_magnitu... | 0.6993941    |
| Train/Action_magnitude  | 0.5556483    |
| Train/Action_max        | 0.24430732   |
| Train/Action_std        | 0.29072675   |
| Train/Entropy           | 0.046775192  |
| Train/Entropy_Loss      | -4.68e-05    |
| Train/Entropy_loss      | -4.68e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.09869626  |
| Train/Loss              | 0.010691876  |
| Train/PolicyClip        | 0.0043136897 |
| Train/Policy_loss       | 0.0050945207 |
| Train/Ratio             | 0.99464077   |
| Train/Return            | 1.0744133    |
| Train/V                 | 1.0887494    |
| Train/Value             | 1.0887494    |
| Train/control_penalty   | 0.5644131    |
| Train/policy_loss       | 0.0050945207 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01725      |
------------------------------------------

 ---------------- Iteration 524 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 523          |
| Time/Actor_Time         | 0.0833       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00308      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24751952   |
| Train/Action_magnitu... | 0.7226529    |
| Train/Action_magnitude  | 0.575204     |
| Train/Action_max        | 0.28360137   |
| Train/Action_std        | 0.2983144    |
| Train/Entropy           | 0.07227315   |
| Train/Entropy_Loss      | -7.23e-05    |
| Train/Entropy_loss      | -7.23e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.16320078  |
| Train/Loss              | 0.023004856  |
| Train/PolicyClip        | 0.0040360396 |
| Train/Policy_loss       | 0.017428668  |
| Train/Ratio             | 0.9933174    |
| Train/Return            | 1.0845197    |
| Train/V                 | 1.1112764    |
| Train/Value             | 1.1112764    |
| Train/control_penalty   | 0.5648461    |
| Train/policy_loss       | 0.017428668  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.019        |
------------------------------------------

 ---------------- Iteration 525 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 524          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00431      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25114375   |
| Train/Action_magnitu... | 0.70683306   |
| Train/Action_magnitude  | 0.56356645   |
| Train/Action_max        | 0.27437547   |
| Train/Action_std        | 0.30694574   |
| Train/Entropy           | 0.08768806   |
| Train/Entropy_Loss      | -8.77e-05    |
| Train/Entropy_loss      | -8.77e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.1775535   |
| Train/Loss              | 0.04842071   |
| Train/PolicyClip        | 0.0057143103 |
| Train/Policy_loss       | 0.042735696  |
| Train/Ratio             | 0.98939764   |
| Train/Return            | 0.97819996   |
| Train/V                 | 1.032141     |
| Train/Value             | 1.032141     |
| Train/control_penalty   | 0.5772702    |
| Train/policy_loss       | 0.042735696  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.015        |
------------------------------------------

 ---------------- Iteration 526 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 525           |
| Time/Actor_Time         | 0.0864        |
| Time/B_Format_Time      | 0.0713        |
| Time/B_Original_Form... | 0.0717        |
| Time/Buffer             | 0.00242       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.24695648    |
| Train/Action_magnitu... | 0.7408695     |
| Train/Action_magnitude  | 0.5917963     |
| Train/Action_max        | 0.32705572    |
| Train/Action_std        | 0.3154959     |
| Train/Entropy           | 0.09505937    |
| Train/Entropy_Loss      | -9.51e-05     |
| Train/Entropy_loss      | -9.51e-05     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.26858136   |
| Train/Loss              | 2.7991366e-05 |
| Train/PolicyClip        | 0.0055063614  |
| Train/Policy_loss       | -0.0058262786 |
| Train/Ratio             | 0.9897313     |
| Train/Return            | 0.94940346    |
| Train/V                 | 0.9523696     |
| Train/Value             | 0.9523696     |
| Train/control_penalty   | 0.5949329     |
| Train/policy_loss       | -0.0058262786 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01525       |
-------------------------------------------

 ---------------- Iteration 527 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 526           |
| Time/Actor_Time         | 0.085         |
| Time/B_Format_Time      | 0.0728        |
| Time/B_Original_Form... | 0.0733        |
| Time/Buffer             | 0.00256       |
| Time/Critic_Time        | 7.15e-07      |
| Train/Action_abs_mean   | 0.24325232    |
| Train/Action_magnitu... | 0.7449286     |
| Train/Action_magnitude  | 0.59754574    |
| Train/Action_max        | 0.34678325    |
| Train/Action_std        | 0.32728347    |
| Train/Entropy           | 0.13842972    |
| Train/Entropy_Loss      | -0.000138     |
| Train/Entropy_loss      | -0.000138     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.32727033   |
| Train/Loss              | -0.0076480764 |
| Train/PolicyClip        | 0.006531784   |
| Train/Policy_loss       | -0.013501231  |
| Train/Ratio             | 0.9948458     |
| Train/Return            | 0.83409196    |
| Train/V                 | 0.83132774    |
| Train/Value             | 0.83132774    |
| Train/control_penalty   | 0.5991584     |
| Train/policy_loss       | -0.013501231  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0145        |
-------------------------------------------

 ---------------- Iteration 528 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 527          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0733       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00247      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24912943   |
| Train/Action_magnitu... | 0.71389955   |
| Train/Action_magnitude  | 0.5743138    |
| Train/Action_max        | 0.33485913   |
| Train/Action_std        | 0.30826318   |
| Train/Entropy           | 0.05556432   |
| Train/Entropy_Loss      | -5.56e-05    |
| Train/Entropy_loss      | -5.56e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.193561    |
| Train/Loss              | -0.027405145 |
| Train/PolicyClip        | 0.010551698  |
| Train/Policy_loss       | -0.03327611  |
| Train/Ratio             | 0.986274     |
| Train/Return            | 0.9973943    |
| Train/V                 | 0.97254103   |
| Train/Value             | 0.97254103   |
| Train/control_penalty   | 0.592653     |
| Train/policy_loss       | -0.03327611  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 529 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 528          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.0023       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24285695   |
| Train/Action_magnitu... | 0.731732     |
| Train/Action_magnitude  | 0.5865556    |
| Train/Action_max        | 0.33179206   |
| Train/Action_std        | 0.31247988   |
| Train/Entropy           | 0.10054456   |
| Train/Entropy_Loss      | -0.000101    |
| Train/Entropy_loss      | -0.000101    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.2678111   |
| Train/Loss              | 0.010161297  |
| Train/PolicyClip        | 0.0072983284 |
| Train/Policy_loss       | 0.0045710076 |
| Train/Ratio             | 0.9888702    |
| Train/Return            | 1.0071691    |
| Train/V                 | 1.0240055    |
| Train/Value             | 1.0240055    |
| Train/control_penalty   | 0.5690834    |
| Train/policy_loss       | 0.0045710076 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0165       |
------------------------------------------

 ---------------- Iteration 530 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 529          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0694       |
| Time/Buffer             | 0.00248      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23352395   |
| Train/Action_magnitu... | 0.7292445    |
| Train/Action_magnitude  | 0.58389515   |
| Train/Action_max        | 0.31235778   |
| Train/Action_std        | 0.3350053    |
| Train/Entropy           | 0.15282625   |
| Train/Entropy_Loss      | -0.000153    |
| Train/Entropy_loss      | -0.000153    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.30002716  |
| Train/Loss              | -0.012531405 |
| Train/PolicyClip        | 0.008961364  |
| Train/Policy_loss       | -0.018282339 |
| Train/Ratio             | 0.9857585    |
| Train/Return            | 0.9237615    |
| Train/V                 | 0.9170753    |
| Train/Value             | 0.9170753    |
| Train/control_penalty   | 0.59037596   |
| Train/policy_loss       | -0.018282339 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.016        |
------------------------------------------

 ---------------- Iteration 531 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 530          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00299      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23194562   |
| Train/Action_magnitu... | 0.7160502    |
| Train/Action_magnitude  | 0.5737896    |
| Train/Action_max        | 0.31077886   |
| Train/Action_std        | 0.32366225   |
| Train/Entropy           | 0.11290213   |
| Train/Entropy_Loss      | -0.000113    |
| Train/Entropy_loss      | -0.000113    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.2300033   |
| Train/Loss              | -0.028129352 |
| Train/PolicyClip        | 0.0061263    |
| Train/Policy_loss       | -0.033832297 |
| Train/Ratio             | 0.99502206   |
| Train/Return            | 0.99247074   |
| Train/V                 | 0.96835315   |
| Train/Value             | 0.96835315   |
| Train/control_penalty   | 0.5815847    |
| Train/policy_loss       | -0.033832297 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01625      |
------------------------------------------

 ---------------- Iteration 532 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 531          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00237      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.23723525   |
| Train/Action_magnitu... | 0.74348617   |
| Train/Action_magnitude  | 0.5933105    |
| Train/Action_max        | 0.31486747   |
| Train/Action_std        | 0.33370134   |
| Train/Entropy           | 0.16473216   |
| Train/Entropy_Loss      | -0.000165    |
| Train/Entropy_loss      | -0.000165    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3712678   |
| Train/Loss              | -0.04293786  |
| Train/PolicyClip        | 0.00822922   |
| Train/Policy_loss       | -0.048625432 |
| Train/Ratio             | 0.9997208    |
| Train/Return            | 1.0628015    |
| Train/V                 | 1.0259188    |
| Train/Value             | 1.0259188    |
| Train/control_penalty   | 0.5852304    |
| Train/policy_loss       | -0.048625432 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 533 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 532            |
| Time/Actor_Time         | 0.0855         |
| Time/B_Format_Time      | 0.0728         |
| Time/B_Original_Form... | 0.0708         |
| Time/Buffer             | 0.00305        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.22789328     |
| Train/Action_magnitu... | 0.71466255     |
| Train/Action_magnitude  | 0.56956744     |
| Train/Action_max        | 0.2773517      |
| Train/Action_std        | 0.33775148     |
| Train/Entropy           | 0.17571118     |
| Train/Entropy_Loss      | -0.000176      |
| Train/Entropy_loss      | -0.000176      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -0.30718544    |
| Train/Loss              | 0.0054025687   |
| Train/PolicyClip        | 0.008860044    |
| Train/Policy_loss       | -0.00015234518 |
| Train/Ratio             | 0.98834264     |
| Train/Return            | 1.0517538      |
| Train/V                 | 1.0634518      |
| Train/Value             | 1.0634518      |
| Train/control_penalty   | 0.57306254     |
| Train/policy_loss       | -0.00015234518 |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.01775        |
--------------------------------------------

 ---------------- Iteration 534 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 533          |
| Time/Actor_Time         | 0.0844       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00359      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.23221439   |
| Train/Action_magnitu... | 0.72770137   |
| Train/Action_magnitude  | 0.5799815    |
| Train/Action_max        | 0.2793361    |
| Train/Action_std        | 0.32645      |
| Train/Entropy           | 0.14332928   |
| Train/Entropy_Loss      | -0.000143    |
| Train/Entropy_loss      | -0.000143    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3264127   |
| Train/Loss              | -0.07380162  |
| Train/PolicyClip        | 0.008511377  |
| Train/Policy_loss       | -0.079280764 |
| Train/Ratio             | 1.0019559    |
| Train/Return            | 1.1712871    |
| Train/V                 | 1.103617     |
| Train/Value             | 1.103617     |
| Train/control_penalty   | 0.56224656   |
| Train/policy_loss       | -0.079280764 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 535 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 534         |
| Time/Actor_Time         | 0.0869      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00209     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23593234  |
| Train/Action_magnitu... | 0.75400347  |
| Train/Action_magnitude  | 0.5982497   |
| Train/Action_max        | 0.27319646  |
| Train/Action_std        | 0.3848902   |
| Train/Entropy           | 0.3063352   |
| Train/Entropy_Loss      | -0.000306   |
| Train/Entropy_loss      | -0.000306   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.54167986 |
| Train/Loss              | 0.056503378 |
| Train/PolicyClip        | 0.006955839 |
| Train/Policy_loss       | 0.050559983 |
| Train/Ratio             | 0.976165    |
| Train/Return            | 1.0687783   |
| Train/V                 | 1.1330305   |
| Train/Value             | 1.1330305   |
| Train/control_penalty   | 0.624973    |
| Train/policy_loss       | 0.050559983 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.013       |
-----------------------------------------

 ---------------- Iteration 536 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 535           |
| Time/Actor_Time         | 0.0835        |
| Time/B_Format_Time      | 0.0707        |
| Time/B_Original_Form... | 0.0704        |
| Time/Buffer             | 0.00282       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.24306887    |
| Train/Action_magnitu... | 0.7419126     |
| Train/Action_magnitude  | 0.5907588     |
| Train/Action_max        | 0.25047562    |
| Train/Action_std        | 0.33678296    |
| Train/Entropy           | 0.18276894    |
| Train/Entropy_Loss      | -0.000183     |
| Train/Entropy_loss      | -0.000183     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.39813617   |
| Train/Loss              | 0.0030444325  |
| Train/PolicyClip        | 0.008936371   |
| Train/Policy_loss       | -0.0027176638 |
| Train/Ratio             | 0.9881603     |
| Train/Return            | 1.1087747     |
| Train/V                 | 1.1172189     |
| Train/Value             | 1.1172189     |
| Train/control_penalty   | 0.59448653    |
| Train/policy_loss       | -0.0027176638 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.019         |
-------------------------------------------

 ---------------- Iteration 537 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 536          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00251      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2423114    |
| Train/Action_magnitu... | 0.7216158    |
| Train/Action_magnitude  | 0.57218766   |
| Train/Action_max        | 0.2311953    |
| Train/Action_std        | 0.34404248   |
| Train/Entropy           | 0.20180604   |
| Train/Entropy_Loss      | -0.000202    |
| Train/Entropy_loss      | -0.000202    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.36827067  |
| Train/Loss              | 0.09123755   |
| Train/PolicyClip        | 0.0060050455 |
| Train/Policy_loss       | 0.085490115  |
| Train/Ratio             | 0.9698915    |
| Train/Return            | 1.0834209    |
| Train/V                 | 1.1803653    |
| Train/Value             | 1.1803653    |
| Train/control_penalty   | 0.5949243    |
| Train/policy_loss       | 0.085490115  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0135       |
------------------------------------------

 ---------------- Iteration 538 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 537           |
| Time/Actor_Time         | 0.0864        |
| Time/B_Format_Time      | 0.0717        |
| Time/B_Original_Form... | 0.0703        |
| Time/Buffer             | 0.00236       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.24037302    |
| Train/Action_magnitu... | 0.7352993     |
| Train/Action_magnitude  | 0.5840765     |
| Train/Action_max        | 0.28648135    |
| Train/Action_std        | 0.33352053    |
| Train/Entropy           | 0.18181376    |
| Train/Entropy_Loss      | -0.000182     |
| Train/Entropy_loss      | -0.000182     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.45598865   |
| Train/Loss              | 0.004011359   |
| Train/PolicyClip        | 0.008572605   |
| Train/Policy_loss       | -0.0016361788 |
| Train/Ratio             | 0.994008      |
| Train/Return            | 0.936993      |
| Train/V                 | 0.9428931     |
| Train/Value             | 0.9428931     |
| Train/control_penalty   | 0.58293515    |
| Train/policy_loss       | -0.0016361788 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01625       |
-------------------------------------------

 ---------------- Iteration 539 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 538          |
| Time/Actor_Time         | 0.084        |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0711       |
| Time/Buffer             | 0.00282      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24905291   |
| Train/Action_magnitu... | 0.7674391    |
| Train/Action_magnitude  | 0.60994714   |
| Train/Action_max        | 0.2679953    |
| Train/Action_std        | 0.3347223    |
| Train/Entropy           | 0.17570515   |
| Train/Entropy_Loss      | -0.000176    |
| Train/Entropy_loss      | -0.000176    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.44739774  |
| Train/Loss              | -0.027326275 |
| Train/PolicyClip        | 0.009878666  |
| Train/Policy_loss       | -0.033131022 |
| Train/Ratio             | 0.9913166    |
| Train/Return            | 0.9263226    |
| Train/V                 | 0.90392303   |
| Train/Value             | 0.90392303   |
| Train/control_penalty   | 0.598045     |
| Train/policy_loss       | -0.033131022 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01825      |
------------------------------------------

 ---------------- Iteration 540 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 539          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0694       |
| Time/Buffer             | 0.00272      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25017717   |
| Train/Action_magnitu... | 0.80478853   |
| Train/Action_magnitude  | 0.6353061    |
| Train/Action_max        | 0.2854361    |
| Train/Action_std        | 0.35647067   |
| Train/Entropy           | 0.2458027    |
| Train/Entropy_Loss      | -0.000246    |
| Train/Entropy_loss      | -0.000246    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6140418   |
| Train/Loss              | -0.06737323  |
| Train/PolicyClip        | 0.0086285705 |
| Train/Policy_loss       | -0.073352866 |
| Train/Ratio             | 0.99967355   |
| Train/Return            | 0.9779296    |
| Train/V                 | 0.9186       |
| Train/Value             | 0.9186       |
| Train/control_penalty   | 0.6225439    |
| Train/policy_loss       | -0.073352866 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 541 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 540          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00315      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24193978   |
| Train/Action_magnitu... | 0.73164344   |
| Train/Action_magnitude  | 0.5798728    |
| Train/Action_max        | 0.26253104   |
| Train/Action_std        | 0.3320896    |
| Train/Entropy           | 0.18391715   |
| Train/Entropy_Loss      | -0.000184    |
| Train/Entropy_loss      | -0.000184    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.37932074  |
| Train/Loss              | -0.036142953 |
| Train/PolicyClip        | 0.009133075  |
| Train/Policy_loss       | -0.0418543   |
| Train/Ratio             | 0.99924713   |
| Train/Return            | 0.96558464   |
| Train/V                 | 0.93654317   |
| Train/Value             | 0.93654317   |
| Train/control_penalty   | 0.5895265    |
| Train/policy_loss       | -0.0418543   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 542 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 541         |
| Time/Actor_Time         | 0.0853      |
| Time/B_Format_Time      | 0.0702      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00315     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24334152  |
| Train/Action_magnitu... | 0.72680044  |
| Train/Action_magnitude  | 0.5754681   |
| Train/Action_max        | 0.25523278  |
| Train/Action_std        | 0.32260498  |
| Train/Entropy           | 0.17088614  |
| Train/Entropy_Loss      | -0.000171   |
| Train/Entropy_loss      | -0.000171   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.40208107 |
| Train/Loss              | 0.022021737 |
| Train/PolicyClip        | 0.009952268 |
| Train/Policy_loss       | 0.016425516 |
| Train/Ratio             | 0.9860945   |
| Train/Return            | 0.9682993   |
| Train/V                 | 0.9957909   |
| Train/Value             | 0.9957909   |
| Train/control_penalty   | 0.5767107   |
| Train/policy_loss       | 0.016425516 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 543 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 542          |
| Time/Actor_Time         | 0.0875       |
| Time/B_Format_Time      | 0.0703       |
| Time/B_Original_Form... | 0.0724       |
| Time/Buffer             | 0.00287      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.25443295   |
| Train/Action_magnitu... | 0.7440139    |
| Train/Action_magnitude  | 0.5853984    |
| Train/Action_max        | 0.21926618   |
| Train/Action_std        | 0.31491286   |
| Train/Entropy           | 0.14427835   |
| Train/Entropy_Loss      | -0.000144    |
| Train/Entropy_loss      | -0.000144    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.33028033  |
| Train/Loss              | -0.010238184 |
| Train/PolicyClip        | 0.0052561234 |
| Train/Policy_loss       | -0.015843485 |
| Train/Ratio             | 0.99120796   |
| Train/Return            | 1.011586     |
| Train/V                 | 1.0057522    |
| Train/Value             | 1.0057522    |
| Train/control_penalty   | 0.5749579    |
| Train/policy_loss       | -0.015843485 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 544 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 543          |
| Time/Actor_Time         | 0.0875       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00338      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25588825   |
| Train/Action_magnitu... | 0.7508641    |
| Train/Action_magnitude  | 0.59426737   |
| Train/Action_max        | 0.24950567   |
| Train/Action_std        | 0.32005653   |
| Train/Entropy           | 0.14295742   |
| Train/Entropy_Loss      | -0.000143    |
| Train/Entropy_loss      | -0.000143    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.36265656  |
| Train/Loss              | -0.031145655 |
| Train/PolicyClip        | 0.0074326047 |
| Train/Policy_loss       | -0.03686748  |
| Train/Ratio             | 0.9965071    |
| Train/Return            | 0.9113401    |
| Train/V                 | 0.8857832    |
| Train/Value             | 0.8857832    |
| Train/control_penalty   | 0.5864784    |
| Train/policy_loss       | -0.03686748  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01925      |
------------------------------------------

 ---------------- Iteration 545 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 544           |
| Time/Actor_Time         | 0.0868        |
| Time/B_Format_Time      | 0.071         |
| Time/B_Original_Form... | 0.0701        |
| Time/Buffer             | 0.00267       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.25683373    |
| Train/Action_magnitu... | 0.7341824     |
| Train/Action_magnitude  | 0.5804264     |
| Train/Action_max        | 0.25960886    |
| Train/Action_std        | 0.32912025    |
| Train/Entropy           | 0.17486718    |
| Train/Entropy_Loss      | -0.000175     |
| Train/Entropy_loss      | -0.000175     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.30484083   |
| Train/Loss              | -0.0068137227 |
| Train/PolicyClip        | 0.011156655   |
| Train/Policy_loss       | -0.01252529   |
| Train/Ratio             | 1.0031083     |
| Train/Return            | 0.8343071     |
| Train/V                 | 0.8280089     |
| Train/Value             | 0.8280089     |
| Train/control_penalty   | 0.5886435     |
| Train/policy_loss       | -0.01252529   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.015         |
-------------------------------------------

 ---------------- Iteration 546 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 545          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00341      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2530162    |
| Train/Action_magnitu... | 0.7305022    |
| Train/Action_magnitude  | 0.5756623    |
| Train/Action_max        | 0.21692334   |
| Train/Action_std        | 0.30346018   |
| Train/Entropy           | 0.1014017    |
| Train/Entropy_Loss      | -0.000101    |
| Train/Entropy_loss      | -0.000101    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.25483868  |
| Train/Loss              | -0.055785283 |
| Train/PolicyClip        | 0.006101153  |
| Train/Policy_loss       | -0.06133944  |
| Train/Ratio             | 1.0101506    |
| Train/Return            | 1.0094002    |
| Train/V                 | 0.96071947   |
| Train/Value             | 0.96071947   |
| Train/control_penalty   | 0.56555593   |
| Train/policy_loss       | -0.06133944  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 547 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 546         |
| Time/Actor_Time         | 0.0835      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.26500988  |
| Train/Action_magnitu... | 0.7254161   |
| Train/Action_magnitude  | 0.5730671   |
| Train/Action_max        | 0.25384215  |
| Train/Action_std        | 0.29838198  |
| Train/Entropy           | 0.073235005 |
| Train/Entropy_Loss      | -7.32e-05   |
| Train/Entropy_loss      | -7.32e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.18585563 |
| Train/Loss              | -0.12157821 |
| Train/PolicyClip        | 0.00574372  |
| Train/Policy_loss       | -0.12731822 |
| Train/Ratio             | 1.0088761   |
| Train/Return            | 1.1589907   |
| Train/V                 | 1.0429565   |
| Train/Value             | 1.0429565   |
| Train/control_penalty   | 0.58132476  |
| Train/policy_loss       | -0.12731822 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 548 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 547          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00332      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25276896   |
| Train/Action_magnitu... | 0.7031256    |
| Train/Action_magnitude  | 0.55448824   |
| Train/Action_max        | 0.23158947   |
| Train/Action_std        | 0.2793438    |
| Train/Entropy           | 0.008861201  |
| Train/Entropy_Loss      | -8.86e-06    |
| Train/Entropy_loss      | -8.86e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.07746082  |
| Train/Loss              | -0.09359872  |
| Train/PolicyClip        | 0.0077831573 |
| Train/Policy_loss       | -0.098967284 |
| Train/Ratio             | 1.0117766    |
| Train/Return            | 1.189897     |
| Train/V                 | 1.103606     |
| Train/Value             | 1.103606     |
| Train/control_penalty   | 0.53774196   |
| Train/policy_loss       | -0.098967284 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02875      |
------------------------------------------

 ---------------- Iteration 549 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 548          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00315      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.24219011   |
| Train/Action_magnitu... | 0.6755121    |
| Train/Action_magnitude  | 0.534381     |
| Train/Action_max        | 0.24777637   |
| Train/Action_std        | 0.2791216    |
| Train/Entropy           | 0.005088273  |
| Train/Entropy_Loss      | -5.09e-06    |
| Train/Entropy_loss      | -5.09e-06    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.044292267 |
| Train/Loss              | -0.03818146  |
| Train/PolicyClip        | 0.008448626  |
| Train/Policy_loss       | -0.043506645 |
| Train/Ratio             | 1.0021172    |
| Train/Return            | 1.1552612    |
| Train/V                 | 1.1224943    |
| Train/Value             | 1.1224943    |
| Train/control_penalty   | 0.5330274    |
| Train/policy_loss       | -0.043506645 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 550 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 549            |
| Time/Actor_Time         | 0.0841         |
| Time/B_Format_Time      | 0.0726         |
| Time/B_Original_Form... | 0.0695         |
| Time/Buffer             | 0.00303        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.25350064     |
| Train/Action_magnitu... | 0.7155084      |
| Train/Action_magnitude  | 0.5673474      |
| Train/Action_max        | 0.2631969      |
| Train/Action_std        | 0.28907537     |
| Train/Entropy           | 0.033523075    |
| Train/Entropy_Loss      | -3.35e-05      |
| Train/Entropy_loss      | -3.35e-05      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -0.17271088    |
| Train/Loss              | -0.00020315498 |
| Train/PolicyClip        | 0.00849461     |
| Train/Policy_loss       | -0.0056548985  |
| Train/Ratio             | 0.99010605     |
| Train/Return            | 1.0592449      |
| Train/V                 | 1.0634574      |
| Train/Value             | 1.0634574      |
| Train/control_penalty   | 0.54852664     |
| Train/policy_loss       | -0.0056548985  |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.017          |
--------------------------------------------

 ---------------- Iteration 551 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 550         |
| Time/Actor_Time         | 0.0862      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.00295     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2659087   |
| Train/Action_magnitu... | 0.72114706  |
| Train/Action_magnitude  | 0.56890327  |
| Train/Action_max        | 0.22928296  |
| Train/Action_std        | 0.3012515   |
| Train/Entropy           | 0.0825744   |
| Train/Entropy_Loss      | -8.26e-05   |
| Train/Entropy_loss      | -8.26e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.1783822  |
| Train/Loss              | 0.026811343 |
| Train/PolicyClip        | 0.008274672 |
| Train/Policy_loss       | 0.021093464 |
| Train/Ratio             | 0.99760216  |
| Train/Return            | 1.2646128   |
| Train/V                 | 1.2931278   |
| Train/Value             | 1.2931278   |
| Train/control_penalty   | 0.5800455   |
| Train/policy_loss       | 0.021093464 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 552 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 551          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.0027       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2426098    |
| Train/Action_magnitu... | 0.7046995    |
| Train/Action_magnitude  | 0.5589293    |
| Train/Action_max        | 0.27769834   |
| Train/Action_std        | 0.31031135   |
| Train/Entropy           | 0.115022756  |
| Train/Entropy_Loss      | -0.000115    |
| Train/Entropy_loss      | -0.000115    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.23813818  |
| Train/Loss              | -0.014973595 |
| Train/PolicyClip        | 0.01114505   |
| Train/Policy_loss       | -0.020541064 |
| Train/Ratio             | 0.9937715    |
| Train/Return            | 1.0899749    |
| Train/V                 | 1.0782757    |
| Train/Value             | 1.0782757    |
| Train/control_penalty   | 0.56824917   |
| Train/policy_loss       | -0.020541064 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 553 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 552          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00253      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26474035   |
| Train/Action_magnitu... | 0.74457824   |
| Train/Action_magnitude  | 0.5914568    |
| Train/Action_max        | 0.25749463   |
| Train/Action_std        | 0.31929418   |
| Train/Entropy           | 0.14334059   |
| Train/Entropy_Loss      | -0.000143    |
| Train/Entropy_loss      | -0.000143    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3227397   |
| Train/Loss              | 0.010353044  |
| Train/PolicyClip        | 0.008103725  |
| Train/Policy_loss       | 0.0045139794 |
| Train/Ratio             | 0.9790492    |
| Train/Return            | 1.1328937    |
| Train/V                 | 1.1525055    |
| Train/Value             | 1.1525055    |
| Train/control_penalty   | 0.5982405    |
| Train/policy_loss       | 0.0045139794 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.018        |
------------------------------------------

 ---------------- Iteration 554 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 553          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.0693       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.003        |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2634613    |
| Train/Action_magnitu... | 0.7467188    |
| Train/Action_magnitude  | 0.59151214   |
| Train/Action_max        | 0.2977937    |
| Train/Action_std        | 0.3135872    |
| Train/Entropy           | 0.13107517   |
| Train/Entropy_Loss      | -0.000131    |
| Train/Entropy_loss      | -0.000131    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.2813864   |
| Train/Loss              | 0.020103207  |
| Train/PolicyClip        | 0.0071207066 |
| Train/Policy_loss       | 0.014347458  |
| Train/Ratio             | 0.98712933   |
| Train/Return            | 0.9809148    |
| Train/V                 | 1.0073704    |
| Train/Value             | 1.0073704    |
| Train/control_penalty   | 0.58868235   |
| Train/policy_loss       | 0.014347458  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01525      |
------------------------------------------

 ---------------- Iteration 555 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 554         |
| Time/Actor_Time         | 0.0871      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0727      |
| Time/Buffer             | 0.00261     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2659733   |
| Train/Action_magnitu... | 0.72056955  |
| Train/Action_magnitude  | 0.5663467   |
| Train/Action_max        | 0.2239006   |
| Train/Action_std        | 0.28734818  |
| Train/Entropy           | 0.048091736 |
| Train/Entropy_Loss      | -4.81e-05   |
| Train/Entropy_loss      | -4.81e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.15823601 |
| Train/Loss              | 0.010106867 |
| Train/PolicyClip        | 0.007919782 |
| Train/Policy_loss       | 0.004494004 |
| Train/Ratio             | 0.9909674   |
| Train/Return            | 1.1091211   |
| Train/V                 | 1.1242266   |
| Train/Value             | 1.1242266   |
| Train/control_penalty   | 0.5660955   |
| Train/policy_loss       | 0.004494004 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 556 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 555          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25763848   |
| Train/Action_magnitu... | 0.7132311    |
| Train/Action_magnitude  | 0.5629355    |
| Train/Action_max        | 0.25115332   |
| Train/Action_std        | 0.3063473    |
| Train/Entropy           | 0.10966055   |
| Train/Entropy_Loss      | -0.00011     |
| Train/Entropy_loss      | -0.00011     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.22580634  |
| Train/Loss              | -0.034127295 |
| Train/PolicyClip        | 0.012469136  |
| Train/Policy_loss       | -0.039789796 |
| Train/Ratio             | 1.0058283    |
| Train/Return            | 1.0828366    |
| Train/V                 | 1.0494484    |
| Train/Value             | 1.0494484    |
| Train/control_penalty   | 0.5772163    |
| Train/policy_loss       | -0.039789796 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 557 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 556          |
| Time/Actor_Time         | 0.0845       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00256      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.25994962   |
| Train/Action_magnitu... | 0.71493083   |
| Train/Action_magnitude  | 0.5649943    |
| Train/Action_max        | 0.24145167   |
| Train/Action_std        | 0.29084703   |
| Train/Entropy           | 0.06241718   |
| Train/Entropy_Loss      | -6.24e-05    |
| Train/Entropy_loss      | -6.24e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.22042544  |
| Train/Loss              | -0.051333614 |
| Train/PolicyClip        | 0.009859363  |
| Train/Policy_loss       | -0.056945354 |
| Train/Ratio             | 1.0090113    |
| Train/Return            | 1.1732469    |
| Train/V                 | 1.1240993    |
| Train/Value             | 1.1240993    |
| Train/control_penalty   | 0.5674158    |
| Train/policy_loss       | -0.056945354 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 558 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 557           |
| Time/Actor_Time         | 0.086         |
| Time/B_Format_Time      | 0.0709        |
| Time/B_Original_Form... | 0.0705        |
| Time/Buffer             | 0.00301       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.26644313    |
| Train/Action_magnitu... | 0.71770364    |
| Train/Action_magnitude  | 0.56796366    |
| Train/Action_max        | 0.2527766     |
| Train/Action_std        | 0.300414      |
| Train/Entropy           | 0.10342837    |
| Train/Entropy_Loss      | -0.000103     |
| Train/Entropy_loss      | -0.000103     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.2536606    |
| Train/Loss              | -0.0015259683 |
| Train/PolicyClip        | 0.011984941   |
| Train/Policy_loss       | -0.007120254  |
| Train/Ratio             | 0.9833078     |
| Train/Return            | 0.95910496    |
| Train/V                 | 0.96144116    |
| Train/Value             | 0.96144116    |
| Train/control_penalty   | 0.5697714     |
| Train/policy_loss       | -0.007120254  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0175        |
-------------------------------------------

 ---------------- Iteration 559 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 558         |
| Time/Actor_Time         | 0.0864      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0751      |
| Time/Buffer             | 0.00295     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2607767   |
| Train/Action_magnitu... | 0.7155974   |
| Train/Action_magnitude  | 0.5646057   |
| Train/Action_max        | 0.21701404  |
| Train/Action_std        | 0.3130798   |
| Train/Entropy           | 0.14082871  |
| Train/Entropy_Loss      | -0.000141   |
| Train/Entropy_loss      | -0.000141   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.2429108  |
| Train/Loss              | 0.026408207 |
| Train/PolicyClip        | 0.012111958 |
| Train/Policy_loss       | 0.020647159 |
| Train/Ratio             | 0.97794986  |
| Train/Return            | 1.1590776   |
| Train/V                 | 1.192464    |
| Train/Value             | 1.192464    |
| Train/control_penalty   | 0.5901876   |
| Train/policy_loss       | 0.020647159 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 560 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 559          |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.0722       |
| Time/Buffer             | 0.00243      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.25465617   |
| Train/Action_magnitu... | 0.719219     |
| Train/Action_magnitude  | 0.5637201    |
| Train/Action_max        | 0.22643358   |
| Train/Action_std        | 0.28961563   |
| Train/Entropy           | 0.06995645   |
| Train/Entropy_Loss      | -7e-05       |
| Train/Entropy_loss      | -7e-05       |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.20967212  |
| Train/Loss              | -0.031606328 |
| Train/PolicyClip        | 0.010910921  |
| Train/Policy_loss       | -0.03715029  |
| Train/Ratio             | 0.99841076   |
| Train/Return            | 1.2061671    |
| Train/V                 | 1.1830738    |
| Train/Value             | 1.1830738    |
| Train/control_penalty   | 0.561392     |
| Train/policy_loss       | -0.03715029  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 561 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 560          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00253      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.26084283   |
| Train/Action_magnitu... | 0.71771866   |
| Train/Action_magnitude  | 0.5662564    |
| Train/Action_max        | 0.24660227   |
| Train/Action_std        | 0.30929327   |
| Train/Entropy           | 0.12959619   |
| Train/Entropy_Loss      | -0.00013     |
| Train/Entropy_loss      | -0.00013     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.2727717   |
| Train/Loss              | -0.059771188 |
| Train/PolicyClip        | 0.015360291  |
| Train/Policy_loss       | -0.065425456 |
| Train/Ratio             | 1.0009366    |
| Train/Return            | 1.216085     |
| Train/V                 | 1.1630017    |
| Train/Value             | 1.1630017    |
| Train/control_penalty   | 0.5783862    |
| Train/policy_loss       | -0.065425456 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 562 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 561          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00263      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.26729313   |
| Train/Action_magnitu... | 0.7549077    |
| Train/Action_magnitude  | 0.5948663    |
| Train/Action_max        | 0.2396357    |
| Train/Action_std        | 0.3257178    |
| Train/Entropy           | 0.17243089   |
| Train/Entropy_Loss      | -0.000172    |
| Train/Entropy_loss      | -0.000172    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3435842   |
| Train/Loss              | -0.05495933  |
| Train/PolicyClip        | 0.0056224894 |
| Train/Policy_loss       | -0.060811885 |
| Train/Ratio             | 1.0011523    |
| Train/Return            | 1.1883239    |
| Train/V                 | 1.1406548    |
| Train/Value             | 1.1406548    |
| Train/control_penalty   | 0.60249853   |
| Train/policy_loss       | -0.060811885 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 563 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 562         |
| Time/Actor_Time         | 0.085       |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00238     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.27971038  |
| Train/Action_magnitu... | 0.74958646  |
| Train/Action_magnitude  | 0.5880183   |
| Train/Action_max        | 0.23907275  |
| Train/Action_std        | 0.30653158  |
| Train/Entropy           | 0.11759958  |
| Train/Entropy_Loss      | -0.000118   |
| Train/Entropy_loss      | -0.000118   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.2613459  |
| Train/Loss              | 0.03336118  |
| Train/PolicyClip        | 0.008142666 |
| Train/Policy_loss       | 0.027553998 |
| Train/Ratio             | 0.9884087   |
| Train/Return            | 1.2166724   |
| Train/V                 | 1.2549117   |
| Train/Value             | 1.2549117   |
| Train/control_penalty   | 0.5924782   |
| Train/policy_loss       | 0.027553998 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 564 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 563          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00231      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27866405   |
| Train/Action_magnitu... | 0.7474847    |
| Train/Action_magnitude  | 0.5874759    |
| Train/Action_max        | 0.25848088   |
| Train/Action_std        | 0.30841586   |
| Train/Entropy           | 0.11950074   |
| Train/Entropy_Loss      | -0.00012     |
| Train/Entropy_loss      | -0.00012     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.27338552  |
| Train/Loss              | -0.01969365  |
| Train/PolicyClip        | 0.010733232  |
| Train/Policy_loss       | -0.025602454 |
| Train/Ratio             | 1.0001053    |
| Train/Return            | 1.3380893    |
| Train/V                 | 1.3279598    |
| Train/Value             | 1.3279598    |
| Train/control_penalty   | 0.60283047   |
| Train/policy_loss       | -0.025602454 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 565 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 564           |
| Time/Actor_Time         | 0.0853        |
| Time/B_Format_Time      | 0.0717        |
| Time/B_Original_Form... | 0.0711        |
| Time/Buffer             | 0.00306       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.27168557    |
| Train/Action_magnitu... | 0.75422955    |
| Train/Action_magnitude  | 0.592276      |
| Train/Action_max        | 0.2615321     |
| Train/Action_std        | 0.31924275    |
| Train/Entropy           | 0.13431445    |
| Train/Entropy_Loss      | -0.000134     |
| Train/Entropy_loss      | -0.000134     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.2667307    |
| Train/Loss              | -0.0011026938 |
| Train/PolicyClip        | 0.01165665    |
| Train/Policy_loss       | -0.0069539235 |
| Train/Ratio             | 0.99222696    |
| Train/Return            | 1.3399811     |
| Train/V                 | 1.3476164     |
| Train/Value             | 1.3476164     |
| Train/control_penalty   | 0.59855443    |
| Train/policy_loss       | -0.0069539235 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02375       |
-------------------------------------------

 ---------------- Iteration 566 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 565         |
| Time/Actor_Time         | 0.0859      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0697      |
| Time/Buffer             | 0.00259     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.27131456  |
| Train/Action_magnitu... | 0.7696697   |
| Train/Action_magnitude  | 0.6060023   |
| Train/Action_max        | 0.2829203   |
| Train/Action_std        | 0.32548308  |
| Train/Entropy           | 0.15620759  |
| Train/Entropy_Loss      | -0.000156   |
| Train/Entropy_loss      | -0.000156   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.33603114 |
| Train/Loss              | 0.017434016 |
| Train/PolicyClip        | 0.010470154 |
| Train/Policy_loss       | 0.011556984 |
| Train/Ratio             | 0.9877256   |
| Train/Return            | 1.2800328   |
| Train/V                 | 1.3041675   |
| Train/Value             | 1.3041675   |
| Train/control_penalty   | 0.603324    |
| Train/policy_loss       | 0.011556984 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 567 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 566          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00251      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2901374    |
| Train/Action_magnitu... | 0.794658     |
| Train/Action_magnitude  | 0.6253795    |
| Train/Action_max        | 0.25136566   |
| Train/Action_std        | 0.32316405   |
| Train/Entropy           | 0.13221098   |
| Train/Entropy_Loss      | -0.000132    |
| Train/Entropy_loss      | -0.000132    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.34878668  |
| Train/Loss              | -0.044960644 |
| Train/PolicyClip        | 0.006742145  |
| Train/Policy_loss       | -0.05109655  |
| Train/Ratio             | 0.99605215   |
| Train/Return            | 1.4337783    |
| Train/V                 | 1.3931794    |
| Train/Value             | 1.3931794    |
| Train/control_penalty   | 0.6268118    |
| Train/policy_loss       | -0.05109655  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02675      |
------------------------------------------

 ---------------- Iteration 568 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 567          |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29735726   |
| Train/Action_magnitu... | 0.8326217    |
| Train/Action_magnitude  | 0.65482265   |
| Train/Action_max        | 0.25144103   |
| Train/Action_std        | 0.33759052   |
| Train/Entropy           | 0.19904831   |
| Train/Entropy_Loss      | -0.000199    |
| Train/Entropy_loss      | -0.000199    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.46122226  |
| Train/Loss              | -0.021204196 |
| Train/PolicyClip        | 0.009284897  |
| Train/Policy_loss       | -0.027407655 |
| Train/Ratio             | 0.9952576    |
| Train/Return            | 1.2674661    |
| Train/V                 | 1.2514968    |
| Train/Value             | 1.2514968    |
| Train/control_penalty   | 0.6402508    |
| Train/policy_loss       | -0.027407655 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 569 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 568          |
| Time/Actor_Time         | 0.0832       |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00268      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29949775   |
| Train/Action_magnitu... | 0.8318625    |
| Train/Action_magnitude  | 0.6596814    |
| Train/Action_max        | 0.29492664   |
| Train/Action_std        | 0.35361385   |
| Train/Entropy           | 0.23813292   |
| Train/Entropy_Loss      | -0.000238    |
| Train/Entropy_loss      | -0.000238    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4499476   |
| Train/Loss              | 0.01376616   |
| Train/PolicyClip        | 0.010220368  |
| Train/Policy_loss       | 0.0072637983 |
| Train/Ratio             | 0.9932331    |
| Train/Return            | 1.258062     |
| Train/V                 | 1.2803875    |
| Train/Value             | 1.2803875    |
| Train/control_penalty   | 0.6740495    |
| Train/policy_loss       | 0.0072637983 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02         |
------------------------------------------

 ---------------- Iteration 570 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 569          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00276      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29450557   |
| Train/Action_magnitu... | 0.81873864   |
| Train/Action_magnitude  | 0.64920986   |
| Train/Action_max        | 0.33989295   |
| Train/Action_std        | 0.35556835   |
| Train/Entropy           | 0.22248401   |
| Train/Entropy_Loss      | -0.000222    |
| Train/Entropy_loss      | -0.000222    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4958166   |
| Train/Loss              | -0.01125217  |
| Train/PolicyClip        | 0.010264247  |
| Train/Policy_loss       | -0.017755844 |
| Train/Ratio             | 0.9946085    |
| Train/Return            | 1.2739153    |
| Train/V                 | 1.2692692    |
| Train/Value             | 1.2692692    |
| Train/control_penalty   | 0.6726157    |
| Train/policy_loss       | -0.017755844 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 571 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 570         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00271     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29119498  |
| Train/Action_magnitu... | 0.8297675   |
| Train/Action_magnitude  | 0.6569094   |
| Train/Action_max        | 0.30258983  |
| Train/Action_std        | 0.35199484  |
| Train/Entropy           | 0.21217667  |
| Train/Entropy_Loss      | -0.000212   |
| Train/Entropy_loss      | -0.000212   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.50151515 |
| Train/Loss              | 0.039703522 |
| Train/PolicyClip        | 0.008609104 |
| Train/Policy_loss       | 0.03334696  |
| Train/Ratio             | 0.98596865  |
| Train/Return            | 1.2680738   |
| Train/V                 | 1.3128301   |
| Train/Value             | 1.3128301   |
| Train/control_penalty   | 0.65687424  |
| Train/policy_loss       | 0.03334696  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 572 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 571          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0699       |
| Time/B_Original_Form... | 0.0699       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30188677   |
| Train/Action_magnitu... | 0.8814882    |
| Train/Action_magnitude  | 0.70153105   |
| Train/Action_max        | 0.3085386    |
| Train/Action_std        | 0.38505337   |
| Train/Entropy           | 0.2953667    |
| Train/Entropy_Loss      | -0.000295    |
| Train/Entropy_loss      | -0.000295    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5800625   |
| Train/Loss              | -0.012657478 |
| Train/PolicyClip        | 0.009330894  |
| Train/Policy_loss       | -0.019356493 |
| Train/Ratio             | 0.9936294    |
| Train/Return            | 1.3096375    |
| Train/V                 | 1.3046086    |
| Train/Value             | 1.3046086    |
| Train/control_penalty   | 0.69943815   |
| Train/policy_loss       | -0.019356493 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 573 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 572         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0724      |
| Time/Buffer             | 0.00313     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.29448205  |
| Train/Action_magnitu... | 0.8653132   |
| Train/Action_magnitude  | 0.68675596  |
| Train/Action_max        | 0.27985513  |
| Train/Action_std        | 0.3775312   |
| Train/Entropy           | 0.27780756  |
| Train/Entropy_Loss      | -0.000278   |
| Train/Entropy_loss      | -0.000278   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5950298  |
| Train/Loss              | 0.02742479  |
| Train/PolicyClip        | 0.007721056 |
| Train/Policy_loss       | 0.020792596 |
| Train/Ratio             | 0.98668194  |
| Train/Return            | 1.3167187   |
| Train/V                 | 1.3520741   |
| Train/Value             | 1.3520741   |
| Train/control_penalty   | 0.6910001   |
| Train/policy_loss       | 0.020792596 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 574 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 573         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2812533   |
| Train/Action_magnitu... | 0.83557105  |
| Train/Action_magnitude  | 0.6627253   |
| Train/Action_max        | 0.29300913  |
| Train/Action_std        | 0.37870649  |
| Train/Entropy           | 0.2864506   |
| Train/Entropy_Loss      | -0.000286   |
| Train/Entropy_loss      | -0.000286   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.60976213 |
| Train/Loss              | 0.030132623 |
| Train/PolicyClip        | 0.009193211 |
| Train/Policy_loss       | 0.023619475 |
| Train/Ratio             | 0.981906    |
| Train/Return            | 1.2353559   |
| Train/V                 | 1.2729945   |
| Train/Value             | 1.2729945   |
| Train/control_penalty   | 0.6799599   |
| Train/policy_loss       | 0.023619475 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 575 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 574          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0711       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28087047   |
| Train/Action_magnitu... | 0.81862813   |
| Train/Action_magnitude  | 0.6515624    |
| Train/Action_max        | 0.28510505   |
| Train/Action_std        | 0.35750708   |
| Train/Entropy           | 0.22389972   |
| Train/Entropy_Loss      | -0.000224    |
| Train/Entropy_loss      | -0.000224    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5316411   |
| Train/Loss              | 0.0012982194 |
| Train/PolicyClip        | 0.0078128185 |
| Train/Policy_loss       | -0.005050064 |
| Train/Ratio             | 0.98890823   |
| Train/Return            | 1.1604612    |
| Train/V                 | 1.1632048    |
| Train/Value             | 1.1632048    |
| Train/control_penalty   | 0.65721834   |
| Train/policy_loss       | -0.005050064 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 576 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 575           |
| Time/Actor_Time         | 0.0852        |
| Time/B_Format_Time      | 0.0721        |
| Time/B_Original_Form... | 0.0716        |
| Time/Buffer             | 0.00375       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.30608398    |
| Train/Action_magnitu... | 0.8492188     |
| Train/Action_magnitude  | 0.68037754    |
| Train/Action_max        | 0.35902226    |
| Train/Action_std        | 0.36287102    |
| Train/Entropy           | 0.24873431    |
| Train/Entropy_Loss      | -0.000249     |
| Train/Entropy_loss      | -0.000249     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.51994514   |
| Train/Loss              | -0.0076725697 |
| Train/PolicyClip        | 0.014069813   |
| Train/Policy_loss       | -0.014309519  |
| Train/Ratio             | 0.99221253    |
| Train/Return            | 1.136572      |
| Train/V                 | 1.1324394     |
| Train/Value             | 1.1324394     |
| Train/control_penalty   | 0.68856835    |
| Train/policy_loss       | -0.014309519  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02325       |
-------------------------------------------

 ---------------- Iteration 577 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 576          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28458026   |
| Train/Action_magnitu... | 0.8392617    |
| Train/Action_magnitude  | 0.6651533    |
| Train/Action_max        | 0.29475564   |
| Train/Action_std        | 0.3684144    |
| Train/Entropy           | 0.25314835   |
| Train/Entropy_Loss      | -0.000253    |
| Train/Entropy_loss      | -0.000253    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5692677   |
| Train/Loss              | -0.02150497  |
| Train/PolicyClip        | 0.008276798  |
| Train/Policy_loss       | -0.027989121 |
| Train/Ratio             | 0.9987994    |
| Train/Return            | 1.2099047    |
| Train/V                 | 1.1928816    |
| Train/Value             | 1.1928816    |
| Train/control_penalty   | 0.67372996   |
| Train/policy_loss       | -0.027989121 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 578 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 577            |
| Time/Actor_Time         | 0.0856         |
| Time/B_Format_Time      | 0.0721         |
| Time/B_Original_Form... | 0.071          |
| Time/Buffer             | 0.00274        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.30863446     |
| Train/Action_magnitu... | 0.8645781      |
| Train/Action_magnitude  | 0.68655705     |
| Train/Action_max        | 0.29261038     |
| Train/Action_std        | 0.38834623     |
| Train/Entropy           | 0.31874847     |
| Train/Entropy_Loss      | -0.000319      |
| Train/Entropy_loss      | -0.000319      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -0.5749712     |
| Train/Loss              | -0.00029284693 |
| Train/PolicyClip        | 0.011065054    |
| Train/Policy_loss       | -0.0070644147  |
| Train/Ratio             | 0.9798081      |
| Train/Return            | 1.2176715      |
| Train/V                 | 1.2240561      |
| Train/Value             | 1.2240561      |
| Train/control_penalty   | 0.70903164     |
| Train/policy_loss       | -0.0070644147  |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.02475        |
--------------------------------------------

 ---------------- Iteration 579 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 578          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0716       |
| Time/Buffer             | 0.00566      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.30734184   |
| Train/Action_magnitu... | 0.885161     |
| Train/Action_magnitude  | 0.706481     |
| Train/Action_max        | 0.36778063   |
| Train/Action_std        | 0.37733713   |
| Train/Entropy           | 0.28491858   |
| Train/Entropy_Loss      | -0.000285    |
| Train/Entropy_loss      | -0.000285    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.64607537  |
| Train/Loss              | -0.04011295  |
| Train/PolicyClip        | 0.009427109  |
| Train/Policy_loss       | -0.046761047 |
| Train/Ratio             | 1.0033982    |
| Train/Return            | 1.2028731    |
| Train/V                 | 1.1682885    |
| Train/Value             | 1.1682885    |
| Train/control_penalty   | 0.6933016    |
| Train/policy_loss       | -0.046761047 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.027        |
------------------------------------------

 ---------------- Iteration 580 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 579          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0742       |
| Time/B_Original_Form... | 0.0882       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.29414144   |
| Train/Action_magnitu... | 0.8964127    |
| Train/Action_magnitude  | 0.71856517   |
| Train/Action_max        | 0.33766538   |
| Train/Action_std        | 0.43436697   |
| Train/Entropy           | 0.42779338   |
| Train/Entropy_Loss      | -0.000428    |
| Train/Entropy_loss      | -0.000428    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8166002   |
| Train/Loss              | -0.007037915 |
| Train/PolicyClip        | 0.009792185  |
| Train/Policy_loss       | -0.013968063 |
| Train/Ratio             | 0.9689844    |
| Train/Return            | 1.1516068    |
| Train/V                 | 1.1506203    |
| Train/Value             | 1.1506203    |
| Train/control_penalty   | 0.7357942    |
| Train/policy_loss       | -0.013968063 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 581 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 580         |
| Time/Actor_Time         | 0.0868      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0724      |
| Time/Buffer             | 0.00278     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31032687  |
| Train/Action_magnitu... | 0.92246586  |
| Train/Action_magnitude  | 0.7386947   |
| Train/Action_max        | 0.38813552  |
| Train/Action_std        | 0.4081366   |
| Train/Entropy           | 0.37536934  |
| Train/Entropy_Loss      | -0.000375   |
| Train/Entropy_loss      | -0.000375   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.79987335 |
| Train/Loss              | -0.09038444 |
| Train/PolicyClip        | 0.008946477 |
| Train/Policy_loss       | -0.09744552 |
| Train/Ratio             | 1.0004137   |
| Train/Return            | 1.2416298   |
| Train/V                 | 1.1586442   |
| Train/Value             | 1.1586442   |
| Train/control_penalty   | 0.7436451   |
| Train/policy_loss       | -0.09744552 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 582 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 581          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0717       |
| Time/Buffer             | 0.00329      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31262594   |
| Train/Action_magnitu... | 0.9150163    |
| Train/Action_magnitude  | 0.7324376    |
| Train/Action_max        | 0.33673725   |
| Train/Action_std        | 0.39198676   |
| Train/Entropy           | 0.32891974   |
| Train/Entropy_Loss      | -0.000329    |
| Train/Entropy_loss      | -0.000329    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.76255876  |
| Train/Loss              | -0.065015495 |
| Train/PolicyClip        | 0.01014342   |
| Train/Policy_loss       | -0.07192931  |
| Train/Ratio             | 1.0058713    |
| Train/Return            | 1.2033857    |
| Train/V                 | 1.1472226    |
| Train/Value             | 1.1472226    |
| Train/control_penalty   | 0.7242738    |
| Train/policy_loss       | -0.07192931  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02875      |
------------------------------------------

 ---------------- Iteration 583 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 582          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0736       |
| Time/Buffer             | 0.00259      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2932518    |
| Train/Action_magnitu... | 0.90164584   |
| Train/Action_magnitude  | 0.720002     |
| Train/Action_max        | 0.35929507   |
| Train/Action_std        | 0.3896165    |
| Train/Entropy           | 0.32671425   |
| Train/Entropy_Loss      | -0.000327    |
| Train/Entropy_loss      | -0.000327    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7313984   |
| Train/Loss              | -0.049068257 |
| Train/PolicyClip        | 0.015449425  |
| Train/Policy_loss       | -0.055704106 |
| Train/Ratio             | 1.004183     |
| Train/Return            | 1.2874135    |
| Train/V                 | 1.2451563    |
| Train/Value             | 1.2451563    |
| Train/control_penalty   | 0.69625646   |
| Train/policy_loss       | -0.055704106 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 584 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 583         |
| Time/Actor_Time         | 0.0847      |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0721      |
| Time/Buffer             | 0.00231     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.29598778  |
| Train/Action_magnitu... | 0.87073445  |
| Train/Action_magnitude  | 0.695031    |
| Train/Action_max        | 0.33034745  |
| Train/Action_std        | 0.38189334  |
| Train/Entropy           | 0.31256366  |
| Train/Entropy_Loss      | -0.000313   |
| Train/Entropy_loss      | -0.000313   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.67199755 |
| Train/Loss              | 0.031712353 |
| Train/PolicyClip        | 0.012309826 |
| Train/Policy_loss       | 0.025091337 |
| Train/Ratio             | 0.9904737   |
| Train/Return            | 1.1623197   |
| Train/V                 | 1.2037076   |
| Train/Value             | 1.2037076   |
| Train/control_penalty   | 0.6933579   |
| Train/policy_loss       | 0.025091337 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------

 ---------------- Iteration 585 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 584          |
| Time/Actor_Time         | 0.11         |
| Time/B_Format_Time      | 0.102        |
| Time/B_Original_Form... | 0.0796       |
| Time/Buffer             | 0.00288      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29568493   |
| Train/Action_magnitu... | 0.8956403    |
| Train/Action_magnitude  | 0.7152207    |
| Train/Action_max        | 0.34407818   |
| Train/Action_std        | 0.38876432   |
| Train/Entropy           | 0.3368104    |
| Train/Entropy_Loss      | -0.000337    |
| Train/Entropy_loss      | -0.000337    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7336809   |
| Train/Loss              | -0.023228604 |
| Train/PolicyClip        | 0.011641289  |
| Train/Policy_loss       | -0.029791217 |
| Train/Ratio             | 0.99604386   |
| Train/Return            | 1.2479762    |
| Train/V                 | 1.2356482    |
| Train/Value             | 1.2356482    |
| Train/control_penalty   | 0.6899423    |
| Train/policy_loss       | -0.029791217 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 586 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 585          |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.082        |
| Time/B_Original_Form... | 0.0826       |
| Time/Buffer             | 0.00366      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2709883    |
| Train/Action_magnitu... | 0.8093215    |
| Train/Action_magnitude  | 0.64369124   |
| Train/Action_max        | 0.3331352    |
| Train/Action_std        | 0.35314795   |
| Train/Entropy           | 0.23420896   |
| Train/Entropy_Loss      | -0.000234    |
| Train/Entropy_loss      | -0.000234    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5233919   |
| Train/Loss              | -0.08351016  |
| Train/PolicyClip        | 0.0048887297 |
| Train/Policy_loss       | -0.08965972  |
| Train/Ratio             | 1.0029035    |
| Train/Return            | 1.2560184    |
| Train/V                 | 1.1761124    |
| Train/Value             | 1.1761124    |
| Train/control_penalty   | 0.6383769    |
| Train/policy_loss       | -0.08965972  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03075      |
------------------------------------------

 ---------------- Iteration 587 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 586         |
| Time/Actor_Time         | 0.0947      |
| Time/B_Format_Time      | 0.0892      |
| Time/B_Original_Form... | 0.0809      |
| Time/Buffer             | 0.00315     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.28748286  |
| Train/Action_magnitu... | 0.8504998   |
| Train/Action_magnitude  | 0.6775249   |
| Train/Action_max        | 0.349228    |
| Train/Action_std        | 0.37725925  |
| Train/Entropy           | 0.28835604  |
| Train/Entropy_Loss      | -0.000288   |
| Train/Entropy_loss      | -0.000288   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.63530076 |
| Train/Loss              | 0.03984143  |
| Train/PolicyClip        | 0.008570266 |
| Train/Policy_loss       | 0.03339655  |
| Train/Ratio             | 0.9824217   |
| Train/Return            | 1.1992208   |
| Train/V                 | 1.2479831   |
| Train/Value             | 1.2479831   |
| Train/control_penalty   | 0.67332345  |
| Train/policy_loss       | 0.03339655  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 588 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 587          |
| Time/Actor_Time         | 0.0832       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00443      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2941739    |
| Train/Action_magnitu... | 0.84999526   |
| Train/Action_magnitude  | 0.6753883    |
| Train/Action_max        | 0.27021223   |
| Train/Action_std        | 0.353897     |
| Train/Entropy           | 0.23554498   |
| Train/Entropy_Loss      | -0.000236    |
| Train/Entropy_loss      | -0.000236    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.71738136  |
| Train/Loss              | -0.040105805 |
| Train/PolicyClip        | 0.009939375  |
| Train/Policy_loss       | -0.046352524 |
| Train/Ratio             | 0.98934835   |
| Train/Return            | 1.2788513    |
| Train/V                 | 1.2404633    |
| Train/Value             | 1.2404633    |
| Train/control_penalty   | 0.64822644   |
| Train/policy_loss       | -0.046352524 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 589 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 588          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0738       |
| Time/B_Original_Form... | 0.0725       |
| Time/Buffer             | 0.00263      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30217326   |
| Train/Action_magnitu... | 0.82607126   |
| Train/Action_magnitude  | 0.65634096   |
| Train/Action_max        | 0.28544253   |
| Train/Action_std        | 0.35307565   |
| Train/Entropy           | 0.23674208   |
| Train/Entropy_Loss      | -0.000237    |
| Train/Entropy_loss      | -0.000237    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4712841   |
| Train/Loss              | 0.007060164  |
| Train/PolicyClip        | 0.0103123    |
| Train/Policy_loss       | 0.0005080415 |
| Train/Ratio             | 0.98333675   |
| Train/Return            | 1.3979075    |
| Train/V                 | 1.4121306    |
| Train/Value             | 1.4121306    |
| Train/control_penalty   | 0.6788865    |
| Train/policy_loss       | 0.0005080415 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 590 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 589         |
| Time/Actor_Time         | 0.0844      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00404     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2861306   |
| Train/Action_magnitu... | 0.7846841   |
| Train/Action_magnitude  | 0.6217212   |
| Train/Action_max        | 0.22491883  |
| Train/Action_std        | 0.3268789   |
| Train/Entropy           | 0.1633277   |
| Train/Entropy_Loss      | -0.000163   |
| Train/Entropy_loss      | -0.000163   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.34463495 |
| Train/Loss              | -0.09415376 |
| Train/PolicyClip        | 0.008552376 |
| Train/Policy_loss       | -0.10016976 |
| Train/Ratio             | 1.0099825   |
| Train/Return            | 1.4143881   |
| Train/V                 | 1.3283815   |
| Train/Value             | 1.3283815   |
| Train/control_penalty   | 0.61793274  |
| Train/policy_loss       | -0.10016976 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0365      |
-----------------------------------------

 ---------------- Iteration 591 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 590          |
| Time/Actor_Time         | 0.0846       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00207      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28938442   |
| Train/Action_magnitu... | 0.7916148    |
| Train/Action_magnitude  | 0.62877136   |
| Train/Action_max        | 0.26134104   |
| Train/Action_std        | 0.33932182   |
| Train/Entropy           | 0.218405     |
| Train/Entropy_Loss      | -0.000218    |
| Train/Entropy_loss      | -0.000218    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.43482512  |
| Train/Loss              | -0.021074722 |
| Train/PolicyClip        | 0.01196822   |
| Train/Policy_loss       | -0.027236655 |
| Train/Ratio             | 0.9927588    |
| Train/Return            | 1.3082538    |
| Train/V                 | 1.2928932    |
| Train/Value             | 1.2928932    |
| Train/control_penalty   | 0.6380338    |
| Train/policy_loss       | -0.027236655 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 592 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 591          |
| Time/Actor_Time         | 0.0918       |
| Time/B_Format_Time      | 0.0784       |
| Time/B_Original_Form... | 0.0813       |
| Time/Buffer             | 0.00326      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3079995    |
| Train/Action_magnitu... | 0.83414227   |
| Train/Action_magnitude  | 0.66221386   |
| Train/Action_max        | 0.25184765   |
| Train/Action_std        | 0.3451792    |
| Train/Entropy           | 0.21235159   |
| Train/Entropy_Loss      | -0.000212    |
| Train/Entropy_loss      | -0.000212    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4625079   |
| Train/Loss              | -0.026927155 |
| Train/PolicyClip        | 0.0106023345 |
| Train/Policy_loss       | -0.033328477 |
| Train/Ratio             | 1.0022045    |
| Train/Return            | 1.3116316    |
| Train/V                 | 1.2914886    |
| Train/Value             | 1.2914886    |
| Train/control_penalty   | 0.6613676    |
| Train/policy_loss       | -0.033328477 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 593 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 592         |
| Time/Actor_Time         | 0.0851      |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.0761      |
| Time/Buffer             | 0.00306     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29565364  |
| Train/Action_magnitu... | 0.77832353  |
| Train/Action_magnitude  | 0.6152538   |
| Train/Action_max        | 0.24446934  |
| Train/Action_std        | 0.32328713  |
| Train/Entropy           | 0.15700755  |
| Train/Entropy_Loss      | -0.000157   |
| Train/Entropy_loss      | -0.000157   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.2902894  |
| Train/Loss              | 0.009308461 |
| Train/PolicyClip        | 0.009040064 |
| Train/Policy_loss       | 0.00322643  |
| Train/Ratio             | 0.98052835  |
| Train/Return            | 1.2692701   |
| Train/V                 | 1.287853    |
| Train/Value             | 1.287853    |
| Train/control_penalty   | 0.6239039   |
| Train/policy_loss       | 0.00322643  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 594 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 593         |
| Time/Actor_Time         | 0.0915      |
| Time/B_Format_Time      | 0.0724      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00324     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29929754  |
| Train/Action_magnitu... | 0.8625678   |
| Train/Action_magnitude  | 0.68234384  |
| Train/Action_max        | 0.22284263  |
| Train/Action_std        | 0.39148948  |
| Train/Entropy           | 0.31039804  |
| Train/Entropy_Loss      | -0.00031    |
| Train/Entropy_loss      | -0.00031    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5597246  |
| Train/Loss              | 0.046348583 |
| Train/PolicyClip        | 0.009300564 |
| Train/Policy_loss       | 0.03961397  |
| Train/Ratio             | 0.97527146  |
| Train/Return            | 1.1717521   |
| Train/V                 | 1.224909    |
| Train/Value             | 1.224909    |
| Train/control_penalty   | 0.7045012   |
| Train/policy_loss       | 0.03961397  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 595 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 594         |
| Time/Actor_Time         | 0.0824      |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.0751      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30479634  |
| Train/Action_magnitu... | 0.87053764  |
| Train/Action_magnitude  | 0.69072837  |
| Train/Action_max        | 0.25758252  |
| Train/Action_std        | 0.37863234  |
| Train/Entropy           | 0.2661792   |
| Train/Entropy_Loss      | -0.000266   |
| Train/Entropy_loss      | -0.000266   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6103213  |
| Train/Loss              | 0.046504073 |
| Train/PolicyClip        | 0.008777976 |
| Train/Policy_loss       | 0.039920066 |
| Train/Ratio             | 0.9822484   |
| Train/Return            | 1.1704592   |
| Train/V                 | 1.2206764   |
| Train/Value             | 1.2206764   |
| Train/control_penalty   | 0.68501866  |
| Train/policy_loss       | 0.039920066 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 596 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 595          |
| Time/Actor_Time         | 0.0922       |
| Time/B_Format_Time      | 0.0886       |
| Time/B_Original_Form... | 0.0813       |
| Time/Buffer             | 0.00285      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2954176    |
| Train/Action_magnitu... | 0.8581669    |
| Train/Action_magnitude  | 0.68230945   |
| Train/Action_max        | 0.27041966   |
| Train/Action_std        | 0.35600975   |
| Train/Entropy           | 0.2164888    |
| Train/Entropy_Loss      | -0.000216    |
| Train/Entropy_loss      | -0.000216    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.51314527  |
| Train/Loss              | -0.04766661  |
| Train/PolicyClip        | 0.0090607945 |
| Train/Policy_loss       | -0.054217897 |
| Train/Ratio             | 1.0008377    |
| Train/Return            | 1.304163     |
| Train/V                 | 1.2605292    |
| Train/Value             | 1.2605292    |
| Train/control_penalty   | 0.6767775    |
| Train/policy_loss       | -0.054217897 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------

 ---------------- Iteration 597 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 596          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0774       |
| Time/B_Original_Form... | 0.0776       |
| Time/Buffer             | 0.00531      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31652656   |
| Train/Action_magnitu... | 0.86319906   |
| Train/Action_magnitude  | 0.6857513    |
| Train/Action_max        | 0.20808126   |
| Train/Action_std        | 0.34467256   |
| Train/Entropy           | 0.15834862   |
| Train/Entropy_Loss      | -0.000158    |
| Train/Entropy_loss      | -0.000158    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.42232934  |
| Train/Loss              | -0.027249113 |
| Train/PolicyClip        | 0.008825974  |
| Train/Policy_loss       | -0.033839762 |
| Train/Ratio             | 0.98834854   |
| Train/Return            | 1.1709267    |
| Train/V                 | 1.150859     |
| Train/Value             | 1.150859     |
| Train/control_penalty   | 0.67489976   |
| Train/policy_loss       | -0.033839762 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------

 ---------------- Iteration 598 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 597          |
| Time/Actor_Time         | 0.088        |
| Time/B_Format_Time      | 0.0789       |
| Time/B_Original_Form... | 0.0807       |
| Time/Buffer             | 0.00264      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30675632   |
| Train/Action_magnitu... | 0.8538087    |
| Train/Action_magnitude  | 0.6756378    |
| Train/Action_max        | 0.25293782   |
| Train/Action_std        | 0.35881263   |
| Train/Entropy           | 0.20506155   |
| Train/Entropy_Loss      | -0.000205    |
| Train/Entropy_loss      | -0.000205    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.40628004  |
| Train/Loss              | -0.041953105 |
| Train/PolicyClip        | 0.010732013  |
| Train/Policy_loss       | -0.048680495 |
| Train/Ratio             | 1.0022523    |
| Train/Return            | 1.1987368    |
| Train/V                 | 1.1561501    |
| Train/Value             | 1.1561501    |
| Train/control_penalty   | 0.69324523   |
| Train/policy_loss       | -0.048680495 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 599 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 598           |
| Time/Actor_Time         | 0.103         |
| Time/B_Format_Time      | 0.0792        |
| Time/B_Original_Form... | 0.0831        |
| Time/Buffer             | 0.00366       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3162666     |
| Train/Action_magnitu... | 0.8563085     |
| Train/Action_magnitude  | 0.67799926    |
| Train/Action_max        | 0.25330722    |
| Train/Action_std        | 0.3665274     |
| Train/Entropy           | 0.24005975    |
| Train/Entropy_Loss      | -0.00024      |
| Train/Entropy_loss      | -0.00024      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.46001887   |
| Train/Loss              | -0.0030757883 |
| Train/PolicyClip        | 0.0067035807  |
| Train/Policy_loss       | -0.009796617  |
| Train/Ratio             | 0.9901896     |
| Train/Return            | 1.119949      |
| Train/V                 | 1.1207435     |
| Train/Value             | 1.1207435     |
| Train/control_penalty   | 0.6960888     |
| Train/policy_loss       | -0.009796617  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02          |
-------------------------------------------

 ---------------- Iteration 600 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 599          |
| Time/Actor_Time         | 0.091        |
| Time/B_Format_Time      | 0.0863       |
| Time/B_Original_Form... | 0.0819       |
| Time/Buffer             | 0.0098       |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.333625     |
| Train/Action_magnitu... | 0.92232126   |
| Train/Action_magnitude  | 0.72946864   |
| Train/Action_max        | 0.2118497    |
| Train/Action_std        | 0.3727802    |
| Train/Entropy           | 0.2444994    |
| Train/Entropy_Loss      | -0.000244    |
| Train/Entropy_loss      | -0.000244    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.53473526  |
| Train/Loss              | -0.03676923  |
| Train/PolicyClip        | 0.010429126  |
| Train/Policy_loss       | -0.043737356 |
| Train/Ratio             | 1.0002463    |
| Train/Return            | 1.3674893    |
| Train/V                 | 1.3359175    |
| Train/Value             | 1.3359175    |
| Train/control_penalty   | 0.7212625    |
| Train/policy_loss       | -0.043737356 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0295       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 601 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 600          |
| Time/Actor_Time         | 0.089        |
| Time/B_Format_Time      | 0.0829       |
| Time/B_Original_Form... | 0.0912       |
| Time/Buffer             | 0.0031       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32302397   |
| Train/Action_magnitu... | 0.91934115   |
| Train/Action_magnitude  | 0.7289068    |
| Train/Action_max        | 0.23248312   |
| Train/Action_std        | 0.37891558   |
| Train/Entropy           | 0.256146     |
| Train/Entropy_Loss      | -0.000256    |
| Train/Entropy_loss      | -0.000256    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.54267466  |
| Train/Loss              | -0.019354323 |
| Train/PolicyClip        | 0.010605874  |
| Train/Policy_loss       | -0.026240265 |
| Train/Ratio             | 0.99658084   |
| Train/Return            | 1.3559895    |
| Train/V                 | 1.3426894    |
| Train/Value             | 1.3426894    |
| Train/control_penalty   | 0.7142087    |
| Train/policy_loss       | -0.026240265 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02675      |
------------------------------------------

 ---------------- Iteration 602 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 601         |
| Time/Actor_Time         | 0.106       |
| Time/B_Format_Time      | 0.0817      |
| Time/B_Original_Form... | 0.0799      |
| Time/Buffer             | 0.00253     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32896027  |
| Train/Action_magnitu... | 0.95084494  |
| Train/Action_magnitude  | 0.75522673  |
| Train/Action_max        | 0.24923262  |
| Train/Action_std        | 0.40898174  |
| Train/Entropy           | 0.3205384   |
| Train/Entropy_Loss      | -0.000321   |
| Train/Entropy_loss      | -0.000321   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6580963  |
| Train/Loss              | 0.026627403 |
| Train/PolicyClip        | 0.012505331 |
| Train/Policy_loss       | 0.019356959 |
| Train/Ratio             | 0.9890396   |
| Train/Return            | 1.3849831   |
| Train/V                 | 1.4174001   |
| Train/Value             | 1.4174001   |
| Train/control_penalty   | 0.7590983   |
| Train/policy_loss       | 0.019356959 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 603 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 602          |
| Time/Actor_Time         | 0.099        |
| Time/B_Format_Time      | 0.0831       |
| Time/B_Original_Form... | 0.0848       |
| Time/Buffer             | 0.00261      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3200354    |
| Train/Action_magnitu... | 0.8992427    |
| Train/Action_magnitude  | 0.7170368    |
| Train/Action_max        | 0.297394     |
| Train/Action_std        | 0.3906606    |
| Train/Entropy           | 0.27712137   |
| Train/Entropy_Loss      | -0.000277    |
| Train/Entropy_loss      | -0.000277    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5718414   |
| Train/Loss              | -0.051493287 |
| Train/PolicyClip        | 0.01065146   |
| Train/Policy_loss       | -0.058435433 |
| Train/Ratio             | 0.99417275   |
| Train/Return            | 1.4208095    |
| Train/V                 | 1.3769101    |
| Train/Value             | 1.3769101    |
| Train/control_penalty   | 0.7219266    |
| Train/policy_loss       | -0.058435433 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.029        |
------------------------------------------

 ---------------- Iteration 604 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 603           |
| Time/Actor_Time         | 0.0867        |
| Time/B_Format_Time      | 0.0715        |
| Time/B_Original_Form... | 0.0714        |
| Time/Buffer             | 0.00272       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.325007      |
| Train/Action_magnitu... | 0.9521442     |
| Train/Action_magnitude  | 0.7661713     |
| Train/Action_max        | 0.3119469     |
| Train/Action_std        | 0.42026734    |
| Train/Entropy           | 0.31666917    |
| Train/Entropy_Loss      | -0.000317     |
| Train/Entropy_loss      | -0.000317     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.67548513   |
| Train/Loss              | 0.0056947004  |
| Train/PolicyClip        | 0.014423832   |
| Train/Policy_loss       | -0.0016074579 |
| Train/Ratio             | 0.99831283    |
| Train/Return            | 1.5390606     |
| Train/V                 | 1.5502399     |
| Train/Value             | 1.5502399     |
| Train/control_penalty   | 0.7618827     |
| Train/policy_loss       | -0.0016074579 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02725       |
-------------------------------------------

 ---------------- Iteration 605 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 604          |
| Time/Actor_Time         | 0.0862       |
| Time/B_Format_Time      | 0.074        |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00246      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31493914   |
| Train/Action_magnitu... | 0.9135416    |
| Train/Action_magnitude  | 0.731758     |
| Train/Action_max        | 0.27731115   |
| Train/Action_std        | 0.40761632   |
| Train/Entropy           | 0.3265029    |
| Train/Entropy_Loss      | -0.000327    |
| Train/Entropy_loss      | -0.000327    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7021844   |
| Train/Loss              | -0.02176977  |
| Train/PolicyClip        | 0.015754553  |
| Train/Policy_loss       | -0.028785998 |
| Train/Ratio             | 1.0024555    |
| Train/Return            | 1.4334395    |
| Train/V                 | 1.4177841    |
| Train/Value             | 1.4177841    |
| Train/control_penalty   | 0.73427314   |
| Train/policy_loss       | -0.028785998 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 606 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 605          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00238      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30070058   |
| Train/Action_magnitu... | 0.8932923    |
| Train/Action_magnitude  | 0.71397847   |
| Train/Action_max        | 0.26836285   |
| Train/Action_std        | 0.38776597   |
| Train/Entropy           | 0.29011077   |
| Train/Entropy_Loss      | -0.00029     |
| Train/Entropy_loss      | -0.00029     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6328495   |
| Train/Loss              | -0.047753572 |
| Train/PolicyClip        | 0.0070589706 |
| Train/Policy_loss       | -0.054556333 |
| Train/Ratio             | 1.0096141    |
| Train/Return            | 1.4537072    |
| Train/V                 | 1.412277     |
| Train/Value             | 1.412277     |
| Train/control_penalty   | 0.70928705   |
| Train/policy_loss       | -0.054556333 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 607 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 606         |
| Time/Actor_Time         | 0.108       |
| Time/B_Format_Time      | 0.077       |
| Time/B_Original_Form... | 0.0786      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.33012533  |
| Train/Action_magnitu... | 0.94464856  |
| Train/Action_magnitude  | 0.75687385  |
| Train/Action_max        | 0.3179109   |
| Train/Action_std        | 0.4036336   |
| Train/Entropy           | 0.3235134   |
| Train/Entropy_Loss      | -0.000324   |
| Train/Entropy_loss      | -0.000324   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.713352   |
| Train/Loss              | 0.050537605 |
| Train/PolicyClip        | 0.00910651  |
| Train/Policy_loss       | 0.04335384  |
| Train/Ratio             | 0.98254395  |
| Train/Return            | 1.498315    |
| Train/V                 | 1.5558205   |
| Train/Value             | 1.5558205   |
| Train/control_penalty   | 0.75072753  |
| Train/policy_loss       | 0.04335384  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 608 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 607          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0706       |
| Time/B_Original_Form... | 0.0696       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32171923   |
| Train/Action_magnitu... | 0.9348506    |
| Train/Action_magnitude  | 0.74902743   |
| Train/Action_max        | 0.3144647    |
| Train/Action_std        | 0.40492004   |
| Train/Entropy           | 0.31816903   |
| Train/Entropy_Loss      | -0.000318    |
| Train/Entropy_loss      | -0.000318    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.63470757  |
| Train/Loss              | -0.02897336  |
| Train/PolicyClip        | 0.009633456  |
| Train/Policy_loss       | -0.036292564 |
| Train/Ratio             | 1.0002319    |
| Train/Return            | 1.6402932    |
| Train/V                 | 1.6182663    |
| Train/Value             | 1.6182663    |
| Train/control_penalty   | 0.7637373    |
| Train/policy_loss       | -0.036292564 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02775      |
------------------------------------------

 ---------------- Iteration 609 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 608         |
| Time/Actor_Time         | 0.0966      |
| Time/B_Format_Time      | 0.0777      |
| Time/B_Original_Form... | 0.0823      |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.32915008  |
| Train/Action_magnitu... | 0.9615244   |
| Train/Action_magnitude  | 0.7679862   |
| Train/Action_max        | 0.28450337  |
| Train/Action_std        | 0.42007032  |
| Train/Entropy           | 0.3662772   |
| Train/Entropy_Loss      | -0.000366   |
| Train/Entropy_loss      | -0.000366   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.71798706 |
| Train/Loss              | 0.01608295  |
| Train/PolicyClip        | 0.014353243 |
| Train/Policy_loss       | 0.008685658 |
| Train/Ratio             | 0.9984521   |
| Train/Return            | 1.5472637   |
| Train/V                 | 1.5708789   |
| Train/Value             | 1.5708789   |
| Train/control_penalty   | 0.77635705  |
| Train/policy_loss       | 0.008685658 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 610 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 609         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.075       |
| Time/B_Original_Form... | 0.0745      |
| Time/Buffer             | 0.00292     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30815578  |
| Train/Action_magnitu... | 0.8975855   |
| Train/Action_magnitude  | 0.71670824  |
| Train/Action_max        | 0.28995255  |
| Train/Action_std        | 0.3964253   |
| Train/Entropy           | 0.3220888   |
| Train/Entropy_Loss      | -0.000322   |
| Train/Entropy_loss      | -0.000322   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.65701014 |
| Train/Loss              | 0.011305604 |
| Train/PolicyClip        | 0.011419931 |
| Train/Policy_loss       | 0.004285673 |
| Train/Ratio             | 0.98646355  |
| Train/Return            | 1.4305675   |
| Train/V                 | 1.4508609   |
| Train/Value             | 1.4508609   |
| Train/control_penalty   | 0.73420197  |
| Train/policy_loss       | 0.004285673 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 611 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 610          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00234      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30828404   |
| Train/Action_magnitu... | 0.9182871    |
| Train/Action_magnitude  | 0.7337004    |
| Train/Action_max        | 0.3140396    |
| Train/Action_std        | 0.4187193    |
| Train/Entropy           | 0.39600542   |
| Train/Entropy_Loss      | -0.000396    |
| Train/Entropy_loss      | -0.000396    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7803981   |
| Train/Loss              | 0.076134495  |
| Train/PolicyClip        | 0.0093565835 |
| Train/Policy_loss       | 0.06908919   |
| Train/Ratio             | 0.9699308    |
| Train/Return            | 1.2187295    |
| Train/V                 | 1.303321     |
| Train/Value             | 1.303321     |
| Train/control_penalty   | 0.74413145   |
| Train/policy_loss       | 0.06908919   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------

 ---------------- Iteration 612 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 611         |
| Time/Actor_Time         | 0.084       |
| Time/B_Format_Time      | 0.0728      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00283     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30349025  |
| Train/Action_magnitu... | 0.91335535  |
| Train/Action_magnitude  | 0.7276896   |
| Train/Action_max        | 0.28959328  |
| Train/Action_std        | 0.42680824  |
| Train/Entropy           | 0.4220234   |
| Train/Entropy_Loss      | -0.000422   |
| Train/Entropy_loss      | -0.000422   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.92547303 |
| Train/Loss              | 0.034654066 |
| Train/PolicyClip        | 0.008412587 |
| Train/Policy_loss       | 0.027725466 |
| Train/Ratio             | 0.9869407   |
| Train/Return            | 1.0797584   |
| Train/V                 | 1.1199324   |
| Train/Value             | 1.1199324   |
| Train/control_penalty   | 0.73506224  |
| Train/policy_loss       | 0.027725466 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01875     |
-----------------------------------------

 ---------------- Iteration 613 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 612          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0737       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00309      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3025186    |
| Train/Action_magnitu... | 0.9644007    |
| Train/Action_magnitude  | 0.7704907    |
| Train/Action_max        | 0.35547656   |
| Train/Action_std        | 0.44284856   |
| Train/Entropy           | 0.43654376   |
| Train/Entropy_Loss      | -0.000437    |
| Train/Entropy_loss      | -0.000437    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.89740837  |
| Train/Loss              | -0.008876367 |
| Train/PolicyClip        | 0.0061334516 |
| Train/Policy_loss       | -0.016034266 |
| Train/Ratio             | 0.98880637   |
| Train/Return            | 1.0454034    |
| Train/V                 | 1.0407239    |
| Train/Value             | 1.0407239    |
| Train/control_penalty   | 0.75944436   |
| Train/policy_loss       | -0.016034266 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01825      |
------------------------------------------

 ---------------- Iteration 614 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 613          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0744       |
| Time/B_Original_Form... | 0.0725       |
| Time/Buffer             | 0.0029       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29906705   |
| Train/Action_magnitu... | 0.94325227   |
| Train/Action_magnitude  | 0.75390375   |
| Train/Action_max        | 0.38937476   |
| Train/Action_std        | 0.43761304   |
| Train/Entropy           | 0.45016667   |
| Train/Entropy_Loss      | -0.00045     |
| Train/Entropy_loss      | -0.00045     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.94810176  |
| Train/Loss              | -0.02675854  |
| Train/PolicyClip        | 0.0076717837 |
| Train/Policy_loss       | -0.033667874 |
| Train/Ratio             | 0.98813283   |
| Train/Return            | 0.98419887   |
| Train/V                 | 0.9638775    |
| Train/Value             | 0.9638775    |
| Train/control_penalty   | 0.7359501    |
| Train/policy_loss       | -0.033667874 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 615 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 614          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0726       |
| Time/Buffer             | 0.00355      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30919853   |
| Train/Action_magnitu... | 0.92513263   |
| Train/Action_magnitude  | 0.7383114    |
| Train/Action_max        | 0.34748057   |
| Train/Action_std        | 0.43307427   |
| Train/Entropy           | 0.44708434   |
| Train/Entropy_Loss      | -0.000447    |
| Train/Entropy_loss      | -0.000447    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.95304316  |
| Train/Loss              | -0.036518093 |
| Train/PolicyClip        | 0.0077741314 |
| Train/Policy_loss       | -0.043529164 |
| Train/Ratio             | 0.9982882    |
| Train/Return            | 0.94722587   |
| Train/V                 | 0.91574025   |
| Train/Value             | 0.91574025   |
| Train/control_penalty   | 0.74581534   |
| Train/policy_loss       | -0.043529164 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 616 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 615          |
| Time/Actor_Time         | 0.0827       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.3048064    |
| Train/Action_magnitu... | 0.9306759    |
| Train/Action_magnitude  | 0.74316424   |
| Train/Action_max        | 0.3750527    |
| Train/Action_std        | 0.4240286    |
| Train/Entropy           | 0.421987     |
| Train/Entropy_Loss      | -0.000422    |
| Train/Entropy_loss      | -0.000422    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9349038   |
| Train/Loss              | -0.030962393 |
| Train/PolicyClip        | 0.008995026  |
| Train/Policy_loss       | -0.03792111  |
| Train/Ratio             | 1.002226     |
| Train/Return            | 1.2304243    |
| Train/V                 | 1.2044033    |
| Train/Value             | 1.2044033    |
| Train/control_penalty   | 0.7380701    |
| Train/policy_loss       | -0.03792111  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 617 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 616          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00357      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.30947566   |
| Train/Action_magnitu... | 0.9514928    |
| Train/Action_magnitude  | 0.7639947    |
| Train/Action_max        | 0.38697094   |
| Train/Action_std        | 0.44559643   |
| Train/Entropy           | 0.45084643   |
| Train/Entropy_Loss      | -0.000451    |
| Train/Entropy_loss      | -0.000451    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9320725   |
| Train/Loss              | 0.01077712   |
| Train/PolicyClip        | 0.01140202   |
| Train/Policy_loss       | 0.0035546904 |
| Train/Ratio             | 0.99260885   |
| Train/Return            | 1.2157453    |
| Train/V                 | 1.2355787    |
| Train/Value             | 1.2355787    |
| Train/control_penalty   | 0.76732755   |
| Train/policy_loss       | 0.0035546904 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 618 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 617          |
| Time/Actor_Time         | 0.0833       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00311      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3046623    |
| Train/Action_magnitu... | 0.8508183    |
| Train/Action_magnitude  | 0.6829115    |
| Train/Action_max        | 0.35609496   |
| Train/Action_std        | 0.38843375   |
| Train/Entropy           | 0.3476746    |
| Train/Entropy_Loss      | -0.000348    |
| Train/Entropy_loss      | -0.000348    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6983248   |
| Train/Loss              | -0.071677305 |
| Train/PolicyClip        | 0.00808835   |
| Train/Policy_loss       | -0.0782938   |
| Train/Ratio             | 1.003705     |
| Train/Return            | 1.4408127    |
| Train/V                 | 1.375742     |
| Train/Value             | 1.375742     |
| Train/control_penalty   | 0.69641674   |
| Train/policy_loss       | -0.0782938   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0315       |
------------------------------------------

 ---------------- Iteration 619 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 618          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0746       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00227      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29985443   |
| Train/Action_magnitu... | 0.90569067   |
| Train/Action_magnitude  | 0.7283305    |
| Train/Action_max        | 0.37508157   |
| Train/Action_std        | 0.40554056   |
| Train/Entropy           | 0.3496478    |
| Train/Entropy_Loss      | -0.00035     |
| Train/Entropy_loss      | -0.00035     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.81194293  |
| Train/Loss              | -0.059109848 |
| Train/PolicyClip        | 0.0089985635 |
| Train/Policy_loss       | -0.06594465  |
| Train/Ratio             | 0.9965798    |
| Train/Return            | 1.2645655    |
| Train/V                 | 1.2103946    |
| Train/Value             | 1.2103946    |
| Train/control_penalty   | 0.7184451    |
| Train/policy_loss       | -0.06594465  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 620 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 619          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0752       |
| Time/B_Original_Form... | 0.075        |
| Time/Buffer             | 0.00706      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29897255   |
| Train/Action_magnitu... | 0.88354504   |
| Train/Action_magnitude  | 0.7076469    |
| Train/Action_max        | 0.33571488   |
| Train/Action_std        | 0.40215766   |
| Train/Entropy           | 0.3662556    |
| Train/Entropy_Loss      | -0.000366    |
| Train/Entropy_loss      | -0.000366    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.76108724  |
| Train/Loss              | -0.03457113  |
| Train/PolicyClip        | 0.013455828  |
| Train/Policy_loss       | -0.041300878 |
| Train/Ratio             | 0.99516094   |
| Train/Return            | 1.3336018    |
| Train/V                 | 1.3108474    |
| Train/Value             | 1.3108474    |
| Train/control_penalty   | 0.7096005    |
| Train/policy_loss       | -0.041300878 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.027        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 621 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 620          |
| Time/Actor_Time         | 0.0825       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00288      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29003096   |
| Train/Action_magnitu... | 0.85670805   |
| Train/Action_magnitude  | 0.6858198    |
| Train/Action_max        | 0.33596164   |
| Train/Action_std        | 0.38114876   |
| Train/Entropy           | 0.31510466   |
| Train/Entropy_Loss      | -0.000315    |
| Train/Entropy_loss      | -0.000315    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6962467   |
| Train/Loss              | -0.05424603  |
| Train/PolicyClip        | 0.00880588   |
| Train/Policy_loss       | -0.060763527 |
| Train/Ratio             | 1.0033478    |
| Train/Return            | 1.3467854    |
| Train/V                 | 1.2998108    |
| Train/Value             | 1.2998108    |
| Train/control_penalty   | 0.68326      |
| Train/policy_loss       | -0.060763527 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03025      |
------------------------------------------

 ---------------- Iteration 622 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 621          |
| Time/Actor_Time         | 0.084        |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00272      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3056002    |
| Train/Action_magnitu... | 0.883497     |
| Train/Action_magnitude  | 0.7091474    |
| Train/Action_max        | 0.3206235    |
| Train/Action_std        | 0.3950581    |
| Train/Entropy           | 0.34150603   |
| Train/Entropy_Loss      | -0.000342    |
| Train/Entropy_loss      | -0.000342    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7460731   |
| Train/Loss              | -0.0318259   |
| Train/PolicyClip        | 0.009484411  |
| Train/Policy_loss       | -0.038569804 |
| Train/Ratio             | 0.99461967   |
| Train/Return            | 1.3130739    |
| Train/V                 | 1.289804     |
| Train/Value             | 1.289804     |
| Train/control_penalty   | 0.70854104   |
| Train/policy_loss       | -0.038569804 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 623 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 622          |
| Time/Actor_Time         | 0.0831       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00327      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31260848   |
| Train/Action_magnitu... | 0.9168005    |
| Train/Action_magnitude  | 0.73459136   |
| Train/Action_max        | 0.36082622   |
| Train/Action_std        | 0.39500427   |
| Train/Entropy           | 0.33478343   |
| Train/Entropy_Loss      | -0.000335    |
| Train/Entropy_loss      | -0.000335    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.72724926  |
| Train/Loss              | -0.0581628   |
| Train/PolicyClip        | 0.0073368005 |
| Train/Policy_loss       | -0.06498938  |
| Train/Ratio             | 1.004902     |
| Train/Return            | 1.4790083    |
| Train/V                 | 1.4293499    |
| Train/Value             | 1.4293499    |
| Train/control_penalty   | 0.7161364    |
| Train/policy_loss       | -0.06498938  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03025      |
------------------------------------------

 ---------------- Iteration 624 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 623          |
| Time/Actor_Time         | 0.0837       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0703       |
| Time/Buffer             | 0.00297      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30668283   |
| Train/Action_magnitu... | 0.8837289    |
| Train/Action_magnitude  | 0.71039796   |
| Train/Action_max        | 0.34362257   |
| Train/Action_std        | 0.37839895   |
| Train/Entropy           | 0.28297606   |
| Train/Entropy_Loss      | -0.000283    |
| Train/Entropy_loss      | -0.000283    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6599695   |
| Train/Loss              | -0.016570777 |
| Train/PolicyClip        | 0.014969944  |
| Train/Policy_loss       | -0.023270274 |
| Train/Ratio             | 0.98691046   |
| Train/Return            | 1.4424206    |
| Train/V                 | 1.439383     |
| Train/Value             | 1.439383     |
| Train/control_penalty   | 0.6982474    |
| Train/policy_loss       | -0.023270274 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02775      |
------------------------------------------

 ---------------- Iteration 625 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 624           |
| Time/Actor_Time         | 0.0838        |
| Time/B_Format_Time      | 0.0734        |
| Time/B_Original_Form... | 0.0732        |
| Time/Buffer             | 0.00278       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31387788    |
| Train/Action_magnitu... | 0.90661865    |
| Train/Action_magnitude  | 0.728034      |
| Train/Action_max        | 0.37396765    |
| Train/Action_std        | 0.3908395     |
| Train/Entropy           | 0.33801535    |
| Train/Entropy_Loss      | -0.000338     |
| Train/Entropy_loss      | -0.000338     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.73365575   |
| Train/Loss              | 0.0053423643  |
| Train/PolicyClip        | 0.0131349135  |
| Train/Policy_loss       | -0.0015694845 |
| Train/Ratio             | 1.0008743     |
| Train/Return            | 1.48742       |
| Train/V                 | 1.496227      |
| Train/Value             | 1.496227      |
| Train/control_penalty   | 0.72498643    |
| Train/policy_loss       | -0.0015694845 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02775       |
-------------------------------------------

 ---------------- Iteration 626 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 625         |
| Time/Actor_Time         | 0.085       |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3324872   |
| Train/Action_magnitu... | 0.89250463  |
| Train/Action_magnitude  | 0.7156839   |
| Train/Action_max        | 0.37372798  |
| Train/Action_std        | 0.37839386  |
| Train/Entropy           | 0.28243378  |
| Train/Entropy_Loss      | -0.000282   |
| Train/Entropy_loss      | -0.000282   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.65408814 |
| Train/Loss              | -0.02332732 |
| Train/PolicyClip        | 0.012643022 |
| Train/Policy_loss       | -0.03023263 |
| Train/Ratio             | 0.99448997  |
| Train/Return            | 1.6270342   |
| Train/V                 | 1.6139346   |
| Train/Value             | 1.6139346   |
| Train/control_penalty   | 0.7187743   |
| Train/policy_loss       | -0.03023263 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.035       |
-----------------------------------------

 ---------------- Iteration 627 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 626           |
| Time/Actor_Time         | 0.0842        |
| Time/B_Format_Time      | 0.071         |
| Time/B_Original_Form... | 0.0711        |
| Time/Buffer             | 0.00249       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.30931443    |
| Train/Action_magnitu... | 0.88601804    |
| Train/Action_magnitude  | 0.7154122     |
| Train/Action_max        | 0.37514925    |
| Train/Action_std        | 0.3877001     |
| Train/Entropy           | 0.3163028     |
| Train/Entropy_Loss      | -0.000316     |
| Train/Entropy_loss      | -0.000316     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6922215    |
| Train/Loss              | -0.0120339375 |
| Train/PolicyClip        | 0.0088573415  |
| Train/Policy_loss       | -0.01876755   |
| Train/Ratio             | 0.99110305    |
| Train/Return            | 1.3446616     |
| Train/V                 | 1.3374466     |
| Train/Value             | 1.3374466     |
| Train/control_penalty   | 0.70499164    |
| Train/policy_loss       | -0.01876755   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02575       |
-------------------------------------------

 ---------------- Iteration 628 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 627         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00308     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32343388  |
| Train/Action_magnitu... | 0.91717833  |
| Train/Action_magnitude  | 0.738548    |
| Train/Action_max        | 0.37239674  |
| Train/Action_std        | 0.4276288   |
| Train/Entropy           | 0.41591823  |
| Train/Entropy_Loss      | -0.000416   |
| Train/Entropy_loss      | -0.000416   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.78487915 |
| Train/Loss              | 0.022507759 |
| Train/PolicyClip        | 0.010554285 |
| Train/Policy_loss       | 0.015301842 |
| Train/Ratio             | 0.9764565   |
| Train/Return            | 1.3692182   |
| Train/V                 | 1.4007137   |
| Train/Value             | 1.4007137   |
| Train/control_penalty   | 0.7621835   |
| Train/policy_loss       | 0.015301842 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 629 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 628           |
| Time/Actor_Time         | 0.0854        |
| Time/B_Format_Time      | 0.0714        |
| Time/B_Original_Form... | 0.0723        |
| Time/Buffer             | 0.00249       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3153147     |
| Train/Action_magnitu... | 0.89523655    |
| Train/Action_magnitude  | 0.71752405    |
| Train/Action_max        | 0.3782982     |
| Train/Action_std        | 0.39980352    |
| Train/Entropy           | 0.3641141     |
| Train/Entropy_Loss      | -0.000364     |
| Train/Entropy_loss      | -0.000364     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.7900203    |
| Train/Loss              | 0.0044779973  |
| Train/PolicyClip        | 0.014233872   |
| Train/Policy_loss       | -0.0023814212 |
| Train/Ratio             | 0.9930961     |
| Train/Return            | 1.3899586     |
| Train/V                 | 1.4019716     |
| Train/Value             | 1.4019716     |
| Train/control_penalty   | 0.7223533     |
| Train/policy_loss       | -0.0023814212 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02425       |
-------------------------------------------

 ---------------- Iteration 630 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 629           |
| Time/Actor_Time         | 0.0848        |
| Time/B_Format_Time      | 0.0701        |
| Time/B_Original_Form... | 0.0694        |
| Time/Buffer             | 0.00545       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3108929     |
| Train/Action_magnitu... | 0.92389834    |
| Train/Action_magnitude  | 0.7411693     |
| Train/Action_max        | 0.36139446    |
| Train/Action_std        | 0.42956445    |
| Train/Entropy           | 0.42092338    |
| Train/Entropy_Loss      | -0.000421     |
| Train/Entropy_loss      | -0.000421     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.89917725   |
| Train/Loss              | 0.004868785   |
| Train/PolicyClip        | 0.008816442   |
| Train/Policy_loss       | -0.0021147595 |
| Train/Ratio             | 0.9981598     |
| Train/Return            | 1.2130597     |
| Train/V                 | 1.2199231     |
| Train/Value             | 1.2199231     |
| Train/control_penalty   | 0.7404468     |
| Train/policy_loss       | -0.0021147595 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02375       |
-------------------------------------------

 ---------------- Iteration 631 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 630          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00266      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32569996   |
| Train/Action_magnitu... | 0.923225     |
| Train/Action_magnitude  | 0.73720074   |
| Train/Action_max        | 0.3365113    |
| Train/Action_std        | 0.42243013   |
| Train/Entropy           | 0.4244608    |
| Train/Entropy_Loss      | -0.000424    |
| Train/Entropy_loss      | -0.000424    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8644136   |
| Train/Loss              | 0.04721      |
| Train/PolicyClip        | 0.0069880625 |
| Train/Policy_loss       | 0.040199764  |
| Train/Ratio             | 0.9812909    |
| Train/Return            | 1.077179     |
| Train/V                 | 1.1300516    |
| Train/Value             | 1.1300516    |
| Train/control_penalty   | 0.74346966   |
| Train/policy_loss       | 0.040199764  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 632 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 631          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00373      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31593546   |
| Train/Action_magnitu... | 0.8984826    |
| Train/Action_magnitude  | 0.7162479    |
| Train/Action_max        | 0.33738574   |
| Train/Action_std        | 0.4127466    |
| Train/Entropy           | 0.3784937    |
| Train/Entropy_Loss      | -0.000378    |
| Train/Entropy_loss      | -0.000378    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8272623   |
| Train/Loss              | -0.03363193  |
| Train/PolicyClip        | 0.011697736  |
| Train/Policy_loss       | -0.040530547 |
| Train/Ratio             | 0.9970138    |
| Train/Return            | 1.051936     |
| Train/V                 | 1.0215915    |
| Train/Value             | 1.0215915    |
| Train/control_penalty   | 0.72771114   |
| Train/policy_loss       | -0.040530547 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 633 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 632          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.0735       |
| Time/Buffer             | 0.00331      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32432216   |
| Train/Action_magnitu... | 0.93264663   |
| Train/Action_magnitude  | 0.7450998    |
| Train/Action_max        | 0.3823751    |
| Train/Action_std        | 0.40593457   |
| Train/Entropy           | 0.36284968   |
| Train/Entropy_Loss      | -0.000363    |
| Train/Entropy_loss      | -0.000363    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.80057794  |
| Train/Loss              | -0.005051707 |
| Train/PolicyClip        | 0.006892219  |
| Train/Policy_loss       | -0.011930963 |
| Train/Ratio             | 0.9952069    |
| Train/Return            | 1.1943982    |
| Train/V                 | 1.1905015    |
| Train/Value             | 1.1905015    |
| Train/control_penalty   | 0.7242106    |
| Train/policy_loss       | -0.011930963 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02575      |
------------------------------------------

 ---------------- Iteration 634 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 633          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00249      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31420898   |
| Train/Action_magnitu... | 0.9766109    |
| Train/Action_magnitude  | 0.77899486   |
| Train/Action_max        | 0.3621123    |
| Train/Action_std        | 0.45780808   |
| Train/Entropy           | 0.47922173   |
| Train/Entropy_Loss      | -0.000479    |
| Train/Entropy_loss      | -0.000479    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.98018616  |
| Train/Loss              | -0.01471501  |
| Train/PolicyClip        | 0.0071933395 |
| Train/Policy_loss       | -0.021939209 |
| Train/Ratio             | 1.0015134    |
| Train/Return            | 0.8540557    |
| Train/V                 | 0.8371513    |
| Train/Value             | 0.8371513    |
| Train/control_penalty   | 0.770342     |
| Train/policy_loss       | -0.021939209 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 635 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 634          |
| Time/Actor_Time         | 0.0837       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00259      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3183345    |
| Train/Action_magnitu... | 0.938566     |
| Train/Action_magnitude  | 0.74806213   |
| Train/Action_max        | 0.3761914    |
| Train/Action_std        | 0.4149826    |
| Train/Entropy           | 0.37236387   |
| Train/Entropy_Loss      | -0.000372    |
| Train/Entropy_loss      | -0.000372    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.79057384  |
| Train/Loss              | -0.07541202  |
| Train/PolicyClip        | 0.0065834057 |
| Train/Policy_loss       | -0.082514554 |
| Train/Ratio             | 1.0082871    |
| Train/Return            | 1.312244     |
| Train/V                 | 1.2396249    |
| Train/Value             | 1.2396249    |
| Train/control_penalty   | 0.7474898    |
| Train/policy_loss       | -0.082514554 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.031        |
------------------------------------------

 ---------------- Iteration 636 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 635         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0711      |
| Time/B_Original_Form... | 0.0701      |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3217409   |
| Train/Action_magnitu... | 0.8949269   |
| Train/Action_magnitude  | 0.7121294   |
| Train/Action_max        | 0.34355202  |
| Train/Action_std        | 0.38814965  |
| Train/Entropy           | 0.29688478  |
| Train/Entropy_Loss      | -0.000297   |
| Train/Entropy_loss      | -0.000297   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6168824  |
| Train/Loss              | 0.015585812 |
| Train/PolicyClip        | 0.013269366 |
| Train/Policy_loss       | 0.008641363 |
| Train/Ratio             | 0.99236065  |
| Train/Return            | 1.1847287   |
| Train/V                 | 1.2047071   |
| Train/Value             | 1.2047071   |
| Train/control_penalty   | 0.72413343  |
| Train/policy_loss       | 0.008641363 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 637 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 636          |
| Time/Actor_Time         | 0.0832       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00256      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3259757    |
| Train/Action_magnitu... | 0.97382575   |
| Train/Action_magnitude  | 0.7737793    |
| Train/Action_max        | 0.35409513   |
| Train/Action_std        | 0.43738616   |
| Train/Entropy           | 0.40017083   |
| Train/Entropy_Loss      | -0.0004      |
| Train/Entropy_loss      | -0.0004      |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8554756   |
| Train/Loss              | -0.030887896 |
| Train/PolicyClip        | 0.010010301  |
| Train/Policy_loss       | -0.038150452 |
| Train/Ratio             | 0.996304     |
| Train/Return            | 1.1862541    |
| Train/V                 | 1.1607144    |
| Train/Value             | 1.1607144    |
| Train/control_penalty   | 0.7662726    |
| Train/policy_loss       | -0.038150452 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 638 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 637          |
| Time/Actor_Time         | 0.0838       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00269      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3199294    |
| Train/Action_magnitu... | 0.9325626    |
| Train/Action_magnitude  | 0.7452845    |
| Train/Action_max        | 0.33800784   |
| Train/Action_std        | 0.4121145    |
| Train/Entropy           | 0.3549779    |
| Train/Entropy_Loss      | -0.000355    |
| Train/Entropy_loss      | -0.000355    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7580875   |
| Train/Loss              | 0.010185817  |
| Train/PolicyClip        | 0.014922562  |
| Train/Policy_loss       | 0.0031055594 |
| Train/Ratio             | 0.9978483    |
| Train/Return            | 1.1691052    |
| Train/V                 | 1.182582     |
| Train/Value             | 1.182582     |
| Train/control_penalty   | 0.7435236    |
| Train/policy_loss       | 0.0031055594 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 639 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 638          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00359      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3269206    |
| Train/Action_magnitu... | 0.9305238    |
| Train/Action_magnitude  | 0.74203664   |
| Train/Action_max        | 0.32137135   |
| Train/Action_std        | 0.3966925    |
| Train/Entropy           | 0.3096493    |
| Train/Entropy_Loss      | -0.00031     |
| Train/Entropy_loss      | -0.00031     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.73950416  |
| Train/Loss              | -0.08101775  |
| Train/PolicyClip        | 0.0070456113 |
| Train/Policy_loss       | -0.088160336 |
| Train/Ratio             | 1.0093787    |
| Train/Return            | 1.2409798    |
| Train/V                 | 1.1621106    |
| Train/Value             | 1.1621106    |
| Train/control_penalty   | 0.7452235    |
| Train/policy_loss       | -0.088160336 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0315       |
------------------------------------------

 ---------------- Iteration 640 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 639          |
| Time/Actor_Time         | 0.0826       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00301      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3108742    |
| Train/Action_magnitu... | 0.9177112    |
| Train/Action_magnitude  | 0.7314571    |
| Train/Action_max        | 0.32175034   |
| Train/Action_std        | 0.40269855   |
| Train/Entropy           | 0.343627     |
| Train/Entropy_Loss      | -0.000344    |
| Train/Entropy_loss      | -0.000344    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.73713136  |
| Train/Loss              | 0.009868358  |
| Train/PolicyClip        | 0.0062720706 |
| Train/Policy_loss       | 0.0029680291 |
| Train/Ratio             | 0.98808664   |
| Train/Return            | 1.153194     |
| Train/V                 | 1.1687409    |
| Train/Value             | 1.1687409    |
| Train/control_penalty   | 0.72439563   |
| Train/policy_loss       | 0.0029680291 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 641 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 640         |
| Time/Actor_Time         | 0.0825      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3175153   |
| Train/Action_magnitu... | 0.9421395   |
| Train/Action_magnitude  | 0.7529917   |
| Train/Action_max        | 0.3609581   |
| Train/Action_std        | 0.42139146  |
| Train/Entropy           | 0.37527594  |
| Train/Entropy_Loss      | -0.000375   |
| Train/Entropy_loss      | -0.000375   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.78807414 |
| Train/Loss              | 0.037467167 |
| Train/PolicyClip        | 0.008221144 |
| Train/Policy_loss       | 0.030350829 |
| Train/Ratio             | 0.9806183   |
| Train/Return            | 1.0028945   |
| Train/V                 | 1.0461142   |
| Train/Value             | 1.0461142   |
| Train/control_penalty   | 0.74916166  |
| Train/policy_loss       | 0.030350829 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 642 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 641          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00238      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3334549    |
| Train/Action_magnitu... | 0.9543683    |
| Train/Action_magnitude  | 0.7623211    |
| Train/Action_max        | 0.31501463   |
| Train/Action_std        | 0.44111237   |
| Train/Entropy           | 0.44204956   |
| Train/Entropy_Loss      | -0.000442    |
| Train/Entropy_loss      | -0.000442    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8392107   |
| Train/Loss              | -0.01784963  |
| Train/PolicyClip        | 0.0073598563 |
| Train/Policy_loss       | -0.025078945 |
| Train/Ratio             | 0.9873151    |
| Train/Return            | 1.2657465    |
| Train/V                 | 1.2527004    |
| Train/Value             | 1.2527004    |
| Train/control_penalty   | 0.76713663   |
| Train/policy_loss       | -0.025078945 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 643 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 642         |
| Time/Actor_Time         | 0.0829      |
| Time/B_Format_Time      | 0.0724      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00296     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3083147   |
| Train/Action_magnitu... | 0.8809413   |
| Train/Action_magnitude  | 0.70559996  |
| Train/Action_max        | 0.3437345   |
| Train/Action_std        | 0.40096703  |
| Train/Entropy           | 0.33147472  |
| Train/Entropy_Loss      | -0.000331   |
| Train/Entropy_loss      | -0.000331   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.67519325 |
| Train/Loss              | 0.033689804 |
| Train/PolicyClip        | 0.010694982 |
| Train/Policy_loss       | 0.02695749  |
| Train/Ratio             | 0.9813847   |
| Train/Return            | 1.1407465   |
| Train/V                 | 1.1805977   |
| Train/Value             | 1.1805977   |
| Train/control_penalty   | 0.706379    |
| Train/policy_loss       | 0.02695749  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 644 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 643         |
| Time/Actor_Time         | 0.0852      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.0695      |
| Time/Buffer             | 0.00429     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31454003  |
| Train/Action_magnitu... | 0.9175833   |
| Train/Action_magnitude  | 0.7322472   |
| Train/Action_max        | 0.32210624  |
| Train/Action_std        | 0.4177422   |
| Train/Entropy           | 0.37640467  |
| Train/Entropy_Loss      | -0.000376   |
| Train/Entropy_loss      | -0.000376   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.79455435 |
| Train/Loss              | 0.03353553  |
| Train/PolicyClip        | 0.010018609 |
| Train/Policy_loss       | 0.026498573 |
| Train/Ratio             | 0.98224723  |
| Train/Return            | 0.93916464  |
| Train/V                 | 0.9799322   |
| Train/Value             | 0.9799322   |
| Train/control_penalty   | 0.741336    |
| Train/policy_loss       | 0.026498573 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0125      |
-----------------------------------------

 ---------------- Iteration 645 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 644          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00295      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31556052   |
| Train/Action_magnitu... | 0.92366004   |
| Train/Action_magnitude  | 0.7360197    |
| Train/Action_max        | 0.32486755   |
| Train/Action_std        | 0.42568544   |
| Train/Entropy           | 0.41201746   |
| Train/Entropy_Loss      | -0.000412    |
| Train/Entropy_loss      | -0.000412    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8313479   |
| Train/Loss              | 0.011135202  |
| Train/PolicyClip        | 0.009623886  |
| Train/Policy_loss       | 0.0039450163 |
| Train/Ratio             | 0.9855709    |
| Train/Return            | 0.9973935    |
| Train/V                 | 1.0146561    |
| Train/Value             | 1.0146561    |
| Train/control_penalty   | 0.76022035   |
| Train/policy_loss       | 0.0039450163 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------

 ---------------- Iteration 646 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 645          |
| Time/Actor_Time         | 0.0846       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00296      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32063767   |
| Train/Action_magnitu... | 0.95216167   |
| Train/Action_magnitude  | 0.75541365   |
| Train/Action_max        | 0.30873248   |
| Train/Action_std        | 0.42495006   |
| Train/Entropy           | 0.40216437   |
| Train/Entropy_Loss      | -0.000402    |
| Train/Entropy_loss      | -0.000402    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8549192   |
| Train/Loss              | -0.054416426 |
| Train/PolicyClip        | 0.009669709  |
| Train/Policy_loss       | -0.06147566  |
| Train/Ratio             | 1.0023383    |
| Train/Return            | 1.0564277    |
| Train/V                 | 1.0075554    |
| Train/Value             | 1.0075554    |
| Train/control_penalty   | 0.74614      |
| Train/policy_loss       | -0.06147566  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 647 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 646          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3358387    |
| Train/Action_magnitu... | 0.94869703   |
| Train/Action_magnitude  | 0.7542073    |
| Train/Action_max        | 0.35373098   |
| Train/Action_std        | 0.41448206   |
| Train/Entropy           | 0.37078246   |
| Train/Entropy_Loss      | -0.000371    |
| Train/Entropy_loss      | -0.000371    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7768513   |
| Train/Loss              | -0.027053494 |
| Train/PolicyClip        | 0.0076540583 |
| Train/Policy_loss       | -0.03433552  |
| Train/Ratio             | 0.9946442    |
| Train/Return            | 1.339726     |
| Train/V                 | 1.316792     |
| Train/Value             | 1.316792     |
| Train/control_penalty   | 0.7652809    |
| Train/policy_loss       | -0.03433552  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 648 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 647           |
| Time/Actor_Time         | 0.0843        |
| Time/B_Format_Time      | 0.0706        |
| Time/B_Original_Form... | 0.0699        |
| Time/Buffer             | 0.00596       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.32139105    |
| Train/Action_magnitu... | 0.903976      |
| Train/Action_magnitude  | 0.7160842     |
| Train/Action_max        | 0.31008655    |
| Train/Action_std        | 0.4090711     |
| Train/Entropy           | 0.3600565     |
| Train/Entropy_Loss      | -0.00036      |
| Train/Entropy_loss      | -0.00036      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6837684    |
| Train/Loss              | 0.0046536904  |
| Train/PolicyClip        | 0.008947484   |
| Train/Policy_loss       | -0.0022925204 |
| Train/Ratio             | 0.99372566    |
| Train/Return            | 1.2131366     |
| Train/V                 | 1.2242378     |
| Train/Value             | 1.2242378     |
| Train/control_penalty   | 0.73062676    |
| Train/policy_loss       | -0.0022925204 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02075       |
-------------------------------------------

 ---------------- Iteration 649 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 648          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0724       |
| Time/Buffer             | 0.00257      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3090485    |
| Train/Action_magnitu... | 0.92734045   |
| Train/Action_magnitude  | 0.742741     |
| Train/Action_max        | 0.36179912   |
| Train/Action_std        | 0.41944793   |
| Train/Entropy           | 0.37180534   |
| Train/Entropy_Loss      | -0.000372    |
| Train/Entropy_loss      | -0.000372    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7495846   |
| Train/Loss              | 0.014209592  |
| Train/PolicyClip        | 0.0077376002 |
| Train/Policy_loss       | 0.007200368  |
| Train/Ratio             | 0.9921642    |
| Train/Return            | 0.9524098    |
| Train/V                 | 0.97197837   |
| Train/Value             | 0.97197837   |
| Train/control_penalty   | 0.7381029    |
| Train/policy_loss       | 0.007200368  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0165       |
------------------------------------------

 ---------------- Iteration 650 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 649          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00246      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32546625   |
| Train/Action_magnitu... | 0.94417053   |
| Train/Action_magnitude  | 0.749412     |
| Train/Action_max        | 0.33147734   |
| Train/Action_std        | 0.43540257   |
| Train/Entropy           | 0.45110327   |
| Train/Entropy_Loss      | -0.000451    |
| Train/Entropy_loss      | -0.000451    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9282187   |
| Train/Loss              | -0.034708023 |
| Train/PolicyClip        | 0.009568857  |
| Train/Policy_loss       | -0.041776557 |
| Train/Ratio             | 0.9886689    |
| Train/Return            | 0.95916677   |
| Train/V                 | 0.930469     |
| Train/Value             | 0.930469     |
| Train/control_penalty   | 0.7519638    |
| Train/policy_loss       | -0.041776557 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 651 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 650           |
| Time/Actor_Time         | 0.0855        |
| Time/B_Format_Time      | 0.0721        |
| Time/B_Original_Form... | 0.0724        |
| Time/Buffer             | 0.00298       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3265929     |
| Train/Action_magnitu... | 0.9204217     |
| Train/Action_magnitude  | 0.7310011     |
| Train/Action_max        | 0.32645527    |
| Train/Action_std        | 0.40387163    |
| Train/Entropy           | 0.35552263    |
| Train/Entropy_Loss      | -0.000356     |
| Train/Entropy_loss      | -0.000356     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.718732     |
| Train/Loss              | -0.0051072063 |
| Train/PolicyClip        | 0.010276114   |
| Train/Policy_loss       | -0.012195751  |
| Train/Ratio             | 0.9889574     |
| Train/Return            | 1.1451876     |
| Train/V                 | 1.1430098     |
| Train/Value             | 1.1430098     |
| Train/control_penalty   | 0.7444068     |
| Train/policy_loss       | -0.012195751  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02225       |
-------------------------------------------

 ---------------- Iteration 652 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 651         |
| Time/Actor_Time         | 0.0865      |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0709      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34749907  |
| Train/Action_magnitu... | 0.9321314   |
| Train/Action_magnitude  | 0.74227023  |
| Train/Action_max        | 0.28492537  |
| Train/Action_std        | 0.37647894  |
| Train/Entropy           | 0.29204282  |
| Train/Entropy_Loss      | -0.000292   |
| Train/Entropy_loss      | -0.000292   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.69798094 |
| Train/Loss              | -0.05510649 |
| Train/PolicyClip        | 0.010101245 |
| Train/Policy_loss       | -0.06215255 |
| Train/Ratio             | 0.99943787  |
| Train/Return            | 1.1022928   |
| Train/V                 | 1.0538902   |
| Train/Value             | 1.0538902   |
| Train/control_penalty   | 0.7338101   |
| Train/policy_loss       | -0.06215255 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 653 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 652          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0729       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00318      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32810023   |
| Train/Action_magnitu... | 0.9195466    |
| Train/Action_magnitude  | 0.7249925    |
| Train/Action_max        | 0.30207533   |
| Train/Action_std        | 0.3733065    |
| Train/Entropy           | 0.27422956   |
| Train/Entropy_Loss      | -0.000274    |
| Train/Entropy_loss      | -0.000274    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.58718807  |
| Train/Loss              | -0.06767598  |
| Train/PolicyClip        | 0.010063103  |
| Train/Policy_loss       | -0.074597694 |
| Train/Ratio             | 1.0047579    |
| Train/Return            | 1.2157423    |
| Train/V                 | 1.1543552    |
| Train/Value             | 1.1543552    |
| Train/control_penalty   | 0.7195942    |
| Train/policy_loss       | -0.074597694 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------

 ---------------- Iteration 654 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 653          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0705       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00328      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34897065   |
| Train/Action_magnitu... | 0.93440366   |
| Train/Action_magnitude  | 0.73843324   |
| Train/Action_max        | 0.31587505   |
| Train/Action_std        | 0.37165934   |
| Train/Entropy           | 0.27676684   |
| Train/Entropy_Loss      | -0.000277    |
| Train/Entropy_loss      | -0.000277    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5924775   |
| Train/Loss              | -0.09975511  |
| Train/PolicyClip        | 0.0072194864 |
| Train/Policy_loss       | -0.106840774 |
| Train/Ratio             | 1.0002891    |
| Train/Return            | 1.2269582    |
| Train/V                 | 1.1329868    |
| Train/Value             | 1.1329868    |
| Train/control_penalty   | 0.7362436    |
| Train/policy_loss       | -0.106840774 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 655 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 654          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.074        |
| Time/Buffer             | 0.00303      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.35710284   |
| Train/Action_magnitu... | 0.9771378    |
| Train/Action_magnitude  | 0.77537024   |
| Train/Action_max        | 0.2819823    |
| Train/Action_std        | 0.38353315   |
| Train/Entropy           | 0.30673712   |
| Train/Entropy_Loss      | -0.000307    |
| Train/Entropy_loss      | -0.000307    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7593315   |
| Train/Loss              | -0.041864533 |
| Train/PolicyClip        | 0.012199031  |
| Train/Policy_loss       | -0.04911856  |
| Train/Ratio             | 0.9999556    |
| Train/Return            | 1.313114     |
| Train/V                 | 1.2755455    |
| Train/Value             | 1.2755455    |
| Train/control_penalty   | 0.7560762    |
| Train/policy_loss       | -0.04911856  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02875      |
------------------------------------------

 ---------------- Iteration 656 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 655         |
| Time/Actor_Time         | 0.0848      |
| Time/B_Format_Time      | 0.0732      |
| Time/B_Original_Form... | 0.0715      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.32964763  |
| Train/Action_magnitu... | 0.8710665   |
| Train/Action_magnitude  | 0.6915414   |
| Train/Action_max        | 0.28474942  |
| Train/Action_std        | 0.3412926   |
| Train/Entropy           | 0.20468697  |
| Train/Entropy_Loss      | -0.000205   |
| Train/Entropy_loss      | -0.000205   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.48515424 |
| Train/Loss              | -0.08813809 |
| Train/PolicyClip        | 0.00918073  |
| Train/Policy_loss       | -0.09480882 |
| Train/Ratio             | 1.0113944   |
| Train/Return            | 1.2855254   |
| Train/V                 | 1.2029053   |
| Train/Value             | 1.2029053   |
| Train/control_penalty   | 0.6875417   |
| Train/policy_loss       | -0.09480882 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 657 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 656         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00281     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33737433  |
| Train/Action_magnitu... | 0.89444923  |
| Train/Action_magnitude  | 0.70863354  |
| Train/Action_max        | 0.28255177  |
| Train/Action_std        | 0.3514931   |
| Train/Entropy           | 0.21234915  |
| Train/Entropy_Loss      | -0.000212   |
| Train/Entropy_loss      | -0.000212   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5096398  |
| Train/Loss              | -0.07876388 |
| Train/PolicyClip        | 0.008580082 |
| Train/Policy_loss       | -0.0856632  |
| Train/Ratio             | 1.0056155   |
| Train/Return            | 1.4338022   |
| Train/V                 | 1.3594477   |
| Train/Value             | 1.3594477   |
| Train/control_penalty   | 0.71116656  |
| Train/policy_loss       | -0.0856632  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03175     |
-----------------------------------------

 ---------------- Iteration 658 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 657          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3416251    |
| Train/Action_magnitu... | 0.9150362    |
| Train/Action_magnitude  | 0.7206901    |
| Train/Action_max        | 0.26307467   |
| Train/Action_std        | 0.36053422   |
| Train/Entropy           | 0.25393242   |
| Train/Entropy_Loss      | -0.000254    |
| Train/Entropy_loss      | -0.000254    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5668998   |
| Train/Loss              | -0.00508263  |
| Train/PolicyClip        | 0.010840352  |
| Train/Policy_loss       | -0.011966923 |
| Train/Ratio             | 0.99728423   |
| Train/Return            | 1.4159262    |
| Train/V                 | 1.4156934    |
| Train/Value             | 1.4156934    |
| Train/control_penalty   | 0.7138226    |
| Train/policy_loss       | -0.011966923 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02725      |
------------------------------------------

 ---------------- Iteration 659 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 658          |
| Time/Actor_Time         | 0.0864       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00336      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33622542   |
| Train/Action_magnitu... | 0.8684248    |
| Train/Action_magnitude  | 0.68968415   |
| Train/Action_max        | 0.28144878   |
| Train/Action_std        | 0.35404208   |
| Train/Entropy           | 0.22920273   |
| Train/Entropy_Loss      | -0.000229    |
| Train/Entropy_loss      | -0.000229    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4665903   |
| Train/Loss              | -0.008871414 |
| Train/PolicyClip        | 0.015356036  |
| Train/Policy_loss       | -0.015739566 |
| Train/Ratio             | 0.99402595   |
| Train/Return            | 1.4151742    |
| Train/V                 | 1.4140425    |
| Train/Value             | 1.4140425    |
| Train/control_penalty   | 0.70973545   |
| Train/policy_loss       | -0.015739566 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.027        |
------------------------------------------

 ---------------- Iteration 660 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 659         |
| Time/Actor_Time         | 0.0848      |
| Time/B_Format_Time      | 0.0735      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34664392  |
| Train/Action_magnitu... | 0.8963104   |
| Train/Action_magnitude  | 0.7205716   |
| Train/Action_max        | 0.30638215  |
| Train/Action_std        | 0.34705788  |
| Train/Entropy           | 0.21616583  |
| Train/Entropy_Loss      | -0.000216   |
| Train/Entropy_loss      | -0.000216   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.44279084 |
| Train/Loss              | 0.055022687 |
| Train/PolicyClip        | 0.008983179 |
| Train/Policy_loss       | 0.04797736  |
| Train/Ratio             | 0.98796093  |
| Train/Return            | 1.311215    |
| Train/V                 | 1.3720361   |
| Train/Value             | 1.3720361   |
| Train/control_penalty   | 0.72614944  |
| Train/policy_loss       | 0.04797736  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 661 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 660          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0731       |
| Time/Buffer             | 0.00296      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33508322   |
| Train/Action_magnitu... | 0.8896301    |
| Train/Action_magnitude  | 0.7087454    |
| Train/Action_max        | 0.3524786    |
| Train/Action_std        | 0.35834813   |
| Train/Entropy           | 0.23962389   |
| Train/Entropy_Loss      | -0.00024     |
| Train/Entropy_loss      | -0.00024     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5056764   |
| Train/Loss              | 0.0416182    |
| Train/PolicyClip        | 0.0116049815 |
| Train/Policy_loss       | 0.034693252  |
| Train/Ratio             | 0.9864156    |
| Train/Return            | 1.4618316    |
| Train/V                 | 1.5099406    |
| Train/Value             | 1.5099406    |
| Train/control_penalty   | 0.71645725   |
| Train/policy_loss       | 0.034693252  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------

 ---------------- Iteration 662 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 661         |
| Time/Actor_Time         | 0.0865      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00279     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3241045   |
| Train/Action_magnitu... | 0.89561856  |
| Train/Action_magnitude  | 0.7084655   |
| Train/Action_max        | 0.2695023   |
| Train/Action_std        | 0.35447913  |
| Train/Entropy           | 0.23235409  |
| Train/Entropy_Loss      | -0.000232   |
| Train/Entropy_loss      | -0.000232   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.47813356 |
| Train/Loss              | 0.041083016 |
| Train/PolicyClip        | 0.008728791 |
| Train/Policy_loss       | 0.034369286 |
| Train/Ratio             | 0.9814674   |
| Train/Return            | 1.2714976   |
| Train/V                 | 1.3166002   |
| Train/Value             | 1.3166002   |
| Train/control_penalty   | 0.6946083   |
| Train/policy_loss       | 0.034369286 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 663 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 662         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00351     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.31931245  |
| Train/Action_magnitu... | 0.8908487   |
| Train/Action_magnitude  | 0.7075011   |
| Train/Action_max        | 0.31172922  |
| Train/Action_std        | 0.36788663  |
| Train/Entropy           | 0.2770555   |
| Train/Entropy_Loss      | -0.000277   |
| Train/Entropy_loss      | -0.000277   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.56787914 |
| Train/Loss              | 0.017938694 |
| Train/PolicyClip        | 0.010532889 |
| Train/Policy_loss       | 0.011186235 |
| Train/Ratio             | 0.9827883   |
| Train/Return            | 1.243554    |
| Train/V                 | 1.2705191   |
| Train/Value             | 1.2705191   |
| Train/control_penalty   | 0.7029515   |
| Train/policy_loss       | 0.011186235 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 664 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 663           |
| Time/Actor_Time         | 0.0851        |
| Time/B_Format_Time      | 0.0709        |
| Time/B_Original_Form... | 0.0705        |
| Time/Buffer             | 0.00225       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.32740405    |
| Train/Action_magnitu... | 0.90744996    |
| Train/Action_magnitude  | 0.7244176     |
| Train/Action_max        | 0.3517759     |
| Train/Action_std        | 0.38074085    |
| Train/Entropy           | 0.29559112    |
| Train/Entropy_Loss      | -0.000296     |
| Train/Entropy_loss      | -0.000296     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.70819944   |
| Train/Loss              | -0.0003966107 |
| Train/PolicyClip        | 0.012053876   |
| Train/Policy_loss       | -0.007413107  |
| Train/Ratio             | 0.9859148     |
| Train/Return            | 1.2365224     |
| Train/V                 | 1.2451136     |
| Train/Value             | 1.2451136     |
| Train/control_penalty   | 0.73120874    |
| Train/policy_loss       | -0.007413107  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.021         |
-------------------------------------------

 ---------------- Iteration 665 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 664          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.00431      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31294867   |
| Train/Action_magnitu... | 0.8817781    |
| Train/Action_magnitude  | 0.7047842    |
| Train/Action_max        | 0.36314046   |
| Train/Action_std        | 0.36688614   |
| Train/Entropy           | 0.2516067    |
| Train/Entropy_Loss      | -0.000252    |
| Train/Entropy_loss      | -0.000252    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5243571   |
| Train/Loss              | 0.002330833  |
| Train/PolicyClip        | 0.007059675  |
| Train/Policy_loss       | -0.004514405 |
| Train/Ratio             | 0.9861417    |
| Train/Return            | 1.206139     |
| Train/V                 | 1.2141726    |
| Train/Value             | 1.2141726    |
| Train/control_penalty   | 0.7096845    |
| Train/policy_loss       | -0.004514405 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 666 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 665          |
| Time/Actor_Time         | 0.0828       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.31027272   |
| Train/Action_magnitu... | 0.8595282    |
| Train/Action_magnitude  | 0.6860968    |
| Train/Action_max        | 0.3454853    |
| Train/Action_std        | 0.34518462   |
| Train/Entropy           | 0.20308673   |
| Train/Entropy_Loss      | -0.000203    |
| Train/Entropy_loss      | -0.000203    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4454058   |
| Train/Loss              | -0.015574425 |
| Train/PolicyClip        | 0.0085679265 |
| Train/Policy_loss       | -0.022142258 |
| Train/Ratio             | 0.9946073    |
| Train/Return            | 1.1750742    |
| Train/V                 | 1.1650804    |
| Train/Value             | 1.1650804    |
| Train/control_penalty   | 0.6770919    |
| Train/policy_loss       | -0.022142258 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 667 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 666         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.0703      |
| Time/Buffer             | 0.00284     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30425528  |
| Train/Action_magnitu... | 0.85133797  |
| Train/Action_magnitude  | 0.67771226  |
| Train/Action_max        | 0.30288896  |
| Train/Action_std        | 0.36344975  |
| Train/Entropy           | 0.26224434  |
| Train/Entropy_Loss      | -0.000262   |
| Train/Entropy_loss      | -0.000262   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.4856512  |
| Train/Loss              | 0.015158056 |
| Train/PolicyClip        | 0.013845295 |
| Train/Policy_loss       | 0.008601157 |
| Train/Ratio             | 0.98187685  |
| Train/Return            | 1.0953739   |
| Train/V                 | 1.120398    |
| Train/Value             | 1.120398    |
| Train/control_penalty   | 0.6819144   |
| Train/policy_loss       | 0.008601157 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 668 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 667          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00292      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3340413    |
| Train/Action_magnitu... | 0.88310516   |
| Train/Action_magnitude  | 0.7122537    |
| Train/Action_max        | 0.39574313   |
| Train/Action_std        | 0.3600094    |
| Train/Entropy           | 0.2677144    |
| Train/Entropy_Loss      | -0.000268    |
| Train/Entropy_loss      | -0.000268    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5426244   |
| Train/Loss              | -0.019927928 |
| Train/PolicyClip        | 0.009624562  |
| Train/Policy_loss       | -0.026705468 |
| Train/Ratio             | 0.9906628    |
| Train/Return            | 1.0543312    |
| Train/V                 | 1.0396707    |
| Train/Value             | 1.0396707    |
| Train/control_penalty   | 0.7045255    |
| Train/policy_loss       | -0.026705468 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 669 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 668          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31647712   |
| Train/Action_magnitu... | 0.87526023   |
| Train/Action_magnitude  | 0.6972057    |
| Train/Action_max        | 0.31776378   |
| Train/Action_std        | 0.3546212    |
| Train/Entropy           | 0.21813242   |
| Train/Entropy_Loss      | -0.000218    |
| Train/Entropy_loss      | -0.000218    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.52141947  |
| Train/Loss              | -0.025849257 |
| Train/PolicyClip        | 0.010059855  |
| Train/Policy_loss       | -0.032649644 |
| Train/Ratio             | 1.0095549    |
| Train/Return            | 1.1559516    |
| Train/V                 | 1.1364715    |
| Train/Value             | 1.1364715    |
| Train/control_penalty   | 0.70185184   |
| Train/policy_loss       | -0.032649644 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 670 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 669          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00253      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31526303   |
| Train/Action_magnitu... | 0.8739382    |
| Train/Action_magnitude  | 0.7015762    |
| Train/Action_max        | 0.34202743   |
| Train/Action_std        | 0.33998817   |
| Train/Entropy           | 0.16639523   |
| Train/Entropy_Loss      | -0.000166    |
| Train/Entropy_loss      | -0.000166    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.44949347  |
| Train/Loss              | -0.109278016 |
| Train/PolicyClip        | 0.012286831  |
| Train/Policy_loss       | -0.116014585 |
| Train/Ratio             | 1.009608     |
| Train/Return            | 1.2132277    |
| Train/V                 | 1.1147141    |
| Train/Value             | 1.1147141    |
| Train/control_penalty   | 0.6902965    |
| Train/policy_loss       | -0.116014585 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0295       |
------------------------------------------

 ---------------- Iteration 671 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 670           |
| Time/Actor_Time         | 0.0869        |
| Time/B_Format_Time      | 0.0719        |
| Time/B_Original_Form... | 0.0715        |
| Time/Buffer             | 0.00263       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31336462    |
| Train/Action_magnitu... | 0.8851832     |
| Train/Action_magnitude  | 0.70635104    |
| Train/Action_max        | 0.32242417    |
| Train/Action_std        | 0.39179295    |
| Train/Entropy           | 0.30139697    |
| Train/Entropy_Loss      | -0.000301     |
| Train/Entropy_loss      | -0.000301     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6639332    |
| Train/Loss              | -0.0013974756 |
| Train/PolicyClip        | 0.014045545   |
| Train/Policy_loss       | -0.008168186  |
| Train/Ratio             | 0.99421144    |
| Train/Return            | 1.0786144     |
| Train/V                 | 1.084633      |
| Train/Value             | 1.084633      |
| Train/control_penalty   | 0.7072108     |
| Train/policy_loss       | -0.008168186  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01775       |
-------------------------------------------

 ---------------- Iteration 672 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 671         |
| Time/Actor_Time         | 0.0857      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31925198  |
| Train/Action_magnitu... | 0.8844579   |
| Train/Action_magnitude  | 0.70447975  |
| Train/Action_max        | 0.2921983   |
| Train/Action_std        | 0.34549502  |
| Train/Entropy           | 0.19085334  |
| Train/Entropy_Loss      | -0.000191   |
| Train/Entropy_loss      | -0.000191   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.46494716 |
| Train/Loss              | -0.03574067 |
| Train/PolicyClip        | 0.009560104 |
| Train/Policy_loss       | -0.04239265 |
| Train/Ratio             | 0.9975932   |
| Train/Return            | 1.2274828   |
| Train/V                 | 1.1988246   |
| Train/Value             | 1.1988246   |
| Train/control_penalty   | 0.68428326  |
| Train/policy_loss       | -0.04239265 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 673 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 672         |
| Time/Actor_Time         | 0.0853      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.28607202  |
| Train/Action_magnitu... | 0.8000918   |
| Train/Action_magnitude  | 0.63930994  |
| Train/Action_max        | 0.33657473  |
| Train/Action_std        | 0.32732677  |
| Train/Entropy           | 0.13171491  |
| Train/Entropy_Loss      | -0.000132   |
| Train/Entropy_loss      | -0.000132   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.28035805 |
| Train/Loss              | 0.012529004 |
| Train/PolicyClip        | 0.011255412 |
| Train/Policy_loss       | 0.006277127 |
| Train/Ratio             | 0.98543924  |
| Train/Return            | 1.3505722   |
| Train/V                 | 1.3684247   |
| Train/Value             | 1.3684247   |
| Train/control_penalty   | 0.6383592   |
| Train/policy_loss       | 0.006277127 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 674 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 673           |
| Time/Actor_Time         | 0.0833        |
| Time/B_Format_Time      | 0.072         |
| Time/B_Original_Form... | 0.0719        |
| Time/Buffer             | 0.00312       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31281       |
| Train/Action_magnitu... | 0.8524377     |
| Train/Action_magnitude  | 0.6799492     |
| Train/Action_max        | 0.30875042    |
| Train/Action_std        | 0.34622973    |
| Train/Entropy           | 0.16055137    |
| Train/Entropy_Loss      | -0.000161     |
| Train/Entropy_loss      | -0.000161     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.36152005   |
| Train/Loss              | -0.0050311033 |
| Train/PolicyClip        | 0.018814296   |
| Train/Policy_loss       | -0.0117957005 |
| Train/Ratio             | 1.001003      |
| Train/Return            | 1.4165686     |
| Train/V                 | 1.4209019     |
| Train/Value             | 1.4209019     |
| Train/control_penalty   | 0.6925149     |
| Train/policy_loss       | -0.0117957005 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.03075       |
-------------------------------------------

 ---------------- Iteration 675 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 674          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00313      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.2995065    |
| Train/Action_magnitu... | 0.7940972    |
| Train/Action_magnitude  | 0.63005644   |
| Train/Action_max        | 0.27653724   |
| Train/Action_std        | 0.31184584   |
| Train/Entropy           | 0.08980378   |
| Train/Entropy_Loss      | -8.98e-05    |
| Train/Entropy_loss      | -8.98e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.23070544  |
| Train/Loss              | -0.004248519 |
| Train/PolicyClip        | 0.010536503  |
| Train/Policy_loss       | -0.010484432 |
| Train/Ratio             | 0.983386     |
| Train/Return            | 1.4227611    |
| Train/V                 | 1.4216288    |
| Train/Value             | 1.4216288    |
| Train/control_penalty   | 0.6325717    |
| Train/policy_loss       | -0.010484432 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0295       |
------------------------------------------

 ---------------- Iteration 676 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 675          |
| Time/Actor_Time         | 0.083        |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00292      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33266532   |
| Train/Action_magnitu... | 0.8604692    |
| Train/Action_magnitude  | 0.6903721    |
| Train/Action_max        | 0.31421876   |
| Train/Action_std        | 0.3536842    |
| Train/Entropy           | 0.15646185   |
| Train/Entropy_Loss      | -0.000156    |
| Train/Entropy_loss      | -0.000156    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.31809452  |
| Train/Loss              | -0.04521306  |
| Train/PolicyClip        | 0.011679588  |
| Train/Policy_loss       | -0.052193217 |
| Train/Ratio             | 0.99979943   |
| Train/Return            | 1.5731347    |
| Train/V                 | 1.5375847    |
| Train/Value             | 1.5375847    |
| Train/control_penalty   | 0.7136621    |
| Train/policy_loss       | -0.052193217 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.034        |
------------------------------------------

 ---------------- Iteration 677 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 676          |
| Time/Actor_Time         | 0.0844       |
| Time/B_Format_Time      | 0.0729       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00327      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32698986   |
| Train/Action_magnitu... | 0.8541604    |
| Train/Action_magnitude  | 0.681349     |
| Train/Action_max        | 0.27524891   |
| Train/Action_std        | 0.31306198   |
| Train/Entropy           | 0.070794284  |
| Train/Entropy_Loss      | -7.08e-05    |
| Train/Entropy_loss      | -7.08e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.24345854  |
| Train/Loss              | -0.071922205 |
| Train/PolicyClip        | 0.010038271  |
| Train/Policy_loss       | -0.078596376 |
| Train/Ratio             | 0.9939693    |
| Train/Return            | 1.6787527    |
| Train/V                 | 1.6146238    |
| Train/Value             | 1.6146238    |
| Train/control_penalty   | 0.67449695   |
| Train/policy_loss       | -0.078596376 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03975      |
------------------------------------------

 ---------------- Iteration 678 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 677         |
| Time/Actor_Time         | 0.0861      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0711      |
| Time/Buffer             | 0.0034      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3161854   |
| Train/Action_magnitu... | 0.8283567   |
| Train/Action_magnitude  | 0.6593616   |
| Train/Action_max        | 0.29852128  |
| Train/Action_std        | 0.32654157  |
| Train/Entropy           | 0.12946966  |
| Train/Entropy_Loss      | -0.000129   |
| Train/Entropy_loss      | -0.000129   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.3120727  |
| Train/Loss              | 0.026320364 |
| Train/PolicyClip        | 0.017972149 |
| Train/Policy_loss       | 0.019816872 |
| Train/Ratio             | 0.99396306  |
| Train/Return            | 1.5216551   |
| Train/V                 | 1.5589238   |
| Train/Value             | 1.5589238   |
| Train/control_penalty   | 0.66329604  |
| Train/policy_loss       | 0.019816872 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 679 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 678           |
| Time/Actor_Time         | 0.083         |
| Time/B_Format_Time      | 0.0727        |
| Time/B_Original_Form... | 0.0706        |
| Time/Buffer             | 0.00284       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.32645717    |
| Train/Action_magnitu... | 0.8446761     |
| Train/Action_magnitude  | 0.6685546     |
| Train/Action_max        | 0.29223317    |
| Train/Action_std        | 0.32736275    |
| Train/Entropy           | 0.14701809    |
| Train/Entropy_Loss      | -0.000147     |
| Train/Entropy_loss      | -0.000147     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.2971135    |
| Train/Loss              | -0.0017748377 |
| Train/PolicyClip        | 0.013319879   |
| Train/Policy_loss       | -0.008412889  |
| Train/Ratio             | 0.98237926    |
| Train/Return            | 1.5312053     |
| Train/V                 | 1.5375344     |
| Train/Value             | 1.5375344     |
| Train/control_penalty   | 0.6785069     |
| Train/policy_loss       | -0.008412889  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02675       |
-------------------------------------------

 ---------------- Iteration 680 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 679          |
| Time/Actor_Time         | 0.0831       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00266      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3100104    |
| Train/Action_magnitu... | 0.84372985   |
| Train/Action_magnitude  | 0.6683427    |
| Train/Action_max        | 0.31122568   |
| Train/Action_std        | 0.33700782   |
| Train/Entropy           | 0.17893483   |
| Train/Entropy_Loss      | -0.000179    |
| Train/Entropy_loss      | -0.000179    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.414799    |
| Train/Loss              | -0.016881727 |
| Train/PolicyClip        | 0.015110147  |
| Train/Policy_loss       | -0.02338776  |
| Train/Ratio             | 0.99207914   |
| Train/Return            | 1.5282878    |
| Train/V                 | 1.5216621    |
| Train/Value             | 1.5216621    |
| Train/control_penalty   | 0.66849685   |
| Train/policy_loss       | -0.02338776  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02975      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 681 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 680         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0717      |
| Time/Buffer             | 0.00329     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30833688  |
| Train/Action_magnitu... | 0.83052754  |
| Train/Action_magnitude  | 0.65366256  |
| Train/Action_max        | 0.28072506  |
| Train/Action_std        | 0.33800066  |
| Train/Entropy           | 0.19457641  |
| Train/Entropy_Loss      | -0.000195   |
| Train/Entropy_loss      | -0.000195   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.37619188 |
| Train/Loss              | 0.046631943 |
| Train/PolicyClip        | 0.010711573 |
| Train/Policy_loss       | 0.040109612 |
| Train/Ratio             | 0.9778383   |
| Train/Return            | 1.5261469   |
| Train/V                 | 1.5822679   |
| Train/Value             | 1.5822679   |
| Train/control_penalty   | 0.67169076  |
| Train/policy_loss       | 0.040109612 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 682 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 681         |
| Time/Actor_Time         | 0.0867      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.00255     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31924045  |
| Train/Action_magnitu... | 0.85123986  |
| Train/Action_magnitude  | 0.6730624   |
| Train/Action_max        | 0.3042386   |
| Train/Action_std        | 0.3342948   |
| Train/Entropy           | 0.18139532  |
| Train/Entropy_Loss      | -0.000181   |
| Train/Entropy_loss      | -0.000181   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.39501825 |
| Train/Loss              | 0.05587396  |
| Train/PolicyClip        | 0.013794958 |
| Train/Policy_loss       | 0.049390197 |
| Train/Ratio             | 0.9833778   |
| Train/Return            | 1.3657779   |
| Train/V                 | 1.4310784   |
| Train/Value             | 1.4310784   |
| Train/control_penalty   | 0.666516    |
| Train/policy_loss       | 0.049390197 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 683 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 682         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0724      |
| Time/Buffer             | 0.00248     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.32837328  |
| Train/Action_magnitu... | 0.8926298   |
| Train/Action_magnitude  | 0.70326084  |
| Train/Action_max        | 0.30702725  |
| Train/Action_std        | 0.36077163  |
| Train/Entropy           | 0.2463579   |
| Train/Entropy_Loss      | -0.000246   |
| Train/Entropy_loss      | -0.000246   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5250737  |
| Train/Loss              | 0.056195777 |
| Train/PolicyClip        | 0.011765371 |
| Train/Policy_loss       | 0.049335815 |
| Train/Ratio             | 0.9803124   |
| Train/Return            | 1.3893249   |
| Train/V                 | 1.4532888   |
| Train/Value             | 1.4532888   |
| Train/control_penalty   | 0.7106318   |
| Train/policy_loss       | 0.049335815 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 684 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 683         |
| Time/Actor_Time         | 0.0837      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.00242     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32589844  |
| Train/Action_magnitu... | 0.9026791   |
| Train/Action_magnitude  | 0.7132344   |
| Train/Action_max        | 0.30985647  |
| Train/Action_std        | 0.3858188   |
| Train/Entropy           | 0.2977005   |
| Train/Entropy_Loss      | -0.000298   |
| Train/Entropy_loss      | -0.000298   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6214855  |
| Train/Loss              | 0.070655964 |
| Train/PolicyClip        | 0.010212412 |
| Train/Policy_loss       | 0.06370937  |
| Train/Ratio             | 0.9776203   |
| Train/Return            | 1.23663     |
| Train/V                 | 1.3140455   |
| Train/Value             | 1.3140455   |
| Train/control_penalty   | 0.72442967  |
| Train/policy_loss       | 0.06370937  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 685 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 684           |
| Time/Actor_Time         | 0.0851        |
| Time/B_Format_Time      | 0.0721        |
| Time/B_Original_Form... | 0.0707        |
| Time/Buffer             | 0.00297       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.3265609     |
| Train/Action_magnitu... | 0.8740358     |
| Train/Action_magnitude  | 0.6915796     |
| Train/Action_max        | 0.3418221     |
| Train/Action_std        | 0.36046556    |
| Train/Entropy           | 0.25523636    |
| Train/Entropy_Loss      | -0.000255     |
| Train/Entropy_loss      | -0.000255     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.5215986    |
| Train/Loss              | -0.0053573274 |
| Train/PolicyClip        | 0.008513238   |
| Train/Policy_loss       | -0.012174355  |
| Train/Ratio             | 0.9918769     |
| Train/Return            | 1.488122      |
| Train/V                 | 1.4897388     |
| Train/Value             | 1.4897388     |
| Train/control_penalty   | 0.7072264     |
| Train/policy_loss       | -0.012174355  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.03225       |
-------------------------------------------

 ---------------- Iteration 686 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 685          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00262      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32375684   |
| Train/Action_magnitu... | 0.85662293   |
| Train/Action_magnitude  | 0.67494      |
| Train/Action_max        | 0.2983466    |
| Train/Action_std        | 0.34660286   |
| Train/Entropy           | 0.2125081    |
| Train/Entropy_Loss      | -0.000213    |
| Train/Entropy_loss      | -0.000213    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.47312444  |
| Train/Loss              | 0.019744061  |
| Train/PolicyClip        | 0.0096470695 |
| Train/Policy_loss       | 0.013088428  |
| Train/Ratio             | 0.9800441    |
| Train/Return            | 1.2674987    |
| Train/V                 | 1.2959254    |
| Train/Value             | 1.2959254    |
| Train/control_penalty   | 0.6868142    |
| Train/policy_loss       | 0.013088428  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 687 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 686         |
| Time/Actor_Time         | 0.0857      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00706     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32303128  |
| Train/Action_magnitu... | 0.855376    |
| Train/Action_magnitude  | 0.6752881   |
| Train/Action_max        | 0.33352175  |
| Train/Action_std        | 0.35668805  |
| Train/Entropy           | 0.24210848  |
| Train/Entropy_Loss      | -0.000242   |
| Train/Entropy_loss      | -0.000242   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.51019216 |
| Train/Loss              | 0.03399948  |
| Train/PolicyClip        | 0.012477042 |
| Train/Policy_loss       | 0.027480954 |
| Train/Ratio             | 0.9800296   |
| Train/Return            | 1.345382    |
| Train/V                 | 1.3889703   |
| Train/Value             | 1.3889703   |
| Train/control_penalty   | 0.67606366  |
| Train/policy_loss       | 0.027480954 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 688 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 687          |
| Time/Actor_Time         | 0.0868       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0706       |
| Time/Buffer             | 0.003        |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.33387765   |
| Train/Action_magnitu... | 0.95104736   |
| Train/Action_magnitude  | 0.75148493   |
| Train/Action_max        | 0.33345026   |
| Train/Action_std        | 0.38746014   |
| Train/Entropy           | 0.31781244   |
| Train/Entropy_Loss      | -0.000318    |
| Train/Entropy_loss      | -0.000318    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6550525   |
| Train/Loss              | 0.004725036  |
| Train/PolicyClip        | 0.010639889  |
| Train/Policy_loss       | -0.002309777 |
| Train/Ratio             | 0.9941786    |
| Train/Return            | 1.3611759    |
| Train/V                 | 1.3727375    |
| Train/Value             | 1.3727375    |
| Train/control_penalty   | 0.7352626    |
| Train/policy_loss       | -0.002309777 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------

 ---------------- Iteration 689 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 688         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.0701      |
| Time/Buffer             | 0.00249     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3303457   |
| Train/Action_magnitu... | 0.9006365   |
| Train/Action_magnitude  | 0.71181136  |
| Train/Action_max        | 0.31692424  |
| Train/Action_std        | 0.3714673   |
| Train/Entropy           | 0.2666681   |
| Train/Entropy_Loss      | -0.000267   |
| Train/Entropy_loss      | -0.000267   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6159784  |
| Train/Loss              | 0.015992407 |
| Train/PolicyClip        | 0.010803241 |
| Train/Policy_loss       | 0.009055405 |
| Train/Ratio             | 0.9891229   |
| Train/Return            | 1.2101866   |
| Train/V                 | 1.2327219   |
| Train/Value             | 1.2327219   |
| Train/control_penalty   | 0.720367    |
| Train/policy_loss       | 0.009055405 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 690 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 689          |
| Time/Actor_Time         | 0.0881       |
| Time/B_Format_Time      | 0.071        |
| Time/B_Original_Form... | 0.0697       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32421002   |
| Train/Action_magnitu... | 0.87855065   |
| Train/Action_magnitude  | 0.69509315   |
| Train/Action_max        | 0.30439875   |
| Train/Action_std        | 0.3629704    |
| Train/Entropy           | 0.25119635   |
| Train/Entropy_Loss      | -0.000251    |
| Train/Entropy_loss      | -0.000251    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5385848   |
| Train/Loss              | -0.015905248 |
| Train/PolicyClip        | 0.008957821  |
| Train/Policy_loss       | -0.02262661  |
| Train/Ratio             | 0.98369944   |
| Train/Return            | 1.1556451    |
| Train/V                 | 1.147828     |
| Train/Value             | 1.147828     |
| Train/control_penalty   | 0.6972559    |
| Train/policy_loss       | -0.02262661  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 691 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 690          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31583816   |
| Train/Action_magnitu... | 0.8836029    |
| Train/Action_magnitude  | 0.7005123    |
| Train/Action_max        | 0.30902815   |
| Train/Action_std        | 0.37541905   |
| Train/Entropy           | 0.27087632   |
| Train/Entropy_Loss      | -0.000271    |
| Train/Entropy_loss      | -0.000271    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.57581615  |
| Train/Loss              | 0.008756649  |
| Train/PolicyClip        | 0.014090517  |
| Train/Policy_loss       | 0.0019318198 |
| Train/Ratio             | 1.0002197    |
| Train/Return            | 1.0654994    |
| Train/V                 | 1.0786324    |
| Train/Value             | 1.0786324    |
| Train/control_penalty   | 0.7095705    |
| Train/policy_loss       | 0.0019318198 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01875      |
------------------------------------------

 ---------------- Iteration 692 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 691          |
| Time/Actor_Time         | 0.0881       |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00281      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3223426    |
| Train/Action_magnitu... | 0.8760852    |
| Train/Action_magnitude  | 0.6972803    |
| Train/Action_max        | 0.33101916   |
| Train/Action_std        | 0.35250145   |
| Train/Entropy           | 0.19241229   |
| Train/Entropy_Loss      | -0.000192    |
| Train/Entropy_loss      | -0.000192    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4467171   |
| Train/Loss              | -0.030354593 |
| Train/PolicyClip        | 0.01411864   |
| Train/Policy_loss       | -0.037070733 |
| Train/Ratio             | 1.0034778    |
| Train/Return            | 1.1434621    |
| Train/V                 | 1.1214979    |
| Train/Value             | 1.1214979    |
| Train/control_penalty   | 0.6908552    |
| Train/policy_loss       | -0.037070733 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------

 ---------------- Iteration 693 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 692          |
| Time/Actor_Time         | 0.0952       |
| Time/B_Format_Time      | 0.082        |
| Time/B_Original_Form... | 0.0771       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.33087698   |
| Train/Action_magnitu... | 0.9255936    |
| Train/Action_magnitude  | 0.7333265    |
| Train/Action_max        | 0.32395154   |
| Train/Action_std        | 0.39274353   |
| Train/Entropy           | 0.3129461    |
| Train/Entropy_Loss      | -0.000313    |
| Train/Entropy_loss      | -0.000313    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5955014   |
| Train/Loss              | 0.01353915   |
| Train/PolicyClip        | 0.008991112  |
| Train/Policy_loss       | 0.0064446563 |
| Train/Ratio             | 0.9866533    |
| Train/Return            | 1.2255871    |
| Train/V                 | 1.2437404    |
| Train/Value             | 1.2437404    |
| Train/control_penalty   | 0.740744     |
| Train/policy_loss       | 0.0064446563 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.022        |
------------------------------------------

 ---------------- Iteration 694 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 693          |
| Time/Actor_Time         | 0.087        |
| Time/B_Format_Time      | 0.0726       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00469      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.3200394    |
| Train/Action_magnitu... | 0.8799901    |
| Train/Action_magnitude  | 0.6956093    |
| Train/Action_max        | 0.32193214   |
| Train/Action_std        | 0.35322672   |
| Train/Entropy           | 0.21444586   |
| Train/Entropy_Loss      | -0.000214    |
| Train/Entropy_loss      | -0.000214    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.44842786  |
| Train/Loss              | -0.026231851 |
| Train/PolicyClip        | 0.009183613  |
| Train/Policy_loss       | -0.032785036 |
| Train/Ratio             | 0.99535036   |
| Train/Return            | 1.0617398    |
| Train/V                 | 1.042857     |
| Train/Value             | 1.042857     |
| Train/control_penalty   | 0.6767632    |
| Train/policy_loss       | -0.032785036 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 695 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 694            |
| Time/Actor_Time         | 0.0845         |
| Time/B_Format_Time      | 0.0739         |
| Time/B_Original_Form... | 0.0717         |
| Time/Buffer             | 0.00282        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.31270894     |
| Train/Action_magnitu... | 0.870962       |
| Train/Action_magnitude  | 0.6913303      |
| Train/Action_max        | 0.3184783      |
| Train/Action_std        | 0.3725452      |
| Train/Entropy           | 0.2677863      |
| Train/Entropy_Loss      | -0.000268      |
| Train/Entropy_loss      | -0.000268      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -0.58012927    |
| Train/Loss              | 0.005691175    |
| Train/PolicyClip        | 0.010381845    |
| Train/Policy_loss       | -0.00096473855 |
| Train/Ratio             | 0.9868406      |
| Train/Return            | 1.1128911      |
| Train/V                 | 1.1269112      |
| Train/Value             | 1.1269112      |
| Train/control_penalty   | 0.69237        |
| Train/policy_loss       | -0.00096473855 |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.02275        |
--------------------------------------------

 ---------------- Iteration 696 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 695         |
| Time/Actor_Time         | 0.0836      |
| Time/B_Format_Time      | 0.0719      |
| Time/B_Original_Form... | 0.0704      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34010404  |
| Train/Action_magnitu... | 0.90428734  |
| Train/Action_magnitude  | 0.71330184  |
| Train/Action_max        | 0.33728227  |
| Train/Action_std        | 0.34807223  |
| Train/Entropy           | 0.19298156  |
| Train/Entropy_Loss      | -0.000193   |
| Train/Entropy_loss      | -0.000193   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.49399614 |
| Train/Loss              | -0.0480289  |
| Train/PolicyClip        | 0.011418017 |
| Train/Policy_loss       | -0.05487346 |
| Train/Ratio             | 1.0084858   |
| Train/Return            | 1.0810245   |
| Train/V                 | 1.0386255   |
| Train/Value             | 1.0386255   |
| Train/control_penalty   | 0.7037541   |
| Train/policy_loss       | -0.05487346 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 697 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 696          |
| Time/Actor_Time         | 0.0832       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00255      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32935742   |
| Train/Action_magnitu... | 0.88170683   |
| Train/Action_magnitude  | 0.6981365    |
| Train/Action_max        | 0.30128586   |
| Train/Action_std        | 0.3591153    |
| Train/Entropy           | 0.2320845    |
| Train/Entropy_Loss      | -0.000232    |
| Train/Entropy_loss      | -0.000232    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.49924347  |
| Train/Loss              | -0.012510452 |
| Train/PolicyClip        | 0.010911319  |
| Train/Policy_loss       | -0.019252151 |
| Train/Ratio             | 0.9951739    |
| Train/Return            | 1.2019231    |
| Train/V                 | 1.1972543    |
| Train/Value             | 1.1972543    |
| Train/control_penalty   | 0.69737834   |
| Train/policy_loss       | -0.019252151 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 698 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 697          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0711       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00327      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34826013   |
| Train/Action_magnitu... | 0.90954053   |
| Train/Action_magnitude  | 0.7213304    |
| Train/Action_max        | 0.31912786   |
| Train/Action_std        | 0.3522862    |
| Train/Entropy           | 0.18988244   |
| Train/Entropy_Loss      | -0.00019     |
| Train/Entropy_loss      | -0.00019     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.38973233  |
| Train/Loss              | -0.004853632 |
| Train/PolicyClip        | 0.012335494  |
| Train/Policy_loss       | -0.011877666 |
| Train/Ratio             | 0.9920386    |
| Train/Return            | 1.206622     |
| Train/V                 | 1.2107867    |
| Train/Value             | 1.2107867    |
| Train/control_penalty   | 0.72139174   |
| Train/policy_loss       | -0.011877666 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 699 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 698          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.072        |
| Time/Buffer             | 0.00326      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32765678   |
| Train/Action_magnitu... | 0.88398504   |
| Train/Action_magnitude  | 0.6988565    |
| Train/Action_max        | 0.31807813   |
| Train/Action_std        | 0.34986266   |
| Train/Entropy           | 0.1993419    |
| Train/Entropy_Loss      | -0.000199    |
| Train/Entropy_loss      | -0.000199    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4655344   |
| Train/Loss              | -0.048717808 |
| Train/PolicyClip        | 0.011016427  |
| Train/Policy_loss       | -0.055522587 |
| Train/Ratio             | 1.0006167    |
| Train/Return            | 1.0511053    |
| Train/V                 | 1.0085844    |
| Train/Value             | 1.0085844    |
| Train/control_penalty   | 0.700412     |
| Train/policy_loss       | -0.055522587 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 700 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 699          |
| Time/Actor_Time         | 0.0828       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00292      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33183634   |
| Train/Action_magnitu... | 0.8830413    |
| Train/Action_magnitude  | 0.7007409    |
| Train/Action_max        | 0.3348234    |
| Train/Action_std        | 0.3428025    |
| Train/Entropy           | 0.15811881   |
| Train/Entropy_Loss      | -0.000158    |
| Train/Entropy_loss      | -0.000158    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.40733314  |
| Train/Loss              | -0.078864865 |
| Train/PolicyClip        | 0.011280344  |
| Train/Policy_loss       | -0.08576015  |
| Train/Ratio             | 1.0041847    |
| Train/Return            | 1.1556312    |
| Train/V                 | 1.0852023    |
| Train/Value             | 1.0852023    |
| Train/control_penalty   | 0.70533943   |
| Train/policy_loss       | -0.08576015  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0325       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 701 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 700         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00245     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31407002  |
| Train/Action_magnitu... | 0.87005264  |
| Train/Action_magnitude  | 0.6937284   |
| Train/Action_max        | 0.3236211   |
| Train/Action_std        | 0.35512075  |
| Train/Entropy           | 0.1721737   |
| Train/Entropy_Loss      | -0.000172   |
| Train/Entropy_loss      | -0.000172   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.39316127 |
| Train/Loss              | 0.018028015 |
| Train/PolicyClip        | 0.013766309 |
| Train/Policy_loss       | 0.011259413 |
| Train/Ratio             | 0.98821443  |
| Train/Return            | 1.1478148   |
| Train/V                 | 1.1746373   |
| Train/Value             | 1.1746373   |
| Train/control_penalty   | 0.69407755  |
| Train/policy_loss       | 0.011259413 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 702 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 701         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0698      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3235544   |
| Train/Action_magnitu... | 0.86153674  |
| Train/Action_magnitude  | 0.6793139   |
| Train/Action_max        | 0.3396966   |
| Train/Action_std        | 0.34775877  |
| Train/Entropy           | 0.17497082  |
| Train/Entropy_Loss      | -0.000175   |
| Train/Entropy_loss      | -0.000175   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.35565916 |
| Train/Loss              | 0.028818546 |
| Train/PolicyClip        | 0.014671985 |
| Train/Policy_loss       | 0.022045245 |
| Train/Ratio             | 0.97982365  |
| Train/Return            | 1.0826321   |
| Train/V                 | 1.1217991   |
| Train/Value             | 1.1217991   |
| Train/control_penalty   | 0.69482714  |
| Train/policy_loss       | 0.022045245 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 703 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 702         |
| Time/Actor_Time         | 0.0861      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0721      |
| Time/Buffer             | 0.0025      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30953887  |
| Train/Action_magnitu... | 0.86542064  |
| Train/Action_magnitude  | 0.6823022   |
| Train/Action_max        | 0.3347301   |
| Train/Action_std        | 0.35686892  |
| Train/Entropy           | 0.207024    |
| Train/Entropy_Loss      | -0.000207   |
| Train/Entropy_loss      | -0.000207   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.46921316 |
| Train/Loss              | 0.022931501 |
| Train/PolicyClip        | 0.014062485 |
| Train/Policy_loss       | 0.01649726  |
| Train/Ratio             | 0.984224    |
| Train/Return            | 1.0534726   |
| Train/V                 | 1.0864904   |
| Train/Value             | 1.0864904   |
| Train/control_penalty   | 0.66412646  |
| Train/policy_loss       | 0.01649726  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------

 ---------------- Iteration 704 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 703          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00306      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.32077423   |
| Train/Action_magnitu... | 0.88225913   |
| Train/Action_magnitude  | 0.6983255    |
| Train/Action_max        | 0.30199814   |
| Train/Action_std        | 0.35358703   |
| Train/Entropy           | 0.20491634   |
| Train/Entropy_Loss      | -0.000205    |
| Train/Entropy_loss      | -0.000205    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4456634   |
| Train/Loss              | 0.04028675   |
| Train/PolicyClip        | 0.0065592197 |
| Train/Policy_loss       | 0.033604793  |
| Train/Ratio             | 0.98528105   |
| Train/Return            | 1.117803     |
| Train/V                 | 1.1620246    |
| Train/Value             | 1.1620246    |
| Train/control_penalty   | 0.6886872    |
| Train/policy_loss       | 0.033604793  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02025      |
------------------------------------------

 ---------------- Iteration 705 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 704         |
| Time/Actor_Time         | 0.0848      |
| Time/B_Format_Time      | 0.0719      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30094567  |
| Train/Action_magnitu... | 0.851196    |
| Train/Action_magnitude  | 0.6759368   |
| Train/Action_max        | 0.29987624  |
| Train/Action_std        | 0.36656538  |
| Train/Entropy           | 0.22947541  |
| Train/Entropy_Loss      | -0.000229   |
| Train/Entropy_loss      | -0.000229   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.4985535  |
| Train/Loss              | 0.015858883 |
| Train/PolicyClip        | 0.008648946 |
| Train/Policy_loss       | 0.009107142 |
| Train/Ratio             | 0.9790553   |
| Train/Return            | 1.0316526   |
| Train/V                 | 1.0526226   |
| Train/Value             | 1.0526226   |
| Train/control_penalty   | 0.6981216   |
| Train/policy_loss       | 0.009107142 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 706 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 705          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0724       |
| Time/Buffer             | 0.00286      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30046684   |
| Train/Action_magnitu... | 0.84347486   |
| Train/Action_magnitude  | 0.6671026    |
| Train/Action_max        | 0.27788037   |
| Train/Action_std        | 0.35291272   |
| Train/Entropy           | 0.1859645    |
| Train/Entropy_Loss      | -0.000186    |
| Train/Entropy_loss      | -0.000186    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3599091   |
| Train/Loss              | -0.041705556 |
| Train/PolicyClip        | 0.014004446  |
| Train/Policy_loss       | -0.04816744  |
| Train/Ratio             | 0.9901616    |
| Train/Return            | 1.1713879    |
| Train/V                 | 1.1370183    |
| Train/Value             | 1.1370183    |
| Train/control_penalty   | 0.6647849    |
| Train/policy_loss       | -0.04816744  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 707 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 706         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30128038  |
| Train/Action_magnitu... | 0.8676073   |
| Train/Action_magnitude  | 0.6883856   |
| Train/Action_max        | 0.29274517  |
| Train/Action_std        | 0.37688494  |
| Train/Entropy           | 0.24562612  |
| Train/Entropy_Loss      | -0.000246   |
| Train/Entropy_loss      | -0.000246   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.50369626 |
| Train/Loss              | 0.05164361  |
| Train/PolicyClip        | 0.010289991 |
| Train/Policy_loss       | 0.044990517 |
| Train/Ratio             | 0.98298496  |
| Train/Return            | 1.0431722   |
| Train/V                 | 1.101535    |
| Train/Value             | 1.101535    |
| Train/control_penalty   | 0.68987215  |
| Train/policy_loss       | 0.044990517 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01725     |
-----------------------------------------

 ---------------- Iteration 708 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 707         |
| Time/Actor_Time         | 0.0841      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00248     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29146138  |
| Train/Action_magnitu... | 0.8601734   |
| Train/Action_magnitude  | 0.6822316   |
| Train/Action_max        | 0.29451042  |
| Train/Action_std        | 0.370047    |
| Train/Entropy           | 0.23004107  |
| Train/Entropy_Loss      | -0.00023    |
| Train/Entropy_loss      | -0.00023    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.45574608 |
| Train/Loss              | 0.010839939 |
| Train/PolicyClip        | 0.009504131 |
| Train/Policy_loss       | 0.004224152 |
| Train/Ratio             | 0.98211163  |
| Train/Return            | 0.9768837   |
| Train/V                 | 0.99561983  |
| Train/Value             | 0.99561983  |
| Train/control_penalty   | 0.68458277  |
| Train/policy_loss       | 0.004224152 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------

 ---------------- Iteration 709 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 708          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00314      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31209075   |
| Train/Action_magnitu... | 0.89794976   |
| Train/Action_magnitude  | 0.7123413    |
| Train/Action_max        | 0.28712147   |
| Train/Action_std        | 0.39255548   |
| Train/Entropy           | 0.27606764   |
| Train/Entropy_Loss      | -0.000276    |
| Train/Entropy_loss      | -0.000276    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.66248155  |
| Train/Loss              | -0.037031494 |
| Train/PolicyClip        | 0.011877624  |
| Train/Policy_loss       | -0.043906115 |
| Train/Ratio             | 1.0040244    |
| Train/Return            | 1.106836     |
| Train/V                 | 1.0763733    |
| Train/Value             | 1.0763733    |
| Train/control_penalty   | 0.71506876   |
| Train/policy_loss       | -0.043906115 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.027        |
------------------------------------------

 ---------------- Iteration 710 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 709         |
| Time/Actor_Time         | 0.0852      |
| Time/B_Format_Time      | 0.0711      |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.29901457  |
| Train/Action_magnitu... | 0.8709385   |
| Train/Action_magnitude  | 0.6917248   |
| Train/Action_max        | 0.31056702  |
| Train/Action_std        | 0.37393603  |
| Train/Entropy           | 0.22381772  |
| Train/Entropy_Loss      | -0.000224   |
| Train/Entropy_loss      | -0.000224   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.44688207 |
| Train/Loss              | 0.020851867 |
| Train/PolicyClip        | 0.013036444 |
| Train/Policy_loss       | 0.014095931 |
| Train/Ratio             | 0.96698475  |
| Train/Return            | 1.0615842   |
| Train/V                 | 1.0882205   |
| Train/Value             | 1.0882205   |
| Train/control_penalty   | 0.69797534  |
| Train/policy_loss       | 0.014095931 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 711 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 710          |
| Time/Actor_Time         | 0.0833       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.072        |
| Time/Buffer             | 0.00323      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28951007   |
| Train/Action_magnitu... | 0.8448147    |
| Train/Action_magnitude  | 0.6673947    |
| Train/Action_max        | 0.29651248   |
| Train/Action_std        | 0.37425786   |
| Train/Entropy           | 0.21445893   |
| Train/Entropy_Loss      | -0.000214    |
| Train/Entropy_loss      | -0.000214    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.41674095  |
| Train/Loss              | -0.067174844 |
| Train/PolicyClip        | 0.011084151  |
| Train/Policy_loss       | -0.07366237  |
| Train/Ratio             | 1.0075288    |
| Train/Return            | 1.2117784    |
| Train/V                 | 1.1528786    |
| Train/Value             | 1.1528786    |
| Train/control_penalty   | 0.6701982    |
| Train/policy_loss       | -0.07366237  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.031        |
------------------------------------------

 ---------------- Iteration 712 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 711         |
| Time/Actor_Time         | 0.0897      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.0693      |
| Time/Buffer             | 0.00228     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2970872   |
| Train/Action_magnitu... | 0.8875353   |
| Train/Action_magnitude  | 0.70196223  |
| Train/Action_max        | 0.30960023  |
| Train/Action_std        | 0.37857965  |
| Train/Entropy           | 0.2330458   |
| Train/Entropy_Loss      | -0.000233   |
| Train/Entropy_loss      | -0.000233   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.53407943 |
| Train/Loss              | 0.041893166 |
| Train/PolicyClip        | 0.008110576 |
| Train/Policy_loss       | 0.03527526  |
| Train/Ratio             | 0.99024165  |
| Train/Return            | 1.0756587   |
| Train/V                 | 1.1228994   |
| Train/Value             | 1.1228994   |
| Train/control_penalty   | 0.68509495  |
| Train/policy_loss       | 0.03527526  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 713 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 712         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0725      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.297179    |
| Train/Action_magnitu... | 0.8752956   |
| Train/Action_magnitude  | 0.69292605  |
| Train/Action_max        | 0.30630797  |
| Train/Action_std        | 0.38698915  |
| Train/Entropy           | 0.24738632  |
| Train/Entropy_Loss      | -0.000247   |
| Train/Entropy_loss      | -0.000247   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.54902834 |
| Train/Loss              | 0.046603825 |
| Train/PolicyClip        | 0.010512347 |
| Train/Policy_loss       | 0.039886355 |
| Train/Ratio             | 0.97649306  |
| Train/Return            | 1.1281562   |
| Train/V                 | 1.1827322   |
| Train/Value             | 1.1827322   |
| Train/control_penalty   | 0.6964856   |
| Train/policy_loss       | 0.039886355 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 714 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 713          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0704       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.325467     |
| Train/Action_magnitu... | 0.9461519    |
| Train/Action_magnitude  | 0.748962     |
| Train/Action_max        | 0.3456487    |
| Train/Action_std        | 0.43033904   |
| Train/Entropy           | 0.33409935   |
| Train/Entropy_Loss      | -0.000334    |
| Train/Entropy_loss      | -0.000334    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.63471794  |
| Train/Loss              | 0.004803984  |
| Train/PolicyClip        | 0.007884556  |
| Train/Policy_loss       | -0.002758382 |
| Train/Ratio             | 0.9905932    |
| Train/Return            | 1.3119922    |
| Train/V                 | 1.3218174    |
| Train/Value             | 1.3218174    |
| Train/control_penalty   | 0.78964657   |
| Train/policy_loss       | -0.002758382 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02625      |
------------------------------------------

 ---------------- Iteration 715 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 714          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0711       |
| Time/Buffer             | 0.00278      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30894887   |
| Train/Action_magnitu... | 0.8761792    |
| Train/Action_magnitude  | 0.6929457    |
| Train/Action_max        | 0.3407676    |
| Train/Action_std        | 0.36924094   |
| Train/Entropy           | 0.20324466   |
| Train/Entropy_Loss      | -0.000203    |
| Train/Entropy_loss      | -0.000203    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.4785666   |
| Train/Loss              | -0.014167989 |
| Train/PolicyClip        | 0.008103903  |
| Train/Policy_loss       | -0.020946482 |
| Train/Ratio             | 0.9918675    |
| Train/Return            | 1.1171243    |
| Train/V                 | 1.1068769    |
| Train/Value             | 1.1068769    |
| Train/control_penalty   | 0.69817376   |
| Train/policy_loss       | -0.020946482 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 716 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 715          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0705       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2942414    |
| Train/Action_magnitu... | 0.85621345   |
| Train/Action_magnitude  | 0.67919385   |
| Train/Action_max        | 0.32252222   |
| Train/Action_std        | 0.35280237   |
| Train/Entropy           | 0.13865617   |
| Train/Entropy_Loss      | -0.000139    |
| Train/Entropy_loss      | -0.000139    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.35634726  |
| Train/Loss              | -0.029836671 |
| Train/PolicyClip        | 0.008355524  |
| Train/Policy_loss       | -0.03639415  |
| Train/Ratio             | 0.9911258    |
| Train/Return            | 1.1685686    |
| Train/V                 | 1.1456951    |
| Train/Value             | 1.1456951    |
| Train/control_penalty   | 0.66961336   |
| Train/policy_loss       | -0.03639415  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 717 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 716         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0705      |
| Time/Buffer             | 0.0033      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30446088  |
| Train/Action_magnitu... | 0.87369066  |
| Train/Action_magnitude  | 0.6929263   |
| Train/Action_max        | 0.31633934  |
| Train/Action_std        | 0.36991566  |
| Train/Entropy           | 0.19149373  |
| Train/Entropy_Loss      | -0.000191   |
| Train/Entropy_loss      | -0.000191   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.40934086 |
| Train/Loss              | 0.050563507 |
| Train/PolicyClip        | 0.00947993  |
| Train/Policy_loss       | 0.043766156 |
| Train/Ratio             | 0.9911074   |
| Train/Return            | 1.1603999   |
| Train/V                 | 1.2172357   |
| Train/Value             | 1.2172357   |
| Train/control_penalty   | 0.6988844   |
| Train/policy_loss       | 0.043766156 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 718 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 717          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00225      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28082266   |
| Train/Action_magnitu... | 0.8623886    |
| Train/Action_magnitude  | 0.6855037    |
| Train/Action_max        | 0.3043764    |
| Train/Action_std        | 0.38390505   |
| Train/Entropy           | 0.24387105   |
| Train/Entropy_Loss      | -0.000244    |
| Train/Entropy_loss      | -0.000244    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.50032234  |
| Train/Loss              | 0.0029169018 |
| Train/PolicyClip        | 0.009555131  |
| Train/Policy_loss       | -0.003608469 |
| Train/Ratio             | 0.9966468    |
| Train/Return            | 1.007712     |
| Train/V                 | 1.016954     |
| Train/Value             | 1.016954     |
| Train/control_penalty   | 0.67692417   |
| Train/policy_loss       | -0.003608469 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 719 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 718          |
| Time/Actor_Time         | 0.0845       |
| Time/B_Format_Time      | 0.0738       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00286      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2847402    |
| Train/Action_magnitu... | 0.8737633    |
| Train/Action_magnitude  | 0.69217503   |
| Train/Action_max        | 0.28856835   |
| Train/Action_std        | 0.36888582   |
| Train/Entropy           | 0.19445372   |
| Train/Entropy_Loss      | -0.000194    |
| Train/Entropy_loss      | -0.000194    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.47451395  |
| Train/Loss              | -0.050041012 |
| Train/PolicyClip        | 0.008098114  |
| Train/Policy_loss       | -0.05664669  |
| Train/Ratio             | 1.0115858    |
| Train/Return            | 1.0725869    |
| Train/V                 | 1.027676     |
| Train/Value             | 1.027676     |
| Train/control_penalty   | 0.68001306   |
| Train/policy_loss       | -0.05664669  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02725      |
------------------------------------------

 ---------------- Iteration 720 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 719          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00312      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29191348   |
| Train/Action_magnitu... | 0.884226     |
| Train/Action_magnitude  | 0.69989294   |
| Train/Action_max        | 0.32551327   |
| Train/Action_std        | 0.3848453    |
| Train/Entropy           | 0.21299161   |
| Train/Entropy_Loss      | -0.000213    |
| Train/Entropy_loss      | -0.000213    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.415485    |
| Train/Loss              | -0.008722063 |
| Train/PolicyClip        | 0.013302268  |
| Train/Policy_loss       | -0.01555279  |
| Train/Ratio             | 0.9946726    |
| Train/Return            | 0.9325749    |
| Train/V                 | 0.9280244    |
| Train/Value             | 0.9280244    |
| Train/control_penalty   | 0.7043719    |
| Train/policy_loss       | -0.01555279  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 721 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 720          |
| Time/Actor_Time         | 0.0829       |
| Time/B_Format_Time      | 0.0734       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00302      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29891467   |
| Train/Action_magnitu... | 0.87268907   |
| Train/Action_magnitude  | 0.6889094    |
| Train/Action_max        | 0.30318072   |
| Train/Action_std        | 0.3746616    |
| Train/Entropy           | 0.20599113   |
| Train/Entropy_Loss      | -0.000206    |
| Train/Entropy_loss      | -0.000206    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.42337468  |
| Train/Loss              | 0.0045106346 |
| Train/PolicyClip        | 0.013839111  |
| Train/Policy_loss       | -0.002285016 |
| Train/Ratio             | 1.0065883    |
| Train/Return            | 0.9441001    |
| Train/V                 | 0.95537084   |
| Train/Value             | 0.95537084   |
| Train/control_penalty   | 0.7001642    |
| Train/policy_loss       | -0.002285016 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 722 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 721          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0698       |
| Time/Buffer             | 0.00283      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2866378    |
| Train/Action_magnitu... | 0.88230526   |
| Train/Action_magnitude  | 0.7006738    |
| Train/Action_max        | 0.2881412    |
| Train/Action_std        | 0.3803403    |
| Train/Entropy           | 0.22100894   |
| Train/Entropy_Loss      | -0.000221    |
| Train/Entropy_loss      | -0.000221    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.49399045  |
| Train/Loss              | -0.028588802 |
| Train/PolicyClip        | 0.0086385235 |
| Train/Policy_loss       | -0.03537716  |
| Train/Ratio             | 1.0028785    |
| Train/Return            | 0.94857156   |
| Train/V                 | 0.92360866   |
| Train/Value             | 0.92360866   |
| Train/control_penalty   | 0.7009369    |
| Train/policy_loss       | -0.03537716  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.024        |
------------------------------------------

 ---------------- Iteration 723 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 722          |
| Time/Actor_Time         | 0.0827       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0715       |
| Time/Buffer             | 0.00274      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.27845484   |
| Train/Action_magnitu... | 0.80589586   |
| Train/Action_magnitude  | 0.6405566    |
| Train/Action_max        | 0.296037     |
| Train/Action_std        | 0.32417446   |
| Train/Entropy           | 0.074762635  |
| Train/Entropy_Loss      | -7.48e-05    |
| Train/Entropy_loss      | -7.48e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.18813163  |
| Train/Loss              | -0.044314157 |
| Train/PolicyClip        | 0.006127136  |
| Train/Policy_loss       | -0.050578706 |
| Train/Ratio             | 1.0012853    |
| Train/Return            | 0.97307956   |
| Train/V                 | 0.93272793   |
| Train/Value             | 0.93272793   |
| Train/control_penalty   | 0.63393116   |
| Train/policy_loss       | -0.050578706 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.026        |
------------------------------------------

 ---------------- Iteration 724 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 723         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.0703      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00252     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.27053887  |
| Train/Action_magnitu... | 0.78960633  |
| Train/Action_magnitude  | 0.6234387   |
| Train/Action_max        | 0.24972388  |
| Train/Action_std        | 0.32066143  |
| Train/Entropy           | 0.0328271   |
| Train/Entropy_Loss      | -3.28e-05   |
| Train/Entropy_loss      | -3.28e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.12959379 |
| Train/Loss              | -0.06094021 |
| Train/PolicyClip        | 0.008551775 |
| Train/Policy_loss       | -0.06708955 |
| Train/Ratio             | 1.0075085   |
| Train/Return            | 1.0058297   |
| Train/V                 | 0.9491782   |
| Train/Value             | 0.9491782   |
| Train/control_penalty   | 0.61821693  |
| Train/policy_loss       | -0.06708955 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 725 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 724         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.28484064  |
| Train/Action_magnitu... | 0.84011155  |
| Train/Action_magnitude  | 0.6641758   |
| Train/Action_max        | 0.302218    |
| Train/Action_std        | 0.35797542  |
| Train/Entropy           | 0.13616723  |
| Train/Entropy_Loss      | -0.000136   |
| Train/Entropy_loss      | -0.000136   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.27096593 |
| Train/Loss              | 0.030014966 |
| Train/PolicyClip        | 0.007044142 |
| Train/Policy_loss       | 0.023525944 |
| Train/Ratio             | 0.9937961   |
| Train/Return            | 0.9173828   |
| Train/V                 | 0.95176905  |
| Train/Value             | 0.95176905  |
| Train/control_penalty   | 0.66251886  |
| Train/policy_loss       | 0.023525944 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01625     |
-----------------------------------------

 ---------------- Iteration 726 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 725          |
| Time/Actor_Time         | 0.088        |
| Time/B_Format_Time      | 0.0709       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00311      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2822818    |
| Train/Action_magnitu... | 0.84059584   |
| Train/Action_magnitude  | 0.66495556   |
| Train/Action_max        | 0.29433897   |
| Train/Action_std        | 0.34220022   |
| Train/Entropy           | 0.09389225   |
| Train/Entropy_Loss      | -9.39e-05    |
| Train/Entropy_loss      | -9.39e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.28246436  |
| Train/Loss              | -0.025990916 |
| Train/PolicyClip        | 0.010240174  |
| Train/Policy_loss       | -0.032460988 |
| Train/Ratio             | 1.0057225    |
| Train/Return            | 0.79051      |
| Train/V                 | 0.766683     |
| Train/Value             | 0.766683     |
| Train/control_penalty   | 0.6563963    |
| Train/policy_loss       | -0.032460988 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0195       |
------------------------------------------

 ---------------- Iteration 727 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 726         |
| Time/Actor_Time         | 0.0845      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00267     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2813547   |
| Train/Action_magnitu... | 0.8167623   |
| Train/Action_magnitude  | 0.64553696  |
| Train/Action_max        | 0.2707775   |
| Train/Action_std        | 0.34612635  |
| Train/Entropy           | 0.12099504  |
| Train/Entropy_Loss      | -0.000121   |
| Train/Entropy_loss      | -0.000121   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.28825292 |
| Train/Loss              | 0.044671874 |
| Train/PolicyClip        | 0.014195921 |
| Train/Policy_loss       | 0.038309325 |
| Train/Ratio             | 0.96580875  |
| Train/Return            | 0.7442709   |
| Train/V                 | 0.79176414  |
| Train/Value             | 0.79176414  |
| Train/control_penalty   | 0.64835435  |
| Train/policy_loss       | 0.038309325 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01425     |
-----------------------------------------

 ---------------- Iteration 728 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 727          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0733       |
| Time/Buffer             | 0.00298      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2990654    |
| Train/Action_magnitu... | 0.90089357   |
| Train/Action_magnitude  | 0.70986146   |
| Train/Action_max        | 0.28854054   |
| Train/Action_std        | 0.36347413   |
| Train/Entropy           | 0.15795244   |
| Train/Entropy_Loss      | -0.000158    |
| Train/Entropy_loss      | -0.000158    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.38754278  |
| Train/Loss              | -0.026336763 |
| Train/PolicyClip        | 0.008176063  |
| Train/Policy_loss       | -0.033061855 |
| Train/Ratio             | 0.99783343   |
| Train/Return            | 0.87191075   |
| Train/V                 | 0.8471643    |
| Train/Value             | 0.8471643    |
| Train/control_penalty   | 0.6883045    |
| Train/policy_loss       | -0.033061855 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 729 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 728          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0742       |
| Time/B_Original_Form... | 0.073        |
| Time/Buffer             | 0.00288      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28653142   |
| Train/Action_magnitu... | 0.81931305   |
| Train/Action_magnitude  | 0.6482183    |
| Train/Action_max        | 0.28297314   |
| Train/Action_std        | 0.3191852    |
| Train/Entropy           | 0.014565033  |
| Train/Entropy_Loss      | -1.46e-05    |
| Train/Entropy_loss      | -1.46e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.079940185 |
| Train/Loss              | -0.06426411  |
| Train/PolicyClip        | 0.0101066595 |
| Train/Policy_loss       | -0.070659615 |
| Train/Ratio             | 1.0090172    |
| Train/Return            | 1.0190642    |
| Train/V                 | 0.9587208    |
| Train/Value             | 0.9587208    |
| Train/control_penalty   | 0.6410071    |
| Train/policy_loss       | -0.070659615 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 730 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 729         |
| Time/Actor_Time         | 0.0836      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0699      |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.28429553  |
| Train/Action_magnitu... | 0.82652164  |
| Train/Action_magnitude  | 0.6570138   |
| Train/Action_max        | 0.30319974  |
| Train/Action_std        | 0.31890988  |
| Train/Entropy           | 0.006874446 |
| Train/Entropy_Loss      | -6.87e-06   |
| Train/Entropy_loss      | -6.87e-06   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.10999667 |
| Train/Loss              | -0.03968904 |
| Train/PolicyClip        | 0.011831045 |
| Train/Policy_loss       | -0.04615824 |
| Train/Ratio             | 0.9996063   |
| Train/Return            | 1.030378    |
| Train/V                 | 0.98947483  |
| Train/Value             | 0.98947483  |
| Train/control_penalty   | 0.64760697  |
| Train/policy_loss       | -0.04615824 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 731 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 730          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2837347    |
| Train/Action_magnitu... | 0.8098662    |
| Train/Action_magnitude  | 0.64079183   |
| Train/Action_max        | 0.25052682   |
| Train/Action_std        | 0.32726827   |
| Train/Entropy           | 0.062841356  |
| Train/Entropy_Loss      | -6.28e-05    |
| Train/Entropy_loss      | -6.28e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.20040643  |
| Train/Loss              | -0.048029773 |
| Train/PolicyClip        | 0.01027835   |
| Train/Policy_loss       | -0.054302864 |
| Train/Ratio             | 1.0111539    |
| Train/Return            | 1.0268558    |
| Train/V                 | 0.9833146    |
| Train/Value             | 0.9833146    |
| Train/control_penalty   | 0.63359326   |
| Train/policy_loss       | -0.054302864 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 732 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 731          |
| Time/Actor_Time         | 0.0861       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00289      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2832538    |
| Train/Action_magnitu... | 0.84623134   |
| Train/Action_magnitude  | 0.6714311    |
| Train/Action_max        | 0.28604677   |
| Train/Action_std        | 0.35371912   |
| Train/Entropy           | 0.1269167    |
| Train/Entropy_Loss      | -0.000127    |
| Train/Entropy_loss      | -0.000127    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3126724   |
| Train/Loss              | -0.014014    |
| Train/PolicyClip        | 0.01235787   |
| Train/Policy_loss       | -0.020606818 |
| Train/Ratio             | 0.99837756   |
| Train/Return            | 0.913752     |
| Train/V                 | 0.90303373   |
| Train/Value             | 0.90303373   |
| Train/control_penalty   | 0.6719735    |
| Train/policy_loss       | -0.020606818 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02225      |
------------------------------------------

 ---------------- Iteration 733 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 732         |
| Time/Actor_Time         | 0.0864      |
| Time/B_Format_Time      | 0.0711      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00288     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.26112527  |
| Train/Action_magnitu... | 0.83881736  |
| Train/Action_magnitude  | 0.6649783   |
| Train/Action_max        | 0.31332496  |
| Train/Action_std        | 0.36964282  |
| Train/Entropy           | 0.16448319  |
| Train/Entropy_Loss      | -0.000164   |
| Train/Entropy_loss      | -0.000164   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.37586462 |
| Train/Loss              | 0.04318502  |
| Train/PolicyClip        | 0.009036365 |
| Train/Policy_loss       | 0.036676712 |
| Train/Ratio             | 0.9790244   |
| Train/Return            | 0.87069774  |
| Train/V                 | 0.91859096  |
| Train/Value             | 0.91859096  |
| Train/control_penalty   | 0.66727924  |
| Train/policy_loss       | 0.036676712 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01475     |
-----------------------------------------

 ---------------- Iteration 734 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 733          |
| Time/Actor_Time         | 0.083        |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28642836   |
| Train/Action_magnitu... | 0.77659154   |
| Train/Action_magnitude  | 0.612165     |
| Train/Action_max        | 0.28022894   |
| Train/Action_std        | 0.3030267    |
| Train/Entropy           | -0.026600102 |
| Train/Entropy_Loss      | 2.66e-05     |
| Train/Entropy_loss      | 2.66e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.005269411 |
| Train/Loss              | -0.007696419 |
| Train/PolicyClip        | 0.013792522  |
| Train/Policy_loss       | -0.013787856 |
| Train/Ratio             | 0.9881688    |
| Train/Return            | 0.92337984   |
| Train/V                 | 0.91340965   |
| Train/Value             | 0.91340965   |
| Train/control_penalty   | 0.6064837    |
| Train/policy_loss       | -0.013787856 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 735 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 734          |
| Time/Actor_Time         | 0.0839       |
| Time/B_Format_Time      | 0.0718       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.00242      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28135774   |
| Train/Action_magnitu... | 0.8069773    |
| Train/Action_magnitude  | 0.63868225   |
| Train/Action_max        | 0.3057353    |
| Train/Action_std        | 0.32861936   |
| Train/Entropy           | 0.030661281  |
| Train/Entropy_Loss      | -3.07e-05    |
| Train/Entropy_loss      | -3.07e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.10413381  |
| Train/Loss              | -0.03169374  |
| Train/PolicyClip        | 0.008431736  |
| Train/Policy_loss       | -0.038178496 |
| Train/Ratio             | 0.9978956    |
| Train/Return            | 0.93415636   |
| Train/V                 | 0.9059556    |
| Train/Value             | 0.9059556    |
| Train/control_penalty   | 0.65154153   |
| Train/policy_loss       | -0.038178496 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 736 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 735         |
| Time/Actor_Time         | 0.0847      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.26792774  |
| Train/Action_magnitu... | 0.88396657  |
| Train/Action_magnitude  | 0.700917    |
| Train/Action_max        | 0.34071252  |
| Train/Action_std        | 0.41296786  |
| Train/Entropy           | 0.25467643  |
| Train/Entropy_Loss      | -0.000255   |
| Train/Entropy_loss      | -0.000255   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.53540736 |
| Train/Loss              | 0.017463641 |
| Train/PolicyClip        | 0.009222886 |
| Train/Policy_loss       | 0.010692053 |
| Train/Ratio             | 0.99401855  |
| Train/Return            | 0.9586816   |
| Train/V                 | 0.9778971   |
| Train/Value             | 0.9778971   |
| Train/control_penalty   | 0.7026265   |
| Train/policy_loss       | 0.010692053 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.017       |
-----------------------------------------

 ---------------- Iteration 737 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 736         |
| Time/Actor_Time         | 0.0827      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00351     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3101398   |
| Train/Action_magnitu... | 0.8828181   |
| Train/Action_magnitude  | 0.6999228   |
| Train/Action_max        | 0.2809122   |
| Train/Action_std        | 0.34974256  |
| Train/Entropy           | 0.08330699  |
| Train/Entropy_Loss      | -8.33e-05   |
| Train/Entropy_loss      | -8.33e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.2696528  |
| Train/Loss              | -0.11790858 |
| Train/PolicyClip        | 0.011027015 |
| Train/Policy_loss       | -0.12476883 |
| Train/Ratio             | 1.0162138   |
| Train/Return            | 1.0776376   |
| Train/V                 | 0.95706517  |
| Train/Value             | 0.95706517  |
| Train/control_penalty   | 0.6943557   |
| Train/policy_loss       | -0.12476883 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 738 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 737           |
| Time/Actor_Time         | 0.0846        |
| Time/B_Format_Time      | 0.0715        |
| Time/B_Original_Form... | 0.0708        |
| Time/Buffer             | 0.00301       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.29550162    |
| Train/Action_magnitu... | 0.8640369     |
| Train/Action_magnitude  | 0.68574274    |
| Train/Action_max        | 0.35981068    |
| Train/Action_std        | 0.36426857    |
| Train/Entropy           | 0.11090022    |
| Train/Entropy_Loss      | -0.000111     |
| Train/Entropy_loss      | -0.000111     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.28078455   |
| Train/Loss              | -0.0024066772 |
| Train/PolicyClip        | 0.012849396   |
| Train/Policy_loss       | -0.009244718  |
| Train/Ratio             | 0.99513847    |
| Train/Return            | 0.94815093    |
| Train/V                 | 0.94356126    |
| Train/Value             | 0.94356126    |
| Train/control_penalty   | 0.6948941     |
| Train/policy_loss       | -0.009244718  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0205        |
-------------------------------------------

 ---------------- Iteration 739 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 738           |
| Time/Actor_Time         | 0.0845        |
| Time/B_Format_Time      | 0.0721        |
| Time/B_Original_Form... | 0.0707        |
| Time/Buffer             | 0.00252       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.2923097     |
| Train/Action_magnitu... | 0.88971484    |
| Train/Action_magnitude  | 0.7053283     |
| Train/Action_max        | 0.32045445    |
| Train/Action_std        | 0.38093865    |
| Train/Entropy           | 0.1482733     |
| Train/Entropy_Loss      | -0.000148     |
| Train/Entropy_loss      | -0.000148     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.32838318   |
| Train/Loss              | 0.0039884886  |
| Train/PolicyClip        | 0.010259987   |
| Train/Policy_loss       | -0.0029479046 |
| Train/Ratio             | 0.9910476     |
| Train/Return            | 1.0765593     |
| Train/V                 | 1.0888197     |
| Train/Value             | 1.0888197     |
| Train/control_penalty   | 0.70846665    |
| Train/policy_loss       | -0.0029479046 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01825       |
-------------------------------------------

 ---------------- Iteration 740 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 739          |
| Time/Actor_Time         | 0.0835       |
| Time/B_Format_Time      | 0.072        |
| Time/B_Original_Form... | 0.0718       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29811957   |
| Train/Action_magnitu... | 0.9062917    |
| Train/Action_magnitude  | 0.7212285    |
| Train/Action_max        | 0.3365457    |
| Train/Action_std        | 0.39191344   |
| Train/Entropy           | 0.16130966   |
| Train/Entropy_Loss      | -0.000161    |
| Train/Entropy_loss      | -0.000161    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.36895618  |
| Train/Loss              | -0.054109044 |
| Train/PolicyClip        | 0.010107956  |
| Train/Policy_loss       | -0.061226483 |
| Train/Ratio             | 1.0016594    |
| Train/Return            | 1.1600875    |
| Train/V                 | 1.1117482    |
| Train/Value             | 1.1117482    |
| Train/control_penalty   | 0.72787493   |
| Train/policy_loss       | -0.061226483 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 741 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 740           |
| Time/Actor_Time         | 0.0846        |
| Time/B_Format_Time      | 0.0718        |
| Time/B_Original_Form... | 0.0711        |
| Time/Buffer             | 0.00243       |
| Time/Critic_Time        | 1.19e-06      |
| Train/Action_abs_mean   | 0.2922719     |
| Train/Action_magnitu... | 0.9558088     |
| Train/Action_magnitude  | 0.76147306    |
| Train/Action_max        | 0.37633678    |
| Train/Action_std        | 0.43268305    |
| Train/Entropy           | 0.28861496    |
| Train/Entropy_Loss      | -0.000289     |
| Train/Entropy_loss      | -0.000289     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.57601535   |
| Train/Loss              | -0.0018087239 |
| Train/PolicyClip        | 0.008172881   |
| Train/Policy_loss       | -0.009188131  |
| Train/Ratio             | 0.996163      |
| Train/Return            | 0.99198335    |
| Train/V                 | 0.9919357     |
| Train/Value             | 0.9919357     |
| Train/control_penalty   | 0.76680225    |
| Train/policy_loss       | -0.009188131  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0185        |
-------------------------------------------

 ---------------- Iteration 742 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 741          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.074        |
| Time/Buffer             | 0.00356      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2989885    |
| Train/Action_magnitu... | 0.9038453    |
| Train/Action_magnitude  | 0.72212404   |
| Train/Action_max        | 0.35400653   |
| Train/Action_std        | 0.38483107   |
| Train/Entropy           | 0.16119318   |
| Train/Entropy_Loss      | -0.000161    |
| Train/Entropy_loss      | -0.000161    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.38645408  |
| Train/Loss              | -0.009432478 |
| Train/PolicyClip        | 0.012176694  |
| Train/Policy_loss       | -0.016495092 |
| Train/Ratio             | 0.9875329    |
| Train/Return            | 1.098742     |
| Train/V                 | 1.0959533    |
| Train/Value             | 1.0959533    |
| Train/control_penalty   | 0.72238076   |
| Train/policy_loss       | -0.016495092 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 743 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 742          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0731       |
| Time/B_Original_Form... | 0.0724       |
| Time/Buffer             | 0.00347      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28080407   |
| Train/Action_magnitu... | 0.91819793   |
| Train/Action_magnitude  | 0.7321277    |
| Train/Action_max        | 0.39705124   |
| Train/Action_std        | 0.4032506    |
| Train/Entropy           | 0.24319117   |
| Train/Entropy_Loss      | -0.000243    |
| Train/Entropy_loss      | -0.000243    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5403093   |
| Train/Loss              | -0.012801759 |
| Train/PolicyClip        | 0.0106062675 |
| Train/Policy_loss       | -0.01961915  |
| Train/Ratio             | 0.99738735   |
| Train/Return            | 1.0281487    |
| Train/V                 | 1.018367     |
| Train/Value             | 1.018367     |
| Train/control_penalty   | 0.7060582    |
| Train/policy_loss       | -0.01961915  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 744 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 743          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0708       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00243      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29513574   |
| Train/Action_magnitu... | 0.9064659    |
| Train/Action_magnitude  | 0.72332615   |
| Train/Action_max        | 0.37568888   |
| Train/Action_std        | 0.3954791    |
| Train/Entropy           | 0.20618615   |
| Train/Entropy_Loss      | -0.000206    |
| Train/Entropy_loss      | -0.000206    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.49099702  |
| Train/Loss              | 0.018022163  |
| Train/PolicyClip        | 0.0076498464 |
| Train/Policy_loss       | 0.011159285  |
| Train/Ratio             | 0.9914377    |
| Train/Return            | 1.0376216    |
| Train/V                 | 1.0589978    |
| Train/Value             | 1.0589978    |
| Train/control_penalty   | 0.7069065    |
| Train/policy_loss       | 0.011159285  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0145       |
------------------------------------------

 ---------------- Iteration 745 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 744         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0725      |
| Time/Buffer             | 0.00326     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30091813  |
| Train/Action_magnitu... | 0.9169138   |
| Train/Action_magnitude  | 0.7278058   |
| Train/Action_max        | 0.3286299   |
| Train/Action_std        | 0.39338616  |
| Train/Entropy           | 0.19237317  |
| Train/Entropy_Loss      | -0.000192   |
| Train/Entropy_loss      | -0.000192   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.4259173  |
| Train/Loss              | -0.0629289  |
| Train/PolicyClip        | 0.009537761 |
| Train/Policy_loss       | -0.07003658 |
| Train/Ratio             | 1.0078934   |
| Train/Return            | 1.2722924   |
| Train/V                 | 1.2126527   |
| Train/Value             | 1.2126527   |
| Train/control_penalty   | 0.7300058   |
| Train/policy_loss       | -0.07003658 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 746 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 745          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0703       |
| Time/B_Original_Form... | 0.07         |
| Time/Buffer             | 0.00239      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2927991    |
| Train/Action_magnitu... | 0.8642025    |
| Train/Action_magnitude  | 0.6840253    |
| Train/Action_max        | 0.32699993   |
| Train/Action_std        | 0.3442867    |
| Train/Entropy           | 0.05609845   |
| Train/Entropy_Loss      | -5.61e-05    |
| Train/Entropy_loss      | -5.61e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.15271169  |
| Train/Loss              | -0.05052578  |
| Train/PolicyClip        | 0.009887077  |
| Train/Policy_loss       | -0.057172004 |
| Train/Ratio             | 0.998093     |
| Train/Return            | 1.0538677    |
| Train/V                 | 1.0023118    |
| Train/Value             | 1.0023118    |
| Train/control_penalty   | 0.67023224   |
| Train/policy_loss       | -0.057172004 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 747 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 746          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0724       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00281      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30605444   |
| Train/Action_magnitu... | 0.9260065    |
| Train/Action_magnitude  | 0.7310757    |
| Train/Action_max        | 0.3351437    |
| Train/Action_std        | 0.41952014   |
| Train/Entropy           | 0.22800547   |
| Train/Entropy_Loss      | -0.000228    |
| Train/Entropy_loss      | -0.000228    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.44129315  |
| Train/Loss              | -0.005116416 |
| Train/PolicyClip        | 0.012563818  |
| Train/Policy_loss       | -0.012435183 |
| Train/Ratio             | 0.9995192    |
| Train/Return            | 0.949383     |
| Train/V                 | 0.94909996   |
| Train/Value             | 0.94909996   |
| Train/control_penalty   | 0.75467724   |
| Train/policy_loss       | -0.012435183 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0185       |
------------------------------------------

 ---------------- Iteration 748 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 747         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.0749      |
| Time/B_Original_Form... | 0.071       |
| Time/Buffer             | 0.00296     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2800781   |
| Train/Action_magnitu... | 0.8657671   |
| Train/Action_magnitude  | 0.6901782   |
| Train/Action_max        | 0.3299912   |
| Train/Action_std        | 0.38477486  |
| Train/Entropy           | 0.16536865  |
| Train/Entropy_Loss      | -0.000165   |
| Train/Entropy_loss      | -0.000165   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.34154347 |
| Train/Loss              | 0.037198048 |
| Train/PolicyClip        | 0.012817398 |
| Train/Policy_loss       | 0.030410344 |
| Train/Ratio             | 0.9741973   |
| Train/Return            | 1.0701321   |
| Train/V                 | 1.1123008   |
| Train/Value             | 1.1123008   |
| Train/control_penalty   | 0.6953075   |
| Train/policy_loss       | 0.030410344 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 749 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 748          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.0715       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00319      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28523982   |
| Train/Action_magnitu... | 0.936962     |
| Train/Action_magnitude  | 0.7410923    |
| Train/Action_max        | 0.34958616   |
| Train/Action_std        | 0.40994793   |
| Train/Entropy           | 0.22225876   |
| Train/Entropy_Loss      | -0.000222    |
| Train/Entropy_loss      | -0.000222    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5739386   |
| Train/Loss              | -0.016010024 |
| Train/PolicyClip        | 0.015775668  |
| Train/Policy_loss       | -0.023012472 |
| Train/Ratio             | 0.99417317   |
| Train/Return            | 1.0517778    |
| Train/V                 | 1.0395757    |
| Train/Value             | 1.0395757    |
| Train/control_penalty   | 0.7224707    |
| Train/policy_loss       | -0.023012472 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.019        |
------------------------------------------

 ---------------- Iteration 750 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 749         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0715      |
| Time/Buffer             | 0.00247     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.27911368  |
| Train/Action_magnitu... | 0.9013978   |
| Train/Action_magnitude  | 0.7175295   |
| Train/Action_max        | 0.34803975  |
| Train/Action_std        | 0.42056334  |
| Train/Entropy           | 0.25441808  |
| Train/Entropy_Loss      | -0.000254   |
| Train/Entropy_loss      | -0.000254   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5206438  |
| Train/Loss              | 0.044653136 |
| Train/PolicyClip        | 0.00728157  |
| Train/Policy_loss       | 0.03762725  |
| Train/Ratio             | 0.9821624   |
| Train/Return            | 1.1681745   |
| Train/V                 | 1.2174346   |
| Train/Value             | 1.2174346   |
| Train/control_penalty   | 0.7280305   |
| Train/policy_loss       | 0.03762725  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02        |
-----------------------------------------

 ---------------- Iteration 751 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 750          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00355      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2787885    |
| Train/Action_magnitu... | 0.8806487    |
| Train/Action_magnitude  | 0.69822514   |
| Train/Action_max        | 0.33797804   |
| Train/Action_std        | 0.36120003   |
| Train/Entropy           | 0.109333426  |
| Train/Entropy_Loss      | -0.000109    |
| Train/Entropy_loss      | -0.000109    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.34737816  |
| Train/Loss              | -0.02046536  |
| Train/PolicyClip        | 0.010711843  |
| Train/Policy_loss       | -0.027105946 |
| Train/Ratio             | 0.9956509    |
| Train/Return            | 1.2402464    |
| Train/V                 | 1.2214516    |
| Train/Value             | 1.2214516    |
| Train/control_penalty   | 0.6749921    |
| Train/policy_loss       | -0.027105946 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02475      |
------------------------------------------

 ---------------- Iteration 752 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 751           |
| Time/Actor_Time         | 0.087         |
| Time/B_Format_Time      | 0.0702        |
| Time/B_Original_Form... | 0.0713        |
| Time/Buffer             | 0.00357       |
| Time/Critic_Time        | 1.19e-06      |
| Train/Action_abs_mean   | 0.290366      |
| Train/Action_magnitu... | 0.90031606    |
| Train/Action_magnitude  | 0.71201104    |
| Train/Action_max        | 0.34680828    |
| Train/Action_std        | 0.39106715    |
| Train/Entropy           | 0.18383297    |
| Train/Entropy_Loss      | -0.000184     |
| Train/Entropy_loss      | -0.000184     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.45949852   |
| Train/Loss              | 0.005232467   |
| Train/PolicyClip        | 0.01014389    |
| Train/Policy_loss       | -0.0017358304 |
| Train/Ratio             | 0.9910182     |
| Train/Return            | 1.1614231     |
| Train/V                 | 1.1715412     |
| Train/Value             | 1.1715412     |
| Train/control_penalty   | 0.71521306    |
| Train/policy_loss       | -0.0017358304 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.01975       |
-------------------------------------------

 ---------------- Iteration 753 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 752         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00268     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29394844  |
| Train/Action_magnitu... | 0.8574829   |
| Train/Action_magnitude  | 0.6794903   |
| Train/Action_max        | 0.3496524   |
| Train/Action_std        | 0.37233704  |
| Train/Entropy           | 0.13039559  |
| Train/Entropy_Loss      | -0.00013    |
| Train/Entropy_loss      | -0.00013    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.29600477 |
| Train/Loss              | 0.04254869  |
| Train/PolicyClip        | 0.005547568 |
| Train/Policy_loss       | 0.035692595 |
| Train/Ratio             | 0.98851335  |
| Train/Return            | 1.2144899   |
| Train/V                 | 1.2606847   |
| Train/Value             | 1.2606847   |
| Train/control_penalty   | 0.6986492   |
| Train/policy_loss       | 0.035692595 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 754 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 753         |
| Time/Actor_Time         | 0.0865      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.28337362  |
| Train/Action_magnitu... | 0.836929    |
| Train/Action_magnitude  | 0.6653239   |
| Train/Action_max        | 0.31575292  |
| Train/Action_std        | 0.36080626  |
| Train/Entropy           | 0.12795967  |
| Train/Entropy_Loss      | -0.000128   |
| Train/Entropy_loss      | -0.000128   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.2222684  |
| Train/Loss              | 0.016755395 |
| Train/PolicyClip        | 0.00832032  |
| Train/Policy_loss       | 0.010158573 |
| Train/Ratio             | 0.99092996  |
| Train/Return            | 1.3535848   |
| Train/V                 | 1.3752608   |
| Train/Value             | 1.3752608   |
| Train/control_penalty   | 0.6724781   |
| Train/policy_loss       | 0.010158573 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 755 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 754          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0713       |
| Time/B_Original_Form... | 0.071        |
| Time/Buffer             | 0.00314      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2920789    |
| Train/Action_magnitu... | 0.87969536   |
| Train/Action_magnitude  | 0.6979639    |
| Train/Action_max        | 0.37390316   |
| Train/Action_std        | 0.3614035    |
| Train/Entropy           | 0.12888904   |
| Train/Entropy_Loss      | -0.000129    |
| Train/Entropy_loss      | -0.000129    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.3378265   |
| Train/Loss              | -0.010640069 |
| Train/PolicyClip        | 0.011640918  |
| Train/Policy_loss       | -0.017290901 |
| Train/Ratio             | 0.9975464    |
| Train/Return            | 1.29157      |
| Train/V                 | 1.2858919    |
| Train/Value             | 1.2858919    |
| Train/control_penalty   | 0.6779722    |
| Train/policy_loss       | -0.017290901 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 756 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 755         |
| Time/Actor_Time         | 0.0839      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2907383   |
| Train/Action_magnitu... | 0.853287    |
| Train/Action_magnitude  | 0.67736846  |
| Train/Action_max        | 0.36355048  |
| Train/Action_std        | 0.35707375  |
| Train/Entropy           | 0.12400379  |
| Train/Entropy_Loss      | -0.000124   |
| Train/Entropy_loss      | -0.000124   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.29204658 |
| Train/Loss              | 0.022185953 |
| Train/PolicyClip        | 0.010322221 |
| Train/Policy_loss       | 0.015539735 |
| Train/Ratio             | 0.9929016   |
| Train/Return            | 1.3246697   |
| Train/V                 | 1.3493855   |
| Train/Value             | 1.3493855   |
| Train/control_penalty   | 0.6770223   |
| Train/policy_loss       | 0.015539735 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 757 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 756         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0711      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31131706  |
| Train/Action_magnitu... | 0.8608311   |
| Train/Action_magnitude  | 0.68297064  |
| Train/Action_max        | 0.37119752  |
| Train/Action_std        | 0.35553554  |
| Train/Entropy           | 0.12104178  |
| Train/Entropy_Loss      | -0.000121   |
| Train/Entropy_loss      | -0.000121   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.28066203 |
| Train/Loss              | 0.027609918 |
| Train/PolicyClip        | 0.009952741 |
| Train/Policy_loss       | 0.020843472 |
| Train/Ratio             | 0.98487693  |
| Train/Return            | 1.3308746   |
| Train/V                 | 1.3640945   |
| Train/Value             | 1.3640945   |
| Train/control_penalty   | 0.68874884  |
| Train/policy_loss       | 0.020843472 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 758 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 757          |
| Time/Actor_Time         | 0.083        |
| Time/B_Format_Time      | 0.0725       |
| Time/B_Original_Form... | 0.0735       |
| Time/Buffer             | 0.00362      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28446832   |
| Train/Action_magnitu... | 0.8459111    |
| Train/Action_magnitude  | 0.66799796   |
| Train/Action_max        | 0.36170074   |
| Train/Action_std        | 0.3639701    |
| Train/Entropy           | 0.13495615   |
| Train/Entropy_Loss      | -0.000135    |
| Train/Entropy_loss      | -0.000135    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.29046208  |
| Train/Loss              | -0.043284353 |
| Train/PolicyClip        | 0.010558953  |
| Train/Policy_loss       | -0.049918056 |
| Train/Ratio             | 0.9933703    |
| Train/Return            | 1.3457308    |
| Train/V                 | 1.3067038    |
| Train/Value             | 1.3067038    |
| Train/control_penalty   | 0.6768658    |
| Train/policy_loss       | -0.049918056 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02975      |
------------------------------------------

 ---------------- Iteration 759 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 758          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0736       |
| Time/B_Original_Form... | 0.0732       |
| Time/Buffer             | 0.00248      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28837252   |
| Train/Action_magnitu... | 0.8878566    |
| Train/Action_magnitude  | 0.7024887    |
| Train/Action_max        | 0.37383586   |
| Train/Action_std        | 0.38720626   |
| Train/Entropy           | 0.24666423   |
| Train/Entropy_Loss      | -0.000247    |
| Train/Entropy_loss      | -0.000247    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.49695662  |
| Train/Loss              | 0.028826652  |
| Train/PolicyClip        | 0.0061459574 |
| Train/Policy_loss       | 0.022079969  |
| Train/Ratio             | 0.9812679    |
| Train/Return            | 1.0851724    |
| Train/V                 | 1.1208779    |
| Train/Value             | 1.1208779    |
| Train/control_penalty   | 0.6993348    |
| Train/policy_loss       | 0.022079969  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01625      |
------------------------------------------

 ---------------- Iteration 760 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 759          |
| Time/Actor_Time         | 0.0842       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0731       |
| Time/Buffer             | 0.00267      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30657598   |
| Train/Action_magnitu... | 0.8981121    |
| Train/Action_magnitude  | 0.7119846    |
| Train/Action_max        | 0.38002196   |
| Train/Action_std        | 0.3980243    |
| Train/Entropy           | 0.24558374   |
| Train/Entropy_Loss      | -0.000246    |
| Train/Entropy_loss      | -0.000246    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.47094074  |
| Train/Loss              | -0.008231882 |
| Train/PolicyClip        | 0.012323699  |
| Train/Policy_loss       | -0.015266963 |
| Train/Ratio             | 0.99562      |
| Train/Return            | 1.2786987    |
| Train/V                 | 1.2750514    |
| Train/Value             | 1.2750514    |
| Train/control_penalty   | 0.72806656   |
| Train/policy_loss       | -0.015266963 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 761 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 760           |
| Time/Actor_Time         | 0.0831        |
| Time/B_Format_Time      | 0.0721        |
| Time/B_Original_Form... | 0.071         |
| Time/Buffer             | 0.00242       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.30846864    |
| Train/Action_magnitu... | 0.9557677     |
| Train/Action_magnitude  | 0.76155293    |
| Train/Action_max        | 0.45084947    |
| Train/Action_std        | 0.4066987     |
| Train/Entropy           | 0.27134797    |
| Train/Entropy_Loss      | -0.000271     |
| Train/Entropy_loss      | -0.000271     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.61797464   |
| Train/Loss              | 0.00541854    |
| Train/PolicyClip        | 0.014415501   |
| Train/Policy_loss       | -0.0017691099 |
| Train/Ratio             | 0.9952305     |
| Train/Return            | 1.2740229     |
| Train/V                 | 1.284558      |
| Train/Value             | 1.284558      |
| Train/control_penalty   | 0.7458998     |
| Train/policy_loss       | -0.0017691099 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02325       |
-------------------------------------------

 ---------------- Iteration 762 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 761          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00272      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31190082   |
| Train/Action_magnitu... | 0.9216167    |
| Train/Action_magnitude  | 0.73219216   |
| Train/Action_max        | 0.41181168   |
| Train/Action_std        | 0.394119     |
| Train/Entropy           | 0.24567811   |
| Train/Entropy_Loss      | -0.000246    |
| Train/Entropy_loss      | -0.000246    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6516091   |
| Train/Loss              | 0.021433445  |
| Train/PolicyClip        | 0.014158258  |
| Train/Policy_loss       | 0.0141573325 |
| Train/Ratio             | 0.98442805   |
| Train/Return            | 1.3382599    |
| Train/V                 | 1.3651227    |
| Train/Value             | 1.3651227    |
| Train/control_penalty   | 0.7521791    |
| Train/policy_loss       | 0.0141573325 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02175      |
------------------------------------------

 ---------------- Iteration 763 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 762          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00366      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29912007   |
| Train/Action_magnitu... | 0.8687728    |
| Train/Action_magnitude  | 0.6893865    |
| Train/Action_max        | 0.3761744    |
| Train/Action_std        | 0.34936163   |
| Train/Entropy           | 0.13332632   |
| Train/Entropy_Loss      | -0.000133    |
| Train/Entropy_loss      | -0.000133    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.34070063  |
| Train/Loss              | -0.025858749 |
| Train/PolicyClip        | 0.011519761  |
| Train/Policy_loss       | -0.03256452  |
| Train/Ratio             | 0.99760556   |
| Train/Return            | 1.4903288    |
| Train/V                 | 1.4696835    |
| Train/Value             | 1.4696835    |
| Train/control_penalty   | 0.68391013   |
| Train/policy_loss       | -0.03256452  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03         |
------------------------------------------

 ---------------- Iteration 764 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 763         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0702      |
| Time/Buffer             | 0.00256     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.29804558  |
| Train/Action_magnitu... | 0.93812627  |
| Train/Action_magnitude  | 0.73903733  |
| Train/Action_max        | 0.36747265  |
| Train/Action_std        | 0.4028146   |
| Train/Entropy           | 0.2427236   |
| Train/Entropy_Loss      | -0.000243   |
| Train/Entropy_loss      | -0.000243   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5472919  |
| Train/Loss              | 0.04371981  |
| Train/PolicyClip        | 0.012440853 |
| Train/Policy_loss       | 0.036771998 |
| Train/Ratio             | 0.9809183   |
| Train/Return            | 1.2233427   |
| Train/V                 | 1.2694404   |
| Train/Value             | 1.2694404   |
| Train/control_penalty   | 0.71905357  |
| Train/policy_loss       | 0.036771998 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 765 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 764           |
| Time/Actor_Time         | 0.0865        |
| Time/B_Format_Time      | 0.0741        |
| Time/B_Original_Form... | 0.073         |
| Time/Buffer             | 0.00286       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31168434    |
| Train/Action_magnitu... | 0.94088745    |
| Train/Action_magnitude  | 0.74550956    |
| Train/Action_max        | 0.33554065    |
| Train/Action_std        | 0.42587557    |
| Train/Entropy           | 0.33400953    |
| Train/Entropy_Loss      | -0.000334     |
| Train/Entropy_loss      | -0.000334     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6890377    |
| Train/Loss              | 0.0009304625  |
| Train/PolicyClip        | 0.011803422   |
| Train/Policy_loss       | -0.0063683013 |
| Train/Ratio             | 0.9890418     |
| Train/Return            | 1.2600771     |
| Train/V                 | 1.264606      |
| Train/Value             | 1.264606      |
| Train/control_penalty   | 0.76327735    |
| Train/policy_loss       | -0.0063683013 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0245        |
-------------------------------------------

 ---------------- Iteration 766 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 765           |
| Time/Actor_Time         | 0.087         |
| Time/B_Format_Time      | 0.073         |
| Time/B_Original_Form... | 0.0726        |
| Time/Buffer             | 0.00359       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3108411     |
| Train/Action_magnitu... | 0.902469      |
| Train/Action_magnitude  | 0.7138235     |
| Train/Action_max        | 0.34791842    |
| Train/Action_std        | 0.36163688    |
| Train/Entropy           | 0.1723232     |
| Train/Entropy_Loss      | -0.000172     |
| Train/Entropy_loss      | -0.000172     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.47548753   |
| Train/Loss              | -0.00648005   |
| Train/PolicyClip        | 0.010933399   |
| Train/Policy_loss       | -0.0131358495 |
| Train/Ratio             | 0.99359       |
| Train/Return            | 1.2545741     |
| Train/V                 | 1.2506577     |
| Train/Value             | 1.2506577     |
| Train/control_penalty   | 0.68281233    |
| Train/policy_loss       | -0.0131358495 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.028         |
-------------------------------------------

 ---------------- Iteration 767 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 766          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0717       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.31988993   |
| Train/Action_magnitu... | 0.91856575   |
| Train/Action_magnitude  | 0.72259015   |
| Train/Action_max        | 0.3385124    |
| Train/Action_std        | 0.3881852    |
| Train/Entropy           | 0.26478988   |
| Train/Entropy_Loss      | -0.000265    |
| Train/Entropy_loss      | -0.000265    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.590187    |
| Train/Loss              | -0.006258625 |
| Train/PolicyClip        | 0.012001659  |
| Train/Policy_loss       | -0.013164448 |
| Train/Ratio             | 0.99490166   |
| Train/Return            | 1.1108547    |
| Train/V                 | 1.1106815    |
| Train/Value             | 1.1106815    |
| Train/control_penalty   | 0.7170613    |
| Train/policy_loss       | -0.013164448 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 768 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 767          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0712       |
| Time/B_Original_Form... | 0.0702       |
| Time/Buffer             | 0.00283      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3051232    |
| Train/Action_magnitu... | 0.87002033   |
| Train/Action_magnitude  | 0.68637717   |
| Train/Action_max        | 0.33455217   |
| Train/Action_std        | 0.3454031    |
| Train/Entropy           | 0.11910347   |
| Train/Entropy_Loss      | -0.000119    |
| Train/Entropy_loss      | -0.000119    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.29966328  |
| Train/Loss              | -0.010120469 |
| Train/PolicyClip        | 0.01575667   |
| Train/Policy_loss       | -0.0167378   |
| Train/Ratio             | 1.0007427    |
| Train/Return            | 1.07795      |
| Train/V                 | 1.0729747    |
| Train/Value             | 1.0729747    |
| Train/control_penalty   | 0.6736434    |
| Train/policy_loss       | -0.0167378   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 769 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 768           |
| Time/Actor_Time         | 0.0883        |
| Time/B_Format_Time      | 0.0801        |
| Time/B_Original_Form... | 0.0784        |
| Time/Buffer             | 0.00279       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.32845783    |
| Train/Action_magnitu... | 0.85857326    |
| Train/Action_magnitude  | 0.6729714     |
| Train/Action_max        | 0.31374693    |
| Train/Action_std        | 0.34255394    |
| Train/Entropy           | 0.1650691     |
| Train/Entropy_Loss      | -0.000165     |
| Train/Entropy_loss      | -0.000165     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.31917688   |
| Train/Loss              | -0.0054028905 |
| Train/PolicyClip        | 0.017703738   |
| Train/Policy_loss       | -0.01211322   |
| Train/Ratio             | 0.9676922     |
| Train/Return            | 1.1463358     |
| Train/V                 | 1.1522083     |
| Train/Value             | 1.1522083     |
| Train/control_penalty   | 0.68753994    |
| Train/policy_loss       | -0.01211322   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02          |
-------------------------------------------

 ---------------- Iteration 770 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 769         |
| Time/Actor_Time         | 0.0835      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30832464  |
| Train/Action_magnitu... | 0.8690583   |
| Train/Action_magnitude  | 0.6855193   |
| Train/Action_max        | 0.3318847   |
| Train/Action_std        | 0.34543756  |
| Train/Entropy           | 0.15327962  |
| Train/Entropy_Loss      | -0.000153   |
| Train/Entropy_loss      | -0.000153   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.38728857 |
| Train/Loss              | 0.02773036  |
| Train/PolicyClip        | 0.01331482  |
| Train/Policy_loss       | 0.021182667 |
| Train/Ratio             | 0.9805408   |
| Train/Return            | 0.99558556  |
| Train/V                 | 1.0306535   |
| Train/Value             | 1.0306535   |
| Train/control_penalty   | 0.6700973   |
| Train/policy_loss       | 0.021182667 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 771 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 770          |
| Time/Actor_Time         | 0.0896       |
| Time/B_Format_Time      | 0.075        |
| Time/B_Original_Form... | 0.0751       |
| Time/Buffer             | 0.00284      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32514843   |
| Train/Action_magnitu... | 0.8645558    |
| Train/Action_magnitude  | 0.67818487   |
| Train/Action_max        | 0.33661878   |
| Train/Action_std        | 0.3430779    |
| Train/Entropy           | 0.14657193   |
| Train/Entropy_Loss      | -0.000147    |
| Train/Entropy_loss      | -0.000147    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.33988276  |
| Train/Loss              | -0.017085794 |
| Train/PolicyClip        | 0.012829804  |
| Train/Policy_loss       | -0.02379507  |
| Train/Ratio             | 0.9860647    |
| Train/Return            | 1.0795128    |
| Train/V                 | 1.0684941    |
| Train/Value             | 1.0684941    |
| Train/control_penalty   | 0.68558484   |
| Train/policy_loss       | -0.02379507  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.022        |
------------------------------------------

 ---------------- Iteration 772 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 771          |
| Time/Actor_Time         | 0.0897       |
| Time/B_Format_Time      | 0.075        |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00339      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31747442   |
| Train/Action_magnitu... | 0.82094216   |
| Train/Action_magnitude  | 0.6462003    |
| Train/Action_max        | 0.291767     |
| Train/Action_std        | 0.29873562   |
| Train/Entropy           | 0.021845255  |
| Train/Entropy_Loss      | -2.18e-05    |
| Train/Entropy_loss      | -2.18e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.18056284  |
| Train/Loss              | -0.019075297 |
| Train/PolicyClip        | 0.019964559  |
| Train/Policy_loss       | -0.025464214 |
| Train/Ratio             | 1.0007789    |
| Train/Return            | 1.2693572    |
| Train/V                 | 1.257263     |
| Train/Value             | 1.257263     |
| Train/control_penalty   | 0.64107627   |
| Train/policy_loss       | -0.025464214 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03         |
------------------------------------------

 ---------------- Iteration 773 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 772          |
| Time/Actor_Time         | 0.0831       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0722       |
| Time/Buffer             | 0.00354      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3207444    |
| Train/Action_magnitu... | 0.81057847   |
| Train/Action_magnitude  | 0.6393977    |
| Train/Action_max        | 0.28379443   |
| Train/Action_std        | 0.3017047    |
| Train/Entropy           | 0.018241033  |
| Train/Entropy_Loss      | -1.82e-05    |
| Train/Entropy_loss      | -1.82e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.032111187 |
| Train/Loss              | -0.094667874 |
| Train/PolicyClip        | 0.009471089  |
| Train/Policy_loss       | -0.10117445  |
| Train/Ratio             | 1.0110343    |
| Train/Return            | 1.4137073    |
| Train/V                 | 1.3184828    |
| Train/Value             | 1.3184828    |
| Train/control_penalty   | 0.6524819    |
| Train/policy_loss       | -0.10117445  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03475      |
------------------------------------------

 ---------------- Iteration 774 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 773           |
| Time/Actor_Time         | 0.0859        |
| Time/B_Format_Time      | 0.0724        |
| Time/B_Original_Form... | 0.0706        |
| Time/Buffer             | 0.00274       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.32248148    |
| Train/Action_magnitu... | 0.8281194     |
| Train/Action_magnitude  | 0.65103906    |
| Train/Action_max        | 0.31369796    |
| Train/Action_std        | 0.29121622    |
| Train/Entropy           | -0.008445039  |
| Train/Entropy_Loss      | 8.45e-06      |
| Train/Entropy_loss      | 8.45e-06      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.0116005    |
| Train/Loss              | -0.0005427799 |
| Train/PolicyClip        | 0.008795126   |
| Train/Policy_loss       | -0.007022379  |
| Train/Ratio             | 0.99115324    |
| Train/Return            | 1.2946726     |
| Train/V                 | 1.3021094     |
| Train/Value             | 1.3021094     |
| Train/control_penalty   | 0.6471154     |
| Train/policy_loss       | -0.007022379  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.025         |
-------------------------------------------

 ---------------- Iteration 775 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 774          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0728       |
| Time/B_Original_Form... | 0.072        |
| Time/Buffer             | 0.00288      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31184834   |
| Train/Action_magnitu... | 0.78346056   |
| Train/Action_magnitude  | 0.61816937   |
| Train/Action_max        | 0.32057363   |
| Train/Action_std        | 0.27996027   |
| Train/Entropy           | -0.046318945 |
| Train/Entropy_Loss      | 4.63e-05     |
| Train/Entropy_loss      | 4.63e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | 0.09406659   |
| Train/Loss              | -0.031448025 |
| Train/PolicyClip        | 0.009344858  |
| Train/Policy_loss       | -0.037608486 |
| Train/Ratio             | 0.99782014   |
| Train/Return            | 1.2757845    |
| Train/V                 | 1.2484225    |
| Train/Value             | 1.2484225    |
| Train/control_penalty   | 0.61141396   |
| Train/policy_loss       | -0.037608486 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.029        |
------------------------------------------

 ---------------- Iteration 776 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 775          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0708       |
| Time/Buffer             | 0.00285      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31902266   |
| Train/Action_magnitu... | 0.81919247   |
| Train/Action_magnitude  | 0.6477305    |
| Train/Action_max        | 0.29383376   |
| Train/Action_std        | 0.30800727   |
| Train/Entropy           | 0.045789674  |
| Train/Entropy_Loss      | -4.58e-05    |
| Train/Entropy_loss      | -4.58e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.09659079  |
| Train/Loss              | 0.0071268566 |
| Train/PolicyClip        | 0.009972407  |
| Train/Policy_loss       | 0.0006224289 |
| Train/Ratio             | 0.98820704   |
| Train/Return            | 1.2265695    |
| Train/V                 | 1.237635     |
| Train/Value             | 1.237635     |
| Train/control_penalty   | 0.6550218    |
| Train/policy_loss       | 0.0006224289 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 777 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 776          |
| Time/Actor_Time         | 0.0853       |
| Time/B_Format_Time      | 0.0727       |
| Time/B_Original_Form... | 0.0712       |
| Time/Buffer             | 0.00317      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32471704   |
| Train/Action_magnitu... | 0.8341597    |
| Train/Action_magnitude  | 0.657567     |
| Train/Action_max        | 0.31730375   |
| Train/Action_std        | 0.315215     |
| Train/Entropy           | 0.06833295   |
| Train/Entropy_Loss      | -6.83e-05    |
| Train/Entropy_loss      | -6.83e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.111620255 |
| Train/Loss              | -0.010576883 |
| Train/PolicyClip        | 0.011941843  |
| Train/Policy_loss       | -0.017106455 |
| Train/Ratio             | 0.99811816   |
| Train/Return            | 1.1892195    |
| Train/V                 | 1.1838433    |
| Train/Value             | 1.1838433    |
| Train/control_penalty   | 0.65979046   |
| Train/policy_loss       | -0.017106455 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.022        |
------------------------------------------

 ---------------- Iteration 778 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 777          |
| Time/Actor_Time         | 0.0836       |
| Time/B_Format_Time      | 0.0707       |
| Time/B_Original_Form... | 0.0709       |
| Time/Buffer             | 0.003        |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31017828   |
| Train/Action_magnitu... | 0.8430551    |
| Train/Action_magnitude  | 0.6690146    |
| Train/Action_max        | 0.32309943   |
| Train/Action_std        | 0.32915986   |
| Train/Entropy           | 0.09935069   |
| Train/Entropy_Loss      | -9.94e-05    |
| Train/Entropy_loss      | -9.94e-05    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.21580467  |
| Train/Loss              | 0.01576946   |
| Train/PolicyClip        | 0.008886315  |
| Train/Policy_loss       | 0.0091680465 |
| Train/Ratio             | 0.9839004    |
| Train/Return            | 1.2200116    |
| Train/V                 | 1.239319     |
| Train/Value             | 1.239319     |
| Train/control_penalty   | 0.6700764    |
| Train/policy_loss       | 0.0091680465 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.023        |
------------------------------------------

 ---------------- Iteration 779 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 778         |
| Time/Actor_Time         | 0.0833      |
| Time/B_Format_Time      | 0.0746      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00342     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.3030777   |
| Train/Action_magnitu... | 0.8260119   |
| Train/Action_magnitude  | 0.6543808   |
| Train/Action_max        | 0.32248017  |
| Train/Action_std        | 0.31999174  |
| Train/Entropy           | 0.08885149  |
| Train/Entropy_Loss      | -8.89e-05   |
| Train/Entropy_loss      | -8.89e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.200422   |
| Train/Loss              | 0.012755588 |
| Train/PolicyClip        | 0.012000206 |
| Train/Policy_loss       | 0.006255986 |
| Train/Ratio             | 0.99101746  |
| Train/Return            | 1.2884978   |
| Train/V                 | 1.3092319   |
| Train/Value             | 1.3092319   |
| Train/control_penalty   | 0.6588453   |
| Train/policy_loss       | 0.006255986 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 780 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 779          |
| Time/Actor_Time         | 0.0874       |
| Time/B_Format_Time      | 0.0788       |
| Time/B_Original_Form... | 0.076        |
| Time/Buffer             | 0.00326      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.302051     |
| Train/Action_magnitu... | 0.8181618    |
| Train/Action_magnitude  | 0.6466367    |
| Train/Action_max        | 0.316115     |
| Train/Action_std        | 0.3303316    |
| Train/Entropy           | 0.12106194   |
| Train/Entropy_Loss      | -0.000121    |
| Train/Entropy_loss      | -0.000121    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.21692695  |
| Train/Loss              | -0.023885958 |
| Train/PolicyClip        | 0.011646841  |
| Train/Policy_loss       | -0.030357443 |
| Train/Ratio             | 0.9913722    |
| Train/Return            | 1.2985718    |
| Train/V                 | 1.2839004    |
| Train/Value             | 1.2839004    |
| Train/control_penalty   | 0.6592548    |
| Train/policy_loss       | -0.030357443 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 781 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 780          |
| Time/Actor_Time         | 0.0865       |
| Time/B_Format_Time      | 0.0845       |
| Time/B_Original_Form... | 0.136        |
| Time/Buffer             | 0.00317      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30647576   |
| Train/Action_magnitu... | 0.81850713   |
| Train/Action_magnitude  | 0.64519185   |
| Train/Action_max        | 0.29503492   |
| Train/Action_std        | 0.31906977   |
| Train/Entropy           | 0.089003235  |
| Train/Entropy_Loss      | -8.9e-05     |
| Train/Entropy_loss      | -8.9e-05     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.21424854  |
| Train/Loss              | -0.018593732 |
| Train/PolicyClip        | 0.010582798  |
| Train/Policy_loss       | -0.025028903 |
| Train/Ratio             | 0.99671143   |
| Train/Return            | 1.3064421    |
| Train/V                 | 1.297929     |
| Train/Value             | 1.297929     |
| Train/control_penalty   | 0.65241736   |
| Train/policy_loss       | -0.025028903 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02675      |
------------------------------------------

 ---------------- Iteration 782 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 781         |
| Time/Actor_Time         | 0.0824      |
| Time/B_Format_Time      | 0.0759      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00631     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3162846   |
| Train/Action_magnitu... | 0.86108285  |
| Train/Action_magnitude  | 0.68260396  |
| Train/Action_max        | 0.34047082  |
| Train/Action_std        | 0.32527784  |
| Train/Entropy           | 0.0980578   |
| Train/Entropy_Loss      | -9.81e-05   |
| Train/Entropy_loss      | -9.81e-05   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.24731767 |
| Train/Loss              | 0.021975897 |
| Train/PolicyClip        | 0.009168091 |
| Train/Policy_loss       | 0.015422415 |
| Train/Ratio             | 0.9827198   |
| Train/Return            | 1.2906908   |
| Train/V                 | 1.3188154   |
| Train/Value             | 1.3188154   |
| Train/control_penalty   | 0.66515404  |
| Train/policy_loss       | 0.015422415 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 783 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 782          |
| Time/Actor_Time         | 0.0841       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00299      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3045979    |
| Train/Action_magnitu... | 0.84067136   |
| Train/Action_magnitude  | 0.66635793   |
| Train/Action_max        | 0.35591385   |
| Train/Action_std        | 0.34809467   |
| Train/Entropy           | 0.16797964   |
| Train/Entropy_Loss      | -0.000168    |
| Train/Entropy_loss      | -0.000168    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.37763283  |
| Train/Loss              | -0.005370401 |
| Train/PolicyClip        | 0.009235619  |
| Train/Policy_loss       | -0.011989981 |
| Train/Ratio             | 0.9992387    |
| Train/Return            | 1.0693827    |
| Train/V                 | 1.0693715    |
| Train/Value             | 1.0693715    |
| Train/control_penalty   | 0.678756     |
| Train/policy_loss       | -0.011989981 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 784 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 783          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0771       |
| Time/B_Original_Form... | 0.079        |
| Time/Buffer             | 0.00378      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30913496   |
| Train/Action_magnitu... | 0.8573983    |
| Train/Action_magnitude  | 0.68208385   |
| Train/Action_max        | 0.36942467   |
| Train/Action_std        | 0.34626105   |
| Train/Entropy           | 0.17005712   |
| Train/Entropy_Loss      | -0.00017     |
| Train/Entropy_loss      | -0.00017     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.40530786  |
| Train/Loss              | -0.047419965 |
| Train/PolicyClip        | 0.009871975  |
| Train/Policy_loss       | -0.054068398 |
| Train/Ratio             | 0.99716157   |
| Train/Return            | 1.1099094    |
| Train/V                 | 1.0687734    |
| Train/Value             | 1.0687734    |
| Train/control_penalty   | 0.68184865   |
| Train/policy_loss       | -0.054068398 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.027        |
------------------------------------------

 ---------------- Iteration 785 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 784          |
| Time/Actor_Time         | 0.103        |
| Time/B_Format_Time      | 0.171        |
| Time/B_Original_Form... | 0.147        |
| Time/Buffer             | 0.00497      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30282217   |
| Train/Action_magnitu... | 0.860648     |
| Train/Action_magnitude  | 0.68081236   |
| Train/Action_max        | 0.37321436   |
| Train/Action_std        | 0.36056623   |
| Train/Entropy           | 0.20740664   |
| Train/Entropy_Loss      | -0.000207    |
| Train/Entropy_loss      | -0.000207    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.42145613  |
| Train/Loss              | -0.01764202  |
| Train/PolicyClip        | 0.01314266   |
| Train/Policy_loss       | -0.024296712 |
| Train/Ratio             | 0.9958571    |
| Train/Return            | 1.0468044    |
| Train/V                 | 1.0394641    |
| Train/Value             | 1.0394641    |
| Train/control_penalty   | 0.6862101    |
| Train/policy_loss       | -0.024296712 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0235       |
------------------------------------------

 ---------------- Iteration 786 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 785         |
| Time/Actor_Time         | 0.0835      |
| Time/B_Format_Time      | 0.105       |
| Time/B_Original_Form... | 0.116       |
| Time/Buffer             | 0.00362     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.311778    |
| Train/Action_magnitu... | 0.8577777   |
| Train/Action_magnitude  | 0.6769535   |
| Train/Action_max        | 0.38934398  |
| Train/Action_std        | 0.36696282  |
| Train/Entropy           | 0.2099928   |
| Train/Entropy_Loss      | -0.00021    |
| Train/Entropy_loss      | -0.00021    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.40225405 |
| Train/Loss              | 0.028518574 |
| Train/PolicyClip        | 0.011856994 |
| Train/Policy_loss       | 0.021630792 |
| Train/Ratio             | 0.9752855   |
| Train/Return            | 0.92354125  |
| Train/V                 | 0.9576552   |
| Train/Value             | 0.9576552   |
| Train/control_penalty   | 0.7097776   |
| Train/policy_loss       | 0.021630792 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0155      |
-----------------------------------------

 ---------------- Iteration 787 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 786          |
| Time/Actor_Time         | 0.0838       |
| Time/B_Format_Time      | 0.0745       |
| Time/B_Original_Form... | 0.0728       |
| Time/Buffer             | 0.00378      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.320165     |
| Train/Action_magnitu... | 0.8620347    |
| Train/Action_magnitude  | 0.68240523   |
| Train/Action_max        | 0.37204814   |
| Train/Action_std        | 0.36633834   |
| Train/Entropy           | 0.21322717   |
| Train/Entropy_Loss      | -0.000213    |
| Train/Entropy_loss      | -0.000213    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.42459106  |
| Train/Loss              | -0.05066537  |
| Train/PolicyClip        | 0.011866251  |
| Train/Policy_loss       | -0.057512425 |
| Train/Ratio             | 0.99029446   |
| Train/Return            | 1.240382     |
| Train/V                 | 1.1967002    |
| Train/Value             | 1.1967002    |
| Train/control_penalty   | 0.7060282    |
| Train/policy_loss       | -0.057512425 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03275      |
------------------------------------------

 ---------------- Iteration 788 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 787          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0722       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.0032       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31518337   |
| Train/Action_magnitu... | 0.9238986    |
| Train/Action_magnitude  | 0.7312084    |
| Train/Action_max        | 0.3917739    |
| Train/Action_std        | 0.40045398   |
| Train/Entropy           | 0.28736842   |
| Train/Entropy_Loss      | -0.000287    |
| Train/Entropy_loss      | -0.000287    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.60866517  |
| Train/Loss              | 0.0230065    |
| Train/PolicyClip        | 0.0089513045 |
| Train/Policy_loss       | 0.015910238  |
| Train/Ratio             | 0.9921866    |
| Train/Return            | 1.0903771    |
| Train/V                 | 1.1180881    |
| Train/Value             | 1.1180881    |
| Train/control_penalty   | 0.7383632    |
| Train/policy_loss       | 0.015910238  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02075      |
------------------------------------------

 ---------------- Iteration 789 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 788           |
| Time/Actor_Time         | 0.0859        |
| Time/B_Format_Time      | 0.0742        |
| Time/B_Original_Form... | 0.074         |
| Time/Buffer             | 0.00321       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.30249524    |
| Train/Action_magnitu... | 0.906137      |
| Train/Action_magnitude  | 0.72161406    |
| Train/Action_max        | 0.39454827    |
| Train/Action_std        | 0.42640382    |
| Train/Entropy           | 0.32060313    |
| Train/Entropy_Loss      | -0.000321     |
| Train/Entropy_loss      | -0.000321     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.5339168    |
| Train/Loss              | -0.0029090154 |
| Train/PolicyClip        | 0.011795091   |
| Train/Policy_loss       | -0.010219645  |
| Train/Ratio             | 0.993162      |
| Train/Return            | 1.2075801     |
| Train/V                 | 1.2101003     |
| Train/Value             | 1.2101003     |
| Train/control_penalty   | 0.7631233     |
| Train/policy_loss       | -0.010219645  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02375       |
-------------------------------------------

 ---------------- Iteration 790 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 789          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0766       |
| Time/B_Original_Form... | 0.0737       |
| Time/Buffer             | 0.00316      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.32536808   |
| Train/Action_magnitu... | 1.0160447    |
| Train/Action_magnitude  | 0.8082796    |
| Train/Action_max        | 0.43766746   |
| Train/Action_std        | 0.46438184   |
| Train/Entropy           | 0.3828571    |
| Train/Entropy_Loss      | -0.000383    |
| Train/Entropy_loss      | -0.000383    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.76946723  |
| Train/Loss              | 0.053817794  |
| Train/PolicyClip        | 0.0103772385 |
| Train/Policy_loss       | 0.04609415   |
| Train/Ratio             | 0.97107196   |
| Train/Return            | 1.0144541    |
| Train/V                 | 1.0742786    |
| Train/Value             | 1.0742786    |
| Train/control_penalty   | 0.8106501    |
| Train/policy_loss       | 0.04609415   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.01775      |
------------------------------------------

 ---------------- Iteration 791 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 790          |
| Time/Actor_Time         | 0.0867       |
| Time/B_Format_Time      | 0.0761       |
| Time/B_Original_Form... | 0.0761       |
| Time/Buffer             | 0.00486      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3143887    |
| Train/Action_magnitu... | 0.98276776   |
| Train/Action_magnitude  | 0.78550917   |
| Train/Action_max        | 0.4055869    |
| Train/Action_std        | 0.42064446   |
| Train/Entropy           | 0.31671128   |
| Train/Entropy_Loss      | -0.000317    |
| Train/Entropy_loss      | -0.000317    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7763275   |
| Train/Loss              | -0.008482965 |
| Train/PolicyClip        | 0.012104419  |
| Train/Policy_loss       | -0.015816862 |
| Train/Ratio             | 0.98923075   |
| Train/Return            | 1.1258123    |
| Train/V                 | 1.1240044    |
| Train/Value             | 1.1240044    |
| Train/control_penalty   | 0.76506084   |
| Train/policy_loss       | -0.015816862 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 792 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 791         |
| Time/Actor_Time         | 0.0832      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00293     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31688678  |
| Train/Action_magnitu... | 0.938837    |
| Train/Action_magnitude  | 0.7460529   |
| Train/Action_max        | 0.39126918  |
| Train/Action_std        | 0.40973186  |
| Train/Entropy           | 0.3197016   |
| Train/Entropy_Loss      | -0.00032    |
| Train/Entropy_loss      | -0.00032    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6878578  |
| Train/Loss              | 0.03526463  |
| Train/PolicyClip        | 0.01252936  |
| Train/Policy_loss       | 0.027959766 |
| Train/Ratio             | 0.9841228   |
| Train/Return            | 1.1160464   |
| Train/V                 | 1.1568176   |
| Train/Value             | 1.1568176   |
| Train/control_penalty   | 0.7624568   |
| Train/policy_loss       | 0.027959766 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 793 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 792          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0739       |
| Time/B_Original_Form... | 0.072        |
| Time/Buffer             | 0.0029       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3275238    |
| Train/Action_magnitu... | 1.0149419    |
| Train/Action_magnitude  | 0.8046626    |
| Train/Action_max        | 0.43380266   |
| Train/Action_std        | 0.43829015   |
| Train/Entropy           | 0.37900034   |
| Train/Entropy_Loss      | -0.000379    |
| Train/Entropy_loss      | -0.000379    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8121815   |
| Train/Loss              | 0.010848051  |
| Train/PolicyClip        | 0.011408547  |
| Train/Policy_loss       | 0.0033533545 |
| Train/Ratio             | 0.99559253   |
| Train/Return            | 1.1700082    |
| Train/V                 | 1.1857483    |
| Train/Value             | 1.1857483    |
| Train/control_penalty   | 0.7873697    |
| Train/policy_loss       | 0.0033533545 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.021        |
------------------------------------------

 ---------------- Iteration 794 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 793          |
| Time/Actor_Time         | 0.0869       |
| Time/B_Format_Time      | 0.077        |
| Time/B_Original_Form... | 0.0742       |
| Time/Buffer             | 0.0037       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32502317   |
| Train/Action_magnitu... | 0.95296586   |
| Train/Action_magnitude  | 0.7579876    |
| Train/Action_max        | 0.40470293   |
| Train/Action_std        | 0.41786122   |
| Train/Entropy           | 0.35813105   |
| Train/Entropy_Loss      | -0.000358    |
| Train/Entropy_loss      | -0.000358    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7535724   |
| Train/Loss              | -0.04205061  |
| Train/PolicyClip        | 0.006117854  |
| Train/Policy_loss       | -0.049334873 |
| Train/Ratio             | 0.993689     |
| Train/Return            | 1.2883227    |
| Train/V                 | 1.2518232    |
| Train/Value             | 1.2518232    |
| Train/control_penalty   | 0.76423913   |
| Train/policy_loss       | -0.049334873 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 795 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 794          |
| Time/Actor_Time         | 0.0897       |
| Time/B_Format_Time      | 0.0799       |
| Time/B_Original_Form... | 0.0863       |
| Time/Buffer             | 0.00293      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32118535   |
| Train/Action_magnitu... | 1.0056254    |
| Train/Action_magnitude  | 0.8039465    |
| Train/Action_max        | 0.41221762   |
| Train/Action_std        | 0.44487292   |
| Train/Entropy           | 0.41088977   |
| Train/Entropy_Loss      | -0.000411    |
| Train/Entropy_loss      | -0.000411    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9556765   |
| Train/Loss              | -0.021784503 |
| Train/PolicyClip        | 0.01158666   |
| Train/Policy_loss       | -0.029150812 |
| Train/Ratio             | 1.0009162    |
| Train/Return            | 1.1603463    |
| Train/V                 | 1.1440511    |
| Train/Value             | 1.1440511    |
| Train/control_penalty   | 0.77771986   |
| Train/policy_loss       | -0.029150812 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 796 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 795          |
| Time/Actor_Time         | 0.0893       |
| Time/B_Format_Time      | 0.0759       |
| Time/B_Original_Form... | 0.0745       |
| Time/Buffer             | 0.00336      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32332373   |
| Train/Action_magnitu... | 0.99093103   |
| Train/Action_magnitude  | 0.78548795   |
| Train/Action_max        | 0.41587603   |
| Train/Action_std        | 0.4355605    |
| Train/Entropy           | 0.38642615   |
| Train/Entropy_Loss      | -0.000386    |
| Train/Entropy_loss      | -0.000386    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.83873665  |
| Train/Loss              | -0.048681304 |
| Train/PolicyClip        | 0.014174505  |
| Train/Policy_loss       | -0.056177758 |
| Train/Ratio             | 0.99129874   |
| Train/Return            | 1.3348241    |
| Train/V                 | 1.2940924    |
| Train/Value             | 1.2940924    |
| Train/control_penalty   | 0.7882877    |
| Train/policy_loss       | -0.056177758 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03         |
------------------------------------------

 ---------------- Iteration 797 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 796         |
| Time/Actor_Time         | 0.0832      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00278     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.31843188  |
| Train/Action_magnitu... | 0.9459005   |
| Train/Action_magnitude  | 0.7517182   |
| Train/Action_max        | 0.43407843  |
| Train/Action_std        | 0.40918267  |
| Train/Entropy           | 0.3567888   |
| Train/Entropy_Loss      | -0.000357   |
| Train/Entropy_loss      | -0.000357   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.7517901  |
| Train/Loss              | 0.019547585 |
| Train/PolicyClip        | 0.009309664 |
| Train/Policy_loss       | 0.012362988 |
| Train/Ratio             | 0.9942302   |
| Train/Return            | 1.1299394   |
| Train/V                 | 1.1547745   |
| Train/Value             | 1.1547745   |
| Train/control_penalty   | 0.75413865  |
| Train/policy_loss       | 0.012362988 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 798 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 797         |
| Time/Actor_Time         | 0.0853      |
| Time/B_Format_Time      | 0.0734      |
| Time/B_Original_Form... | 0.075       |
| Time/Buffer             | 0.00359     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32935232  |
| Train/Action_magnitu... | 0.98937404  |
| Train/Action_magnitude  | 0.78512025  |
| Train/Action_max        | 0.39626166  |
| Train/Action_std        | 0.4267799   |
| Train/Entropy           | 0.357882    |
| Train/Entropy_Loss      | -0.000358   |
| Train/Entropy_loss      | -0.000358   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.71978956 |
| Train/Loss              | -0.03746291 |
| Train/PolicyClip        | 0.013426408 |
| Train/Policy_loss       | -0.04495519 |
| Train/Ratio             | 0.9943551   |
| Train/Return            | 1.4684902   |
| Train/V                 | 1.4383464   |
| Train/Value             | 1.4383464   |
| Train/control_penalty   | 0.78501624  |
| Train/policy_loss       | -0.04495519 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0305      |
-----------------------------------------

 ---------------- Iteration 799 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 798          |
| Time/Actor_Time         | 0.0845       |
| Time/B_Format_Time      | 0.0781       |
| Time/B_Original_Form... | 0.0727       |
| Time/Buffer             | 0.00317      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34617803   |
| Train/Action_magnitu... | 0.97101575   |
| Train/Action_magnitude  | 0.7687244    |
| Train/Action_max        | 0.4278031    |
| Train/Action_std        | 0.38540295   |
| Train/Entropy           | 0.28666174   |
| Train/Entropy_Loss      | -0.000287    |
| Train/Entropy_loss      | -0.000287    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.6594424   |
| Train/Loss              | -0.05289865  |
| Train/PolicyClip        | 0.011878672  |
| Train/Policy_loss       | -0.060276628 |
| Train/Ratio             | 1.003435     |
| Train/Return            | 1.248601     |
| Train/V                 | 1.2016749    |
| Train/Value             | 1.2016749    |
| Train/control_penalty   | 0.7664639    |
| Train/policy_loss       | -0.060276628 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02525      |
------------------------------------------

 ---------------- Iteration 800 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 799          |
| Time/Actor_Time         | 0.087        |
| Time/B_Format_Time      | 0.0898       |
| Time/B_Original_Form... | 0.0791       |
| Time/Buffer             | 0.00857      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32745597   |
| Train/Action_magnitu... | 0.92879725   |
| Train/Action_magnitude  | 0.7351641    |
| Train/Action_max        | 0.38974428   |
| Train/Action_std        | 0.41592193   |
| Train/Entropy           | 0.3709177    |
| Train/Entropy_Loss      | -0.000371    |
| Train/Entropy_loss      | -0.000371    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.71891236  |
| Train/Loss              | 0.01053451   |
| Train/PolicyClip        | 0.014657723  |
| Train/Policy_loss       | 0.0034177809 |
| Train/Ratio             | 0.9798427    |
| Train/Return            | 1.3728551    |
| Train/V                 | 1.3930511    |
| Train/Value             | 1.3930511    |
| Train/control_penalty   | 0.74876475   |
| Train/policy_loss       | 0.0034177809 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 801 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 800          |
| Time/Actor_Time         | 0.0958       |
| Time/B_Format_Time      | 0.0898       |
| Time/B_Original_Form... | 0.266        |
| Time/Buffer             | 0.00322      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3157419    |
| Train/Action_magnitu... | 0.89430904   |
| Train/Action_magnitude  | 0.70514476   |
| Train/Action_max        | 0.42071983   |
| Train/Action_std        | 0.3684596    |
| Train/Entropy           | 0.25534096   |
| Train/Entropy_Loss      | -0.000255    |
| Train/Entropy_loss      | -0.000255    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.57146215  |
| Train/Loss              | 0.0031462908 |
| Train/PolicyClip        | 0.01353782   |
| Train/Policy_loss       | -0.003539081 |
| Train/Ratio             | 0.9898538    |
| Train/Return            | 1.3059609    |
| Train/V                 | 1.3165911    |
| Train/Value             | 1.3165911    |
| Train/control_penalty   | 0.6940713    |
| Train/policy_loss       | -0.003539081 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02425      |
------------------------------------------

 ---------------- Iteration 802 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 801          |
| Time/Actor_Time         | 0.0863       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00313      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3299088    |
| Train/Action_magnitu... | 0.92222905   |
| Train/Action_magnitude  | 0.72713035   |
| Train/Action_max        | 0.41879815   |
| Train/Action_std        | 0.38349414   |
| Train/Entropy           | 0.27222562   |
| Train/Entropy_Loss      | -0.000272    |
| Train/Entropy_loss      | -0.000272    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.59820783  |
| Train/Loss              | -0.060877632 |
| Train/PolicyClip        | 0.012592572  |
| Train/Policy_loss       | -0.06789646  |
| Train/Ratio             | 1.0115162    |
| Train/Return            | 1.409188     |
| Train/V                 | 1.3558694    |
| Train/Value             | 1.3558694    |
| Train/control_penalty   | 0.7291061    |
| Train/policy_loss       | -0.06789646  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02925      |
------------------------------------------

 ---------------- Iteration 803 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 802          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0751       |
| Time/B_Original_Form... | 0.075        |
| Time/Buffer             | 0.00409      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3124635    |
| Train/Action_magnitu... | 0.9116182    |
| Train/Action_magnitude  | 0.7187046    |
| Train/Action_max        | 0.383867     |
| Train/Action_std        | 0.3956024    |
| Train/Entropy           | 0.30972502   |
| Train/Entropy_Loss      | -0.00031     |
| Train/Entropy_loss      | -0.00031     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.61706126  |
| Train/Loss              | -0.005484246 |
| Train/PolicyClip        | 0.011635147  |
| Train/Policy_loss       | -0.012312857 |
| Train/Ratio             | 1.0018055    |
| Train/Return            | 1.4114162    |
| Train/V                 | 1.4150939    |
| Train/Value             | 1.4150939    |
| Train/control_penalty   | 0.7138337    |
| Train/policy_loss       | -0.012312857 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02575      |
------------------------------------------

 ---------------- Iteration 804 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 803           |
| Time/Actor_Time         | 0.0839        |
| Time/B_Format_Time      | 0.0739        |
| Time/B_Original_Form... | 0.0713        |
| Time/Buffer             | 0.00471       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.3167615     |
| Train/Action_magnitu... | 0.8976247     |
| Train/Action_magnitude  | 0.70903236    |
| Train/Action_max        | 0.40689942    |
| Train/Action_std        | 0.3966326     |
| Train/Entropy           | 0.29916975    |
| Train/Entropy_Loss      | -0.000299     |
| Train/Entropy_loss      | -0.000299     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.587703     |
| Train/Loss              | -0.0035042567 |
| Train/PolicyClip        | 0.012478703   |
| Train/Policy_loss       | -0.010514366  |
| Train/Ratio             | 0.99044645    |
| Train/Return            | 1.4842701     |
| Train/V                 | 1.4883744     |
| Train/Value             | 1.4883744     |
| Train/control_penalty   | 0.73092794    |
| Train/policy_loss       | -0.010514366  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0265        |
-------------------------------------------

 ---------------- Iteration 805 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 804          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0762       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.0031       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3165764    |
| Train/Action_magnitu... | 0.8908492    |
| Train/Action_magnitude  | 0.7030558    |
| Train/Action_max        | 0.39072528   |
| Train/Action_std        | 0.36982045   |
| Train/Entropy           | 0.23875253   |
| Train/Entropy_Loss      | -0.000239    |
| Train/Entropy_loss      | -0.000239    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5373619   |
| Train/Loss              | -0.024231177 |
| Train/PolicyClip        | 0.012809907  |
| Train/Policy_loss       | -0.030966567 |
| Train/Ratio             | 0.9936893    |
| Train/Return            | 1.7265615    |
| Train/V                 | 1.7131491    |
| Train/Value             | 1.7131491    |
| Train/control_penalty   | 0.6974142    |
| Train/policy_loss       | -0.030966567 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03625      |
------------------------------------------

 ---------------- Iteration 806 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 805         |
| Time/Actor_Time         | 0.0869      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00241     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30330372  |
| Train/Action_magnitu... | 0.900476    |
| Train/Action_magnitude  | 0.7104738   |
| Train/Action_max        | 0.3854166   |
| Train/Action_std        | 0.39752325  |
| Train/Entropy           | 0.3000577   |
| Train/Entropy_Loss      | -0.0003     |
| Train/Entropy_loss      | -0.0003     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.68003404 |
| Train/Loss              | 0.07073313  |
| Train/PolicyClip        | 0.012029224 |
| Train/Policy_loss       | 0.06387944  |
| Train/Ratio             | 0.9924487   |
| Train/Return            | 1.4099861   |
| Train/V                 | 1.487498    |
| Train/Value             | 1.487498    |
| Train/control_penalty   | 0.7153752   |
| Train/policy_loss       | 0.06387944  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 807 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 806         |
| Time/Actor_Time         | 0.0847      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00275     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30173168  |
| Train/Action_magnitu... | 0.8849926   |
| Train/Action_magnitude  | 0.69996923  |
| Train/Action_max        | 0.37194628  |
| Train/Action_std        | 0.39243764  |
| Train/Entropy           | 0.26982594  |
| Train/Entropy_Loss      | -0.00027    |
| Train/Entropy_loss      | -0.00027    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.542963   |
| Train/Loss              | 0.038960107 |
| Train/PolicyClip        | 0.015682524 |
| Train/Policy_loss       | 0.032107975 |
| Train/Ratio             | 0.9944374   |
| Train/Return            | 1.4966595   |
| Train/V                 | 1.5469701   |
| Train/Value             | 1.5469701   |
| Train/control_penalty   | 0.7121957   |
| Train/policy_loss       | 0.032107975 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 808 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 807          |
| Time/Actor_Time         | 0.0875       |
| Time/B_Format_Time      | 0.0774       |
| Time/B_Original_Form... | 0.0764       |
| Time/Buffer             | 0.00306      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31635797   |
| Train/Action_magnitu... | 0.8824939    |
| Train/Action_magnitude  | 0.69380677   |
| Train/Action_max        | 0.37289286   |
| Train/Action_std        | 0.37549186   |
| Train/Entropy           | 0.26082477   |
| Train/Entropy_Loss      | -0.000261    |
| Train/Entropy_loss      | -0.000261    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.5446519   |
| Train/Loss              | -0.030774472 |
| Train/PolicyClip        | 0.009364669  |
| Train/Policy_loss       | -0.03754125  |
| Train/Ratio             | 1.001861     |
| Train/Return            | 1.4698181    |
| Train/V                 | 1.4443042    |
| Train/Value             | 1.4443042    |
| Train/control_penalty   | 0.70276064   |
| Train/policy_loss       | -0.03754125  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0295       |
------------------------------------------

 ---------------- Iteration 809 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 808           |
| Time/Actor_Time         | 0.0871        |
| Time/B_Format_Time      | 0.0724        |
| Time/B_Original_Form... | 0.072         |
| Time/Buffer             | 0.00254       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.32181928    |
| Train/Action_magnitu... | 0.8948016     |
| Train/Action_magnitude  | 0.70441926    |
| Train/Action_max        | 0.39201796    |
| Train/Action_std        | 0.3748096     |
| Train/Entropy           | 0.23425561    |
| Train/Entropy_Loss      | -0.000234     |
| Train/Entropy_loss      | -0.000234     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.5010295    |
| Train/Loss              | -0.0010990943 |
| Train/PolicyClip        | 0.015921917   |
| Train/Policy_loss       | -0.007980874  |
| Train/Ratio             | 0.9921953     |
| Train/Return            | 1.2881885     |
| Train/V                 | 1.2948152     |
| Train/Value             | 1.2948152     |
| Train/control_penalty   | 0.7116035     |
| Train/policy_loss       | -0.007980874  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.024         |
-------------------------------------------

 ---------------- Iteration 810 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 809         |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0731      |
| Time/Buffer             | 0.00288     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2970816   |
| Train/Action_magnitu... | 0.8637249   |
| Train/Action_magnitude  | 0.68358326  |
| Train/Action_max        | 0.3704031   |
| Train/Action_std        | 0.39577845  |
| Train/Entropy           | 0.28506327  |
| Train/Entropy_Loss      | -0.000285   |
| Train/Entropy_loss      | -0.000285   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.52268946 |
| Train/Loss              | 0.0751338   |
| Train/PolicyClip        | 0.012534615 |
| Train/Policy_loss       | 0.0682885   |
| Train/Ratio             | 0.9697048   |
| Train/Return            | 1.2488018   |
| Train/V                 | 1.3340187   |
| Train/Value             | 1.3340187   |
| Train/control_penalty   | 0.71303695  |
| Train/policy_loss       | 0.0682885   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 811 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 810         |
| Time/Actor_Time         | 0.087       |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0757      |
| Time/Buffer             | 0.00279     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.30727872  |
| Train/Action_magnitu... | 0.9679063   |
| Train/Action_magnitude  | 0.765282    |
| Train/Action_max        | 0.39419258  |
| Train/Action_std        | 0.43402693  |
| Train/Entropy           | 0.36869803  |
| Train/Entropy_Loss      | -0.000369   |
| Train/Entropy_loss      | -0.000369   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.7412851  |
| Train/Loss              | 0.10794865  |
| Train/PolicyClip        | 0.013201448 |
| Train/Policy_loss       | 0.10066583  |
| Train/Ratio             | 0.97105855  |
| Train/Return            | 1.4657304   |
| Train/V                 | 1.5849197   |
| Train/Value             | 1.5849197   |
| Train/control_penalty   | 0.76515216  |
| Train/policy_loss       | 0.10066583  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.021       |
-----------------------------------------

 ---------------- Iteration 812 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 811          |
| Time/Actor_Time         | 0.0836       |
| Time/B_Format_Time      | 0.073        |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.0028       |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30411014   |
| Train/Action_magnitu... | 0.88882273   |
| Train/Action_magnitude  | 0.7005757    |
| Train/Action_max        | 0.40336797   |
| Train/Action_std        | 0.38516954   |
| Train/Entropy           | 0.26068553   |
| Train/Entropy_Loss      | -0.000261    |
| Train/Entropy_loss      | -0.000261    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.557307    |
| Train/Loss              | -0.040465713 |
| Train/PolicyClip        | 0.013169762  |
| Train/Policy_loss       | -0.0471816   |
| Train/Ratio             | 1.000345     |
| Train/Return            | 1.5358363    |
| Train/V                 | 1.5023943    |
| Train/Value             | 1.5023943    |
| Train/control_penalty   | 0.69765705   |
| Train/policy_loss       | -0.0471816   |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03275      |
------------------------------------------

 ---------------- Iteration 813 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 812         |
| Time/Actor_Time         | 0.0853      |
| Time/B_Format_Time      | 0.0734      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00335     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3107384   |
| Train/Action_magnitu... | 0.90223145  |
| Train/Action_magnitude  | 0.71469843  |
| Train/Action_max        | 0.39811584  |
| Train/Action_std        | 0.39962146  |
| Train/Entropy           | 0.30608067  |
| Train/Entropy_Loss      | -0.000306   |
| Train/Entropy_loss      | -0.000306   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6514359  |
| Train/Loss              | 0.045811478 |
| Train/PolicyClip        | 0.013708575 |
| Train/Policy_loss       | 0.03882192  |
| Train/Ratio             | 0.986114    |
| Train/Return            | 1.2797768   |
| Train/V                 | 1.3343039   |
| Train/Value             | 1.3343039   |
| Train/control_penalty   | 0.7295639   |
| Train/policy_loss       | 0.03882192  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 814 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 813           |
| Time/Actor_Time         | 0.0861        |
| Time/B_Format_Time      | 0.0711        |
| Time/B_Original_Form... | 0.0712        |
| Time/Buffer             | 0.00283       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.29979914    |
| Train/Action_magnitu... | 0.8571085     |
| Train/Action_magnitude  | 0.67413574    |
| Train/Action_max        | 0.37870327    |
| Train/Action_std        | 0.35263148    |
| Train/Entropy           | 0.16557173    |
| Train/Entropy_Loss      | -0.000166     |
| Train/Entropy_loss      | -0.000166     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.4102554    |
| Train/Loss              | -0.0132455155 |
| Train/PolicyClip        | 0.018071229   |
| Train/Policy_loss       | -0.019784864  |
| Train/Ratio             | 1.0021877     |
| Train/Return            | 1.4300412     |
| Train/V                 | 1.4210012     |
| Train/Value             | 1.4210012     |
| Train/control_penalty   | 0.6704922     |
| Train/policy_loss       | -0.019784864  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0275        |
-------------------------------------------

 ---------------- Iteration 815 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 814          |
| Time/Actor_Time         | 0.0849       |
| Time/B_Format_Time      | 0.0714       |
| Time/B_Original_Form... | 0.0707       |
| Time/Buffer             | 0.00271      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30776665   |
| Train/Action_magnitu... | 0.8767089    |
| Train/Action_magnitude  | 0.692054     |
| Train/Action_max        | 0.39289507   |
| Train/Action_std        | 0.35904303   |
| Train/Entropy           | 0.20747545   |
| Train/Entropy_Loss      | -0.000207    |
| Train/Entropy_loss      | -0.000207    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.49654177  |
| Train/Loss              | -0.014165323 |
| Train/PolicyClip        | 0.018607898  |
| Train/Policy_loss       | -0.020788448 |
| Train/Ratio             | 1.0019306    |
| Train/Return            | 1.46268      |
| Train/V                 | 1.455688     |
| Train/Value             | 1.455688     |
| Train/control_penalty   | 0.6830601    |
| Train/policy_loss       | -0.020788448 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02825      |
------------------------------------------

 ---------------- Iteration 816 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 815          |
| Time/Actor_Time         | 0.0852       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00305      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31159908   |
| Train/Action_magnitu... | 0.8853749    |
| Train/Action_magnitude  | 0.697785     |
| Train/Action_max        | 0.4070178    |
| Train/Action_std        | 0.37776428   |
| Train/Entropy           | 0.24841666   |
| Train/Entropy_Loss      | -0.000248    |
| Train/Entropy_loss      | -0.000248    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.48904827  |
| Train/Loss              | 0.00906857   |
| Train/PolicyClip        | 0.019719696  |
| Train/Policy_loss       | 0.0022370883 |
| Train/Ratio             | 0.99398607   |
| Train/Return            | 1.406561     |
| Train/V                 | 1.4270936    |
| Train/Value             | 1.4270936    |
| Train/control_penalty   | 0.7079899    |
| Train/policy_loss       | 0.0022370883 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------

 ---------------- Iteration 817 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 816         |
| Time/Actor_Time         | 0.085       |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0738      |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3153124   |
| Train/Action_magnitu... | 0.92677397  |
| Train/Action_magnitude  | 0.73057514  |
| Train/Action_max        | 0.41484776  |
| Train/Action_std        | 0.42055187  |
| Train/Entropy           | 0.3774913   |
| Train/Entropy_Loss      | -0.000377   |
| Train/Entropy_loss      | -0.000377   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.7352688  |
| Train/Loss              | 0.06880373  |
| Train/PolicyClip        | 0.012802955 |
| Train/Policy_loss       | 0.0617665   |
| Train/Ratio             | 0.9771752   |
| Train/Return            | 1.2986709   |
| Train/V                 | 1.3797172   |
| Train/Value             | 1.3797172   |
| Train/control_penalty   | 0.7414718   |
| Train/policy_loss       | 0.0617665   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 818 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 817          |
| Time/Actor_Time         | 0.0836       |
| Time/B_Format_Time      | 0.0777       |
| Time/B_Original_Form... | 0.0764       |
| Time/Buffer             | 0.00338      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30093938   |
| Train/Action_magnitu... | 0.89638096   |
| Train/Action_magnitude  | 0.7120415    |
| Train/Action_max        | 0.4037179    |
| Train/Action_std        | 0.38069916   |
| Train/Entropy           | 0.26022702   |
| Train/Entropy_Loss      | -0.00026     |
| Train/Entropy_loss      | -0.00026     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.62046444  |
| Train/Loss              | -0.045454245 |
| Train/PolicyClip        | 0.013666282  |
| Train/Policy_loss       | -0.052149937 |
| Train/Ratio             | 0.9964825    |
| Train/Return            | 1.4698476    |
| Train/V                 | 1.4323037    |
| Train/Value             | 1.4323037    |
| Train/control_penalty   | 0.6955919    |
| Train/policy_loss       | -0.052149937 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.029        |
------------------------------------------

 ---------------- Iteration 819 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 818         |
| Time/Actor_Time         | 0.0837      |
| Time/B_Format_Time      | 0.0756      |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00345     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30291307  |
| Train/Action_magnitu... | 0.90807706  |
| Train/Action_magnitude  | 0.71796507  |
| Train/Action_max        | 0.3723162   |
| Train/Action_std        | 0.38466677  |
| Train/Entropy           | 0.2690203   |
| Train/Entropy_Loss      | -0.000269   |
| Train/Entropy_loss      | -0.000269   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.6347667  |
| Train/Loss              | 0.030816145 |
| Train/PolicyClip        | 0.01621654  |
| Train/Policy_loss       | 0.02401587  |
| Train/Ratio             | 0.9903236   |
| Train/Return            | 1.3672224   |
| Train/V                 | 1.4066942   |
| Train/Value             | 1.4066942   |
| Train/control_penalty   | 0.7069295   |
| Train/policy_loss       | 0.02401587  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 820 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 819         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0703      |
| Time/Buffer             | 0.0035      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3112034   |
| Train/Action_magnitu... | 0.9244863   |
| Train/Action_magnitude  | 0.73095095  |
| Train/Action_max        | 0.40697068  |
| Train/Action_std        | 0.3928985   |
| Train/Entropy           | 0.29767796  |
| Train/Entropy_Loss      | -0.000298   |
| Train/Entropy_loss      | -0.000298   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.68596417 |
| Train/Loss              | -0.03662627 |
| Train/PolicyClip        | 0.013233685 |
| Train/Policy_loss       | -0.04358451 |
| Train/Ratio             | 0.9965758   |
| Train/Return            | 1.648996    |
| Train/V                 | 1.6193657   |
| Train/Value             | 1.6193657   |
| Train/control_penalty   | 0.7255919   |
| Train/policy_loss       | -0.04358451 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 821 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 820         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.00345     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31913653  |
| Train/Action_magnitu... | 0.92008346  |
| Train/Action_magnitude  | 0.725536    |
| Train/Action_max        | 0.39044744  |
| Train/Action_std        | 0.39324418  |
| Train/Entropy           | 0.2884093   |
| Train/Entropy_Loss      | -0.000288   |
| Train/Entropy_loss      | -0.000288   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.59219617 |
| Train/Loss              | 0.015265189 |
| Train/PolicyClip        | 0.014255133 |
| Train/Policy_loss       | 0.0082045   |
| Train/Ratio             | 0.99311227  |
| Train/Return            | 1.7308708   |
| Train/V                 | 1.7549281   |
| Train/Value             | 1.7549281   |
| Train/control_penalty   | 0.7349098   |
| Train/policy_loss       | 0.0082045   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 822 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 821           |
| Time/Actor_Time         | 0.0898        |
| Time/B_Format_Time      | 0.0892        |
| Time/B_Original_Form... | 0.106         |
| Time/Buffer             | 0.00298       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31054857    |
| Train/Action_magnitu... | 0.9083186     |
| Train/Action_magnitude  | 0.7199331     |
| Train/Action_max        | 0.38891754    |
| Train/Action_std        | 0.3688739     |
| Train/Entropy           | 0.22548659    |
| Train/Entropy_Loss      | -0.000225     |
| Train/Entropy_loss      | -0.000225     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.5574514    |
| Train/Loss              | -0.0046485034 |
| Train/PolicyClip        | 0.016573042   |
| Train/Policy_loss       | -0.011384947  |
| Train/Ratio             | 0.99028623    |
| Train/Return            | 1.3559153     |
| Train/V                 | 1.3640776     |
| Train/Value             | 1.3640776     |
| Train/control_penalty   | 0.69619304    |
| Train/policy_loss       | -0.011384947  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02775       |
-------------------------------------------

 ---------------- Iteration 823 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 822         |
| Time/Actor_Time         | 0.145       |
| Time/B_Format_Time      | 0.112       |
| Time/B_Original_Form... | 0.121       |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32586664  |
| Train/Action_magnitu... | 0.8929934   |
| Train/Action_magnitude  | 0.7090208   |
| Train/Action_max        | 0.3385641   |
| Train/Action_std        | 0.37735546  |
| Train/Entropy           | 0.24194257  |
| Train/Entropy_Loss      | -0.000242   |
| Train/Entropy_loss      | -0.000242   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.50337356 |
| Train/Loss              | 0.043663144 |
| Train/PolicyClip        | 0.011265208 |
| Train/Policy_loss       | 0.036702007 |
| Train/Ratio             | 0.9797468   |
| Train/Return            | 1.4696099   |
| Train/V                 | 1.5204561   |
| Train/Value             | 1.5204561   |
| Train/control_penalty   | 0.7203081   |
| Train/policy_loss       | 0.036702007 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 824 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 823         |
| Time/Actor_Time         | 0.0841      |
| Time/B_Format_Time      | 0.0755      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00669     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34555164  |
| Train/Action_magnitu... | 0.9348982   |
| Train/Action_magnitude  | 0.7377577   |
| Train/Action_max        | 0.40191463  |
| Train/Action_std        | 0.3959821   |
| Train/Entropy           | 0.29327077  |
| Train/Entropy_Loss      | -0.000293   |
| Train/Entropy_loss      | -0.000293   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.5455724  |
| Train/Loss              | 0.041490927 |
| Train/PolicyClip        | 0.016260765 |
| Train/Policy_loss       | 0.034144256 |
| Train/Ratio             | 0.9865441   |
| Train/Return            | 1.3242645   |
| Train/V                 | 1.3738256   |
| Train/Value             | 1.3738256   |
| Train/control_penalty   | 0.76399416  |
| Train/policy_loss       | 0.034144256 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 825 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 824           |
| Time/Actor_Time         | 0.0872        |
| Time/B_Format_Time      | 0.0761        |
| Time/B_Original_Form... | 0.075         |
| Time/Buffer             | 0.00382       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.3268629     |
| Train/Action_magnitu... | 0.9237124     |
| Train/Action_magnitude  | 0.73030066    |
| Train/Action_max        | 0.38410836    |
| Train/Action_std        | 0.40629068    |
| Train/Entropy           | 0.3187892     |
| Train/Entropy_Loss      | -0.000319     |
| Train/Entropy_loss      | -0.000319     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.6664994    |
| Train/Loss              | -0.0067742094 |
| Train/PolicyClip        | 0.021665705   |
| Train/Policy_loss       | -0.013908782  |
| Train/Ratio             | 0.9811732     |
| Train/Return            | 1.2721541     |
| Train/V                 | 1.2705883     |
| Train/Value             | 1.2705883     |
| Train/control_penalty   | 0.7453362     |
| Train/policy_loss       | -0.013908782  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0285        |
-------------------------------------------

 ---------------- Iteration 826 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 825           |
| Time/Actor_Time         | 0.0844        |
| Time/B_Format_Time      | 0.0753        |
| Time/B_Original_Form... | 0.0733        |
| Time/Buffer             | 0.00371       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.32661057    |
| Train/Action_magnitu... | 0.97969383    |
| Train/Action_magnitude  | 0.77352357    |
| Train/Action_max        | 0.4275362     |
| Train/Action_std        | 0.4393787     |
| Train/Entropy           | 0.38275644    |
| Train/Entropy_Loss      | -0.000383     |
| Train/Entropy_loss      | -0.000383     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.8025973    |
| Train/Loss              | 0.004650837   |
| Train/PolicyClip        | 0.012674796   |
| Train/Policy_loss       | -0.0027167997 |
| Train/Ratio             | 0.992443      |
| Train/Return            | 1.3901101     |
| Train/V                 | 1.3998578     |
| Train/Value             | 1.3998578     |
| Train/control_penalty   | 0.7750393     |
| Train/policy_loss       | -0.0027167997 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0305        |
-------------------------------------------

 ---------------- Iteration 827 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 826          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0749       |
| Time/B_Original_Form... | 0.0734       |
| Time/Buffer             | 0.00346      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32763594   |
| Train/Action_magnitu... | 0.985885     |
| Train/Action_magnitude  | 0.7787948    |
| Train/Action_max        | 0.42997247   |
| Train/Action_std        | 0.42229015   |
| Train/Entropy           | 0.36015162   |
| Train/Entropy_Loss      | -0.00036     |
| Train/Entropy_loss      | -0.00036     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7934441   |
| Train/Loss              | -0.06391851  |
| Train/PolicyClip        | 0.017305076  |
| Train/Policy_loss       | -0.071132794 |
| Train/Ratio             | 0.99341744   |
| Train/Return            | 1.6408695    |
| Train/V                 | 1.5879589    |
| Train/Value             | 1.5879589    |
| Train/control_penalty   | 0.7574441    |
| Train/policy_loss       | -0.071132794 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03875      |
------------------------------------------

 ---------------- Iteration 828 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 827          |
| Time/Actor_Time         | 0.0846       |
| Time/B_Format_Time      | 0.0843       |
| Time/B_Original_Form... | 0.0817       |
| Time/Buffer             | 0.00285      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32071197   |
| Train/Action_magnitu... | 0.94407654   |
| Train/Action_magnitude  | 0.7485424    |
| Train/Action_max        | 0.40106156   |
| Train/Action_std        | 0.42081836   |
| Train/Entropy           | 0.35970733   |
| Train/Entropy_Loss      | -0.00036     |
| Train/Entropy_loss      | -0.00036     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7614683   |
| Train/Loss              | -0.038812067 |
| Train/PolicyClip        | 0.0131172715 |
| Train/Policy_loss       | -0.045955267 |
| Train/Ratio             | 0.9934113    |
| Train/Return            | 1.4519043    |
| Train/V                 | 1.4204386    |
| Train/Value             | 1.4204386    |
| Train/control_penalty   | 0.7502906    |
| Train/policy_loss       | -0.045955267 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.035        |
------------------------------------------

 ---------------- Iteration 829 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 828         |
| Time/Actor_Time         | 0.0848      |
| Time/B_Format_Time      | 0.0744      |
| Time/B_Original_Form... | 0.0742      |
| Time/Buffer             | 0.00342     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.3327331   |
| Train/Action_magnitu... | 0.9524856   |
| Train/Action_magnitude  | 0.75213796  |
| Train/Action_max        | 0.41410273  |
| Train/Action_std        | 0.42558044  |
| Train/Entropy           | 0.38472483  |
| Train/Entropy_Loss      | -0.000385   |
| Train/Entropy_loss      | -0.000385   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.78930306 |
| Train/Loss              | 0.024359234 |
| Train/PolicyClip        | 0.01724684  |
| Train/Policy_loss       | 0.017102297 |
| Train/Ratio             | 0.98386735  |
| Train/Return            | 1.5022674   |
| Train/V                 | 1.5360652   |
| Train/Value             | 1.5360652   |
| Train/control_penalty   | 0.7641663   |
| Train/policy_loss       | 0.017102297 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 830 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 829           |
| Time/Actor_Time         | 0.0844        |
| Time/B_Format_Time      | 0.0712        |
| Time/B_Original_Form... | 0.0706        |
| Time/Buffer             | 0.00257       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.33395007    |
| Train/Action_magnitu... | 0.9823856     |
| Train/Action_magnitude  | 0.77907705    |
| Train/Action_max        | 0.42846552    |
| Train/Action_std        | 0.42857474    |
| Train/Entropy           | 0.367092      |
| Train/Entropy_Loss      | -0.000367     |
| Train/Entropy_loss      | -0.000367     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.7727876    |
| Train/Loss              | -0.0047088545 |
| Train/PolicyClip        | 0.010094134   |
| Train/Policy_loss       | -0.012014349  |
| Train/Ratio             | 0.9944026     |
| Train/Return            | 1.4767393     |
| Train/V                 | 1.482184      |
| Train/Value             | 1.482184      |
| Train/control_penalty   | 0.76725864    |
| Train/policy_loss       | -0.012014349  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.03          |
-------------------------------------------

 ---------------- Iteration 831 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 830          |
| Time/Actor_Time         | 0.0908       |
| Time/B_Format_Time      | 0.0721       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00314      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.33095455   |
| Train/Action_magnitu... | 0.9754911    |
| Train/Action_magnitude  | 0.773093     |
| Train/Action_max        | 0.3976153    |
| Train/Action_std        | 0.44116235   |
| Train/Entropy           | 0.3973393    |
| Train/Entropy_Loss      | -0.000397    |
| Train/Entropy_loss      | -0.000397    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.83171415  |
| Train/Loss              | -0.027818587 |
| Train/PolicyClip        | 0.011692478  |
| Train/Policy_loss       | -0.035378207 |
| Train/Ratio             | 0.9936884    |
| Train/Return            | 1.4841164    |
| Train/V                 | 1.4652761    |
| Train/Value             | 1.4652761    |
| Train/control_penalty   | 0.79569584   |
| Train/policy_loss       | -0.035378207 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02775      |
------------------------------------------

 ---------------- Iteration 832 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 831           |
| Time/Actor_Time         | 0.0845        |
| Time/B_Format_Time      | 0.0722        |
| Time/B_Original_Form... | 0.0697        |
| Time/Buffer             | 0.00266       |
| Time/Critic_Time        | 9.54e-07      |
| Train/Action_abs_mean   | 0.32027406    |
| Train/Action_magnitu... | 0.96558857    |
| Train/Action_magnitude  | 0.76701283    |
| Train/Action_max        | 0.42473233    |
| Train/Action_std        | 0.44670504    |
| Train/Entropy           | 0.38588175    |
| Train/Entropy_Loss      | -0.000386     |
| Train/Entropy_loss      | -0.000386     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.8113217    |
| Train/Loss              | -0.0006738268 |
| Train/PolicyClip        | 0.013382343   |
| Train/Policy_loss       | -0.008104675  |
| Train/Ratio             | 0.98787934    |
| Train/Return            | 1.5937129     |
| Train/V                 | 1.6038903     |
| Train/Value             | 1.6038903     |
| Train/control_penalty   | 0.781673      |
| Train/policy_loss       | -0.008104675  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.03125       |
-------------------------------------------

 ---------------- Iteration 833 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 832          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0713       |
| Time/Buffer             | 0.00268      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.321093     |
| Train/Action_magnitu... | 1.0041673    |
| Train/Action_magnitude  | 0.79501575   |
| Train/Action_max        | 0.40939385   |
| Train/Action_std        | 0.4877232    |
| Train/Entropy           | 0.4987748    |
| Train/Entropy_Loss      | -0.000499    |
| Train/Entropy_loss      | -0.000499    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9150204   |
| Train/Loss              | -0.030848496 |
| Train/PolicyClip        | 0.01367824   |
| Train/Policy_loss       | -0.038588315 |
| Train/Ratio             | 0.9998945    |
| Train/Return            | 1.487434     |
| Train/V                 | 1.4607838    |
| Train/Value             | 1.4607838    |
| Train/control_penalty   | 0.82385963   |
| Train/policy_loss       | -0.038588315 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03025      |
------------------------------------------

 ---------------- Iteration 834 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 833         |
| Time/Actor_Time         | 0.0858      |
| Time/B_Format_Time      | 0.0732      |
| Time/B_Original_Form... | 0.0743      |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3147442   |
| Train/Action_magnitu... | 0.9620771   |
| Train/Action_magnitude  | 0.7585105   |
| Train/Action_max        | 0.3424279   |
| Train/Action_std        | 0.46444866  |
| Train/Entropy           | 0.4667783   |
| Train/Entropy_Loss      | -0.000467   |
| Train/Entropy_loss      | -0.000467   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.9295604  |
| Train/Loss              | 0.03176435  |
| Train/PolicyClip        | 0.012319709 |
| Train/Policy_loss       | 0.024270471 |
| Train/Ratio             | 0.98274976  |
| Train/Return            | 1.5015802   |
| Train/V                 | 1.5437254   |
| Train/Value             | 1.5437254   |
| Train/control_penalty   | 0.79606575  |
| Train/policy_loss       | 0.024270471 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 835 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 834         |
| Time/Actor_Time         | 0.0856      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.321166    |
| Train/Action_magnitu... | 0.9764093   |
| Train/Action_magnitude  | 0.7717273   |
| Train/Action_max        | 0.3555013   |
| Train/Action_std        | 0.4493644   |
| Train/Entropy           | 0.3908007   |
| Train/Entropy_Loss      | -0.000391   |
| Train/Entropy_loss      | -0.000391   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.7503262  |
| Train/Loss              | 0.06024883  |
| Train/PolicyClip        | 0.010998411 |
| Train/Policy_loss       | 0.052824777 |
| Train/Ratio             | 0.9795233   |
| Train/Return            | 1.6475886   |
| Train/V                 | 1.7141508   |
| Train/Value             | 1.7141508   |
| Train/control_penalty   | 0.7814854   |
| Train/policy_loss       | 0.052824777 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 836 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 835          |
| Time/Actor_Time         | 0.0843       |
| Time/B_Format_Time      | 0.0757       |
| Time/B_Original_Form... | 0.0733       |
| Time/Buffer             | 0.00287      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32626042   |
| Train/Action_magnitu... | 0.9976797    |
| Train/Action_magnitude  | 0.7894326    |
| Train/Action_max        | 0.38746142   |
| Train/Action_std        | 0.4394471    |
| Train/Entropy           | 0.3575924    |
| Train/Entropy_Loss      | -0.000358    |
| Train/Entropy_loss      | -0.000358    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7716204   |
| Train/Loss              | 0.007551449  |
| Train/PolicyClip        | 0.008590198  |
| Train/Policy_loss       | 6.930089e-05 |
| Train/Ratio             | 0.9930234    |
| Train/Return            | 1.5272062    |
| Train/V                 | 1.53986      |
| Train/Value             | 1.53986      |
| Train/control_penalty   | 0.78397405   |
| Train/policy_loss       | 6.930089e-05 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03225      |
------------------------------------------

 ---------------- Iteration 837 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 836          |
| Time/Actor_Time         | 0.122        |
| Time/B_Format_Time      | 0.079        |
| Time/B_Original_Form... | 0.0749       |
| Time/Buffer             | 0.00306      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.34523705   |
| Train/Action_magnitu... | 1.0116435    |
| Train/Action_magnitude  | 0.80088073   |
| Train/Action_max        | 0.38189483   |
| Train/Action_std        | 0.44269872   |
| Train/Entropy           | 0.38181636   |
| Train/Entropy_Loss      | -0.000382    |
| Train/Entropy_loss      | -0.000382    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7653553   |
| Train/Loss              | -0.03643027  |
| Train/PolicyClip        | 0.012690102  |
| Train/Policy_loss       | -0.044176556 |
| Train/Ratio             | 0.9991709    |
| Train/Return            | 1.6398296    |
| Train/V                 | 1.6094247    |
| Train/Value             | 1.6094247    |
| Train/control_penalty   | 0.81281024   |
| Train/policy_loss       | -0.044176556 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03675      |
------------------------------------------

 ---------------- Iteration 838 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 837          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0719       |
| Time/B_Original_Form... | 0.0739       |
| Time/Buffer             | 0.00308      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3280048    |
| Train/Action_magnitu... | 0.9782358    |
| Train/Action_magnitude  | 0.7758777    |
| Train/Action_max        | 0.45149347   |
| Train/Action_std        | 0.4223995    |
| Train/Entropy           | 0.33137816   |
| Train/Entropy_Loss      | -0.000331    |
| Train/Entropy_loss      | -0.000331    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7257984   |
| Train/Loss              | 0.0007999358 |
| Train/PolicyClip        | 0.021224633  |
| Train/Policy_loss       | -0.006462056 |
| Train/Ratio             | 0.9874346    |
| Train/Return            | 1.3730078    |
| Train/V                 | 1.3799136    |
| Train/Value             | 1.3799136    |
| Train/control_penalty   | 0.759337     |
| Train/policy_loss       | -0.006462056 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 839 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 838         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.0733      |
| Time/Buffer             | 0.00241     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.33988914  |
| Train/Action_magnitu... | 1.0115007   |
| Train/Action_magnitude  | 0.8034276   |
| Train/Action_max        | 0.44566134  |
| Train/Action_std        | 0.45904073  |
| Train/Entropy           | 0.40080237  |
| Train/Entropy_Loss      | -0.000401   |
| Train/Entropy_loss      | -0.000401   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.82523733 |
| Train/Loss              | 0.026234508 |
| Train/PolicyClip        | 0.01296963  |
| Train/Policy_loss       | 0.018622726 |
| Train/Ratio             | 0.97202146  |
| Train/Return            | 1.3505498   |
| Train/V                 | 1.3838122   |
| Train/Value             | 1.3838122   |
| Train/control_penalty   | 0.80125844  |
| Train/policy_loss       | 0.018622726 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 840 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 839         |
| Time/Actor_Time         | 0.0866      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0702      |
| Time/Buffer             | 0.00274     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.3382697   |
| Train/Action_magnitu... | 1.0337052   |
| Train/Action_magnitude  | 0.8183891   |
| Train/Action_max        | 0.4330603   |
| Train/Action_std        | 0.48872235  |
| Train/Entropy           | 0.5010136   |
| Train/Entropy_Loss      | -0.000501   |
| Train/Entropy_loss      | -0.000501   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.9857604  |
| Train/Loss              | 0.023912556 |
| Train/PolicyClip        | 0.007899162 |
| Train/Policy_loss       | 0.01608672  |
| Train/Ratio             | 0.98446023  |
| Train/Return            | 1.3418459   |
| Train/V                 | 1.3741488   |
| Train/Value             | 1.3741488   |
| Train/control_penalty   | 0.8326849   |
| Train/policy_loss       | 0.01608672  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 841 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 840          |
| Time/Actor_Time         | 0.184        |
| Time/B_Format_Time      | 0.0818       |
| Time/B_Original_Form... | 0.0745       |
| Time/Buffer             | 0.00944      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32950947   |
| Train/Action_magnitu... | 1.0631374    |
| Train/Action_magnitude  | 0.84489024   |
| Train/Action_max        | 0.45286757   |
| Train/Action_std        | 0.49244356   |
| Train/Entropy           | 0.49847823   |
| Train/Entropy_Loss      | -0.000498    |
| Train/Entropy_loss      | -0.000498    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0630714   |
| Train/Loss              | -0.04563381  |
| Train/PolicyClip        | 0.012261588  |
| Train/Policy_loss       | -0.053485885 |
| Train/Ratio             | 1.0067348    |
| Train/Return            | 1.4387469    |
| Train/V                 | 1.4019698    |
| Train/Value             | 1.4019698    |
| Train/control_penalty   | 0.8350552    |
| Train/policy_loss       | -0.053485885 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0315       |
------------------------------------------

 ---------------- Iteration 842 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 841         |
| Time/Actor_Time         | 0.0851      |
| Time/B_Format_Time      | 0.0708      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32984373  |
| Train/Action_magnitu... | 1.0453705   |
| Train/Action_magnitude  | 0.83252805  |
| Train/Action_max        | 0.4570114   |
| Train/Action_std        | 0.48701626  |
| Train/Entropy           | 0.49691343  |
| Train/Entropy_Loss      | -0.000497   |
| Train/Entropy_loss      | -0.000497   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.0305346  |
| Train/Loss              | 0.013816963 |
| Train/PolicyClip        | 0.010707606 |
| Train/Policy_loss       | 0.006041748 |
| Train/Ratio             | 1.000685    |
| Train/Return            | 1.3386433   |
| Train/V                 | 1.3592521   |
| Train/Value             | 1.3592521   |
| Train/control_penalty   | 0.82721287  |
| Train/policy_loss       | 0.006041748 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 843 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 842          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0723       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00307      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32299197   |
| Train/Action_magnitu... | 1.0612499    |
| Train/Action_magnitude  | 0.8424322    |
| Train/Action_max        | 0.45238698   |
| Train/Action_std        | 0.50502753   |
| Train/Entropy           | 0.5401664    |
| Train/Entropy_Loss      | -0.00054     |
| Train/Entropy_loss      | -0.00054     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1321658   |
| Train/Loss              | -0.027333077 |
| Train/PolicyClip        | 0.011231546  |
| Train/Policy_loss       | -0.035130717 |
| Train/Ratio             | 0.9922438    |
| Train/Return            | 1.104782     |
| Train/V                 | 1.0832349    |
| Train/Value             | 1.0832349    |
| Train/control_penalty   | 0.83378077   |
| Train/policy_loss       | -0.035130717 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02275      |
------------------------------------------

 ---------------- Iteration 844 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 843         |
| Time/Actor_Time         | 0.0852      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0724      |
| Time/Buffer             | 0.00245     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33224523  |
| Train/Action_magnitu... | 1.0460321   |
| Train/Action_magnitude  | 0.82957804  |
| Train/Action_max        | 0.4348994   |
| Train/Action_std        | 0.5224702   |
| Train/Entropy           | 0.5965279   |
| Train/Entropy_Loss      | -0.000597   |
| Train/Entropy_loss      | -0.000597   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1728581  |
| Train/Loss              | 0.037193563 |
| Train/PolicyClip        | 0.010182215 |
| Train/Policy_loss       | 0.029230453 |
| Train/Ratio             | 0.9835425   |
| Train/Return            | 1.0594966   |
| Train/V                 | 1.103551    |
| Train/Value             | 1.103551    |
| Train/control_penalty   | 0.8559638   |
| Train/policy_loss       | 0.029230453 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01675     |
-----------------------------------------

 ---------------- Iteration 845 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 844           |
| Time/Actor_Time         | 0.0855        |
| Time/B_Format_Time      | 0.0715        |
| Time/B_Original_Form... | 0.071         |
| Time/Buffer             | 0.00303       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.32045966    |
| Train/Action_magnitu... | 1.017471      |
| Train/Action_magnitude  | 0.806603      |
| Train/Action_max        | 0.4364777     |
| Train/Action_std        | 0.48015702    |
| Train/Entropy           | 0.47222066    |
| Train/Entropy_Loss      | -0.000472     |
| Train/Entropy_loss      | -0.000472     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.9947571    |
| Train/Loss              | 0.004338996   |
| Train/PolicyClip        | 0.0114156455  |
| Train/Policy_loss       | -0.0033032992 |
| Train/Ratio             | 1.0021404     |
| Train/Return            | 1.2542402     |
| Train/V                 | 1.2675302     |
| Train/Value             | 1.2675302     |
| Train/control_penalty   | 0.81145155    |
| Train/policy_loss       | -0.0033032992 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02525       |
-------------------------------------------

 ---------------- Iteration 846 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 845           |
| Time/Actor_Time         | 0.0849        |
| Time/B_Format_Time      | 0.0716        |
| Time/B_Original_Form... | 0.0707        |
| Time/Buffer             | 0.00294       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.33627903    |
| Train/Action_magnitu... | 1.0258864     |
| Train/Action_magnitude  | 0.8143876     |
| Train/Action_max        | 0.43732724    |
| Train/Action_std        | 0.5098785     |
| Train/Entropy           | 0.5382547     |
| Train/Entropy_Loss      | -0.000538     |
| Train/Entropy_loss      | -0.000538     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.0352815    |
| Train/Loss              | 0.00067131594 |
| Train/PolicyClip        | 0.010597578   |
| Train/Policy_loss       | -0.007358626  |
| Train/Ratio             | 0.97946364    |
| Train/Return            | 1.0904964     |
| Train/V                 | 1.0968789     |
| Train/Value             | 1.0968789     |
| Train/control_penalty   | 0.8568197     |
| Train/policy_loss       | -0.007358626  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02275       |
-------------------------------------------

 ---------------- Iteration 847 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 846         |
| Time/Actor_Time         | 0.0848      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00238     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.34931082  |
| Train/Action_magnitu... | 1.0520804   |
| Train/Action_magnitude  | 0.8313051   |
| Train/Action_max        | 0.44125727  |
| Train/Action_std        | 0.50415605  |
| Train/Entropy           | 0.5517279   |
| Train/Entropy_Loss      | -0.000552   |
| Train/Entropy_loss      | -0.000552   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.100834   |
| Train/Loss              | 0.04153301  |
| Train/PolicyClip        | 0.010422758 |
| Train/Policy_loss       | 0.033414796 |
| Train/Ratio             | 0.98552746  |
| Train/Return            | 1.0201423   |
| Train/V                 | 1.0682198   |
| Train/Value             | 1.0682198   |
| Train/control_penalty   | 0.86699384  |
| Train/policy_loss       | 0.033414796 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01625     |
-----------------------------------------

 ---------------- Iteration 848 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 847          |
| Time/Actor_Time         | 0.139        |
| Time/B_Format_Time      | 0.112        |
| Time/B_Original_Form... | 0.0971       |
| Time/Buffer             | 0.00298      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.33524135   |
| Train/Action_magnitu... | 1.0336567    |
| Train/Action_magnitude  | 0.8194367    |
| Train/Action_max        | 0.432744     |
| Train/Action_std        | 0.46540016   |
| Train/Entropy           | 0.41763994   |
| Train/Entropy_Loss      | -0.000418    |
| Train/Entropy_loss      | -0.000418    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.89891005  |
| Train/Loss              | -0.069921866 |
| Train/PolicyClip        | 0.011475462  |
| Train/Policy_loss       | -0.07771881  |
| Train/Ratio             | 1.0025795    |
| Train/Return            | 1.1456523    |
| Train/V                 | 1.0797926    |
| Train/Value             | 1.0797926    |
| Train/control_penalty   | 0.8214583    |
| Train/policy_loss       | -0.07771881  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 849 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 848          |
| Time/Actor_Time         | 0.0855       |
| Time/B_Format_Time      | 0.0734       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00347      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30976826   |
| Train/Action_magnitu... | 1.005174     |
| Train/Action_magnitude  | 0.7917986    |
| Train/Action_max        | 0.36785537   |
| Train/Action_std        | 0.45219213   |
| Train/Entropy           | 0.37697527   |
| Train/Entropy_Loss      | -0.000377    |
| Train/Entropy_loss      | -0.000377    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8197793   |
| Train/Loss              | -0.022192601 |
| Train/PolicyClip        | 0.01126574   |
| Train/Policy_loss       | -0.029592704 |
| Train/Ratio             | 0.9935597    |
| Train/Return            | 1.2143403    |
| Train/V                 | 1.1971552    |
| Train/Value             | 1.1971552    |
| Train/control_penalty   | 0.7777079    |
| Train/policy_loss       | -0.029592704 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0265       |
------------------------------------------

 ---------------- Iteration 850 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 849          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0716       |
| Time/B_Original_Form... | 0.0735       |
| Time/Buffer             | 0.00311      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32605496   |
| Train/Action_magnitu... | 1.0512507    |
| Train/Action_magnitude  | 0.8302917    |
| Train/Action_max        | 0.43932983   |
| Train/Action_std        | 0.4849985    |
| Train/Entropy           | 0.45809048   |
| Train/Entropy_Loss      | -0.000458    |
| Train/Entropy_loss      | -0.000458    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.9245459   |
| Train/Loss              | -0.03712119  |
| Train/PolicyClip        | 0.011565085  |
| Train/Policy_loss       | -0.044831462 |
| Train/Ratio             | 0.9963655    |
| Train/Return            | 1.2262667    |
| Train/V                 | 1.1950243    |
| Train/Value             | 1.1950243    |
| Train/control_penalty   | 0.8168364    |
| Train/policy_loss       | -0.044831462 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 851 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 850         |
| Time/Actor_Time         | 0.09        |
| Time/B_Format_Time      | 0.0781      |
| Time/B_Original_Form... | 0.0762      |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.31599194  |
| Train/Action_magnitu... | 1.0252306   |
| Train/Action_magnitude  | 0.81104     |
| Train/Action_max        | 0.42106578  |
| Train/Action_std        | 0.48522237  |
| Train/Entropy           | 0.4309479   |
| Train/Entropy_Loss      | -0.000431   |
| Train/Entropy_loss      | -0.000431   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.8961456  |
| Train/Loss              | -0.02075038 |
| Train/PolicyClip        | 0.009005082 |
| Train/Policy_loss       | -0.02860336 |
| Train/Ratio             | 0.9950997   |
| Train/Return            | 1.3681526   |
| Train/V                 | 1.3515458   |
| Train/Value             | 1.3515458   |
| Train/control_penalty   | 0.82839286  |
| Train/policy_loss       | -0.02860336 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 852 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 851         |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0722      |
| Time/Buffer             | 0.00306     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.31509152  |
| Train/Action_magnitu... | 1.032166    |
| Train/Action_magnitude  | 0.8177315   |
| Train/Action_max        | 0.40186384  |
| Train/Action_std        | 0.48608723  |
| Train/Entropy           | 0.44286606  |
| Train/Entropy_Loss      | -0.000443   |
| Train/Entropy_loss      | -0.000443   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.90807587 |
| Train/Loss              | 0.05468714  |
| Train/PolicyClip        | 0.010267242 |
| Train/Policy_loss       | 0.04678511  |
| Train/Ratio             | 0.98237     |
| Train/Return            | 1.3516272   |
| Train/V                 | 1.4119263   |
| Train/Value             | 1.4119263   |
| Train/control_penalty   | 0.83448964  |
| Train/policy_loss       | 0.04678511  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 853 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 852          |
| Time/Actor_Time         | 0.0859       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0714       |
| Time/Buffer             | 0.00312      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31816643   |
| Train/Action_magnitu... | 0.98826355   |
| Train/Action_magnitude  | 0.78425455   |
| Train/Action_max        | 0.39995316   |
| Train/Action_std        | 0.4781257    |
| Train/Entropy           | 0.43655956   |
| Train/Entropy_Loss      | -0.000437    |
| Train/Entropy_loss      | -0.000437    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7983246   |
| Train/Loss              | -0.057346355 |
| Train/PolicyClip        | 0.0114503605 |
| Train/Policy_loss       | -0.06507899  |
| Train/Ratio             | 0.9942747    |
| Train/Return            | 1.2527908    |
| Train/V                 | 1.2014328    |
| Train/Value             | 1.2014328    |
| Train/control_penalty   | 0.8169192    |
| Train/policy_loss       | -0.06507899  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.031        |
------------------------------------------

 ---------------- Iteration 854 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 853          |
| Time/Actor_Time         | 0.0854       |
| Time/B_Format_Time      | 0.0755       |
| Time/B_Original_Form... | 0.0735       |
| Time/Buffer             | 0.00333      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31563064   |
| Train/Action_magnitu... | 0.9748435    |
| Train/Action_magnitude  | 0.7718717    |
| Train/Action_max        | 0.3832659    |
| Train/Action_std        | 0.4427788    |
| Train/Entropy           | 0.38440716   |
| Train/Entropy_Loss      | -0.000384    |
| Train/Entropy_loss      | -0.000384    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.8392886   |
| Train/Loss              | -0.02427541  |
| Train/PolicyClip        | 0.010161979  |
| Train/Policy_loss       | -0.031711437 |
| Train/Ratio             | 1.0049233    |
| Train/Return            | 1.233863     |
| Train/V                 | 1.21803      |
| Train/Value             | 1.21803      |
| Train/control_penalty   | 0.7820436    |
| Train/policy_loss       | -0.031711437 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02725      |
------------------------------------------

 ---------------- Iteration 855 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 854         |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00382     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31665155  |
| Train/Action_magnitu... | 1.0077137   |
| Train/Action_magnitude  | 0.7982784   |
| Train/Action_max        | 0.40096328  |
| Train/Action_std        | 0.44924897  |
| Train/Entropy           | 0.38261232  |
| Train/Entropy_Loss      | -0.000383   |
| Train/Entropy_loss      | -0.000383   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.8140925  |
| Train/Loss              | -0.07223153 |
| Train/PolicyClip        | 0.013071539 |
| Train/Policy_loss       | -0.07970527 |
| Train/Ratio             | 1.0082088   |
| Train/Return            | 1.5288862   |
| Train/V                 | 1.4673994   |
| Train/Value             | 1.4673994   |
| Train/control_penalty   | 0.78563476  |
| Train/policy_loss       | -0.07970527 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0365      |
-----------------------------------------

 ---------------- Iteration 856 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 855           |
| Time/Actor_Time         | 0.0857        |
| Time/B_Format_Time      | 0.0728        |
| Time/B_Original_Form... | 0.0825        |
| Time/Buffer             | 0.00241       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3079778     |
| Train/Action_magnitu... | 1.0152761     |
| Train/Action_magnitude  | 0.8042485     |
| Train/Action_max        | 0.4320335     |
| Train/Action_std        | 0.48921886    |
| Train/Entropy           | 0.4457665     |
| Train/Entropy_Loss      | -0.000446     |
| Train/Entropy_loss      | -0.000446     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.8882529    |
| Train/Loss              | 0.0060536605  |
| Train/PolicyClip        | 0.01203064    |
| Train/Policy_loss       | -0.0017813159 |
| Train/Ratio             | 0.98482555    |
| Train/Return            | 1.3005602     |
| Train/V                 | 1.314139      |
| Train/Value             | 1.314139      |
| Train/control_penalty   | 0.82807434    |
| Train/policy_loss       | -0.0017813159 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02275       |
-------------------------------------------

 ---------------- Iteration 857 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 856         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31170976  |
| Train/Action_magnitu... | 0.99784756  |
| Train/Action_magnitude  | 0.79089177  |
| Train/Action_max        | 0.3870255   |
| Train/Action_std        | 0.47149903  |
| Train/Entropy           | 0.42751154  |
| Train/Entropy_Loss      | -0.000428   |
| Train/Entropy_loss      | -0.000428   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.8901027  |
| Train/Loss              | -0.0696625  |
| Train/PolicyClip        | 0.011913705 |
| Train/Policy_loss       | -0.07740442 |
| Train/Ratio             | 1.0026913   |
| Train/Return            | 1.4905438   |
| Train/V                 | 1.427528    |
| Train/Value             | 1.427528    |
| Train/control_penalty   | 0.8169436   |
| Train/policy_loss       | -0.07740442 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 858 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 857         |
| Time/Actor_Time         | 0.0849      |
| Time/B_Format_Time      | 0.0768      |
| Time/B_Original_Form... | 0.0982      |
| Time/Buffer             | 0.00306     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31485608  |
| Train/Action_magnitu... | 1.0885578   |
| Train/Action_magnitude  | 0.8650749   |
| Train/Action_max        | 0.39998958  |
| Train/Action_std        | 0.50218475  |
| Train/Entropy           | 0.4442729   |
| Train/Entropy_Loss      | -0.000444   |
| Train/Entropy_loss      | -0.000444   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.96251833 |
| Train/Loss              | -0.03809084 |
| Train/PolicyClip        | 0.01138934  |
| Train/Policy_loss       | -0.0461168  |
| Train/Ratio             | 0.9950689   |
| Train/Return            | 1.3911208   |
| Train/V                 | 1.3598897   |
| Train/Value             | 1.3598897   |
| Train/control_penalty   | 0.8470234   |
| Train/policy_loss       | -0.0461168  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 859 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 858          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0739       |
| Time/B_Original_Form... | 0.0747       |
| Time/Buffer             | 0.00294      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3058954    |
| Train/Action_magnitu... | 1.0122906    |
| Train/Action_magnitude  | 0.80634964   |
| Train/Action_max        | 0.4149487    |
| Train/Action_std        | 0.47642532   |
| Train/Entropy           | 0.42115843   |
| Train/Entropy_Loss      | -0.000421    |
| Train/Entropy_loss      | -0.000421    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.89453614  |
| Train/Loss              | -0.025579691 |
| Train/PolicyClip        | 0.009966392  |
| Train/Policy_loss       | -0.033142976 |
| Train/Ratio             | 0.99879694   |
| Train/Return            | 1.2668562    |
| Train/V                 | 1.2482426    |
| Train/Value             | 1.2482426    |
| Train/control_penalty   | 0.79844457   |
| Train/policy_loss       | -0.033142976 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0225       |
------------------------------------------

 ---------------- Iteration 860 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 859         |
| Time/Actor_Time         | 0.223       |
| Time/B_Format_Time      | 0.12        |
| Time/B_Original_Form... | 0.0826      |
| Time/Buffer             | 0.00311     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3074657   |
| Train/Action_magnitu... | 1.0087377   |
| Train/Action_magnitude  | 0.79775566  |
| Train/Action_max        | 0.41288018  |
| Train/Action_std        | 0.45917517  |
| Train/Entropy           | 0.3940577   |
| Train/Entropy_Loss      | -0.000394   |
| Train/Entropy_loss      | -0.000394   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.82783145 |
| Train/Loss              | -0.09100966 |
| Train/PolicyClip        | 0.009324421 |
| Train/Policy_loss       | -0.0985862  |
| Train/Ratio             | 1.0078554   |
| Train/Return            | 1.3576456   |
| Train/V                 | 1.2739366   |
| Train/Value             | 1.2739366   |
| Train/control_penalty   | 0.79706013  |
| Train/policy_loss       | -0.0985862  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03175     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 861 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 860          |
| Time/Actor_Time         | 0.116        |
| Time/B_Format_Time      | 0.0817       |
| Time/B_Original_Form... | 0.085        |
| Time/Buffer             | 0.00497      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30038774   |
| Train/Action_magnitu... | 0.99583906   |
| Train/Action_magnitude  | 0.7863188    |
| Train/Action_max        | 0.41306502   |
| Train/Action_std        | 0.45694152   |
| Train/Entropy           | 0.39407596   |
| Train/Entropy_Loss      | -0.000394    |
| Train/Entropy_loss      | -0.000394    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.85259193  |
| Train/Loss              | -0.013444509 |
| Train/PolicyClip        | 0.013737035  |
| Train/Policy_loss       | -0.020786183 |
| Train/Ratio             | 0.99379885   |
| Train/Return            | 1.0864322    |
| Train/V                 | 1.0758203    |
| Train/Value             | 1.0758203    |
| Train/control_penalty   | 0.77357495   |
| Train/policy_loss       | -0.020786183 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------

 ---------------- Iteration 862 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 861          |
| Time/Actor_Time         | 0.086        |
| Time/B_Format_Time      | 0.0733       |
| Time/B_Original_Form... | 0.0731       |
| Time/Buffer             | 0.00516      |
| Time/Critic_Time        | 9.54e-07     |
| Train/Action_abs_mean   | 0.30992025   |
| Train/Action_magnitu... | 0.9974779    |
| Train/Action_magnitude  | 0.7882142    |
| Train/Action_max        | 0.40008342   |
| Train/Action_std        | 0.45185256   |
| Train/Entropy           | 0.402264     |
| Train/Entropy_Loss      | -0.000402    |
| Train/Entropy_loss      | -0.000402    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.85690993  |
| Train/Loss              | -0.027275126 |
| Train/PolicyClip        | 0.0084452275 |
| Train/Policy_loss       | -0.034708243 |
| Train/Ratio             | 0.99955076   |
| Train/Return            | 1.2054021    |
| Train/V                 | 1.1834885    |
| Train/Value             | 1.1834885    |
| Train/control_penalty   | 0.7835381    |
| Train/policy_loss       | -0.034708243 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 863 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 862          |
| Time/Actor_Time         | 0.0847       |
| Time/B_Format_Time      | 0.0742       |
| Time/B_Original_Form... | 0.0741       |
| Time/Buffer             | 0.00572      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.2931059    |
| Train/Action_magnitu... | 0.9325085    |
| Train/Action_magnitude  | 0.7365537    |
| Train/Action_max        | 0.35028595   |
| Train/Action_std        | 0.4262853    |
| Train/Entropy           | 0.33908162   |
| Train/Entropy_Loss      | -0.000339    |
| Train/Entropy_loss      | -0.000339    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7572926   |
| Train/Loss              | -0.045458857 |
| Train/PolicyClip        | 0.0141058955 |
| Train/Policy_loss       | -0.05253123  |
| Train/Ratio             | 0.9960735    |
| Train/Return            | 1.2264917    |
| Train/V                 | 1.1856345    |
| Train/Value             | 1.1856345    |
| Train/control_penalty   | 0.7411457    |
| Train/policy_loss       | -0.05253123  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0285       |
------------------------------------------

 ---------------- Iteration 864 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 863          |
| Time/Actor_Time         | 0.0851       |
| Time/B_Format_Time      | 0.0775       |
| Time/B_Original_Form... | 0.0764       |
| Time/Buffer             | 0.00386      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31540576   |
| Train/Action_magnitu... | 0.9838775    |
| Train/Action_magnitude  | 0.7776394    |
| Train/Action_max        | 0.39598116   |
| Train/Action_std        | 0.44739985   |
| Train/Entropy           | 0.37453684   |
| Train/Entropy_Loss      | -0.000375    |
| Train/Entropy_loss      | -0.000375    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.7064999   |
| Train/Loss              | -0.041298117 |
| Train/PolicyClip        | 0.008774622  |
| Train/Policy_loss       | -0.048699856 |
| Train/Ratio             | 0.99834013   |
| Train/Return            | 1.436956     |
| Train/V                 | 1.398936     |
| Train/Value             | 1.398936     |
| Train/control_penalty   | 0.7776274    |
| Train/policy_loss       | -0.048699856 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.035        |
------------------------------------------

 ---------------- Iteration 865 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 864           |
| Time/Actor_Time         | 0.0841        |
| Time/B_Format_Time      | 0.0801        |
| Time/B_Original_Form... | 0.0732        |
| Time/Buffer             | 0.0037        |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3133646     |
| Train/Action_magnitu... | 0.9901097     |
| Train/Action_magnitude  | 0.7832912     |
| Train/Action_max        | 0.3947532     |
| Train/Action_std        | 0.48589468    |
| Train/Entropy           | 0.46717706    |
| Train/Entropy_Loss      | -0.000467     |
| Train/Entropy_loss      | -0.000467     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -0.8478518    |
| Train/Loss              | 0.0025279694  |
| Train/PolicyClip        | 0.014439025   |
| Train/Policy_loss       | -0.0051391395 |
| Train/Ratio             | 0.9989443     |
| Train/Return            | 1.4249257     |
| Train/V                 | 1.4302279     |
| Train/Value             | 1.4302279     |
| Train/control_penalty   | 0.81342864    |
| Train/policy_loss       | -0.0051391395 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0275        |
-------------------------------------------

 ---------------- Iteration 866 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 865          |
| Time/Actor_Time         | 0.0833       |
| Time/B_Format_Time      | 0.0751       |
| Time/B_Original_Form... | 0.0897       |
| Time/Buffer             | 0.00343      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31860375   |
| Train/Action_magnitu... | 1.0472205    |
| Train/Action_magnitude  | 0.83154184   |
| Train/Action_max        | 0.40878507   |
| Train/Action_std        | 0.4813663    |
| Train/Entropy           | 0.4632748    |
| Train/Entropy_Loss      | -0.000463    |
| Train/Entropy_loss      | -0.000463    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0226995   |
| Train/Loss              | -0.022578321 |
| Train/PolicyClip        | 0.010466057  |
| Train/Policy_loss       | -0.030435476 |
| Train/Ratio             | 1.0054032    |
| Train/Return            | 1.5045415    |
| Train/V                 | 1.4891834    |
| Train/Value             | 1.4891834    |
| Train/control_penalty   | 0.8320431    |
| Train/policy_loss       | -0.030435476 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0305       |
------------------------------------------

 ---------------- Iteration 867 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 866         |
| Time/Actor_Time         | 0.087       |
| Time/B_Format_Time      | 0.0786      |
| Time/B_Original_Form... | 0.0773      |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32288313  |
| Train/Action_magnitu... | 1.0618727   |
| Train/Action_magnitude  | 0.8399608   |
| Train/Action_max        | 0.39943263  |
| Train/Action_std        | 0.515369    |
| Train/Entropy           | 0.5459611   |
| Train/Entropy_Loss      | -0.000546   |
| Train/Entropy_loss      | -0.000546   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1333897  |
| Train/Loss              | -0.03924624 |
| Train/PolicyClip        | 0.013225517 |
| Train/Policy_loss       | -0.04722397 |
| Train/Ratio             | 1.0069337   |
| Train/Return            | 1.359265    |
| Train/V                 | 1.3261057   |
| Train/Value             | 1.3261057   |
| Train/control_penalty   | 0.8523692   |
| Train/policy_loss       | -0.04722397 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03325     |
-----------------------------------------

 ---------------- Iteration 868 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 867         |
| Time/Actor_Time         | 0.0837      |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00265     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3312508   |
| Train/Action_magnitu... | 1.0394366   |
| Train/Action_magnitude  | 0.82120633  |
| Train/Action_max        | 0.426546    |
| Train/Action_std        | 0.48970395  |
| Train/Entropy           | 0.5032459   |
| Train/Entropy_Loss      | -0.000503   |
| Train/Entropy_loss      | -0.000503   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.0252632  |
| Train/Loss              | 0.057178166 |
| Train/PolicyClip        | 0.010710978 |
| Train/Policy_loss       | 0.04925913  |
| Train/Ratio             | 0.97818285  |
| Train/Return            | 1.036601    |
| Train/V                 | 1.1002522   |
| Train/Value             | 1.1002522   |
| Train/control_penalty   | 0.8422283   |
| Train/policy_loss       | 0.04925913  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01375     |
-----------------------------------------

 ---------------- Iteration 869 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 868         |
| Time/Actor_Time         | 0.0857      |
| Time/B_Format_Time      | 0.0787      |
| Time/B_Original_Form... | 0.0739      |
| Time/Buffer             | 0.00377     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30648398  |
| Train/Action_magnitu... | 1.0779972   |
| Train/Action_magnitude  | 0.8556797   |
| Train/Action_max        | 0.41311222  |
| Train/Action_std        | 0.51366454  |
| Train/Entropy           | 0.54736876  |
| Train/Entropy_Loss      | -0.000547   |
| Train/Entropy_loss      | -0.000547   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1760892  |
| Train/Loss              | -0.07945667 |
| Train/PolicyClip        | 0.008592861 |
| Train/Policy_loss       | -0.08727754 |
| Train/Ratio             | 1.0113535   |
| Train/Return            | 1.0704079   |
| Train/V                 | 0.9953524   |
| Train/Value             | 0.9953524   |
| Train/control_penalty   | 0.83682394  |
| Train/policy_loss       | -0.08727754 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 870 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 869           |
| Time/Actor_Time         | 0.0829        |
| Time/B_Format_Time      | 0.0765        |
| Time/B_Original_Form... | 0.0731        |
| Time/Buffer             | 0.00279       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.3181047     |
| Train/Action_magnitu... | 1.0784348     |
| Train/Action_magnitude  | 0.85199106    |
| Train/Action_max        | 0.42991456    |
| Train/Action_std        | 0.49129406    |
| Train/Entropy           | 0.49195454    |
| Train/Entropy_Loss      | -0.000492     |
| Train/Entropy_loss      | -0.000492     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.0685395    |
| Train/Loss              | -0.0036603212 |
| Train/PolicyClip        | 0.007355457   |
| Train/Policy_loss       | -0.011574637  |
| Train/Ratio             | 0.9844068     |
| Train/Return            | 1.1392214     |
| Train/V                 | 1.1423674     |
| Train/Value             | 1.1423674     |
| Train/control_penalty   | 0.840627      |
| Train/policy_loss       | -0.011574637  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02025       |
-------------------------------------------

 ---------------- Iteration 871 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 870         |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0759      |
| Time/B_Original_Form... | 0.079       |
| Time/Buffer             | 0.0028      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31247187  |
| Train/Action_magnitu... | 1.0136454   |
| Train/Action_magnitude  | 0.80044603  |
| Train/Action_max        | 0.4186541   |
| Train/Action_std        | 0.46037358  |
| Train/Entropy           | 0.4191043   |
| Train/Entropy_Loss      | -0.000419   |
| Train/Entropy_loss      | -0.000419   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.8975264  |
| Train/Loss              | -0.02962458 |
| Train/PolicyClip        | 0.009584339 |
| Train/Policy_loss       | -0.03712923 |
| Train/Ratio             | 0.9949303   |
| Train/Return            | 1.213517    |
| Train/V                 | 1.1884986   |
| Train/Value             | 1.1884986   |
| Train/control_penalty   | 0.7923754   |
| Train/policy_loss       | -0.03712923 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 872 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 871         |
| Time/Actor_Time         | 0.0852      |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00279     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3284862   |
| Train/Action_magnitu... | 1.0364581   |
| Train/Action_magnitude  | 0.8178958   |
| Train/Action_max        | 0.4511624   |
| Train/Action_std        | 0.47876075  |
| Train/Entropy           | 0.47162703  |
| Train/Entropy_Loss      | -0.000472   |
| Train/Entropy_loss      | -0.000472   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.98373675 |
| Train/Loss              | 0.02193294  |
| Train/PolicyClip        | 0.012152671 |
| Train/Policy_loss       | 0.014228034 |
| Train/Ratio             | 0.97135085  |
| Train/Return            | 1.075387    |
| Train/V                 | 1.1037796   |
| Train/Value             | 1.1037796   |
| Train/control_penalty   | 0.81765354  |
| Train/policy_loss       | 0.014228034 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 873 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 872          |
| Time/Actor_Time         | 0.0848       |
| Time/B_Format_Time      | 0.0739       |
| Time/B_Original_Form... | 0.0735       |
| Time/Buffer             | 0.00511      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32874733   |
| Train/Action_magnitu... | 1.0429217    |
| Train/Action_magnitude  | 0.8225841    |
| Train/Action_max        | 0.40968305   |
| Train/Action_std        | 0.46923527   |
| Train/Entropy           | 0.46873823   |
| Train/Entropy_Loss      | -0.000469    |
| Train/Entropy_loss      | -0.000469    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.966631    |
| Train/Loss              | -0.036025688 |
| Train/PolicyClip        | 0.011261981  |
| Train/Policy_loss       | -0.043557372 |
| Train/Ratio             | 0.9924679    |
| Train/Return            | 1.3282912    |
| Train/V                 | 1.2992171    |
| Train/Value             | 1.2992171    |
| Train/control_penalty   | 0.80004203   |
| Train/policy_loss       | -0.043557372 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02375      |
------------------------------------------

 ---------------- Iteration 874 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 873          |
| Time/Actor_Time         | 0.083        |
| Time/B_Format_Time      | 0.0737       |
| Time/B_Original_Form... | 0.0745       |
| Time/Buffer             | 0.00279      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3056808    |
| Train/Action_magnitu... | 1.0183277    |
| Train/Action_magnitude  | 0.8061926    |
| Train/Action_max        | 0.36848882   |
| Train/Action_std        | 0.507895     |
| Train/Entropy           | 0.54174405   |
| Train/Entropy_Loss      | -0.000542    |
| Train/Entropy_loss      | -0.000542    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0506306   |
| Train/Loss              | 0.05980953   |
| Train/PolicyClip        | 0.0073802527 |
| Train/Policy_loss       | 0.052173935  |
| Train/Ratio             | 0.9699888    |
| Train/Return            | 1.0672456    |
| Train/V                 | 1.1359733    |
| Train/Value             | 1.1359733    |
| Train/control_penalty   | 0.81773365   |
| Train/policy_loss       | 0.052173935  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0175       |
------------------------------------------

 ---------------- Iteration 875 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 874           |
| Time/Actor_Time         | 0.0914        |
| Time/B_Format_Time      | 0.0878        |
| Time/B_Original_Form... | 0.0759        |
| Time/Buffer             | 0.00342       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31034106    |
| Train/Action_magnitu... | 1.066228      |
| Train/Action_magnitude  | 0.8423362     |
| Train/Action_max        | 0.41801503    |
| Train/Action_std        | 0.49767613    |
| Train/Entropy           | 0.5262167     |
| Train/Entropy_Loss      | -0.000526     |
| Train/Entropy_loss      | -0.000526     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.1187658    |
| Train/Loss              | -0.0024258904 |
| Train/PolicyClip        | 0.009238949   |
| Train/Policy_loss       | -0.010167694  |
| Train/Ratio             | 0.9903478     |
| Train/Return            | 1.0034983     |
| Train/V                 | 1.007368      |
| Train/Value             | 1.007368      |
| Train/control_penalty   | 0.826802      |
| Train/policy_loss       | -0.010167694  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.019         |
-------------------------------------------

 ---------------- Iteration 876 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 875         |
| Time/Actor_Time         | 0.0833      |
| Time/B_Format_Time      | 0.0744      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00439     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31389517  |
| Train/Action_magnitu... | 0.9802486   |
| Train/Action_magnitude  | 0.7743142   |
| Train/Action_max        | 0.41097724  |
| Train/Action_std        | 0.44579843  |
| Train/Entropy           | 0.41224614  |
| Train/Entropy_Loss      | -0.000412   |
| Train/Entropy_loss      | -0.000412   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.81709665 |
| Train/Loss              | 0.021158883 |
| Train/PolicyClip        | 0.00914728  |
| Train/Policy_loss       | 0.013843836 |
| Train/Ratio             | 0.9909344   |
| Train/Return            | 1.2532938   |
| Train/V                 | 1.2838416   |
| Train/Value             | 1.2838416   |
| Train/control_penalty   | 0.77272934  |
| Train/policy_loss       | 0.013843836 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 877 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 876          |
| Time/Actor_Time         | 0.0837       |
| Time/B_Format_Time      | 0.076        |
| Time/B_Original_Form... | 0.076        |
| Time/Buffer             | 0.00254      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.3070666    |
| Train/Action_magnitu... | 1.0377843    |
| Train/Action_magnitude  | 0.82101727   |
| Train/Action_max        | 0.3969478    |
| Train/Action_std        | 0.4936234    |
| Train/Entropy           | 0.52544224   |
| Train/Entropy_Loss      | -0.000525    |
| Train/Entropy_loss      | -0.000525    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0479568   |
| Train/Loss              | -0.035729967 |
| Train/PolicyClip        | 0.013539524  |
| Train/Policy_loss       | -0.04333299  |
| Train/Ratio             | 1.0137024    |
| Train/Return            | 1.1319072    |
| Train/V                 | 1.0989391    |
| Train/Value             | 1.0989391    |
| Train/control_penalty   | 0.8128465    |
| Train/policy_loss       | -0.04333299  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0205       |
------------------------------------------

 ---------------- Iteration 878 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 877         |
| Time/Actor_Time         | 0.0879      |
| Time/B_Format_Time      | 0.0772      |
| Time/B_Original_Form... | 0.0773      |
| Time/Buffer             | 0.00294     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.31853718  |
| Train/Action_magnitu... | 1.058241    |
| Train/Action_magnitude  | 0.83548474  |
| Train/Action_max        | 0.4336978   |
| Train/Action_std        | 0.50757194  |
| Train/Entropy           | 0.54037255  |
| Train/Entropy_Loss      | -0.00054    |
| Train/Entropy_loss      | -0.00054    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1022437  |
| Train/Loss              | 0.011244488 |
| Train/PolicyClip        | 0.013785028 |
| Train/Policy_loss       | 0.003548116 |
| Train/Ratio             | 0.984592    |
| Train/Return            | 1.2335591   |
| Train/V                 | 1.2544539   |
| Train/Value             | 1.2544539   |
| Train/control_penalty   | 0.8236745   |
| Train/policy_loss       | 0.003548116 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 879 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 878           |
| Time/Actor_Time         | 0.0869        |
| Time/B_Format_Time      | 0.084         |
| Time/B_Original_Form... | 0.0846        |
| Time/Buffer             | 0.00285       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31440943    |
| Train/Action_magnitu... | 1.0346605     |
| Train/Action_magnitude  | 0.8141538     |
| Train/Action_max        | 0.42085865    |
| Train/Action_std        | 0.49504325    |
| Train/Entropy           | 0.54271543    |
| Train/Entropy_Loss      | -0.000543     |
| Train/Entropy_loss      | -0.000543     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.1506711    |
| Train/Loss              | -0.0002560811 |
| Train/PolicyClip        | 0.0111153815  |
| Train/Policy_loss       | -0.00797475   |
| Train/Ratio             | 0.98565996    |
| Train/Return            | 1.123851      |
| Train/V                 | 1.1311522     |
| Train/Value             | 1.1311522     |
| Train/control_penalty   | 0.82613844    |
| Train/policy_loss       | -0.00797475   |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02          |
-------------------------------------------

 ---------------- Iteration 880 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 879          |
| Time/Actor_Time         | 0.0834       |
| Time/B_Format_Time      | 0.0743       |
| Time/B_Original_Form... | 0.0742       |
| Time/Buffer             | 0.00309      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3063795    |
| Train/Action_magnitu... | 1.0760908    |
| Train/Action_magnitude  | 0.84908354   |
| Train/Action_max        | 0.42855197   |
| Train/Action_std        | 0.5183785    |
| Train/Entropy           | 0.56787694   |
| Train/Entropy_Loss      | -0.000568    |
| Train/Entropy_loss      | -0.000568    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1991311   |
| Train/Loss              | -0.051743276 |
| Train/PolicyClip        | 0.008989454  |
| Train/Policy_loss       | -0.059477486 |
| Train/Ratio             | 1.0016928    |
| Train/Return            | 1.1965388    |
| Train/V                 | 1.1483746    |
| Train/Value             | 1.1483746    |
| Train/control_penalty   | 0.83020836   |
| Train/policy_loss       | -0.059477486 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 881 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 880          |
| Time/Actor_Time         | 0.0905       |
| Time/B_Format_Time      | 0.0773       |
| Time/B_Original_Form... | 0.0747       |
| Time/Buffer             | 0.00345      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32341948   |
| Train/Action_magnitu... | 1.1249988    |
| Train/Action_magnitude  | 0.88799644   |
| Train/Action_max        | 0.42701724   |
| Train/Action_std        | 0.54267234   |
| Train/Entropy           | 0.5980572    |
| Train/Entropy_Loss      | -0.000598    |
| Train/Entropy_loss      | -0.000598    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.2588716   |
| Train/Loss              | -0.07363581  |
| Train/PolicyClip        | 0.0069454047 |
| Train/Policy_loss       | -0.08179509  |
| Train/Ratio             | 0.9991896    |
| Train/Return            | 1.2213132    |
| Train/V                 | 1.1514981    |
| Train/Value             | 1.1514981    |
| Train/control_penalty   | 0.87573403   |
| Train/policy_loss       | -0.08179509  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02975      |
------------------------------------------

 ---------------- Iteration 882 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 881         |
| Time/Actor_Time         | 0.0832      |
| Time/B_Format_Time      | 0.0724      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31646782  |
| Train/Action_magnitu... | 1.0951545   |
| Train/Action_magnitude  | 0.863483    |
| Train/Action_max        | 0.42978206  |
| Train/Action_std        | 0.5351352   |
| Train/Entropy           | 0.5879833   |
| Train/Entropy_Loss      | -0.000588   |
| Train/Entropy_loss      | -0.000588   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.2265431  |
| Train/Loss              | -0.0625532  |
| Train/PolicyClip        | 0.00686073  |
| Train/Policy_loss       | -0.07068582 |
| Train/Ratio             | 1.0080261   |
| Train/Return            | 1.1521685   |
| Train/V                 | 1.0947697   |
| Train/Value             | 1.0947697   |
| Train/control_penalty   | 0.8720611   |
| Train/policy_loss       | -0.07068582 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 883 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 882          |
| Time/Actor_Time         | 0.0917       |
| Time/B_Format_Time      | 0.0862       |
| Time/B_Original_Form... | 0.099        |
| Time/Buffer             | 0.0035       |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31722498   |
| Train/Action_magnitu... | 1.0786113    |
| Train/Action_magnitude  | 0.852005     |
| Train/Action_max        | 0.43728966   |
| Train/Action_std        | 0.5124483    |
| Train/Entropy           | 0.5475099    |
| Train/Entropy_Loss      | -0.000548    |
| Train/Entropy_loss      | -0.000548    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1576023   |
| Train/Loss              | -0.014633254 |
| Train/PolicyClip        | 0.009418813  |
| Train/Policy_loss       | -0.02234457  |
| Train/Ratio             | 0.99817395   |
| Train/Return            | 1.2165176    |
| Train/V                 | 1.20627      |
| Train/Value             | 1.20627      |
| Train/control_penalty   | 0.8258827    |
| Train/policy_loss       | -0.02234457  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0255       |
------------------------------------------

 ---------------- Iteration 884 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 883          |
| Time/Actor_Time         | 0.085        |
| Time/B_Format_Time      | 0.0758       |
| Time/B_Original_Form... | 0.0723       |
| Time/Buffer             | 0.00309      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32996252   |
| Train/Action_magnitu... | 1.092188     |
| Train/Action_magnitude  | 0.86620814   |
| Train/Action_max        | 0.40165436   |
| Train/Action_std        | 0.5146224    |
| Train/Entropy           | 0.5359289    |
| Train/Entropy_Loss      | -0.000536    |
| Train/Entropy_loss      | -0.000536    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.2043338   |
| Train/Loss              | -0.059863504 |
| Train/PolicyClip        | 0.009504471  |
| Train/Policy_loss       | -0.06789451  |
| Train/Ratio             | 1.0012693    |
| Train/Return            | 1.3277816    |
| Train/V                 | 1.2726918    |
| Train/Value             | 1.2726918    |
| Train/control_penalty   | 0.8566935    |
| Train/policy_loss       | -0.06789451  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.031        |
------------------------------------------

 ---------------- Iteration 885 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 884         |
| Time/Actor_Time         | 0.0844      |
| Time/B_Format_Time      | 0.0759      |
| Time/B_Original_Form... | 0.0745      |
| Time/Buffer             | 0.00389     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33728105  |
| Train/Action_magnitu... | 1.0369682   |
| Train/Action_magnitude  | 0.81936115  |
| Train/Action_max        | 0.4097682   |
| Train/Action_std        | 0.46275514  |
| Train/Entropy           | 0.40660253  |
| Train/Entropy_Loss      | -0.000407   |
| Train/Entropy_loss      | -0.000407   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.9071671  |
| Train/Loss              | -0.07459186 |
| Train/PolicyClip        | 0.013649147 |
| Train/Policy_loss       | -0.08231096 |
| Train/Ratio             | 1.0090276   |
| Train/Return            | 1.4285979   |
| Train/V                 | 1.3609449   |
| Train/Value             | 1.3609449   |
| Train/control_penalty   | 0.8125697   |
| Train/policy_loss       | -0.08231096 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.035       |
-----------------------------------------

 ---------------- Iteration 886 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 885         |
| Time/Actor_Time         | 0.0922      |
| Time/B_Format_Time      | 0.0754      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00361     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.32406354  |
| Train/Action_magnitu... | 0.98921096  |
| Train/Action_magnitude  | 0.78212136  |
| Train/Action_max        | 0.3849333   |
| Train/Action_std        | 0.46057835  |
| Train/Entropy           | 0.40985918  |
| Train/Entropy_Loss      | -0.00041    |
| Train/Entropy_loss      | -0.00041    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.8335911  |
| Train/Loss              | 0.03580055  |
| Train/PolicyClip        | 0.010646797 |
| Train/Policy_loss       | 0.028264541 |
| Train/Ratio             | 0.98378295  |
| Train/Return            | 1.508648    |
| Train/V                 | 1.5540681   |
| Train/Value             | 1.5540681   |
| Train/control_penalty   | 0.79458666  |
| Train/policy_loss       | 0.028264541 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 887 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 886         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.33247742  |
| Train/Action_magnitu... | 1.0738015   |
| Train/Action_magnitude  | 0.8491818   |
| Train/Action_max        | 0.38707528  |
| Train/Action_std        | 0.50516886  |
| Train/Entropy           | 0.49491927  |
| Train/Entropy_Loss      | -0.000495   |
| Train/Entropy_loss      | -0.000495   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.0753869  |
| Train/Loss              | 0.059210815 |
| Train/PolicyClip        | 0.01447039  |
| Train/Policy_loss       | 0.051317595 |
| Train/Ratio             | 0.9859956   |
| Train/Return            | 1.4202476   |
| Train/V                 | 1.488866    |
| Train/Value             | 1.488866    |
| Train/control_penalty   | 0.8388137   |
| Train/policy_loss       | 0.051317595 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 888 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 887         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00246     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3356318   |
| Train/Action_magnitu... | 1.1588992   |
| Train/Action_magnitude  | 0.9196481   |
| Train/Action_max        | 0.43658295  |
| Train/Action_std        | 0.6061729   |
| Train/Entropy           | 0.65616006  |
| Train/Entropy_Loss      | -0.000656   |
| Train/Entropy_loss      | -0.000656   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.247569   |
| Train/Loss              | 0.05131346  |
| Train/PolicyClip        | 0.010959031 |
| Train/Policy_loss       | 0.042310443 |
| Train/Ratio             | 0.9780866   |
| Train/Return            | 1.1657659   |
| Train/V                 | 1.2228248   |
| Train/Value             | 1.2228248   |
| Train/control_penalty   | 0.96591765  |
| Train/policy_loss       | 0.042310443 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 889 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 888         |
| Time/Actor_Time         | 0.0836      |
| Time/B_Format_Time      | 0.0761      |
| Time/B_Original_Form... | 0.0751      |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.31586316  |
| Train/Action_magnitu... | 1.1301575   |
| Train/Action_magnitude  | 0.8974721   |
| Train/Action_max        | 0.39420784  |
| Train/Action_std        | 0.5428694   |
| Train/Entropy           | 0.56200415  |
| Train/Entropy_Loss      | -0.000562   |
| Train/Entropy_loss      | -0.000562   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.2032766  |
| Train/Loss              | -0.08561446 |
| Train/PolicyClip        | 0.008967454 |
| Train/Policy_loss       | -0.09375706 |
| Train/Ratio             | 1.0011265   |
| Train/Return            | 1.5032302   |
| Train/V                 | 1.4258513   |
| Train/Value             | 1.4258513   |
| Train/control_penalty   | 0.8704608   |
| Train/policy_loss       | -0.09375706 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03575     |
-----------------------------------------

 ---------------- Iteration 890 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 889         |
| Time/Actor_Time         | 0.09        |
| Time/B_Format_Time      | 0.093       |
| Time/B_Original_Form... | 0.0933      |
| Time/Buffer             | 0.00299     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31062868  |
| Train/Action_magnitu... | 1.0642934   |
| Train/Action_magnitude  | 0.8427863   |
| Train/Action_max        | 0.38755852  |
| Train/Action_std        | 0.5252515   |
| Train/Entropy           | 0.52444005  |
| Train/Entropy_Loss      | -0.000524   |
| Train/Entropy_loss      | -0.000524   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1561006  |
| Train/Loss              | 0.07067851  |
| Train/PolicyClip        | 0.015290606 |
| Train/Policy_loss       | 0.06258783  |
| Train/Ratio             | 0.95713687  |
| Train/Return            | 1.2833018   |
| Train/V                 | 1.3587472   |
| Train/Value             | 1.3587472   |
| Train/control_penalty   | 0.861512    |
| Train/policy_loss       | 0.06258783  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 891 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 890          |
| Time/Actor_Time         | 0.0873       |
| Time/B_Format_Time      | 0.0774       |
| Time/B_Original_Form... | 0.0744       |
| Time/Buffer             | 0.00296      |
| Time/Critic_Time        | 1.19e-06     |
| Train/Action_abs_mean   | 0.2996889    |
| Train/Action_magnitu... | 1.0444674    |
| Train/Action_magnitude  | 0.8239234    |
| Train/Action_max        | 0.36976385   |
| Train/Action_std        | 0.48189166   |
| Train/Entropy           | 0.45676258   |
| Train/Entropy_Loss      | -0.000457    |
| Train/Entropy_loss      | -0.000457    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.047237    |
| Train/Loss              | -0.054135308 |
| Train/PolicyClip        | 0.009510911  |
| Train/Policy_loss       | -0.06158407  |
| Train/Ratio             | 1.0031624    |
| Train/Return            | 1.465616     |
| Train/V                 | 1.4154003    |
| Train/Value             | 1.4154003    |
| Train/control_penalty   | 0.79055256   |
| Train/policy_loss       | -0.06158407  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.035        |
------------------------------------------

 ---------------- Iteration 892 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
--------------------------------------------
| Itr                     | 891            |
| Time/Actor_Time         | 0.0885         |
| Time/B_Format_Time      | 0.0794         |
| Time/B_Original_Form... | 0.0781         |
| Time/Buffer             | 0.00355        |
| Time/Critic_Time        | 0              |
| Train/Action_abs_mean   | 0.3218349      |
| Train/Action_magnitu... | 1.1298963      |
| Train/Action_magnitude  | 0.89582205     |
| Train/Action_max        | 0.44301942     |
| Train/Action_std        | 0.5717742      |
| Train/Entropy           | 0.63158894     |
| Train/Entropy_Loss      | -0.000632      |
| Train/Entropy_loss      | -0.000632      |
| Train/Grad_norm_actor   | 0.0            |
| Train/LogProb           | -1.2934834     |
| Train/Loss              | 0.008141091    |
| Train/PolicyClip        | 0.010839504    |
| Train/Policy_loss       | -0.00028810414 |
| Train/Ratio             | 0.9768108      |
| Train/Return            | 1.0943269      |
| Train/V                 | 1.1089303      |
| Train/Value             | 1.1089303      |
| Train/control_penalty   | 0.9060784      |
| Train/policy_loss       | -0.00028810414 |
| Train/recon_loss        | 0.0            |
| train/batch_reward      | 0.02175        |
--------------------------------------------

 ---------------- Iteration 893 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 892          |
| Time/Actor_Time         | 0.0858       |
| Time/B_Format_Time      | 0.0754       |
| Time/B_Original_Form... | 0.0815       |
| Time/Buffer             | 0.00347      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30915228   |
| Train/Action_magnitu... | 1.0527458    |
| Train/Action_magnitude  | 0.83190393   |
| Train/Action_max        | 0.39122427   |
| Train/Action_std        | 0.513526     |
| Train/Entropy           | 0.5101671    |
| Train/Entropy_Loss      | -0.00051     |
| Train/Entropy_loss      | -0.00051     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0530838   |
| Train/Loss              | -0.045187697 |
| Train/PolicyClip        | 0.009058696  |
| Train/Policy_loss       | -0.053113412 |
| Train/Ratio             | 1.004033     |
| Train/Return            | 1.346012     |
| Train/V                 | 1.3072405    |
| Train/Value             | 1.3072405    |
| Train/control_penalty   | 0.8435881    |
| Train/policy_loss       | -0.053113412 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02975      |
------------------------------------------

 ---------------- Iteration 894 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 893         |
| Time/Actor_Time         | 0.0943      |
| Time/B_Format_Time      | 0.0803      |
| Time/B_Original_Form... | 0.0768      |
| Time/Buffer             | 0.00351     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31008095  |
| Train/Action_magnitu... | 1.0770508   |
| Train/Action_magnitude  | 0.85036206  |
| Train/Action_max        | 0.40773278  |
| Train/Action_std        | 0.55955935  |
| Train/Entropy           | 0.60606647  |
| Train/Entropy_Loss      | -0.000606   |
| Train/Entropy_loss      | -0.000606   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1959695  |
| Train/Loss              | 0.06430516  |
| Train/PolicyClip        | 0.011119172 |
| Train/Policy_loss       | 0.0559262   |
| Train/Ratio             | 0.9640525   |
| Train/Return            | 1.2773135   |
| Train/V                 | 1.3490719   |
| Train/Value             | 1.3490719   |
| Train/control_penalty   | 0.89850277  |
| Train/policy_loss       | 0.0559262   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 895 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 894         |
| Time/Actor_Time         | 0.0835      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.0829      |
| Time/Buffer             | 0.00353     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.30740243  |
| Train/Action_magnitu... | 1.1176038   |
| Train/Action_magnitude  | 0.87959653  |
| Train/Action_max        | 0.41427755  |
| Train/Action_std        | 0.5656221   |
| Train/Entropy           | 0.628285    |
| Train/Entropy_Loss      | -0.000628   |
| Train/Entropy_loss      | -0.000628   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.2601304  |
| Train/Loss              | 0.060674347 |
| Train/PolicyClip        | 0.008495571 |
| Train/Policy_loss       | 0.052382283 |
| Train/Ratio             | 0.97397155  |
| Train/Return            | 1.1413431   |
| Train/V                 | 1.2081861   |
| Train/Value             | 1.2081861   |
| Train/control_penalty   | 0.8920347   |
| Train/policy_loss       | 0.052382283 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 896 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 895           |
| Time/Actor_Time         | 0.0939        |
| Time/B_Format_Time      | 0.0819        |
| Time/B_Original_Form... | 0.0873        |
| Time/Buffer             | 0.00487       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.28121114    |
| Train/Action_magnitu... | 1.1092324     |
| Train/Action_magnitude  | 0.87588173    |
| Train/Action_max        | 0.42854044    |
| Train/Action_std        | 0.53570664    |
| Train/Entropy           | 0.59371156    |
| Train/Entropy_Loss      | -0.000594     |
| Train/Entropy_loss      | -0.000594     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.3131455    |
| Train/Loss              | -0.0076028137 |
| Train/PolicyClip        | 0.011003782   |
| Train/Policy_loss       | -0.015314719  |
| Train/Ratio             | 0.9939596     |
| Train/Return            | 1.1026523     |
| Train/V                 | 1.0976481     |
| Train/Value             | 1.0976481     |
| Train/control_penalty   | 0.8305616     |
| Train/policy_loss       | -0.015314719  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.025         |
-------------------------------------------

 ---------------- Iteration 897 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 896         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0734      |
| Time/B_Original_Form... | 0.0742      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31765956  |
| Train/Action_magnitu... | 1.1662889   |
| Train/Action_magnitude  | 0.92450166  |
| Train/Action_max        | 0.44329286  |
| Train/Action_std        | 0.6058144   |
| Train/Entropy           | 0.7017523   |
| Train/Entropy_Loss      | -0.000702   |
| Train/Entropy_loss      | -0.000702   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.4296317  |
| Train/Loss              | 0.05088695  |
| Train/PolicyClip        | 0.008789388 |
| Train/Policy_loss       | 0.042369395 |
| Train/Ratio             | 0.98371756  |
| Train/Return            | 1.2174286   |
| Train/V                 | 1.2739586   |
| Train/Value             | 1.2739586   |
| Train/control_penalty   | 0.92193097  |
| Train/policy_loss       | 0.042369395 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 898 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 897          |
| Time/Actor_Time         | 0.0902       |
| Time/B_Format_Time      | 0.0773       |
| Time/B_Original_Form... | 0.0766       |
| Time/Buffer             | 0.00371      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.28703222   |
| Train/Action_magnitu... | 1.0337232    |
| Train/Action_magnitude  | 0.8183663    |
| Train/Action_max        | 0.38745674   |
| Train/Action_std        | 0.5108823    |
| Train/Entropy           | 0.5234428    |
| Train/Entropy_Loss      | -0.000523    |
| Train/Entropy_loss      | -0.000523    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1175826   |
| Train/Loss              | 0.005045265  |
| Train/PolicyClip        | 0.01054821   |
| Train/Policy_loss       | -0.002576482 |
| Train/Ratio             | 0.99278706   |
| Train/Return            | 1.2908467    |
| Train/V                 | 1.3033491    |
| Train/Value             | 1.3033491    |
| Train/control_penalty   | 0.81451905   |
| Train/policy_loss       | -0.002576482 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02675      |
------------------------------------------

 ---------------- Iteration 899 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 898          |
| Time/Actor_Time         | 0.0856       |
| Time/B_Format_Time      | 0.0736       |
| Time/B_Original_Form... | 0.0719       |
| Time/Buffer             | 0.00308      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.294423     |
| Train/Action_magnitu... | 1.134685     |
| Train/Action_magnitude  | 0.8964996    |
| Train/Action_max        | 0.41990906   |
| Train/Action_std        | 0.58366746   |
| Train/Entropy           | 0.67437404   |
| Train/Entropy_Loss      | -0.000674    |
| Train/Entropy_loss      | -0.000674    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.3630369   |
| Train/Loss              | -0.004915744 |
| Train/PolicyClip        | 0.010878838  |
| Train/Policy_loss       | -0.013112116 |
| Train/Ratio             | 0.99379796   |
| Train/Return            | 1.131746     |
| Train/V                 | 1.131834     |
| Train/Value             | 1.131834     |
| Train/control_penalty   | 0.8870746    |
| Train/policy_loss       | -0.013112116 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 900 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 899          |
| Time/Actor_Time         | 0.0971       |
| Time/B_Format_Time      | 0.0816       |
| Time/B_Original_Form... | 0.0776       |
| Time/Buffer             | 0.00426      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29781574   |
| Train/Action_magnitu... | 1.1051608    |
| Train/Action_magnitude  | 0.8734353    |
| Train/Action_max        | 0.37933263   |
| Train/Action_std        | 0.55255526   |
| Train/Entropy           | 0.6294409    |
| Train/Entropy_Loss      | -0.000629    |
| Train/Entropy_loss      | -0.000629    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.3121865   |
| Train/Loss              | -0.008061666 |
| Train/PolicyClip        | 0.012214707  |
| Train/Policy_loss       | -0.016153159 |
| Train/Ratio             | 0.9960554    |
| Train/Return            | 1.1745322    |
| Train/V                 | 1.1726657    |
| Train/Value             | 1.1726657    |
| Train/control_penalty   | 0.8720935    |
| Train/policy_loss       | -0.016153159 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02325      |
------------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 901 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 900          |
| Time/Actor_Time         | 0.0857       |
| Time/B_Format_Time      | 0.0776       |
| Time/B_Original_Form... | 0.0794       |
| Time/Buffer             | 0.003        |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29671368   |
| Train/Action_magnitu... | 1.0783238    |
| Train/Action_magnitude  | 0.8496009    |
| Train/Action_max        | 0.39980242   |
| Train/Action_std        | 0.5370968    |
| Train/Entropy           | 0.57324535   |
| Train/Entropy_Loss      | -0.000573    |
| Train/Entropy_loss      | -0.000573    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1940178   |
| Train/Loss              | -0.023008812 |
| Train/PolicyClip        | 0.013197427  |
| Train/Policy_loss       | -0.030999882 |
| Train/Ratio             | 0.9975954    |
| Train/Return            | 1.1741896    |
| Train/V                 | 1.1540033    |
| Train/Value             | 1.1540033    |
| Train/control_penalty   | 0.85643154   |
| Train/policy_loss       | -0.030999882 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02125      |
------------------------------------------

 ---------------- Iteration 902 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 901          |
| Time/Actor_Time         | 0.127        |
| Time/B_Format_Time      | 0.0895       |
| Time/B_Original_Form... | 0.0792       |
| Time/Buffer             | 0.00283      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.29584864   |
| Train/Action_magnitu... | 1.0578202    |
| Train/Action_magnitude  | 0.8327063    |
| Train/Action_max        | 0.3915595    |
| Train/Action_std        | 0.49749148   |
| Train/Entropy           | 0.5094609    |
| Train/Entropy_Loss      | -0.000509    |
| Train/Entropy_loss      | -0.000509    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0943103   |
| Train/Loss              | -0.08653647  |
| Train/PolicyClip        | 0.011259092  |
| Train/Policy_loss       | -0.094054714 |
| Train/Ratio             | 1.0211514    |
| Train/Return            | 1.3390572    |
| Train/V                 | 1.2592449    |
| Train/Value             | 1.2592449    |
| Train/control_penalty   | 0.8027711    |
| Train/policy_loss       | -0.094054714 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.03225      |
------------------------------------------

 ---------------- Iteration 903 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 902          |
| Time/Actor_Time         | 0.0866       |
| Time/B_Format_Time      | 0.0755       |
| Time/B_Original_Form... | 0.0784       |
| Time/Buffer             | 0.00382      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.3067576    |
| Train/Action_magnitu... | 1.1154231    |
| Train/Action_magnitude  | 0.8812177    |
| Train/Action_max        | 0.45061702   |
| Train/Action_std        | 0.5360902    |
| Train/Entropy           | 0.5567624    |
| Train/Entropy_Loss      | -0.000557    |
| Train/Entropy_loss      | -0.000557    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.192743    |
| Train/Loss              | -0.03410329  |
| Train/PolicyClip        | 0.0113137355 |
| Train/Policy_loss       | -0.04213455  |
| Train/Ratio             | 0.9945258    |
| Train/Return            | 1.238127     |
| Train/V                 | 1.2045293    |
| Train/Value             | 1.2045293    |
| Train/control_penalty   | 0.85880214   |
| Train/policy_loss       | -0.04213455  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.02775      |
------------------------------------------

 ---------------- Iteration 904 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 903          |
| Time/Actor_Time         | 0.0919       |
| Time/B_Format_Time      | 0.0813       |
| Time/B_Original_Form... | 0.101        |
| Time/Buffer             | 0.00367      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31461346   |
| Train/Action_magnitu... | 1.0698798    |
| Train/Action_magnitude  | 0.8449946    |
| Train/Action_max        | 0.3860516    |
| Train/Action_std        | 0.51802987   |
| Train/Entropy           | 0.56807      |
| Train/Entropy_Loss      | -0.000568    |
| Train/Entropy_loss      | -0.000568    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.1769221   |
| Train/Loss              | -0.034940295 |
| Train/PolicyClip        | 0.012887042  |
| Train/Policy_loss       | -0.04282583  |
| Train/Ratio             | 0.99553496   |
| Train/Return            | 1.2274016    |
| Train/V                 | 1.2000251    |
| Train/Value             | 1.2000251    |
| Train/control_penalty   | 0.8453605    |
| Train/policy_loss       | -0.04282583  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 905 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 904         |
| Time/Actor_Time         | 0.0842      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0764      |
| Time/Buffer             | 0.00246     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3073587   |
| Train/Action_magnitu... | 1.0721686   |
| Train/Action_magnitude  | 0.8471842   |
| Train/Action_max        | 0.4544685   |
| Train/Action_std        | 0.53328645  |
| Train/Entropy           | 0.59928894  |
| Train/Entropy_Loss      | -0.000599   |
| Train/Entropy_loss      | -0.000599   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.2268898  |
| Train/Loss              | 0.021779852 |
| Train/PolicyClip        | 0.009002175 |
| Train/Policy_loss       | 0.013841006 |
| Train/Ratio             | 0.98759246  |
| Train/Return            | 1.0156418   |
| Train/V                 | 1.0428007   |
| Train/Value             | 1.0428007   |
| Train/control_penalty   | 0.8538135   |
| Train/policy_loss       | 0.013841006 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0165      |
-----------------------------------------

 ---------------- Iteration 906 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 905          |
| Time/Actor_Time         | 0.0905       |
| Time/B_Format_Time      | 0.0777       |
| Time/B_Original_Form... | 0.0769       |
| Time/Buffer             | 0.00339      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.30999607   |
| Train/Action_magnitu... | 1.0409427    |
| Train/Action_magnitude  | 0.82492644   |
| Train/Action_max        | 0.39536455   |
| Train/Action_std        | 0.52625185   |
| Train/Entropy           | 0.5518356    |
| Train/Entropy_Loss      | -0.000552    |
| Train/Entropy_loss      | -0.000552    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.047853    |
| Train/Loss              | -0.01756389  |
| Train/PolicyClip        | 0.009714182  |
| Train/Policy_loss       | -0.025582965 |
| Train/Ratio             | 0.998671     |
| Train/Return            | 1.2726344    |
| Train/V                 | 1.2570121    |
| Train/Value             | 1.2570121    |
| Train/control_penalty   | 0.85709083   |
| Train/policy_loss       | -0.025582965 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0245       |
------------------------------------------

 ---------------- Iteration 907 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 906           |
| Time/Actor_Time         | 0.0867        |
| Time/B_Format_Time      | 0.0732        |
| Time/B_Original_Form... | 0.0724        |
| Time/Buffer             | 0.00554       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.30547878    |
| Train/Action_magnitu... | 1.0785602     |
| Train/Action_magnitude  | 0.85654306    |
| Train/Action_max        | 0.4537797     |
| Train/Action_std        | 0.5199046     |
| Train/Entropy           | 0.5080311     |
| Train/Entropy_Loss      | -0.000508     |
| Train/Entropy_loss      | -0.000508     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.1031562    |
| Train/Loss              | 0.003113064   |
| Train/PolicyClip        | 0.011824095   |
| Train/Policy_loss       | -0.0049376106 |
| Train/Ratio             | 0.9880942     |
| Train/Return            | 1.3722836     |
| Train/V                 | 1.3781459     |
| Train/Value             | 1.3781459     |
| Train/control_penalty   | 0.85587054    |
| Train/policy_loss       | -0.0049376106 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0215        |
-------------------------------------------

 ---------------- Iteration 908 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 907          |
| Time/Actor_Time         | 0.0838       |
| Time/B_Format_Time      | 0.0747       |
| Time/B_Original_Form... | 0.0721       |
| Time/Buffer             | 0.00277      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31444275   |
| Train/Action_magnitu... | 1.0144666    |
| Train/Action_magnitude  | 0.80296206   |
| Train/Action_max        | 0.42551702   |
| Train/Action_std        | 0.4738292    |
| Train/Entropy           | 0.4295807    |
| Train/Entropy_Loss      | -0.00043     |
| Train/Entropy_loss      | -0.00043     |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -0.86360013  |
| Train/Loss              | -0.010101734 |
| Train/PolicyClip        | 0.011323425  |
| Train/Policy_loss       | -0.017893696 |
| Train/Ratio             | 0.99922717   |
| Train/Return            | 1.1832535    |
| Train/V                 | 1.17519      |
| Train/Value             | 1.17519      |
| Train/control_penalty   | 0.8221543    |
| Train/policy_loss       | -0.017893696 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.025        |
------------------------------------------

 ---------------- Iteration 909 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 908         |
| Time/Actor_Time         | 0.0868      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0725      |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.30776203  |
| Train/Action_magnitu... | 1.1354276   |
| Train/Action_magnitude  | 0.9031505   |
| Train/Action_max        | 0.4693982   |
| Train/Action_std        | 0.5614461   |
| Train/Entropy           | 0.59119684  |
| Train/Entropy_Loss      | -0.000591   |
| Train/Entropy_loss      | -0.000591   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1839759  |
| Train/Loss              | 0.050585862 |
| Train/PolicyClip        | 0.010540542 |
| Train/Policy_loss       | 0.042005766 |
| Train/Ratio             | 0.9813429   |
| Train/Return            | 1.0983562   |
| Train/V                 | 1.1507792   |
| Train/Value             | 1.1507792   |
| Train/control_penalty   | 0.91712916  |
| Train/policy_loss       | 0.042005766 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 910 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-------------------------------------------
| Itr                     | 909           |
| Time/Actor_Time         | 0.084         |
| Time/B_Format_Time      | 0.0734        |
| Time/B_Original_Form... | 0.0725        |
| Time/Buffer             | 0.00291       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.31613052    |
| Train/Action_magnitu... | 1.1002473     |
| Train/Action_magnitude  | 0.8658592     |
| Train/Action_max        | 0.43769178    |
| Train/Action_std        | 0.5482118     |
| Train/Entropy           | 0.5887088     |
| Train/Entropy_Loss      | -0.000589     |
| Train/Entropy_loss      | -0.000589     |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | -1.1700598    |
| Train/Loss              | -0.0038332948 |
| Train/PolicyClip        | 0.010480642   |
| Train/Policy_loss       | -0.012148928  |
| Train/Ratio             | 1.0039978     |
| Train/Return            | 1.1133738     |
| Train/V                 | 1.1113507     |
| Train/Value             | 1.1113507     |
| Train/control_penalty   | 0.89043415    |
| Train/policy_loss       | -0.012148928  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.02175       |
-------------------------------------------

 ---------------- Iteration 911 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 910          |
| Time/Actor_Time         | 0.0837       |
| Time/B_Format_Time      | 0.0732       |
| Time/B_Original_Form... | 0.0736       |
| Time/Buffer             | 0.00536      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.31105423   |
| Train/Action_magnitu... | 1.1121503    |
| Train/Action_magnitude  | 0.88091797   |
| Train/Action_max        | 0.43961382   |
| Train/Action_std        | 0.5229457    |
| Train/Entropy           | 0.5326192    |
| Train/Entropy_Loss      | -0.000533    |
| Train/Entropy_loss      | -0.000533    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.125287    |
| Train/Loss              | -0.09545149  |
| Train/PolicyClip        | 0.0063668224 |
| Train/Policy_loss       | -0.10351529  |
| Train/Ratio             | 1.006905     |
| Train/Return            | 1.2615676    |
| Train/V                 | 1.1674571    |
| Train/Value             | 1.1674571    |
| Train/control_penalty   | 0.8596422    |
| Train/policy_loss       | -0.10351529  |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0305       |
------------------------------------------

 ---------------- Iteration 912 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 911         |
| Time/Actor_Time         | 0.0835      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.0725      |
| Time/Buffer             | 0.00253     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.31906962  |
| Train/Action_magnitu... | 1.0667871   |
| Train/Action_magnitude  | 0.8443748   |
| Train/Action_max        | 0.43909547  |
| Train/Action_std        | 0.50876766  |
| Train/Entropy           | 0.51151943  |
| Train/Entropy_Loss      | -0.000512   |
| Train/Entropy_loss      | -0.000512   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.0351993  |
| Train/Loss              | 0.03148053  |
| Train/PolicyClip        | 0.010225964 |
| Train/Policy_loss       | 0.023503965 |
| Train/Ratio             | 0.9919193   |
| Train/Return            | 1.2020257   |
| Train/V                 | 1.2379198   |
| Train/Value             | 1.2379198   |
| Train/control_penalty   | 0.8488084   |
| Train/policy_loss       | 0.023503965 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 913 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 912         |
| Time/Actor_Time         | 0.0837      |
| Time/B_Format_Time      | 0.0744      |
| Time/B_Original_Form... | 0.0772      |
| Time/Buffer             | 0.00323     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3332318   |
| Train/Action_magnitu... | 1.1231335   |
| Train/Action_magnitude  | 0.8875569   |
| Train/Action_max        | 0.48600203  |
| Train/Action_std        | 0.5262409   |
| Train/Entropy           | 0.55868894  |
| Train/Entropy_Loss      | -0.000559   |
| Train/Entropy_loss      | -0.000559   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1672549  |
| Train/Loss              | 0.02093466  |
| Train/PolicyClip        | 0.012741651 |
| Train/Policy_loss       | 0.012841085 |
| Train/Ratio             | 0.98758644  |
| Train/Return            | 1.251592    |
| Train/V                 | 1.277881    |
| Train/Value             | 1.277881    |
| Train/control_penalty   | 0.8652263   |
| Train/policy_loss       | 0.012841085 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 914 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 913          |
| Time/Actor_Time         | 0.0894       |
| Time/B_Format_Time      | 0.0794       |
| Time/B_Original_Form... | 0.0793       |
| Time/Buffer             | 0.0031       |
| Time/Critic_Time        | 7.15e-07     |
| Train/Action_abs_mean   | 0.3014669    |
| Train/Action_magnitu... | 1.0564901    |
| Train/Action_magnitude  | 0.8349179    |
| Train/Action_max        | 0.40949383   |
| Train/Action_std        | 0.51603746   |
| Train/Entropy           | 0.5776486    |
| Train/Entropy_Loss      | -0.000578    |
| Train/Entropy_loss      | -0.000578    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.2288984   |
| Train/Loss              | -0.002791523 |
| Train/PolicyClip        | 0.013202476  |
| Train/Policy_loss       | -0.010549889 |
| Train/Ratio             | 0.9903554    |
| Train/Return            | 1.0143915    |
| Train/V                 | 1.0140817    |
| Train/Value             | 1.0140817    |
| Train/control_penalty   | 0.8336014    |
| Train/policy_loss       | -0.010549889 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.0215       |
------------------------------------------

 ---------------- Iteration 915 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 914         |
| Time/Actor_Time         | 0.0841      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.33807868  |
| Train/Action_magnitu... | 1.1360388   |
| Train/Action_magnitude  | 0.89640194  |
| Train/Action_max        | 0.44572803  |
| Train/Action_std        | 0.5253779   |
| Train/Entropy           | 0.55670196  |
| Train/Entropy_Loss      | -0.000557   |
| Train/Entropy_loss      | -0.000557   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1815752  |
| Train/Loss              | 0.019480098 |
| Train/PolicyClip        | 0.012721341 |
| Train/Policy_loss       | 0.011123621 |
| Train/Ratio             | 0.9872724   |
| Train/Return            | 1.4236631   |
| Train/V                 | 1.4462957   |
| Train/Value             | 1.4462957   |
| Train/control_penalty   | 0.89131796  |
| Train/policy_loss       | 0.011123621 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 916 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 915         |
| Time/Actor_Time         | 0.0974      |
| Time/B_Format_Time      | 0.0843      |
| Time/B_Original_Form... | 0.078       |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3315492   |
| Train/Action_magnitu... | 1.0636052   |
| Train/Action_magnitude  | 0.83879757  |
| Train/Action_max        | 0.38386288  |
| Train/Action_std        | 0.5215472   |
| Train/Entropy           | 0.55988824  |
| Train/Entropy_Loss      | -0.00056    |
| Train/Entropy_loss      | -0.00056    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.1190534  |
| Train/Loss              | 0.039758623 |
| Train/PolicyClip        | 0.01256374  |
| Train/Policy_loss       | 0.031577557 |
| Train/Ratio             | 0.9790307   |
| Train/Return            | 1.4686522   |
| Train/V                 | 1.5177858   |
| Train/Value             | 1.5177858   |
| Train/control_penalty   | 0.87409556  |
| Train/policy_loss       | 0.031577557 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 917 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 916         |
| Time/Actor_Time         | 0.085       |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0725      |
| Time/Buffer             | 0.00261     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3418711   |
| Train/Action_magnitu... | 1.0506126   |
| Train/Action_magnitude  | 0.82382005  |
| Train/Action_max        | 0.42743438  |
| Train/Action_std        | 0.45438972  |
| Train/Entropy           | 0.38729793  |
| Train/Entropy_Loss      | -0.000387   |
| Train/Entropy_loss      | -0.000387   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -0.8935199  |
| Train/Loss              | -0.03888636 |
| Train/PolicyClip        | 0.023891795 |
| Train/Policy_loss       | -0.04667594 |
| Train/Ratio             | 0.9964158   |
| Train/Return            | 1.3098917   |
| Train/V                 | 1.2784839   |
| Train/Value             | 1.2784839   |
| Train/control_penalty   | 0.81768775  |
| Train/policy_loss       | -0.04667594 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 918 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
-----------------------------------------
| Itr                     | 917         |
| Time/Actor_Time         | 0.0925      |
| Time/B_Format_Time      | 0.0814      |
| Time/B_Original_Form... | 0.0815      |
| Time/Buffer             | 0.00252     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.3230844   |
| Train/Action_magnitu... | 1.1319975   |
| Train/Action_magnitude  | 0.8914974   |
| Train/Action_max        | 0.40152207  |
| Train/Action_std        | 0.53814286  |
| Train/Entropy           | 0.59833384  |
| Train/Entropy_Loss      | -0.000598   |
| Train/Entropy_loss      | -0.000598   |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | -1.251903   |
| Train/Loss              | 0.0487716   |
| Train/PolicyClip        | 0.012158084 |
| Train/Policy_loss       | 0.040537916 |
| Train/Ratio             | 0.98520285  |
| Train/Return            | 1.3340627   |
| Train/V                 | 1.3866949   |
| Train/Value             | 1.3866949   |
| Train/control_penalty   | 0.88320196  |
| Train/policy_loss       | 0.040537916 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 919 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distillation_well_trained_converter
------------------------------------------
| Itr                     | 918          |
| Time/Actor_Time         | 0.14         |
| Time/B_Format_Time      | 0.137        |
| Time/B_Original_Form... | 0.13         |
| Time/Buffer             | 0.00406      |
| Time/Critic_Time        | 0            |
| Train/Action_abs_mean   | 0.32102314   |
| Train/Action_magnitu... | 1.0670903    |
| Train/Action_magnitude  | 0.84364283   |
| Train/Action_max        | 0.38776416   |
| Train/Action_std        | 0.527056     |
| Train/Entropy           | 0.5917398    |
| Train/Entropy_Loss      | -0.000592    |
| Train/Entropy_loss      | -0.000592    |
| Train/Grad_norm_actor   | 0.0          |
| Train/LogProb           | -1.0927644   |
| Train/Loss              | -0.037560027 |
| Train/PolicyClip        | 0.013043511  |
| Train/Policy_loss       | -0.045570977 |
| Train/Ratio             | 0.9884       |
| Train/Return            | 1.3866155    |
| Train/V                 | 1.3547606    |
| Train/Value             | 1.3547606    |
| Train/control_penalty   | 0.860269     |
| Train/policy_loss       | -0.045570977 |
| Train/recon_loss        | 0.0          |
| train/batch_reward      | 0.028        |
------------------------------------------
