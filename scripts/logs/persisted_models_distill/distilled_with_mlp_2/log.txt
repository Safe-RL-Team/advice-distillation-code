Logging to /Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2

 ---------------- Iteration 0 ----------------
Obtaining samples...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
---------------------------
| Time/Buffer | 0.00703   |
| Train/Value | 1.7445183 |
---------------------------
Saving snapshot...
Saved

 ---------------- Iteration 1 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 0          |
| Time/Actor_Time         | 0.0686     |
| Time/B_Format_Time      | 0.0717     |
| Time/B_Original_Form... | 0.0726     |
| Time/Buffer             | 0.00373    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24001482 |
| Train/Action_magnitu... | 0.53290266 |
| Train/Action_magnitude  | 0.4077     |
| Train/Action_max        | 0.20633335 |
| Train/Action_std        | 0.13544491 |
| Train/Entropy           | -0.6077472 |
| Train/Entropy_Loss      | 0.000608   |
| Train/Entropy_loss      | 0.000608   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2191447  |
| Train/Loss              | 0.17588013 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17121509 |
| Train/Ratio             | 0.99998933 |
| Train/Return            | 1.570766   |
| Train/V                 | 1.7419792  |
| Train/Value             | 1.7419792  |
| Train/control_penalty   | 0.4057302  |
| Train/policy_loss       | 0.17121509 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02525    |
----------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1           |
| Time/Actor_Time         | 0.0684      |
| Time/B_Format_Time      | 0.0798      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00247     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.20596173  |
| Train/Action_magnitu... | 0.47101212  |
| Train/Action_magnitude  | 0.36319098  |
| Train/Action_max        | 0.17647904  |
| Train/Action_std        | 0.13847685  |
| Train/Entropy           | -0.58942455 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.179399    |
| Train/Loss              | 0.25859565  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.25433528  |
| Train/Ratio             | 0.99999654  |
| Train/Return            | 1.0902256   |
| Train/V                 | 1.3445488   |
| Train/Value             | 1.3445488   |
| Train/control_penalty   | 0.36709443  |
| Train/policy_loss       | 0.25433528  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.006       |
-----------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-------------------------------------------
| Itr                     | 2             |
| Time/Actor_Time         | 0.0824        |
| Time/B_Format_Time      | 0.125         |
| Time/B_Original_Form... | 0.0921        |
| Time/Buffer             | 0.00455       |
| Time/Critic_Time        | 1.19e-06      |
| Train/Action_abs_mean   | 0.24457064    |
| Train/Action_magnitu... | 0.5490726     |
| Train/Action_magnitude  | 0.4311864     |
| Train/Action_max        | 0.18863359    |
| Train/Action_std        | 0.14212564    |
| Train/Entropy           | -0.5653268    |
| Train/Entropy_Loss      | 0.000565      |
| Train/Entropy_loss      | 0.000565      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 1.1416297     |
| Train/Loss              | -0.005241944  |
| Train/PolicyClip        | 0.0           |
| Train/Policy_loss       | -0.0101044625 |
| Train/Ratio             | 1.0000012     |
| Train/Return            | 2.1558578     |
| Train/V                 | 2.1457486     |
| Train/Value             | 2.1457486     |
| Train/control_penalty   | 0.42971918    |
| Train/policy_loss       | -0.0101044625 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.04775       |
-------------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-------------------------------------------
| Itr                     | 3             |
| Time/Actor_Time         | 0.0635        |
| Time/B_Format_Time      | 0.111         |
| Time/B_Original_Form... | 0.0692        |
| Time/Buffer             | 0.00366       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.2550676     |
| Train/Action_magnitu... | 0.5606311     |
| Train/Action_magnitude  | 0.44034734    |
| Train/Action_max        | 0.18951175    |
| Train/Action_std        | 0.1436085     |
| Train/Entropy           | -0.55323905   |
| Train/Entropy_Loss      | 0.000553      |
| Train/Entropy_loss      | 0.000553      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 1.0949492     |
| Train/Loss              | -0.0061430046 |
| Train/PolicyClip        | 0.0           |
| Train/Policy_loss       | -0.011094423  |
| Train/Ratio             | 0.9999841     |
| Train/Return            | 1.9986631     |
| Train/V                 | 1.9875741     |
| Train/Value             | 1.9875741     |
| Train/control_penalty   | 0.43981794    |
| Train/policy_loss       | -0.011094423  |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.0495        |
-------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 4           |
| Time/Actor_Time         | 0.0673      |
| Time/B_Format_Time      | 0.081       |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2539142   |
| Train/Action_magnitu... | 0.56466466  |
| Train/Action_magnitude  | 0.44580954  |
| Train/Action_max        | 0.19372201  |
| Train/Action_std        | 0.14340277  |
| Train/Entropy           | -0.54814005 |
| Train/Entropy_Loss      | 0.000548    |
| Train/Entropy_loss      | 0.000548    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0894531   |
| Train/Loss              | 0.08930461  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08435513  |
| Train/Ratio             | 0.99999815  |
| Train/Return            | 1.6536648   |
| Train/V                 | 1.7380224   |
| Train/Value             | 1.7380224   |
| Train/control_penalty   | 0.4401345   |
| Train/policy_loss       | 0.08435513  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 5          |
| Time/Actor_Time         | 0.0967     |
| Time/B_Format_Time      | 0.0811     |
| Time/B_Original_Form... | 0.0758     |
| Time/Buffer             | 0.00326    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25505146 |
| Train/Action_magnitu... | 0.5635071  |
| Train/Action_magnitude  | 0.44682088 |
| Train/Action_max        | 0.23368114 |
| Train/Action_std        | 0.14582056 |
| Train/Entropy           | -0.5368752 |
| Train/Entropy_Loss      | 0.000537   |
| Train/Entropy_loss      | 0.000537   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.080906   |
| Train/Loss              | 0.18120997 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17617838 |
| Train/Ratio             | 0.9999833  |
| Train/Return            | 1.8426714  |
| Train/V                 | 2.0188572  |
| Train/Value             | 2.0188572  |
| Train/control_penalty   | 0.44947162 |
| Train/policy_loss       | 0.17617838 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 6           |
| Time/Actor_Time         | 0.0951      |
| Time/B_Format_Time      | 0.097       |
| Time/B_Original_Form... | 0.151       |
| Time/Buffer             | 0.00421     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23369306  |
| Train/Action_magnitu... | 0.5188584   |
| Train/Action_magnitude  | 0.4068356   |
| Train/Action_max        | 0.21229811  |
| Train/Action_std        | 0.14131615  |
| Train/Entropy           | -0.56911016 |
| Train/Entropy_Loss      | 0.000569    |
| Train/Entropy_loss      | 0.000569    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1512812   |
| Train/Loss              | 0.12982228  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1251523   |
| Train/Ratio             | 0.9999834   |
| Train/Return            | 1.80487     |
| Train/V                 | 1.930017    |
| Train/Value             | 1.930017    |
| Train/control_penalty   | 0.41008717  |
| Train/policy_loss       | 0.1251523   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03325     |
-----------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 7          |
| Time/Actor_Time         | 0.0992     |
| Time/B_Format_Time      | 0.087      |
| Time/B_Original_Form... | 0.0924     |
| Time/Buffer             | 0.00456    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23092034 |
| Train/Action_magnitu... | 0.5188768  |
| Train/Action_magnitude  | 0.40425506 |
| Train/Action_max        | 0.21667019 |
| Train/Action_std        | 0.14441031 |
| Train/Entropy           | -0.5530344 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0723562  |
| Train/Loss              | 0.14704686 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14241342 |
| Train/Ratio             | 1.000009   |
| Train/Return            | 1.6432931  |
| Train/V                 | 1.7857004  |
| Train/Value             | 1.7857004  |
| Train/control_penalty   | 0.40803927 |
| Train/policy_loss       | 0.14241342 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.034      |
----------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 8          |
| Time/Actor_Time         | 0.107      |
| Time/B_Format_Time      | 0.0962     |
| Time/B_Original_Form... | 0.112      |
| Time/Buffer             | 0.00503    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2421001  |
| Train/Action_magnitu... | 0.5435286  |
| Train/Action_magnitude  | 0.4195266  |
| Train/Action_max        | 0.22730757 |
| Train/Action_std        | 0.14180014 |
| Train/Entropy           | -0.5752739 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1489849  |
| Train/Loss              | 0.16290206 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15815243 |
| Train/Ratio             | 0.9999827  |
| Train/Return            | 1.9637241  |
| Train/V                 | 2.1218777  |
| Train/Value             | 2.1218777  |
| Train/control_penalty   | 0.41743496 |
| Train/policy_loss       | 0.15815243 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 9          |
| Time/Actor_Time         | 0.0654     |
| Time/B_Format_Time      | 0.102      |
| Time/B_Original_Form... | 0.105      |
| Time/Buffer             | 0.00777    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2544326  |
| Train/Action_magnitu... | 0.55793244 |
| Train/Action_magnitude  | 0.42762986 |
| Train/Action_max        | 0.25632474 |
| Train/Action_std        | 0.13994429 |
| Train/Entropy           | -0.5855185 |
| Train/Entropy_Loss      | 0.000586   |
| Train/Entropy_loss      | 0.000586   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1398796  |
| Train/Loss              | 0.17574434 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1708844  |
| Train/Ratio             | 1.000016   |
| Train/Return            | 1.6532006  |
| Train/V                 | 1.8240727  |
| Train/Value             | 1.8240727  |
| Train/control_penalty   | 0.42744252 |
| Train/policy_loss       | 0.1708844  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 11 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 10          |
| Time/Actor_Time         | 0.0669      |
| Time/B_Format_Time      | 0.0901      |
| Time/B_Original_Form... | 0.0892      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24485534  |
| Train/Action_magnitu... | 0.54507136  |
| Train/Action_magnitude  | 0.4160156   |
| Train/Action_max        | 0.24664612  |
| Train/Action_std        | 0.1435967   |
| Train/Entropy           | -0.56136817 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1303025   |
| Train/Loss              | 0.1560359   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15131849  |
| Train/Ratio             | 0.99999714  |
| Train/Return            | 1.389753    |
| Train/V                 | 1.5410703   |
| Train/Value             | 1.5410703   |
| Train/control_penalty   | 0.41560385  |
| Train/policy_loss       | 0.15131849  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02        |
-----------------------------------------

 ---------------- Iteration 12 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 11         |
| Time/Actor_Time         | 0.0681     |
| Time/B_Format_Time      | 0.107      |
| Time/B_Original_Form... | 0.0846     |
| Time/Buffer             | 0.00332    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2501869  |
| Train/Action_magnitu... | 0.5506501  |
| Train/Action_magnitude  | 0.42240143 |
| Train/Action_max        | 0.20455553 |
| Train/Action_std        | 0.14296523 |
| Train/Entropy           | -0.5566294 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1335243  |
| Train/Loss              | 0.20719473 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20240715 |
| Train/Ratio             | 0.9999735  |
| Train/Return            | 1.5095096  |
| Train/V                 | 1.7119261  |
| Train/Value             | 1.7119261  |
| Train/control_penalty   | 0.4230942  |
| Train/policy_loss       | 0.20240715 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.016      |
----------------------------------------

 ---------------- Iteration 13 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 12          |
| Time/Actor_Time         | 0.125       |
| Time/B_Format_Time      | 0.139       |
| Time/B_Original_Form... | 0.168       |
| Time/Buffer             | 0.00453     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2428759   |
| Train/Action_magnitu... | 0.5472704   |
| Train/Action_magnitude  | 0.42710063  |
| Train/Action_max        | 0.14363332  |
| Train/Action_std        | 0.14004941  |
| Train/Entropy           | -0.57378584 |
| Train/Entropy_Loss      | 0.000574    |
| Train/Entropy_loss      | 0.000574    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1631857   |
| Train/Loss              | 0.34427124  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.33946377  |
| Train/Ratio             | 1.0000092   |
| Train/Return            | 2.1114159   |
| Train/V                 | 2.4508793   |
| Train/Value             | 2.4508793   |
| Train/control_penalty   | 0.42336884  |
| Train/policy_loss       | 0.33946377  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 14 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 13          |
| Time/Actor_Time         | 0.0647      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0783      |
| Time/Buffer             | 0.00416     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24468513  |
| Train/Action_magnitu... | 0.5481035   |
| Train/Action_magnitude  | 0.43186462  |
| Train/Action_max        | 0.10932543  |
| Train/Action_std        | 0.13523103  |
| Train/Entropy           | -0.60566986 |
| Train/Entropy_Loss      | 0.000606    |
| Train/Entropy_loss      | 0.000606    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2050859   |
| Train/Loss              | 0.55452406  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.54960114  |
| Train/Ratio             | 0.99999046  |
| Train/Return            | 3.1183598   |
| Train/V                 | 3.6679692   |
| Train/Value             | 3.6679692   |
| Train/control_penalty   | 0.4317276   |
| Train/policy_loss       | 0.54960114  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0205      |
-----------------------------------------

 ---------------- Iteration 15 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 14         |
| Time/Actor_Time         | 0.0657     |
| Time/B_Format_Time      | 0.071      |
| Time/B_Original_Form... | 0.105      |
| Time/Buffer             | 0.003      |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23722783 |
| Train/Action_magnitu... | 0.5440605  |
| Train/Action_magnitude  | 0.42926222 |
| Train/Action_max        | 0.12694979 |
| Train/Action_std        | 0.13944115 |
| Train/Entropy           | -0.5811893 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1354712  |
| Train/Loss              | 0.6680916  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.6632607  |
| Train/Ratio             | 1.0000056  |
| Train/Return            | 3.3030686  |
| Train/V                 | 3.966313   |
| Train/Value             | 3.966313   |
| Train/control_penalty   | 0.42496988 |
| Train/policy_loss       | 0.6632607  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0175     |
----------------------------------------

 ---------------- Iteration 16 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 15         |
| Time/Actor_Time         | 0.0933     |
| Time/B_Format_Time      | 0.0773     |
| Time/B_Original_Form... | 0.071      |
| Time/Buffer             | 0.00582    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24140944 |
| Train/Action_magnitu... | 0.5416494  |
| Train/Action_magnitude  | 0.42049265 |
| Train/Action_max        | 0.21622111 |
| Train/Action_std        | 0.14125848 |
| Train/Entropy           | -0.5775003 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1605668  |
| Train/Loss              | 0.34147984 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.33673438 |
| Train/Ratio             | 0.99999225 |
| Train/Return            | 2.310647   |
| Train/V                 | 2.647381   |
| Train/Value             | 2.647381   |
| Train/control_penalty   | 0.41679323 |
| Train/policy_loss       | 0.33673438 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0385     |
----------------------------------------

 ---------------- Iteration 17 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 16         |
| Time/Actor_Time         | 0.0974     |
| Time/B_Format_Time      | 0.13       |
| Time/B_Original_Form... | 0.0826     |
| Time/Buffer             | 0.00979    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23043926 |
| Train/Action_magnitu... | 0.5078031  |
| Train/Action_magnitude  | 0.3920356  |
| Train/Action_max        | 0.23288342 |
| Train/Action_std        | 0.13862665 |
| Train/Entropy           | -0.5979376 |
| Train/Entropy_Loss      | 0.000598   |
| Train/Entropy_loss      | 0.000598   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2238672  |
| Train/Loss              | 0.14707561 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14252998 |
| Train/Ratio             | 1.0        |
| Train/Return            | 1.526804   |
| Train/V                 | 1.6693417  |
| Train/Value             | 1.6693417  |
| Train/control_penalty   | 0.39476907 |
| Train/policy_loss       | 0.14252998 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02725    |
----------------------------------------

 ---------------- Iteration 18 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 17          |
| Time/Actor_Time         | 0.0807      |
| Time/B_Format_Time      | 0.0747      |
| Time/B_Original_Form... | 0.0816      |
| Time/Buffer             | 0.0107      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23483042  |
| Train/Action_magnitu... | 0.5178507   |
| Train/Action_magnitude  | 0.3991732   |
| Train/Action_max        | 0.22452752  |
| Train/Action_std        | 0.13410771  |
| Train/Entropy           | -0.63167745 |
| Train/Entropy_Loss      | 0.000632    |
| Train/Entropy_loss      | 0.000632    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2701402   |
| Train/Loss              | 0.09414051  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08952153  |
| Train/Ratio             | 0.99999964  |
| Train/Return            | 1.618365    |
| Train/V                 | 1.7078768   |
| Train/Value             | 1.7078768   |
| Train/control_penalty   | 0.39873052  |
| Train/policy_loss       | 0.08952153  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03725     |
-----------------------------------------

 ---------------- Iteration 19 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 18         |
| Time/Actor_Time         | 0.0717     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.0734     |
| Time/Buffer             | 0.00456    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2298453  |
| Train/Action_magnitu... | 0.5095696  |
| Train/Action_magnitude  | 0.3913477  |
| Train/Action_max        | 0.19996841 |
| Train/Action_std        | 0.13291667 |
| Train/Entropy           | -0.6369221 |
| Train/Entropy_Loss      | 0.000637   |
| Train/Entropy_loss      | 0.000637   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2763163  |
| Train/Loss              | 0.12339435 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11886605 |
| Train/Ratio             | 1.000004   |
| Train/Return            | 1.7730122  |
| Train/V                 | 1.8918878  |
| Train/Value             | 1.8918878  |
| Train/control_penalty   | 0.38913757 |
| Train/policy_loss       | 0.11886605 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03575    |
----------------------------------------

 ---------------- Iteration 20 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 19         |
| Time/Actor_Time         | 0.0692     |
| Time/B_Format_Time      | 0.0707     |
| Time/B_Original_Form... | 0.0746     |
| Time/Buffer             | 0.00389    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21863529 |
| Train/Action_magnitu... | 0.5003471  |
| Train/Action_magnitude  | 0.3847792  |
| Train/Action_max        | 0.19393976 |
| Train/Action_std        | 0.1365311  |
| Train/Entropy           | -0.6069449 |
| Train/Entropy_Loss      | 0.000607   |
| Train/Entropy_loss      | 0.000607   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2179326  |
| Train/Loss              | 0.18394898 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17953771 |
| Train/Ratio             | 0.99999994 |
| Train/Return            | 1.3826172  |
| Train/V                 | 1.5621476  |
| Train/Value             | 1.5621476  |
| Train/control_penalty   | 0.3804319  |
| Train/policy_loss       | 0.17953771 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 21 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 20          |
| Time/Actor_Time         | 0.071       |
| Time/B_Format_Time      | 0.0867      |
| Time/B_Original_Form... | 0.115       |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22315134  |
| Train/Action_magnitu... | 0.5032836   |
| Train/Action_magnitude  | 0.39048404  |
| Train/Action_max        | 0.20462939  |
| Train/Action_std        | 0.14081597  |
| Train/Entropy           | -0.57695305 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1542561   |
| Train/Loss              | 0.16533831  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1608323   |
| Train/Ratio             | 1.0000074   |
| Train/Return            | 1.2970419   |
| Train/V                 | 1.4578682   |
| Train/Value             | 1.4578682   |
| Train/control_penalty   | 0.39290565  |
| Train/policy_loss       | 0.1608323   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 22 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 21         |
| Time/Actor_Time         | 0.064      |
| Time/B_Format_Time      | 0.0705     |
| Time/B_Original_Form... | 0.0716     |
| Time/Buffer             | 0.00342    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23574093 |
| Train/Action_magnitu... | 0.5319702  |
| Train/Action_magnitude  | 0.4109942  |
| Train/Action_max        | 0.23084773 |
| Train/Action_std        | 0.13994285 |
| Train/Entropy           | -0.5809241 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.124587   |
| Train/Loss              | 0.12393521 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11929069 |
| Train/Ratio             | 0.99999076 |
| Train/Return            | 1.255925   |
| Train/V                 | 1.3752126  |
| Train/Value             | 1.3752126  |
| Train/control_penalty   | 0.40635967 |
| Train/policy_loss       | 0.11929069 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02125    |
----------------------------------------

 ---------------- Iteration 23 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 22         |
| Time/Actor_Time         | 0.0657     |
| Time/B_Format_Time      | 0.0711     |
| Time/B_Original_Form... | 0.0701     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24226648 |
| Train/Action_magnitu... | 0.5389394  |
| Train/Action_magnitude  | 0.41748786 |
| Train/Action_max        | 0.22478512 |
| Train/Action_std        | 0.14044242 |
| Train/Entropy           | -0.5786348 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1374642  |
| Train/Loss              | 0.08875109 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08398752 |
| Train/Ratio             | 1.0000062  |
| Train/Return            | 1.4361748  |
| Train/V                 | 1.5201685  |
| Train/Value             | 1.5201685  |
| Train/control_penalty   | 0.41849357 |
| Train/policy_loss       | 0.08398752 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 24 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 23          |
| Time/Actor_Time         | 0.0697      |
| Time/B_Format_Time      | 0.0748      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00385     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25827026  |
| Train/Action_magnitu... | 0.5721586   |
| Train/Action_magnitude  | 0.44245464  |
| Train/Action_max        | 0.20613134  |
| Train/Action_std        | 0.14123502  |
| Train/Entropy           | -0.5686126  |
| Train/Entropy_Loss      | 0.000569    |
| Train/Entropy_loss      | 0.000569    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1162002   |
| Train/Loss              | 0.061078895 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.056114234 |
| Train/Ratio             | 1.0000141   |
| Train/Return            | 1.7358723   |
| Train/V                 | 1.7919859   |
| Train/Value             | 1.7919859   |
| Train/control_penalty   | 0.43960488  |
| Train/policy_loss       | 0.056114234 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0405      |
-----------------------------------------

 ---------------- Iteration 25 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 24         |
| Time/Actor_Time         | 0.0723     |
| Time/B_Format_Time      | 0.0746     |
| Time/B_Original_Form... | 0.0783     |
| Time/Buffer             | 0.00507    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23667006 |
| Train/Action_magnitu... | 0.53568685 |
| Train/Action_magnitude  | 0.41865778 |
| Train/Action_max        | 0.2005638  |
| Train/Action_std        | 0.14339639 |
| Train/Entropy           | -0.5561671 |
| Train/Entropy_Loss      | 0.000556   |
| Train/Entropy_loss      | 0.000556   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0908827  |
| Train/Loss              | 0.0823462  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.0776328  |
| Train/Ratio             | 0.99999547 |
| Train/Return            | 1.814323   |
| Train/V                 | 1.8919512  |
| Train/Value             | 1.8919512  |
| Train/control_penalty   | 0.41572392 |
| Train/policy_loss       | 0.0776328  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03675    |
----------------------------------------

 ---------------- Iteration 26 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 25          |
| Time/Actor_Time         | 0.0881      |
| Time/B_Format_Time      | 0.0882      |
| Time/B_Original_Form... | 0.0914      |
| Time/Buffer             | 0.00368     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23568055  |
| Train/Action_magnitu... | 0.5255016   |
| Train/Action_magnitude  | 0.4113336   |
| Train/Action_max        | 0.17629012  |
| Train/Action_std        | 0.13685381  |
| Train/Entropy           | -0.6032106  |
| Train/Entropy_Loss      | 0.000603    |
| Train/Entropy_loss      | 0.000603    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1855865   |
| Train/Loss              | 0.061835483 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.05711257  |
| Train/Ratio             | 0.999984    |
| Train/Return            | 1.5366      |
| Train/V                 | 1.5937151   |
| Train/Value             | 1.5937151   |
| Train/control_penalty   | 0.41197005  |
| Train/policy_loss       | 0.05711257  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 27 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 26          |
| Time/Actor_Time         | 0.105       |
| Time/B_Format_Time      | 0.0849      |
| Time/B_Original_Form... | 0.0693      |
| Time/Buffer             | 0.00613     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23910037  |
| Train/Action_magnitu... | 0.53282744  |
| Train/Action_magnitude  | 0.41609198  |
| Train/Action_max        | 0.208586    |
| Train/Action_std        | 0.14017135  |
| Train/Entropy           | -0.58423203 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1440562   |
| Train/Loss              | 0.13689582  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13218738  |
| Train/Ratio             | 1.0000038   |
| Train/Return            | 1.77431     |
| Train/V                 | 1.9064944   |
| Train/Value             | 1.9064944   |
| Train/control_penalty   | 0.41242114  |
| Train/policy_loss       | 0.13218738  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 28 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 27          |
| Time/Actor_Time         | 0.0652      |
| Time/B_Format_Time      | 0.0749      |
| Time/B_Original_Form... | 0.0786      |
| Time/Buffer             | 0.00394     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24516118  |
| Train/Action_magnitu... | 0.54339767  |
| Train/Action_magnitude  | 0.42082176  |
| Train/Action_max        | 0.22217488  |
| Train/Action_std        | 0.13991693  |
| Train/Entropy           | -0.58519405 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1402665   |
| Train/Loss              | 0.13938372  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13459818  |
| Train/Ratio             | 0.9999966   |
| Train/Return            | 1.4005892   |
| Train/V                 | 1.5351982   |
| Train/Value             | 1.5351982   |
| Train/control_penalty   | 0.4200338   |
| Train/policy_loss       | 0.13459818  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 29 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 28          |
| Time/Actor_Time         | 0.0711      |
| Time/B_Format_Time      | 0.139       |
| Time/B_Original_Form... | 0.143       |
| Time/Buffer             | 0.00478     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23740803  |
| Train/Action_magnitu... | 0.53658885  |
| Train/Action_magnitude  | 0.41802394  |
| Train/Action_max        | 0.20926827  |
| Train/Action_std        | 0.14184304  |
| Train/Entropy           | -0.57646203 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1791998   |
| Train/Loss              | 0.12292288  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11818344  |
| Train/Ratio             | 1.0000019   |
| Train/Return            | 1.5490692   |
| Train/V                 | 1.6672586   |
| Train/Value             | 1.6672586   |
| Train/control_penalty   | 0.41629815  |
| Train/policy_loss       | 0.11818344  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 30 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 29          |
| Time/Actor_Time         | 0.179       |
| Time/B_Format_Time      | 0.114       |
| Time/B_Original_Form... | 0.142       |
| Time/Buffer             | 0.00515     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24223994  |
| Train/Action_magnitu... | 0.5436772   |
| Train/Action_magnitude  | 0.42378518  |
| Train/Action_max        | 0.23538335  |
| Train/Action_std        | 0.14567679  |
| Train/Entropy           | -0.54888755 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1311082   |
| Train/Loss              | 0.18794781  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18314481  |
| Train/Ratio             | 1.0000494   |
| Train/Return            | 1.4270838   |
| Train/V                 | 1.6102214   |
| Train/Value             | 1.6102214   |
| Train/control_penalty   | 0.42541218  |
| Train/policy_loss       | 0.18314481  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01725     |
-----------------------------------------

 ---------------- Iteration 31 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 30          |
| Time/Actor_Time         | 0.102       |
| Time/B_Format_Time      | 0.0841      |
| Time/B_Original_Form... | 0.069       |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24559818  |
| Train/Action_magnitu... | 0.54738057  |
| Train/Action_magnitude  | 0.42510813  |
| Train/Action_max        | 0.23110306  |
| Train/Action_std        | 0.13493787  |
| Train/Entropy           | -0.62224716 |
| Train/Entropy_Loss      | 0.000622    |
| Train/Entropy_loss      | 0.000622    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2453254   |
| Train/Loss              | 0.10998394  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10512599  |
| Train/Ratio             | 1.0000077   |
| Train/Return            | 1.7246438   |
| Train/V                 | 1.829762    |
| Train/Value             | 1.829762    |
| Train/control_penalty   | 0.4235701   |
| Train/policy_loss       | 0.10512599  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0285      |
-----------------------------------------

 ---------------- Iteration 32 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 31         |
| Time/Actor_Time         | 0.0754     |
| Time/B_Format_Time      | 0.0867     |
| Time/B_Original_Form... | 0.0796     |
| Time/Buffer             | 0.0064     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23290339 |
| Train/Action_magnitu... | 0.5175826  |
| Train/Action_magnitude  | 0.40027466 |
| Train/Action_max        | 0.22303638 |
| Train/Action_std        | 0.13476826 |
| Train/Entropy           | -0.6216382 |
| Train/Entropy_Loss      | 0.000622   |
| Train/Entropy_loss      | 0.000622   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2493184  |
| Train/Loss              | 0.1488537  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14424708 |
| Train/Ratio             | 0.9999952  |
| Train/Return            | 1.5935273  |
| Train/V                 | 1.7377754  |
| Train/Value             | 1.7377754  |
| Train/control_penalty   | 0.39849883 |
| Train/policy_loss       | 0.14424708 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 33 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 32         |
| Time/Actor_Time         | 0.068      |
| Time/B_Format_Time      | 0.0992     |
| Time/B_Original_Form... | 0.0761     |
| Time/Buffer             | 0.00513    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.21378343 |
| Train/Action_magnitu... | 0.48376    |
| Train/Action_magnitude  | 0.37655458 |
| Train/Action_max        | 0.20749246 |
| Train/Action_std        | 0.13272838 |
| Train/Entropy           | -0.6372257 |
| Train/Entropy_Loss      | 0.000637   |
| Train/Entropy_loss      | 0.000637   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2967138  |
| Train/Loss              | 0.15572557 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15131417 |
| Train/Ratio             | 0.9999906  |
| Train/Return            | 1.5746706  |
| Train/V                 | 1.7259946  |
| Train/Value             | 1.7259946  |
| Train/control_penalty   | 0.37741765 |
| Train/policy_loss       | 0.15131417 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 34 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 33         |
| Time/Actor_Time         | 0.0975     |
| Time/B_Format_Time      | 0.0724     |
| Time/B_Original_Form... | 0.099      |
| Time/Buffer             | 0.00499    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22632018 |
| Train/Action_magnitu... | 0.50789183 |
| Train/Action_magnitude  | 0.39405    |
| Train/Action_max        | 0.19241029 |
| Train/Action_std        | 0.13463606 |
| Train/Entropy           | -0.6173332 |
| Train/Entropy_Loss      | 0.000617   |
| Train/Entropy_loss      | 0.000617   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2433237  |
| Train/Loss              | 0.2060909  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20153055 |
| Train/Ratio             | 0.99998796 |
| Train/Return            | 1.4105698  |
| Train/V                 | 1.6121022  |
| Train/Value             | 1.6121022  |
| Train/control_penalty   | 0.3943019  |
| Train/policy_loss       | 0.20153055 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0145     |
----------------------------------------

 ---------------- Iteration 35 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 34          |
| Time/Actor_Time         | 0.0729      |
| Time/B_Format_Time      | 0.0878      |
| Time/B_Original_Form... | 0.0926      |
| Time/Buffer             | 0.00333     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21225552  |
| Train/Action_magnitu... | 0.47773722  |
| Train/Action_magnitude  | 0.3684263   |
| Train/Action_max        | 0.18270399  |
| Train/Action_std        | 0.1303641   |
| Train/Entropy           | -0.65189785 |
| Train/Entropy_Loss      | 0.000652    |
| Train/Entropy_loss      | 0.000652    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2996677   |
| Train/Loss              | 0.21444415  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21012409  |
| Train/Ratio             | 1.0000101   |
| Train/Return            | 1.4148928   |
| Train/V                 | 1.6250085   |
| Train/Value             | 1.6250085   |
| Train/control_penalty   | 0.36681646  |
| Train/policy_loss       | 0.21012409  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.015       |
-----------------------------------------

 ---------------- Iteration 36 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 35         |
| Time/Actor_Time         | 0.116      |
| Time/B_Format_Time      | 0.0863     |
| Time/B_Original_Form... | 0.0773     |
| Time/Buffer             | 0.00649    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.21714993 |
| Train/Action_magnitu... | 0.48203814 |
| Train/Action_magnitude  | 0.3700506  |
| Train/Action_max        | 0.18898499 |
| Train/Action_std        | 0.1281534  |
| Train/Entropy           | -0.669518  |
| Train/Entropy_Loss      | 0.00067    |
| Train/Entropy_loss      | 0.00067    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.3532822  |
| Train/Loss              | 0.22551315 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22112958 |
| Train/Ratio             | 0.9999983  |
| Train/Return            | 1.7312745  |
| Train/V                 | 1.9524075  |
| Train/Value             | 1.9524075  |
| Train/control_penalty   | 0.3714043  |
| Train/policy_loss       | 0.22112958 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01925    |
----------------------------------------

 ---------------- Iteration 37 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 36          |
| Time/Actor_Time         | 0.0703      |
| Time/B_Format_Time      | 0.0741      |
| Time/B_Original_Form... | 0.0792      |
| Time/Buffer             | 0.00275     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20438407  |
| Train/Action_magnitu... | 0.45916352  |
| Train/Action_magnitude  | 0.3528245   |
| Train/Action_max        | 0.18363798  |
| Train/Action_std        | 0.12622018  |
| Train/Entropy           | -0.68543166 |
| Train/Entropy_Loss      | 0.000685    |
| Train/Entropy_loss      | 0.000685    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.3611445   |
| Train/Loss              | 0.25339118  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24918826  |
| Train/Ratio             | 0.99999076  |
| Train/Return            | 1.728552    |
| Train/V                 | 1.9777453   |
| Train/Value             | 1.9777453   |
| Train/control_penalty   | 0.35174897  |
| Train/policy_loss       | 0.24918826  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01725     |
-----------------------------------------

 ---------------- Iteration 38 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 37          |
| Time/Actor_Time         | 0.0827      |
| Time/B_Format_Time      | 0.0701      |
| Time/B_Original_Form... | 0.0841      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.22763395  |
| Train/Action_magnitu... | 0.5032129   |
| Train/Action_magnitude  | 0.3902316   |
| Train/Action_max        | 0.19439676  |
| Train/Action_std        | 0.13327856  |
| Train/Entropy           | -0.63028026 |
| Train/Entropy_Loss      | 0.00063     |
| Train/Entropy_loss      | 0.00063     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2811298   |
| Train/Loss              | 0.17053398  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16599973  |
| Train/Ratio             | 0.9999893   |
| Train/Return            | 1.8437464   |
| Train/V                 | 2.009741    |
| Train/Value             | 2.009741    |
| Train/control_penalty   | 0.39039874  |
| Train/policy_loss       | 0.16599973  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 39 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 38         |
| Time/Actor_Time         | 0.079      |
| Time/B_Format_Time      | 0.0726     |
| Time/B_Original_Form... | 0.0819     |
| Time/Buffer             | 0.0103     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22356762 |
| Train/Action_magnitu... | 0.49893183 |
| Train/Action_magnitude  | 0.38586786 |
| Train/Action_max        | 0.19570866 |
| Train/Action_std        | 0.13330472 |
| Train/Entropy           | -0.6319472 |
| Train/Entropy_Loss      | 0.000632   |
| Train/Entropy_loss      | 0.000632   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2642334  |
| Train/Loss              | 0.18926708 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18477449 |
| Train/Ratio             | 0.99998826 |
| Train/Return            | 1.589807   |
| Train/V                 | 1.7745787  |
| Train/Value             | 1.7745787  |
| Train/control_penalty   | 0.38606587 |
| Train/policy_loss       | 0.18477449 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01725    |
----------------------------------------

 ---------------- Iteration 40 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 39          |
| Time/Actor_Time         | 0.0745      |
| Time/B_Format_Time      | 0.103       |
| Time/B_Original_Form... | 0.0854      |
| Time/Buffer             | 0.00331     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2437374   |
| Train/Action_magnitu... | 0.53936225  |
| Train/Action_magnitude  | 0.4183212   |
| Train/Action_max        | 0.2185759   |
| Train/Action_std        | 0.13877888  |
| Train/Entropy           | -0.59178454 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2203774   |
| Train/Loss              | 0.14413318  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13934778  |
| Train/Ratio             | 1.0000067   |
| Train/Return            | 1.9240957   |
| Train/V                 | 2.0634542   |
| Train/Value             | 2.0634542   |
| Train/control_penalty   | 0.41936126  |
| Train/policy_loss       | 0.13934778  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 41 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 40         |
| Time/Actor_Time         | 0.0699     |
| Time/B_Format_Time      | 0.0767     |
| Time/B_Original_Form... | 0.0791     |
| Time/Buffer             | 0.00344    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2286592  |
| Train/Action_magnitu... | 0.50626636 |
| Train/Action_magnitude  | 0.39218298 |
| Train/Action_max        | 0.18325773 |
| Train/Action_std        | 0.1335211  |
| Train/Entropy           | -0.6313957 |
| Train/Entropy_Loss      | 0.000631   |
| Train/Entropy_loss      | 0.000631   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2571602  |
| Train/Loss              | 0.15628357 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15173183 |
| Train/Ratio             | 0.9999843  |
| Train/Return            | 1.8861654  |
| Train/V                 | 2.037902   |
| Train/Value             | 2.037902   |
| Train/control_penalty   | 0.39203465 |
| Train/policy_loss       | 0.15173183 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 42 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 41         |
| Time/Actor_Time         | 0.0645     |
| Time/B_Format_Time      | 0.0729     |
| Time/B_Original_Form... | 0.0688     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23298025 |
| Train/Action_magnitu... | 0.52130675 |
| Train/Action_magnitude  | 0.40412056 |
| Train/Action_max        | 0.20813662 |
| Train/Action_std        | 0.1395694  |
| Train/Entropy           | -0.5876604 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.167274   |
| Train/Loss              | 0.1300278  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12544127 |
| Train/Ratio             | 1.0000042  |
| Train/Return            | 1.6669604  |
| Train/V                 | 1.7923917  |
| Train/Value             | 1.7923917  |
| Train/control_penalty   | 0.3998873  |
| Train/policy_loss       | 0.12544127 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 43 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 42         |
| Time/Actor_Time         | 0.0717     |
| Time/B_Format_Time      | 0.0778     |
| Time/B_Original_Form... | 0.0804     |
| Time/Buffer             | 0.00366    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22877231 |
| Train/Action_magnitu... | 0.5122059  |
| Train/Action_magnitude  | 0.39568675 |
| Train/Action_max        | 0.20859532 |
| Train/Action_std        | 0.13661562 |
| Train/Entropy           | -0.6124681 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2377932  |
| Train/Loss              | 0.14490618 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14030996 |
| Train/Ratio             | 0.9999843  |
| Train/Return            | 1.7361108  |
| Train/V                 | 1.8764157  |
| Train/Value             | 1.8764157  |
| Train/control_penalty   | 0.39837447 |
| Train/policy_loss       | 0.14030996 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 44 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 43         |
| Time/Actor_Time         | 0.0734     |
| Time/B_Format_Time      | 0.0775     |
| Time/B_Original_Form... | 0.0799     |
| Time/Buffer             | 0.00712    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21791531 |
| Train/Action_magnitu... | 0.49009094 |
| Train/Action_magnitude  | 0.3779997  |
| Train/Action_max        | 0.20373121 |
| Train/Action_std        | 0.13555552 |
| Train/Entropy           | -0.6160855 |
| Train/Entropy_Loss      | 0.000616   |
| Train/Entropy_loss      | 0.000616   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2227346  |
| Train/Loss              | 0.13655443 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1321245  |
| Train/Ratio             | 1.0000061  |
| Train/Return            | 1.6305971  |
| Train/V                 | 1.7627292  |
| Train/Value             | 1.7627292  |
| Train/control_penalty   | 0.38138422 |
| Train/policy_loss       | 0.1321245  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 45 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 44          |
| Time/Actor_Time         | 0.0684      |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00415     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22697845  |
| Train/Action_magnitu... | 0.51460314  |
| Train/Action_magnitude  | 0.40247682  |
| Train/Action_max        | 0.22217116  |
| Train/Action_std        | 0.13981287  |
| Train/Entropy           | -0.58901894 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1703293   |
| Train/Loss              | 0.09826945  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.093661495 |
| Train/Ratio             | 1.0000054   |
| Train/Return            | 1.9668509   |
| Train/V                 | 2.0605125   |
| Train/Value             | 2.0605125   |
| Train/control_penalty   | 0.40189344  |
| Train/policy_loss       | 0.093661495 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0445      |
-----------------------------------------

 ---------------- Iteration 46 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 45         |
| Time/Actor_Time         | 0.0797     |
| Time/B_Format_Time      | 0.0719     |
| Time/B_Original_Form... | 0.0934     |
| Time/Buffer             | 0.00296    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.19045953 |
| Train/Action_magnitu... | 0.4397046  |
| Train/Action_magnitude  | 0.34221318 |
| Train/Action_max        | 0.16862348 |
| Train/Action_std        | 0.12765862 |
| Train/Entropy           | -0.6803343 |
| Train/Entropy_Loss      | 0.00068    |
| Train/Entropy_loss      | 0.00068    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.3693379  |
| Train/Loss              | 0.10106898 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09693546 |
| Train/Ratio             | 1.0000039  |
| Train/Return            | 1.3791964  |
| Train/V                 | 1.4761299  |
| Train/Value             | 1.4761299  |
| Train/control_penalty   | 0.34531873 |
| Train/policy_loss       | 0.09693546 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 47 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 46         |
| Time/Actor_Time         | 0.102      |
| Time/B_Format_Time      | 0.0812     |
| Time/B_Original_Form... | 0.0919     |
| Time/Buffer             | 0.00452    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.19867487 |
| Train/Action_magnitu... | 0.4515581  |
| Train/Action_magnitude  | 0.35098132 |
| Train/Action_max        | 0.20186403 |
| Train/Action_std        | 0.13022989 |
| Train/Entropy           | -0.6622197 |
| Train/Entropy_Loss      | 0.000662   |
| Train/Entropy_loss      | 0.000662   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.337626   |
| Train/Loss              | 0.15139577 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14715002 |
| Train/Ratio             | 0.9999858  |
| Train/Return            | 1.6226712  |
| Train/V                 | 1.7698203  |
| Train/Value             | 1.7698203  |
| Train/control_penalty   | 0.3583516  |
| Train/policy_loss       | 0.14715002 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 48 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 47          |
| Time/Actor_Time         | 0.0785      |
| Time/B_Format_Time      | 0.0701      |
| Time/B_Original_Form... | 0.0962      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.20641871  |
| Train/Action_magnitu... | 0.47525433  |
| Train/Action_magnitude  | 0.36824542  |
| Train/Action_max        | 0.22614168  |
| Train/Action_std        | 0.13417341  |
| Train/Entropy           | -0.63410544 |
| Train/Entropy_Loss      | 0.000634    |
| Train/Entropy_loss      | 0.000634    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2687671   |
| Train/Loss              | 0.10020693  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0958941   |
| Train/Ratio             | 1.0000015   |
| Train/Return            | 1.4868482   |
| Train/V                 | 1.5827358   |
| Train/Value             | 1.5827358   |
| Train/control_penalty   | 0.36787215  |
| Train/policy_loss       | 0.0958941   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 49 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 48         |
| Time/Actor_Time         | 0.0828     |
| Time/B_Format_Time      | 0.111      |
| Time/B_Original_Form... | 0.103      |
| Time/Buffer             | 0.00403    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22384043 |
| Train/Action_magnitu... | 0.5041025  |
| Train/Action_magnitude  | 0.38993016 |
| Train/Action_max        | 0.23967914 |
| Train/Action_std        | 0.13661447 |
| Train/Entropy           | -0.6122238 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2308389  |
| Train/Loss              | 0.12929136 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12479979 |
| Train/Ratio             | 1.0000038  |
| Train/Return            | 1.449034   |
| Train/V                 | 1.5738342  |
| Train/Value             | 1.5738342  |
| Train/control_penalty   | 0.38793406 |
| Train/policy_loss       | 0.12479979 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 50 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 49          |
| Time/Actor_Time         | 0.0884      |
| Time/B_Format_Time      | 0.112       |
| Time/B_Original_Form... | 0.0838      |
| Time/Buffer             | 0.00402     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22698662  |
| Train/Action_magnitu... | 0.5036561   |
| Train/Action_magnitude  | 0.3894501   |
| Train/Action_max        | 0.2012489   |
| Train/Action_std        | 0.13387676  |
| Train/Entropy           | -0.6300016  |
| Train/Entropy_Loss      | 0.00063     |
| Train/Entropy_loss      | 0.00063     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2744554   |
| Train/Loss              | 0.115618035 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11113861  |
| Train/Ratio             | 0.9999882   |
| Train/Return            | 1.3891753   |
| Train/V                 | 1.5003176   |
| Train/Value             | 1.5003176   |
| Train/control_penalty   | 0.3849423   |
| Train/policy_loss       | 0.11113861  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 51 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 50         |
| Time/Actor_Time         | 0.07       |
| Time/B_Format_Time      | 0.102      |
| Time/B_Original_Form... | 0.0767     |
| Time/Buffer             | 0.0039     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23246378 |
| Train/Action_magnitu... | 0.51468605 |
| Train/Action_magnitude  | 0.3973909  |
| Train/Action_max        | 0.21612506 |
| Train/Action_std        | 0.1338438  |
| Train/Entropy           | -0.6289795 |
| Train/Entropy_Loss      | 0.000629   |
| Train/Entropy_loss      | 0.000629   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2295569  |
| Train/Loss              | 0.16657527 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16199721 |
| Train/Ratio             | 1.0000042  |
| Train/Return            | 1.5341897  |
| Train/V                 | 1.6961874  |
| Train/Value             | 1.6961874  |
| Train/control_penalty   | 0.3949072  |
| Train/policy_loss       | 0.16199721 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 52 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 51         |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0676     |
| Time/B_Original_Form... | 0.0701     |
| Time/Buffer             | 0.00303    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.2233054  |
| Train/Action_magnitu... | 0.4993981  |
| Train/Action_magnitude  | 0.38854158 |
| Train/Action_max        | 0.1743523  |
| Train/Action_std        | 0.13438332 |
| Train/Entropy           | -0.6239447 |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2482996  |
| Train/Loss              | 0.11399088 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10948967 |
| Train/Ratio             | 0.9999821  |
| Train/Return            | 1.5136428  |
| Train/V                 | 1.6231353  |
| Train/Value             | 1.6231353  |
| Train/control_penalty   | 0.38772702 |
| Train/policy_loss       | 0.10948967 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 53 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 52          |
| Time/Actor_Time         | 0.0682      |
| Time/B_Format_Time      | 0.0746      |
| Time/B_Original_Form... | 0.0778      |
| Time/Buffer             | 0.00423     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23032047  |
| Train/Action_magnitu... | 0.51566446  |
| Train/Action_magnitude  | 0.40241024  |
| Train/Action_max        | 0.17811036  |
| Train/Action_std        | 0.13795839  |
| Train/Entropy           | -0.59400177 |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.181481    |
| Train/Loss              | 0.123166874 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1185675   |
| Train/Ratio             | 0.99999774  |
| Train/Return            | 1.7940149   |
| Train/V                 | 1.9125788   |
| Train/Value             | 1.9125788   |
| Train/control_penalty   | 0.40053698  |
| Train/policy_loss       | 0.1185675   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 54 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 53          |
| Time/Actor_Time         | 0.0673      |
| Time/B_Format_Time      | 0.07        |
| Time/B_Original_Form... | 0.115       |
| Time/Buffer             | 0.0034      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23085015  |
| Train/Action_magnitu... | 0.5164432   |
| Train/Action_magnitude  | 0.40081927  |
| Train/Action_max        | 0.17689654  |
| Train/Action_std        | 0.13607885  |
| Train/Entropy           | -0.60273135 |
| Train/Entropy_Loss      | 0.000603    |
| Train/Entropy_loss      | 0.000603    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2344297   |
| Train/Loss              | 0.15667379  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15208577  |
| Train/Ratio             | 1.0000169   |
| Train/Return            | 1.5596066   |
| Train/V                 | 1.7116841   |
| Train/Value             | 1.7116841   |
| Train/control_penalty   | 0.39852905  |
| Train/policy_loss       | 0.15208577  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 55 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 54         |
| Time/Actor_Time         | 0.0889     |
| Time/B_Format_Time      | 0.0725     |
| Time/B_Original_Form... | 0.0818     |
| Time/Buffer             | 0.00734    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25813732 |
| Train/Action_magnitu... | 0.572336   |
| Train/Action_magnitude  | 0.44212252 |
| Train/Action_max        | 0.20321012 |
| Train/Action_std        | 0.14034185 |
| Train/Entropy           | -0.5737075 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1389751  |
| Train/Loss              | 0.23401216 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22903876 |
| Train/Ratio             | 0.9999987  |
| Train/Return            | 1.8328857  |
| Train/V                 | 2.0619204  |
| Train/Value             | 2.0619204  |
| Train/control_penalty   | 0.43996832 |
| Train/policy_loss       | 0.22903876 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 56 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 55          |
| Time/Actor_Time         | 0.0909      |
| Time/B_Format_Time      | 0.108       |
| Time/B_Original_Form... | 0.0749      |
| Time/Buffer             | 0.0034      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25194114  |
| Train/Action_magnitu... | 0.555001    |
| Train/Action_magnitude  | 0.42959213  |
| Train/Action_max        | 0.20667589  |
| Train/Action_std        | 0.14082976  |
| Train/Entropy           | -0.5695118  |
| Train/Entropy_Loss      | 0.00057     |
| Train/Entropy_loss      | 0.00057     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1503599   |
| Train/Loss              | 0.116169915 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.111313686 |
| Train/Ratio             | 1.0000137   |
| Train/Return            | 1.8552821   |
| Train/V                 | 1.9665904   |
| Train/Value             | 1.9665904   |
| Train/control_penalty   | 0.42867178  |
| Train/policy_loss       | 0.111313686 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.038       |
-----------------------------------------

 ---------------- Iteration 57 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 56          |
| Time/Actor_Time         | 0.1         |
| Time/B_Format_Time      | 0.0791      |
| Time/B_Original_Form... | 0.0934      |
| Time/Buffer             | 0.00396     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24525906  |
| Train/Action_magnitu... | 0.550835    |
| Train/Action_magnitude  | 0.42980278  |
| Train/Action_max        | 0.19850159  |
| Train/Action_std        | 0.14095488  |
| Train/Entropy           | -0.57161146 |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1574492   |
| Train/Loss              | 0.07506829  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.07022888  |
| Train/Ratio             | 1.0000083   |
| Train/Return            | 1.8630488   |
| Train/V                 | 1.9332738   |
| Train/Value             | 1.9332738   |
| Train/control_penalty   | 0.42677972  |
| Train/policy_loss       | 0.07022888  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.042       |
-----------------------------------------

 ---------------- Iteration 58 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 57          |
| Time/Actor_Time         | 0.102       |
| Time/B_Format_Time      | 0.153       |
| Time/B_Original_Form... | 0.197       |
| Time/Buffer             | 0.00423     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2446972   |
| Train/Action_magnitu... | 0.5445216   |
| Train/Action_magnitude  | 0.4265606   |
| Train/Action_max        | 0.20197824  |
| Train/Action_std        | 0.1416656   |
| Train/Entropy           | -0.56503314 |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1390072   |
| Train/Loss              | 0.06888708  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0640656   |
| Train/Ratio             | 0.9999834   |
| Train/Return            | 1.6295416   |
| Train/V                 | 1.6936053   |
| Train/Value             | 1.6936053   |
| Train/control_penalty   | 0.42564481  |
| Train/policy_loss       | 0.0640656   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.035       |
-----------------------------------------

 ---------------- Iteration 59 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 58          |
| Time/Actor_Time         | 0.0694      |
| Time/B_Format_Time      | 0.0772      |
| Time/B_Original_Form... | 0.0781      |
| Time/Buffer             | 0.00571     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24091563  |
| Train/Action_magnitu... | 0.54123104  |
| Train/Action_magnitude  | 0.42383507  |
| Train/Action_max        | 0.21632761  |
| Train/Action_std        | 0.14436398  |
| Train/Entropy           | -0.54987925 |
| Train/Entropy_Loss      | 0.00055     |
| Train/Entropy_loss      | 0.00055     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1038848   |
| Train/Loss              | 0.13058288  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12582964  |
| Train/Ratio             | 1.0000266   |
| Train/Return            | 1.7448575   |
| Train/V                 | 1.870684    |
| Train/Value             | 1.870684    |
| Train/control_penalty   | 0.42033586  |
| Train/policy_loss       | 0.12582964  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 60 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 59          |
| Time/Actor_Time         | 0.063       |
| Time/B_Format_Time      | 0.0888      |
| Time/B_Original_Form... | 0.0721      |
| Time/Buffer             | 0.00693     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2373042   |
| Train/Action_magnitu... | 0.53552985  |
| Train/Action_magnitude  | 0.42224926  |
| Train/Action_max        | 0.18403593  |
| Train/Action_std        | 0.14393662  |
| Train/Entropy           | -0.54881656 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.101395    |
| Train/Loss              | 0.13668409  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13191698  |
| Train/Ratio             | 1.0000142   |
| Train/Return            | 1.6042492   |
| Train/V                 | 1.7361641   |
| Train/Value             | 1.7361641   |
| Train/control_penalty   | 0.42182896  |
| Train/policy_loss       | 0.13191698  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 61 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 60         |
| Time/Actor_Time         | 0.0704     |
| Time/B_Format_Time      | 0.0762     |
| Time/B_Original_Form... | 0.0805     |
| Time/Buffer             | 0.00372    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21739493 |
| Train/Action_magnitu... | 0.4960681  |
| Train/Action_magnitude  | 0.38964948 |
| Train/Action_max        | 0.18124345 |
| Train/Action_std        | 0.13829167 |
| Train/Entropy           | -0.5940015 |
| Train/Entropy_Loss      | 0.000594   |
| Train/Entropy_loss      | 0.000594   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1791058  |
| Train/Loss              | 0.09314199 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08865417 |
| Train/Ratio             | 0.99999654 |
| Train/Return            | 1.662066   |
| Train/V                 | 1.750725   |
| Train/Value             | 1.750725   |
| Train/control_penalty   | 0.3893818  |
| Train/policy_loss       | 0.08865417 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.032      |
----------------------------------------

 ---------------- Iteration 62 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 61          |
| Time/Actor_Time         | 0.114       |
| Time/B_Format_Time      | 0.0947      |
| Time/B_Original_Form... | 0.0868      |
| Time/Buffer             | 0.00408     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23680767  |
| Train/Action_magnitu... | 0.52416503  |
| Train/Action_magnitude  | 0.4099628   |
| Train/Action_max        | 0.19578494  |
| Train/Action_std        | 0.1364097   |
| Train/Entropy           | -0.60531616 |
| Train/Entropy_Loss      | 0.000605    |
| Train/Entropy_loss      | 0.000605    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.215452    |
| Train/Loss              | 0.08987612  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.085180394 |
| Train/Ratio             | 1.0000039   |
| Train/Return            | 1.7384045   |
| Train/V                 | 1.823595    |
| Train/Value             | 1.823595    |
| Train/control_penalty   | 0.40904108  |
| Train/policy_loss       | 0.085180394 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.032       |
-----------------------------------------

 ---------------- Iteration 63 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 62          |
| Time/Actor_Time         | 0.0732      |
| Time/B_Format_Time      | 0.133       |
| Time/B_Original_Form... | 0.0932      |
| Time/Buffer             | 0.00337     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23106986  |
| Train/Action_magnitu... | 0.515278    |
| Train/Action_magnitude  | 0.4012263   |
| Train/Action_max        | 0.18944488  |
| Train/Action_std        | 0.13729933  |
| Train/Entropy           | -0.59864223 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.180473    |
| Train/Loss              | 0.13656375  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13194202  |
| Train/Ratio             | 0.9999857   |
| Train/Return            | 1.6516345   |
| Train/V                 | 1.783579    |
| Train/Value             | 1.783579    |
| Train/control_penalty   | 0.4023094   |
| Train/policy_loss       | 0.13194202  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 64 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 63         |
| Time/Actor_Time         | 0.0704     |
| Time/B_Format_Time      | 0.0943     |
| Time/B_Original_Form... | 0.1        |
| Time/Buffer             | 0.0117     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2246498  |
| Train/Action_magnitu... | 0.5001034  |
| Train/Action_magnitude  | 0.38936594 |
| Train/Action_max        | 0.17845984 |
| Train/Action_std        | 0.13695951 |
| Train/Entropy           | -0.5989597 |
| Train/Entropy_Loss      | 0.000599   |
| Train/Entropy_loss      | 0.000599   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2176702  |
| Train/Loss              | 0.08485352 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08033655 |
| Train/Ratio             | 0.9999847  |
| Train/Return            | 1.843884   |
| Train/V                 | 1.9242285  |
| Train/Value             | 1.9242285  |
| Train/control_penalty   | 0.39180174 |
| Train/policy_loss       | 0.08033655 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03975    |
----------------------------------------

 ---------------- Iteration 65 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 64          |
| Time/Actor_Time         | 0.109       |
| Time/B_Format_Time      | 0.104       |
| Time/B_Original_Form... | 0.125       |
| Time/Buffer             | 0.00354     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23120727  |
| Train/Action_magnitu... | 0.5172993   |
| Train/Action_magnitude  | 0.40148216  |
| Train/Action_max        | 0.19326924  |
| Train/Action_std        | 0.14114167  |
| Train/Entropy           | -0.5735568  |
| Train/Entropy_Loss      | 0.000574    |
| Train/Entropy_loss      | 0.000574    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1517644   |
| Train/Loss              | 0.104693145 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10010162  |
| Train/Ratio             | 0.99999815  |
| Train/Return            | 1.650754    |
| Train/V                 | 1.7508545   |
| Train/Value             | 1.7508545   |
| Train/control_penalty   | 0.4017974   |
| Train/policy_loss       | 0.10010162  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 66 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 65         |
| Time/Actor_Time         | 0.0872     |
| Time/B_Format_Time      | 0.178      |
| Time/B_Original_Form... | 0.0972     |
| Time/Buffer             | 0.00457    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23300274 |
| Train/Action_magnitu... | 0.5272663  |
| Train/Action_magnitude  | 0.40981153 |
| Train/Action_max        | 0.21461475 |
| Train/Action_std        | 0.1421055  |
| Train/Entropy           | -0.5679645 |
| Train/Entropy_Loss      | 0.000568   |
| Train/Entropy_loss      | 0.000568   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1055236  |
| Train/Loss              | 0.1336571  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12902701 |
| Train/Ratio             | 1.0000079  |
| Train/Return            | 1.6169869  |
| Train/V                 | 1.7460033  |
| Train/Value             | 1.7460033  |
| Train/control_penalty   | 0.4062132  |
| Train/policy_loss       | 0.12902701 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0305     |
----------------------------------------

 ---------------- Iteration 67 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 66         |
| Time/Actor_Time         | 0.106      |
| Time/B_Format_Time      | 0.121      |
| Time/B_Original_Form... | 0.111      |
| Time/Buffer             | 0.00389    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2405731  |
| Train/Action_magnitu... | 0.5339389  |
| Train/Action_magnitude  | 0.41619238 |
| Train/Action_max        | 0.20772573 |
| Train/Action_std        | 0.14178254 |
| Train/Entropy           | -0.5661338 |
| Train/Entropy_Loss      | 0.000566   |
| Train/Entropy_loss      | 0.000566   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1424633  |
| Train/Loss              | 0.05256497 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.04782092 |
| Train/Ratio             | 0.9999989  |
| Train/Return            | 1.8172386  |
| Train/V                 | 1.865056   |
| Train/Value             | 1.865056   |
| Train/control_penalty   | 0.41779187 |
| Train/policy_loss       | 0.04782092 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0395     |
----------------------------------------

 ---------------- Iteration 68 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 67          |
| Time/Actor_Time         | 0.0653      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.075       |
| Time/Buffer             | 0.00423     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24114662  |
| Train/Action_magnitu... | 0.5376443   |
| Train/Action_magnitude  | 0.41974476  |
| Train/Action_max        | 0.2131933   |
| Train/Action_std        | 0.1459698   |
| Train/Entropy           | -0.5416567  |
| Train/Entropy_Loss      | 0.000542    |
| Train/Entropy_loss      | 0.000542    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0938013   |
| Train/Loss              | 0.071005456 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.06623569  |
| Train/Ratio             | 1.0000038   |
| Train/Return            | 1.7604935   |
| Train/V                 | 1.8267218   |
| Train/Value             | 1.8267218   |
| Train/control_penalty   | 0.42281076  |
| Train/policy_loss       | 0.06623569  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04475     |
-----------------------------------------

 ---------------- Iteration 69 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 68         |
| Time/Actor_Time         | 0.0675     |
| Time/B_Format_Time      | 0.0715     |
| Time/B_Original_Form... | 0.0741     |
| Time/Buffer             | 0.00329    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24129914 |
| Train/Action_magnitu... | 0.5423466  |
| Train/Action_magnitude  | 0.42139766 |
| Train/Action_max        | 0.2206929  |
| Train/Action_std        | 0.14444509 |
| Train/Entropy           | -0.5483457 |
| Train/Entropy_Loss      | 0.000548   |
| Train/Entropy_loss      | 0.000548   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0785029  |
| Train/Loss              | 0.08998216 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08521619 |
| Train/Ratio             | 0.9999866  |
| Train/Return            | 1.6226391  |
| Train/V                 | 1.7078586  |
| Train/Value             | 1.7078586  |
| Train/control_penalty   | 0.4217622  |
| Train/policy_loss       | 0.08521619 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 70 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 69          |
| Time/Actor_Time         | 0.0726      |
| Time/B_Format_Time      | 0.0705      |
| Time/B_Original_Form... | 0.0711      |
| Time/Buffer             | 0.00372     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24320705  |
| Train/Action_magnitu... | 0.5358533   |
| Train/Action_magnitude  | 0.41636038  |
| Train/Action_max        | 0.2323086   |
| Train/Action_std        | 0.14304008  |
| Train/Entropy           | -0.5592469  |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1041949   |
| Train/Loss              | 0.107356764 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.102610126 |
| Train/Ratio             | 1.0000201   |
| Train/Return            | 1.8014319   |
| Train/V                 | 1.9040444   |
| Train/Value             | 1.9040444   |
| Train/control_penalty   | 0.41873872  |
| Train/policy_loss       | 0.102610126 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04275     |
-----------------------------------------

 ---------------- Iteration 71 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 70          |
| Time/Actor_Time         | 0.0645      |
| Time/B_Format_Time      | 0.104       |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00347     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2350625   |
| Train/Action_magnitu... | 0.532511    |
| Train/Action_magnitude  | 0.41431525  |
| Train/Action_max        | 0.2191781   |
| Train/Action_std        | 0.14303736  |
| Train/Entropy           | -0.56272906 |
| Train/Entropy_Loss      | 0.000563    |
| Train/Entropy_loss      | 0.000563    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.104742    |
| Train/Loss              | 0.061935097 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.05730182  |
| Train/Ratio             | 0.99997866  |
| Train/Return            | 1.8318329   |
| Train/V                 | 1.889129    |
| Train/Value             | 1.889129    |
| Train/control_penalty   | 0.4070549   |
| Train/policy_loss       | 0.05730182  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04425     |
-----------------------------------------

 ---------------- Iteration 72 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 71         |
| Time/Actor_Time         | 0.069      |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.0683     |
| Time/Buffer             | 0.00323    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22749913 |
| Train/Action_magnitu... | 0.51890403 |
| Train/Action_magnitude  | 0.4039257  |
| Train/Action_max        | 0.21768647 |
| Train/Action_std        | 0.1423437  |
| Train/Entropy           | -0.5644743 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1345415  |
| Train/Loss              | 0.11108379 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10653445 |
| Train/Ratio             | 1.0000056  |
| Train/Return            | 1.665109   |
| Train/V                 | 1.7716488  |
| Train/Value             | 1.7716488  |
| Train/control_penalty   | 0.39848694 |
| Train/policy_loss       | 0.10653445 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------

 ---------------- Iteration 73 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 72          |
| Time/Actor_Time         | 0.0674      |
| Time/B_Format_Time      | 0.0783      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00798     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2314235   |
| Train/Action_magnitu... | 0.5244075   |
| Train/Action_magnitude  | 0.40710676  |
| Train/Action_max        | 0.207545    |
| Train/Action_std        | 0.1438071   |
| Train/Entropy           | -0.5553804  |
| Train/Entropy_Loss      | 0.000555    |
| Train/Entropy_loss      | 0.000555    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1111089   |
| Train/Loss              | 0.05812785  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.053503446 |
| Train/Ratio             | 1.0000181   |
| Train/Return            | 1.8631849   |
| Train/V                 | 1.9166775   |
| Train/Value             | 1.9166775   |
| Train/control_penalty   | 0.40690228  |
| Train/policy_loss       | 0.053503446 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04425     |
-----------------------------------------

 ---------------- Iteration 74 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 73         |
| Time/Actor_Time         | 0.0799     |
| Time/B_Format_Time      | 0.0972     |
| Time/B_Original_Form... | 0.089      |
| Time/Buffer             | 0.00326    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23046567 |
| Train/Action_magnitu... | 0.5179299  |
| Train/Action_magnitude  | 0.39953187 |
| Train/Action_max        | 0.22148147 |
| Train/Action_std        | 0.1411227  |
| Train/Entropy           | -0.5751387 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1428453  |
| Train/Loss              | 0.12055762 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11601349 |
| Train/Ratio             | 0.99998814 |
| Train/Return            | 1.6574329  |
| Train/V                 | 1.773443   |
| Train/Value             | 1.773443   |
| Train/control_penalty   | 0.39689884 |
| Train/policy_loss       | 0.11601349 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.031      |
----------------------------------------

 ---------------- Iteration 75 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 74          |
| Time/Actor_Time         | 0.0719      |
| Time/B_Format_Time      | 0.13        |
| Time/B_Original_Form... | 0.13        |
| Time/Buffer             | 0.0084      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23026288  |
| Train/Action_magnitu... | 0.52128744  |
| Train/Action_magnitude  | 0.4078146   |
| Train/Action_max        | 0.2079043   |
| Train/Action_std        | 0.14419924  |
| Train/Entropy           | -0.5527962  |
| Train/Entropy_Loss      | 0.000553    |
| Train/Entropy_loss      | 0.000553    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0930372   |
| Train/Loss              | 0.06179381  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.057163555 |
| Train/Ratio             | 0.99998415  |
| Train/Return            | 2.0129938   |
| Train/V                 | 2.070161    |
| Train/Value             | 2.070161    |
| Train/control_penalty   | 0.40774626  |
| Train/policy_loss       | 0.057163555 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04675     |
-----------------------------------------

 ---------------- Iteration 76 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 75          |
| Time/Actor_Time         | 0.112       |
| Time/B_Format_Time      | 0.0753      |
| Time/B_Original_Form... | 0.101       |
| Time/Buffer             | 0.00422     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23663022  |
| Train/Action_magnitu... | 0.5345287   |
| Train/Action_magnitude  | 0.4144278   |
| Train/Action_max        | 0.21266967  |
| Train/Action_std        | 0.14416723  |
| Train/Entropy           | -0.55116916 |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.090974    |
| Train/Loss              | 0.13589779  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13116333  |
| Train/Ratio             | 0.9999818   |
| Train/Return            | 1.611482    |
| Train/V                 | 1.7426482   |
| Train/Value             | 1.7426482   |
| Train/control_penalty   | 0.41832936  |
| Train/policy_loss       | 0.13116333  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 77 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 76          |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0892      |
| Time/B_Original_Form... | 0.0786      |
| Time/Buffer             | 0.00406     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23762783  |
| Train/Action_magnitu... | 0.5302863   |
| Train/Action_magnitude  | 0.41142237  |
| Train/Action_max        | 0.1901487   |
| Train/Action_std        | 0.14178805  |
| Train/Entropy           | -0.56154615 |
| Train/Entropy_Loss      | 0.000562    |
| Train/Entropy_loss      | 0.000562    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1200513   |
| Train/Loss              | 0.08999906  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08532382  |
| Train/Ratio             | 0.9999858   |
| Train/Return            | 1.9484105   |
| Train/V                 | 2.0337322   |
| Train/Value             | 2.0337322   |
| Train/control_penalty   | 0.41136965  |
| Train/policy_loss       | 0.08532382  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04225     |
-----------------------------------------

 ---------------- Iteration 78 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 77         |
| Time/Actor_Time         | 0.0885     |
| Time/B_Format_Time      | 0.075      |
| Time/B_Original_Form... | 0.136      |
| Time/Buffer             | 0.00475    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24434738 |
| Train/Action_magnitu... | 0.54954493 |
| Train/Action_magnitude  | 0.42740133 |
| Train/Action_max        | 0.1863797  |
| Train/Action_std        | 0.14820953 |
| Train/Entropy           | -0.5209454 |
| Train/Entropy_Loss      | 0.000521   |
| Train/Entropy_loss      | 0.000521   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0711658  |
| Train/Loss              | 0.09761336 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09281909 |
| Train/Ratio             | 1.0000118  |
| Train/Return            | 1.7873563  |
| Train/V                 | 1.8801774  |
| Train/Value             | 1.8801774  |
| Train/control_penalty   | 0.42733237 |
| Train/policy_loss       | 0.09281909 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.04075    |
----------------------------------------

 ---------------- Iteration 79 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 78          |
| Time/Actor_Time         | 0.066       |
| Time/B_Format_Time      | 0.123       |
| Time/B_Original_Form... | 0.0806      |
| Time/Buffer             | 0.00496     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2455931   |
| Train/Action_magnitu... | 0.55031466  |
| Train/Action_magnitude  | 0.42878476  |
| Train/Action_max        | 0.1942872   |
| Train/Action_std        | 0.14554453  |
| Train/Entropy           | -0.53568196 |
| Train/Entropy_Loss      | 0.000536    |
| Train/Entropy_loss      | 0.000536    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.071475    |
| Train/Loss              | 0.05379639  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.048997685 |
| Train/Ratio             | 1.0000099   |
| Train/Return            | 2.0246687   |
| Train/V                 | 2.0736666   |
| Train/Value             | 2.0736666   |
| Train/control_penalty   | 0.42630213  |
| Train/policy_loss       | 0.048997685 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04775     |
-----------------------------------------

 ---------------- Iteration 80 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 79         |
| Time/Actor_Time         | 0.0712     |
| Time/B_Format_Time      | 0.089      |
| Time/B_Original_Form... | 0.0778     |
| Time/Buffer             | 0.00979    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.242922   |
| Train/Action_magnitu... | 0.5445339  |
| Train/Action_magnitude  | 0.42670432 |
| Train/Action_max        | 0.18605615 |
| Train/Action_std        | 0.14364983 |
| Train/Entropy           | -0.5493867 |
| Train/Entropy_Loss      | 0.000549   |
| Train/Entropy_loss      | 0.000549   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0731342  |
| Train/Loss              | 0.06768815 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.06288458 |
| Train/Ratio             | 1.0000188  |
| Train/Return            | 1.7599516  |
| Train/V                 | 1.8228273  |
| Train/Value             | 1.8228273  |
| Train/control_penalty   | 0.42541933 |
| Train/policy_loss       | 0.06288458 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.04025    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 81 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 80         |
| Time/Actor_Time         | 0.0794     |
| Time/B_Format_Time      | 0.0775     |
| Time/B_Original_Form... | 0.1        |
| Time/Buffer             | 0.004      |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2490741  |
| Train/Action_magnitu... | 0.5541422  |
| Train/Action_magnitude  | 0.4327435  |
| Train/Action_max        | 0.20314623 |
| Train/Action_std        | 0.14731002 |
| Train/Entropy           | -0.5233087 |
| Train/Entropy_Loss      | 0.000523   |
| Train/Entropy_loss      | 0.000523   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0455313  |
| Train/Loss              | 0.05468853 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.04984279 |
| Train/Ratio             | 1.000006   |
| Train/Return            | 1.7811598  |
| Train/V                 | 1.8310093  |
| Train/Value             | 1.8310093  |
| Train/control_penalty   | 0.4322429  |
| Train/policy_loss       | 0.04984279 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03925    |
----------------------------------------

 ---------------- Iteration 82 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 81          |
| Time/Actor_Time         | 0.101       |
| Time/B_Format_Time      | 0.102       |
| Time/B_Original_Form... | 0.0773      |
| Time/Buffer             | 0.00647     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25855485  |
| Train/Action_magnitu... | 0.57341915  |
| Train/Action_magnitude  | 0.45080662  |
| Train/Action_max        | 0.18767281  |
| Train/Action_std        | 0.14593492  |
| Train/Entropy           | -0.5350163  |
| Train/Entropy_Loss      | 0.000535    |
| Train/Entropy_loss      | 0.000535    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0863562   |
| Train/Loss              | 0.07053639  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.065446936 |
| Train/Ratio             | 0.99999243  |
| Train/Return            | 1.9041318   |
| Train/V                 | 1.9695756   |
| Train/Value             | 1.9695756   |
| Train/control_penalty   | 0.4554433   |
| Train/policy_loss       | 0.065446936 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.046       |
-----------------------------------------

 ---------------- Iteration 83 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 82          |
| Time/Actor_Time         | 0.0686      |
| Time/B_Format_Time      | 0.0846      |
| Time/B_Original_Form... | 0.0756      |
| Time/Buffer             | 0.00477     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2547806   |
| Train/Action_magnitu... | 0.5679171   |
| Train/Action_magnitude  | 0.44394687  |
| Train/Action_max        | 0.19818045  |
| Train/Action_std        | 0.14618345  |
| Train/Entropy           | -0.5313052  |
| Train/Entropy_Loss      | 0.000531    |
| Train/Entropy_loss      | 0.000531    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0687029   |
| Train/Loss              | 0.028449915 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0234745   |
| Train/Ratio             | 0.9999977   |
| Train/Return            | 1.9103242   |
| Train/V                 | 1.9337978   |
| Train/Value             | 1.9337978   |
| Train/control_penalty   | 0.4444111   |
| Train/policy_loss       | 0.0234745   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0485      |
-----------------------------------------

 ---------------- Iteration 84 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 83         |
| Time/Actor_Time         | 0.102      |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0854     |
| Time/Buffer             | 0.00451    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.250149   |
| Train/Action_magnitu... | 0.5599157  |
| Train/Action_magnitude  | 0.43713826 |
| Train/Action_max        | 0.20630981 |
| Train/Action_std        | 0.14526406 |
| Train/Entropy           | -0.5405719 |
| Train/Entropy_Loss      | 0.000541   |
| Train/Entropy_loss      | 0.000541   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0703615  |
| Train/Loss              | 0.10781439 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10292235 |
| Train/Ratio             | 0.99999267 |
| Train/Return            | 1.8690479  |
| Train/V                 | 1.971973   |
| Train/Value             | 1.971973   |
| Train/control_penalty   | 0.43514708 |
| Train/policy_loss       | 0.10292235 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.041      |
----------------------------------------

 ---------------- Iteration 85 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 84          |
| Time/Actor_Time         | 0.0638      |
| Time/B_Format_Time      | 0.0688      |
| Time/B_Original_Form... | 0.0688      |
| Time/Buffer             | 0.00387     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23612748  |
| Train/Action_magnitu... | 0.5299802   |
| Train/Action_magnitude  | 0.4102353   |
| Train/Action_max        | 0.21362744  |
| Train/Action_std        | 0.14118561  |
| Train/Entropy           | -0.57140845 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1495742   |
| Train/Loss              | 0.12530304  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12059584  |
| Train/Ratio             | 1.0000068   |
| Train/Return            | 1.5901282   |
| Train/V                 | 1.7107109   |
| Train/Value             | 1.7107109   |
| Train/control_penalty   | 0.4135801   |
| Train/policy_loss       | 0.12059584  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0285      |
-----------------------------------------

 ---------------- Iteration 86 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 85          |
| Time/Actor_Time         | 0.0782      |
| Time/B_Format_Time      | 0.0821      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00474     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24507535  |
| Train/Action_magnitu... | 0.5460988   |
| Train/Action_magnitude  | 0.42256638  |
| Train/Action_max        | 0.22412257  |
| Train/Action_std        | 0.13956532  |
| Train/Entropy           | -0.58314276 |
| Train/Entropy_Loss      | 0.000583    |
| Train/Entropy_loss      | 0.000583    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1582232   |
| Train/Loss              | 0.083312616 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0784635   |
| Train/Ratio             | 1.0000068   |
| Train/Return            | 1.7341101   |
| Train/V                 | 1.8125694   |
| Train/Value             | 1.8125694   |
| Train/control_penalty   | 0.42659718  |
| Train/policy_loss       | 0.0784635   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.041       |
-----------------------------------------

 ---------------- Iteration 87 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 86         |
| Time/Actor_Time         | 0.0639     |
| Time/B_Format_Time      | 0.0723     |
| Time/B_Original_Form... | 0.113      |
| Time/Buffer             | 0.00341    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24294287 |
| Train/Action_magnitu... | 0.5414774  |
| Train/Action_magnitude  | 0.41683656 |
| Train/Action_max        | 0.2203419  |
| Train/Action_std        | 0.13779835 |
| Train/Entropy           | -0.5961334 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1548607  |
| Train/Loss              | 0.09027264 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08558538 |
| Train/Ratio             | 0.99999535 |
| Train/Return            | 1.8505571  |
| Train/V                 | 1.9361471  |
| Train/Value             | 1.9361471  |
| Train/control_penalty   | 0.40911323 |
| Train/policy_loss       | 0.08558538 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0375     |
----------------------------------------

 ---------------- Iteration 88 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 87          |
| Time/Actor_Time         | 0.0701      |
| Time/B_Format_Time      | 0.0695      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.004       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24231388  |
| Train/Action_magnitu... | 0.5350299   |
| Train/Action_magnitude  | 0.41345623  |
| Train/Action_max        | 0.20384747  |
| Train/Action_std        | 0.13473201  |
| Train/Entropy           | -0.61795557 |
| Train/Entropy_Loss      | 0.000618    |
| Train/Entropy_loss      | 0.000618    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2105583   |
| Train/Loss              | 0.15110458  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14634393  |
| Train/Ratio             | 1.000003    |
| Train/Return            | 1.6274216   |
| Train/V                 | 1.7737665   |
| Train/Value             | 1.7737665   |
| Train/control_penalty   | 0.41426998  |
| Train/policy_loss       | 0.14634393  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 89 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 88          |
| Time/Actor_Time         | 0.071       |
| Time/B_Format_Time      | 0.0791      |
| Time/B_Original_Form... | 0.0842      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24012023  |
| Train/Action_magnitu... | 0.537255    |
| Train/Action_magnitude  | 0.41648975  |
| Train/Action_max        | 0.21335898  |
| Train/Action_std        | 0.14020541  |
| Train/Entropy           | -0.57998204 |
| Train/Entropy_Loss      | 0.00058     |
| Train/Entropy_loss      | 0.00058     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1719173   |
| Train/Loss              | 0.0986596   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09392478  |
| Train/Ratio             | 0.99999475  |
| Train/Return            | 1.919575    |
| Train/V                 | 2.013501    |
| Train/Value             | 2.013501    |
| Train/control_penalty   | 0.415483    |
| Train/policy_loss       | 0.09392478  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03925     |
-----------------------------------------

 ---------------- Iteration 90 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 89          |
| Time/Actor_Time         | 0.0932      |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.0783      |
| Time/Buffer             | 0.00481     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22496353  |
| Train/Action_magnitu... | 0.4956562   |
| Train/Action_magnitude  | 0.38593507  |
| Train/Action_max        | 0.18037285  |
| Train/Action_std        | 0.133495    |
| Train/Entropy           | -0.62830174 |
| Train/Entropy_Loss      | 0.000628    |
| Train/Entropy_loss      | 0.000628    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2725466   |
| Train/Loss              | 0.1128787   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10838937  |
| Train/Ratio             | 1.0000077   |
| Train/Return            | 1.6567786   |
| Train/V                 | 1.7651649   |
| Train/Value             | 1.7651649   |
| Train/control_penalty   | 0.38610336  |
| Train/policy_loss       | 0.10838937  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03625     |
-----------------------------------------

 ---------------- Iteration 91 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 90         |
| Time/Actor_Time         | 0.0626     |
| Time/B_Format_Time      | 0.0819     |
| Time/B_Original_Form... | 0.0918     |
| Time/Buffer             | 0.00579    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24609244 |
| Train/Action_magnitu... | 0.5390408  |
| Train/Action_magnitude  | 0.41903785 |
| Train/Action_max        | 0.20942208 |
| Train/Action_std        | 0.13819255 |
| Train/Entropy           | -0.5946686 |
| Train/Entropy_Loss      | 0.000595   |
| Train/Entropy_loss      | 0.000595   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1874114  |
| Train/Loss              | 0.10627331 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10150831 |
| Train/Ratio             | 1.0000209  |
| Train/Return            | 1.8288027  |
| Train/V                 | 1.9303062  |
| Train/Value             | 1.9303062  |
| Train/control_penalty   | 0.4170332  |
| Train/policy_loss       | 0.10150831 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.036      |
----------------------------------------

 ---------------- Iteration 92 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 91         |
| Time/Actor_Time         | 0.0815     |
| Time/B_Format_Time      | 0.0723     |
| Time/B_Original_Form... | 0.083      |
| Time/Buffer             | 0.00584    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23266001 |
| Train/Action_magnitu... | 0.52182996 |
| Train/Action_magnitude  | 0.4067677  |
| Train/Action_max        | 0.1867542  |
| Train/Action_std        | 0.1377045  |
| Train/Entropy           | -0.5975557 |
| Train/Entropy_Loss      | 0.000598   |
| Train/Entropy_loss      | 0.000598   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1824619  |
| Train/Loss              | 0.15166116 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14697447 |
| Train/Ratio             | 0.99999446 |
| Train/Return            | 1.6284136  |
| Train/V                 | 1.7753811  |
| Train/Value             | 1.7753811  |
| Train/control_penalty   | 0.40891284 |
| Train/policy_loss       | 0.14697447 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 93 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 92          |
| Time/Actor_Time         | 0.0644      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.071       |
| Time/Buffer             | 0.0041      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2456431   |
| Train/Action_magnitu... | 0.5519154   |
| Train/Action_magnitude  | 0.43092552  |
| Train/Action_max        | 0.19711658  |
| Train/Action_std        | 0.14482145  |
| Train/Entropy           | -0.54365796 |
| Train/Entropy_Loss      | 0.000544    |
| Train/Entropy_loss      | 0.000544    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0977144   |
| Train/Loss              | 0.04584182  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.040983565 |
| Train/Ratio             | 0.99999684  |
| Train/Return            | 1.6577444   |
| Train/V                 | 1.6987197   |
| Train/Value             | 1.6987197   |
| Train/control_penalty   | 0.43145996  |
| Train/policy_loss       | 0.040983565 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03675     |
-----------------------------------------

 ---------------- Iteration 94 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 93          |
| Time/Actor_Time         | 0.0656      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0735      |
| Time/Buffer             | 0.00351     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2379104   |
| Train/Action_magnitu... | 0.5387688   |
| Train/Action_magnitude  | 0.42112014  |
| Train/Action_max        | 0.20243542  |
| Train/Action_std        | 0.14296082  |
| Train/Entropy           | -0.56277406 |
| Train/Entropy_Loss      | 0.000563    |
| Train/Entropy_loss      | 0.000563    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1274954   |
| Train/Loss              | 0.19303527  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18823975  |
| Train/Ratio             | 0.9999889   |
| Train/Return            | 1.7817906   |
| Train/V                 | 1.9700314   |
| Train/Value             | 1.9700314   |
| Train/control_penalty   | 0.4232748   |
| Train/policy_loss       | 0.18823975  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 95 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 94          |
| Time/Actor_Time         | 0.0699      |
| Time/B_Format_Time      | 0.0742      |
| Time/B_Original_Form... | 0.0771      |
| Time/Buffer             | 0.00423     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24171844  |
| Train/Action_magnitu... | 0.53855026  |
| Train/Action_magnitude  | 0.42023662  |
| Train/Action_max        | 0.20746914  |
| Train/Action_std        | 0.14129065  |
| Train/Entropy           | -0.57168186 |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1271045   |
| Train/Loss              | 0.15002763  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14522244  |
| Train/Ratio             | 1.0000196   |
| Train/Return            | 1.5286511   |
| Train/V                 | 1.673874    |
| Train/Value             | 1.673874    |
| Train/control_penalty   | 0.4233511   |
| Train/policy_loss       | 0.14522244  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 96 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 95         |
| Time/Actor_Time         | 0.0716     |
| Time/B_Format_Time      | 0.0684     |
| Time/B_Original_Form... | 0.0717     |
| Time/Buffer             | 0.00293    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25097293 |
| Train/Action_magnitu... | 0.5593447  |
| Train/Action_magnitude  | 0.43407723 |
| Train/Action_max        | 0.21846718 |
| Train/Action_std        | 0.14297555 |
| Train/Entropy           | -0.5613048 |
| Train/Entropy_Loss      | 0.000561   |
| Train/Entropy_loss      | 0.000561   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1383553  |
| Train/Loss              | 0.15726346 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15238665 |
| Train/Ratio             | 1.0000045  |
| Train/Return            | 1.6729004  |
| Train/V                 | 1.8252909  |
| Train/Value             | 1.8252909  |
| Train/control_penalty   | 0.43155006 |
| Train/policy_loss       | 0.15238665 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------

 ---------------- Iteration 97 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 96          |
| Time/Actor_Time         | 0.0729      |
| Time/B_Format_Time      | 0.0785      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00369     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23919614  |
| Train/Action_magnitu... | 0.5363801   |
| Train/Action_magnitude  | 0.41493958  |
| Train/Action_max        | 0.21945335  |
| Train/Action_std        | 0.1400426   |
| Train/Entropy           | -0.58519745 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1573813   |
| Train/Loss              | 0.1481711   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14342368  |
| Train/Ratio             | 0.9999979   |
| Train/Return            | 1.4897295   |
| Train/V                 | 1.6331534   |
| Train/Value             | 1.6331534   |
| Train/control_penalty   | 0.4162227   |
| Train/policy_loss       | 0.14342368  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 98 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 97         |
| Time/Actor_Time         | 0.0791     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.132      |
| Time/Buffer             | 0.00302    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2355347  |
| Train/Action_magnitu... | 0.53299946 |
| Train/Action_magnitude  | 0.41411895 |
| Train/Action_max        | 0.21364522 |
| Train/Action_std        | 0.14007203 |
| Train/Entropy           | -0.5842236 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1375442  |
| Train/Loss              | 0.1669757  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16228417 |
| Train/Ratio             | 0.9999968  |
| Train/Return            | 1.5016253  |
| Train/V                 | 1.6639099  |
| Train/Value             | 1.6639099  |
| Train/control_penalty   | 0.41073066 |
| Train/policy_loss       | 0.16228417 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------

 ---------------- Iteration 99 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 98          |
| Time/Actor_Time         | 0.0749      |
| Time/B_Format_Time      | 0.0708      |
| Time/B_Original_Form... | 0.0825      |
| Time/Buffer             | 0.00375     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24642536  |
| Train/Action_magnitu... | 0.54470086  |
| Train/Action_magnitude  | 0.4228784   |
| Train/Action_max        | 0.21628255  |
| Train/Action_std        | 0.14184266  |
| Train/Entropy           | -0.57058287 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.157357    |
| Train/Loss              | 0.10539247  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10064486  |
| Train/Ratio             | 1.0000112   |
| Train/Return            | 1.677813    |
| Train/V                 | 1.7784579   |
| Train/Value             | 1.7784579   |
| Train/control_penalty   | 0.41770354  |
| Train/policy_loss       | 0.10064486  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------

 ---------------- Iteration 100 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 99         |
| Time/Actor_Time         | 0.0883     |
| Time/B_Format_Time      | 0.083      |
| Time/B_Original_Form... | 0.106      |
| Time/Buffer             | 0.005      |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24179761 |
| Train/Action_magnitu... | 0.53088516 |
| Train/Action_magnitude  | 0.41193926 |
| Train/Action_max        | 0.21907747 |
| Train/Action_std        | 0.1368467  |
| Train/Entropy           | -0.6039655 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2258638  |
| Train/Loss              | 0.17813185 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17338131 |
| Train/Ratio             | 0.99998355 |
| Train/Return            | 1.4709417  |
| Train/V                 | 1.6443179  |
| Train/Value             | 1.6443179  |
| Train/control_penalty   | 0.41465715 |
| Train/policy_loss       | 0.17338131 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02175    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 101 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 100        |
| Time/Actor_Time         | 0.122      |
| Time/B_Format_Time      | 0.0743     |
| Time/B_Original_Form... | 0.118      |
| Time/Buffer             | 0.00439    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23099254 |
| Train/Action_magnitu... | 0.511288   |
| Train/Action_magnitude  | 0.3931422  |
| Train/Action_max        | 0.21086653 |
| Train/Action_std        | 0.13601711 |
| Train/Entropy           | -0.6147748 |
| Train/Entropy_Loss      | 0.000615   |
| Train/Entropy_loss      | 0.000615   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2432092  |
| Train/Loss              | 0.16630574 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1617783  |
| Train/Ratio             | 0.99999565 |
| Train/Return            | 1.7000462  |
| Train/V                 | 1.8618276  |
| Train/Value             | 1.8618276  |
| Train/control_penalty   | 0.39126515 |
| Train/policy_loss       | 0.1617783  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 102 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 101         |
| Time/Actor_Time         | 0.064       |
| Time/B_Format_Time      | 0.0724      |
| Time/B_Original_Form... | 0.122       |
| Time/Buffer             | 0.00299     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2342813   |
| Train/Action_magnitu... | 0.5250557   |
| Train/Action_magnitude  | 0.40953547  |
| Train/Action_max        | 0.20910953  |
| Train/Action_std        | 0.14116818  |
| Train/Entropy           | -0.57643753 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1601623   |
| Train/Loss              | 0.20481579  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20015675  |
| Train/Ratio             | 0.99999905  |
| Train/Return            | 1.7044818   |
| Train/V                 | 1.9046383   |
| Train/Value             | 1.9046383   |
| Train/control_penalty   | 0.4082606   |
| Train/policy_loss       | 0.20015675  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 103 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 102        |
| Time/Actor_Time         | 0.0744     |
| Time/B_Format_Time      | 0.0739     |
| Time/B_Original_Form... | 0.0858     |
| Time/Buffer             | 0.00301    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23579168 |
| Train/Action_magnitu... | 0.5309903  |
| Train/Action_magnitude  | 0.4132842  |
| Train/Action_max        | 0.20709364 |
| Train/Action_std        | 0.14185475 |
| Train/Entropy           | -0.5716474 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1286768  |
| Train/Loss              | 0.15503223 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15038186 |
| Train/Ratio             | 0.9999968  |
| Train/Return            | 1.8262222  |
| Train/V                 | 1.9766093  |
| Train/Value             | 1.9766093  |
| Train/control_penalty   | 0.40787226 |
| Train/policy_loss       | 0.15038186 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 104 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 103         |
| Time/Actor_Time         | 0.116       |
| Time/B_Format_Time      | 0.116       |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.0044      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23532593  |
| Train/Action_magnitu... | 0.5274448   |
| Train/Action_magnitude  | 0.40814945  |
| Train/Action_max        | 0.1904137   |
| Train/Action_std        | 0.1372247   |
| Train/Entropy           | -0.60352373 |
| Train/Entropy_Loss      | 0.000604    |
| Train/Entropy_loss      | 0.000604    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1932356   |
| Train/Loss              | 0.19581145  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19116548  |
| Train/Ratio             | 0.999983    |
| Train/Return            | 1.9335749   |
| Train/V                 | 2.1247385   |
| Train/Value             | 2.1247385   |
| Train/control_penalty   | 0.404245    |
| Train/policy_loss       | 0.19116548  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 105 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 104        |
| Time/Actor_Time         | 0.0819     |
| Time/B_Format_Time      | 0.0827     |
| Time/B_Original_Form... | 0.105      |
| Time/Buffer             | 0.00305    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22572179 |
| Train/Action_magnitu... | 0.51182383 |
| Train/Action_magnitude  | 0.39790234 |
| Train/Action_max        | 0.2124788  |
| Train/Action_std        | 0.13770571 |
| Train/Entropy           | -0.601883  |
| Train/Entropy_Loss      | 0.000602   |
| Train/Entropy_loss      | 0.000602   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1910228  |
| Train/Loss              | 0.2328645  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22828674 |
| Train/Ratio             | 1.0000013  |
| Train/Return            | 1.8539747  |
| Train/V                 | 2.0822585  |
| Train/Value             | 2.0822585  |
| Train/control_penalty   | 0.39758623 |
| Train/policy_loss       | 0.22828674 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 106 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 105         |
| Time/Actor_Time         | 0.133       |
| Time/B_Format_Time      | 0.117       |
| Time/B_Original_Form... | 0.0775      |
| Time/Buffer             | 0.00788     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2411736   |
| Train/Action_magnitu... | 0.53524095  |
| Train/Action_magnitude  | 0.4168553   |
| Train/Action_max        | 0.2113349   |
| Train/Action_std        | 0.13879465  |
| Train/Entropy           | -0.59078366 |
| Train/Entropy_Loss      | 0.000591    |
| Train/Entropy_loss      | 0.000591    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2061118   |
| Train/Loss              | 0.20579907  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20106614  |
| Train/Ratio             | 0.9999844   |
| Train/Return            | 1.7918968   |
| Train/V                 | 1.9929631   |
| Train/Value             | 1.9929631   |
| Train/control_penalty   | 0.41421553  |
| Train/policy_loss       | 0.20106614  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 107 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 106         |
| Time/Actor_Time         | 0.0741      |
| Time/B_Format_Time      | 0.0966      |
| Time/B_Original_Form... | 0.0823      |
| Time/Buffer             | 0.00288     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22836946  |
| Train/Action_magnitu... | 0.5206571   |
| Train/Action_magnitude  | 0.40449792  |
| Train/Action_max        | 0.20177858  |
| Train/Action_std        | 0.14510496  |
| Train/Entropy           | -0.54746777 |
| Train/Entropy_Loss      | 0.000547    |
| Train/Entropy_loss      | 0.000547    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0985948   |
| Train/Loss              | 0.18287465  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17824948  |
| Train/Ratio             | 0.999979    |
| Train/Return            | 1.7397165   |
| Train/V                 | 1.9179721   |
| Train/Value             | 1.9179721   |
| Train/control_penalty   | 0.4077696   |
| Train/policy_loss       | 0.17824948  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 108 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 107         |
| Time/Actor_Time         | 0.0739      |
| Time/B_Format_Time      | 0.069       |
| Time/B_Original_Form... | 0.0797      |
| Time/Buffer             | 0.00393     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24432144  |
| Train/Action_magnitu... | 0.5436056   |
| Train/Action_magnitude  | 0.42075583  |
| Train/Action_max        | 0.21529964  |
| Train/Action_std        | 0.14125177  |
| Train/Entropy           | -0.57665557 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1509914   |
| Train/Loss              | 0.18286434  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17808355  |
| Train/Ratio             | 1.0000168   |
| Train/Return            | 1.8494209   |
| Train/V                 | 2.0275028   |
| Train/Value             | 2.0275028   |
| Train/control_penalty   | 0.42041194  |
| Train/policy_loss       | 0.17808355  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 109 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 108         |
| Time/Actor_Time         | 0.0785      |
| Time/B_Format_Time      | 0.0769      |
| Time/B_Original_Form... | 0.138       |
| Time/Buffer             | 0.00292     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23361726  |
| Train/Action_magnitu... | 0.52145827  |
| Train/Action_magnitude  | 0.40387288  |
| Train/Action_max        | 0.1977898   |
| Train/Action_std        | 0.14043728  |
| Train/Entropy           | -0.58090997 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1598508   |
| Train/Loss              | 0.18096155  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17630751  |
| Train/Ratio             | 0.9999828   |
| Train/Return            | 1.6024437   |
| Train/V                 | 1.7787584   |
| Train/Value             | 1.7787584   |
| Train/control_penalty   | 0.40731347  |
| Train/policy_loss       | 0.17630751  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02        |
-----------------------------------------

 ---------------- Iteration 110 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 109        |
| Time/Actor_Time         | 0.063      |
| Time/B_Format_Time      | 0.0686     |
| Time/B_Original_Form... | 0.0731     |
| Time/Buffer             | 0.00402    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24305554 |
| Train/Action_magnitu... | 0.5396788  |
| Train/Action_magnitude  | 0.4209651  |
| Train/Action_max        | 0.21012235 |
| Train/Action_std        | 0.14488643 |
| Train/Entropy           | -0.552484  |
| Train/Entropy_Loss      | 0.000552   |
| Train/Entropy_loss      | 0.000552   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1242795  |
| Train/Loss              | 0.15966707 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1548874  |
| Train/Ratio             | 1.0000046  |
| Train/Return            | 1.5062622  |
| Train/V                 | 1.6611502  |
| Train/Value             | 1.6611502  |
| Train/control_penalty   | 0.4227186  |
| Train/policy_loss       | 0.1548874  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 111 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 110        |
| Time/Actor_Time         | 0.0677     |
| Time/B_Format_Time      | 0.0738     |
| Time/B_Original_Form... | 0.119      |
| Time/Buffer             | 0.00275    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22589728 |
| Train/Action_magnitu... | 0.50086373 |
| Train/Action_magnitude  | 0.3874762  |
| Train/Action_max        | 0.18944523 |
| Train/Action_std        | 0.13544749 |
| Train/Entropy           | -0.6147158 |
| Train/Entropy_Loss      | 0.000615   |
| Train/Entropy_loss      | 0.000615   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2322984  |
| Train/Loss              | 0.153876   |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14937583 |
| Train/Ratio             | 0.99998623 |
| Train/Return            | 1.4049975  |
| Train/V                 | 1.5543773  |
| Train/Value             | 1.5543773  |
| Train/control_penalty   | 0.38854638 |
| Train/policy_loss       | 0.14937583 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02       |
----------------------------------------

 ---------------- Iteration 112 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 111        |
| Time/Actor_Time         | 0.114      |
| Time/B_Format_Time      | 0.0827     |
| Time/B_Original_Form... | 0.0719     |
| Time/Buffer             | 0.00509    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23055092 |
| Train/Action_magnitu... | 0.51737577 |
| Train/Action_magnitude  | 0.4001372  |
| Train/Action_max        | 0.20487106 |
| Train/Action_std        | 0.1405107  |
| Train/Entropy           | -0.5784497 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.183935   |
| Train/Loss              | 0.11152163 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10693559 |
| Train/Ratio             | 1.0000088  |
| Train/Return            | 1.5284519  |
| Train/V                 | 1.6353875  |
| Train/Value             | 1.6353875  |
| Train/control_penalty   | 0.40075964 |
| Train/policy_loss       | 0.10693559 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 113 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 112         |
| Time/Actor_Time         | 0.0908      |
| Time/B_Format_Time      | 0.0874      |
| Time/B_Original_Form... | 0.126       |
| Time/Buffer             | 0.00303     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23447223  |
| Train/Action_magnitu... | 0.5329382   |
| Train/Action_magnitude  | 0.41337442  |
| Train/Action_max        | 0.19985852  |
| Train/Action_std        | 0.14349177  |
| Train/Entropy           | -0.55764836 |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0892324   |
| Train/Loss              | 0.13574423  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1310744   |
| Train/Ratio             | 1.0000094   |
| Train/Return            | 1.543572    |
| Train/V                 | 1.6746455   |
| Train/Value             | 1.6746455   |
| Train/control_penalty   | 0.41121796  |
| Train/policy_loss       | 0.1310744   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 114 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 113        |
| Time/Actor_Time         | 0.0626     |
| Time/B_Format_Time      | 0.102      |
| Time/B_Original_Form... | 0.0685     |
| Time/Buffer             | 0.00304    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23887946 |
| Train/Action_magnitu... | 0.5283648  |
| Train/Action_magnitude  | 0.41046718 |
| Train/Action_max        | 0.19875693 |
| Train/Action_std        | 0.14040616 |
| Train/Entropy           | -0.5774406 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1770833  |
| Train/Loss              | 0.1505904  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14590262 |
| Train/Ratio             | 1.0000112  |
| Train/Return            | 1.5000609  |
| Train/V                 | 1.6459614  |
| Train/Value             | 1.6459614  |
| Train/control_penalty   | 0.41103446 |
| Train/policy_loss       | 0.14590262 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 115 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 114        |
| Time/Actor_Time         | 0.0674     |
| Time/B_Format_Time      | 0.0671     |
| Time/B_Original_Form... | 0.0715     |
| Time/Buffer             | 0.0032     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23679408 |
| Train/Action_magnitu... | 0.53493994 |
| Train/Action_magnitude  | 0.4151353  |
| Train/Action_max        | 0.20256963 |
| Train/Action_std        | 0.14270875 |
| Train/Entropy           | -0.5591998 |
| Train/Entropy_Loss      | 0.000559   |
| Train/Entropy_loss      | 0.000559   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1128366  |
| Train/Loss              | 0.1359786  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13131861 |
| Train/Ratio             | 1.0000012  |
| Train/Return            | 1.508796   |
| Train/V                 | 1.6401159  |
| Train/Value             | 1.6401159  |
| Train/control_penalty   | 0.4100791  |
| Train/policy_loss       | 0.13131861 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 116 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 115        |
| Time/Actor_Time         | 0.0928     |
| Time/B_Format_Time      | 0.0857     |
| Time/B_Original_Form... | 0.077      |
| Time/Buffer             | 0.00298    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24008675 |
| Train/Action_magnitu... | 0.53911847 |
| Train/Action_magnitude  | 0.42020324 |
| Train/Action_max        | 0.2010005  |
| Train/Action_std        | 0.14027114 |
| Train/Entropy           | -0.579301  |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1611282  |
| Train/Loss              | 0.10096895 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09620971 |
| Train/Ratio             | 0.9999909  |
| Train/Return            | 1.4393277  |
| Train/V                 | 1.5355428  |
| Train/Value             | 1.5355428  |
| Train/control_penalty   | 0.41799396 |
| Train/policy_loss       | 0.09620971 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 117 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 116         |
| Time/Actor_Time         | 0.138       |
| Time/B_Format_Time      | 0.111       |
| Time/B_Original_Form... | 0.0908      |
| Time/Buffer             | 0.00291     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22385517  |
| Train/Action_magnitu... | 0.5031203   |
| Train/Action_magnitude  | 0.3896913   |
| Train/Action_max        | 0.1859451   |
| Train/Action_std        | 0.13965657  |
| Train/Entropy           | -0.58486253 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1597879   |
| Train/Loss              | 0.16122676  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15675451  |
| Train/Ratio             | 0.99999547  |
| Train/Return            | 1.4541858   |
| Train/V                 | 1.6109306   |
| Train/Value             | 1.6109306   |
| Train/control_penalty   | 0.38874057  |
| Train/policy_loss       | 0.15675451  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 118 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 117         |
| Time/Actor_Time         | 0.12        |
| Time/B_Format_Time      | 0.162       |
| Time/B_Original_Form... | 0.185       |
| Time/Buffer             | 0.00325     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24415776  |
| Train/Action_magnitu... | 0.5448972   |
| Train/Action_magnitude  | 0.42345187  |
| Train/Action_max        | 0.18841824  |
| Train/Action_std        | 0.14431441  |
| Train/Entropy           | -0.54683584 |
| Train/Entropy_Loss      | 0.000547    |
| Train/Entropy_loss      | 0.000547    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0676696   |
| Train/Loss              | 0.12463819  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11986157  |
| Train/Ratio             | 1.000013    |
| Train/Return            | 1.6312362   |
| Train/V                 | 1.7510935   |
| Train/Value             | 1.7510935   |
| Train/control_penalty   | 0.42297816  |
| Train/policy_loss       | 0.11986157  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 119 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 118        |
| Time/Actor_Time         | 0.142      |
| Time/B_Format_Time      | 0.142      |
| Time/B_Original_Form... | 0.117      |
| Time/Buffer             | 0.00309    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24486108 |
| Train/Action_magnitu... | 0.5419607  |
| Train/Action_magnitude  | 0.42323726 |
| Train/Action_max        | 0.19382511 |
| Train/Action_std        | 0.1413783  |
| Train/Entropy           | -0.567018  |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1502501  |
| Train/Loss              | 0.14358452 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13879746 |
| Train/Ratio             | 0.9999767  |
| Train/Return            | 1.5587308  |
| Train/V                 | 1.6975324  |
| Train/Value             | 1.6975324  |
| Train/control_penalty   | 0.4220035  |
| Train/policy_loss       | 0.13879746 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 120 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 119        |
| Time/Actor_Time         | 0.0814     |
| Time/B_Format_Time      | 0.0812     |
| Time/B_Original_Form... | 0.0842     |
| Time/Buffer             | 0.0039     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23900023 |
| Train/Action_magnitu... | 0.53269565 |
| Train/Action_magnitude  | 0.41522187 |
| Train/Action_max        | 0.20371059 |
| Train/Action_std        | 0.14481527 |
| Train/Entropy           | -0.5438896 |
| Train/Entropy_Loss      | 0.000544   |
| Train/Entropy_loss      | 0.000544   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0822018  |
| Train/Loss              | 0.11143603 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10668237 |
| Train/Ratio             | 0.9999873  |
| Train/Return            | 1.6014439  |
| Train/V                 | 1.7081288  |
| Train/Value             | 1.7081288  |
| Train/control_penalty   | 0.42097697 |
| Train/policy_loss       | 0.10668237 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 121 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 120         |
| Time/Actor_Time         | 0.066       |
| Time/B_Format_Time      | 0.109       |
| Time/B_Original_Form... | 0.078       |
| Time/Buffer             | 0.00672     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24259096  |
| Train/Action_magnitu... | 0.53596276  |
| Train/Action_magnitude  | 0.4163334   |
| Train/Action_max        | 0.19496118  |
| Train/Action_std        | 0.14320321  |
| Train/Entropy           | -0.557343   |
| Train/Entropy_Loss      | 0.000557    |
| Train/Entropy_loss      | 0.000557    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1100112   |
| Train/Loss              | 0.109203    |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.104477905 |
| Train/Ratio             | 1.0000024   |
| Train/Return            | 1.7730339   |
| Train/V                 | 1.8775021   |
| Train/Value             | 1.8775021   |
| Train/control_penalty   | 0.41677552  |
| Train/policy_loss       | 0.104477905 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 122 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 121         |
| Time/Actor_Time         | 0.077       |
| Time/B_Format_Time      | 0.0947      |
| Time/B_Original_Form... | 0.08        |
| Time/Buffer             | 0.00374     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24020647  |
| Train/Action_magnitu... | 0.539324    |
| Train/Action_magnitude  | 0.4192653   |
| Train/Action_max        | 0.20821537  |
| Train/Action_std        | 0.14385669  |
| Train/Entropy           | -0.55149746 |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0756897   |
| Train/Loss              | 0.14427486  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13955005  |
| Train/Ratio             | 1.0000008   |
| Train/Return            | 1.6970203   |
| Train/V                 | 1.8365688   |
| Train/Value             | 1.8365688   |
| Train/control_penalty   | 0.4173328   |
| Train/policy_loss       | 0.13955005  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 123 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 122        |
| Time/Actor_Time         | 0.0691     |
| Time/B_Format_Time      | 0.0738     |
| Time/B_Original_Form... | 0.0747     |
| Time/Buffer             | 0.00555    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23947197 |
| Train/Action_magnitu... | 0.5278724  |
| Train/Action_magnitude  | 0.4097479  |
| Train/Action_max        | 0.20666818 |
| Train/Action_std        | 0.14156723 |
| Train/Entropy           | -0.5675354 |
| Train/Entropy_Loss      | 0.000568   |
| Train/Entropy_loss      | 0.000568   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1698884  |
| Train/Loss              | 0.13094778 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12624827 |
| Train/Ratio             | 0.9999927  |
| Train/Return            | 1.6307194  |
| Train/V                 | 1.7569686  |
| Train/Value             | 1.7569686  |
| Train/control_penalty   | 0.4131967  |
| Train/policy_loss       | 0.12624827 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 124 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 123        |
| Time/Actor_Time         | 0.0703     |
| Time/B_Format_Time      | 0.0825     |
| Time/B_Original_Form... | 0.0809     |
| Time/Buffer             | 0.00454    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2392145  |
| Train/Action_magnitu... | 0.5327731  |
| Train/Action_magnitude  | 0.41393375 |
| Train/Action_max        | 0.20946944 |
| Train/Action_std        | 0.139737   |
| Train/Entropy           | -0.5820408 |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1677341  |
| Train/Loss              | 0.15113969 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14642946 |
| Train/Ratio             | 1.0000069  |
| Train/Return            | 1.8194212  |
| Train/V                 | 1.9658473  |
| Train/Value             | 1.9658473  |
| Train/control_penalty   | 0.41281933 |
| Train/policy_loss       | 0.14642946 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0365     |
----------------------------------------

 ---------------- Iteration 125 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 124        |
| Time/Actor_Time         | 0.0899     |
| Time/B_Format_Time      | 0.0727     |
| Time/B_Original_Form... | 0.0835     |
| Time/Buffer             | 0.00496    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24442121 |
| Train/Action_magnitu... | 0.5405474  |
| Train/Action_magnitude  | 0.41801575 |
| Train/Action_max        | 0.21082833 |
| Train/Action_std        | 0.14229609 |
| Train/Entropy           | -0.5653776 |
| Train/Entropy_Loss      | 0.000565   |
| Train/Entropy_loss      | 0.000565   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1518135  |
| Train/Loss              | 0.12875128 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12400132 |
| Train/Ratio             | 0.99999833 |
| Train/Return            | 1.6657659  |
| Train/V                 | 1.7897776  |
| Train/Value             | 1.7897776  |
| Train/control_penalty   | 0.41845885 |
| Train/policy_loss       | 0.12400132 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 126 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 125        |
| Time/Actor_Time         | 0.0645     |
| Time/B_Format_Time      | 0.0776     |
| Time/B_Original_Form... | 0.0988     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24533218 |
| Train/Action_magnitu... | 0.54134214 |
| Train/Action_magnitude  | 0.4194893  |
| Train/Action_max        | 0.2162309  |
| Train/Action_std        | 0.1383243  |
| Train/Entropy           | -0.5930302 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1919273  |
| Train/Loss              | 0.13415779 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1293749  |
| Train/Ratio             | 1.0000111  |
| Train/Return            | 1.8272728  |
| Train/V                 | 1.956642   |
| Train/Value             | 1.956642   |
| Train/control_penalty   | 0.41898444 |
| Train/policy_loss       | 0.1293749  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0335     |
----------------------------------------

 ---------------- Iteration 127 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 126        |
| Time/Actor_Time         | 0.067      |
| Time/B_Format_Time      | 0.0912     |
| Time/B_Original_Form... | 0.0735     |
| Time/Buffer             | 0.00403    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23200428 |
| Train/Action_magnitu... | 0.51481146 |
| Train/Action_magnitude  | 0.39826885 |
| Train/Action_max        | 0.18801427 |
| Train/Action_std        | 0.13725668 |
| Train/Entropy           | -0.6003206 |
| Train/Entropy_Loss      | 0.0006     |
| Train/Entropy_loss      | 0.0006     |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1860945  |
| Train/Loss              | 0.10780658 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1031782  |
| Train/Ratio             | 1.0000215  |
| Train/Return            | 1.5932212  |
| Train/V                 | 1.6964052  |
| Train/Value             | 1.6964052  |
| Train/control_penalty   | 0.40280512 |
| Train/policy_loss       | 0.1031782  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 128 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 127        |
| Time/Actor_Time         | 0.0878     |
| Time/B_Format_Time      | 0.0751     |
| Time/B_Original_Form... | 0.0808     |
| Time/Buffer             | 0.00321    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23315257 |
| Train/Action_magnitu... | 0.5176779  |
| Train/Action_magnitude  | 0.40108758 |
| Train/Action_max        | 0.20077512 |
| Train/Action_std        | 0.13582014 |
| Train/Entropy           | -0.614835  |
| Train/Entropy_Loss      | 0.000615   |
| Train/Entropy_loss      | 0.000615   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2373822  |
| Train/Loss              | 0.14445348 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13983513 |
| Train/Ratio             | 0.999994   |
| Train/Return            | 1.7171215  |
| Train/V                 | 1.8569678  |
| Train/Value             | 1.8569678  |
| Train/control_penalty   | 0.40035108 |
| Train/policy_loss       | 0.13983513 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 129 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 128         |
| Time/Actor_Time         | 0.0676      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00421     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2375657   |
| Train/Action_magnitu... | 0.5261964   |
| Train/Action_magnitude  | 0.4080912   |
| Train/Action_max        | 0.20527034  |
| Train/Action_std        | 0.13519858  |
| Train/Entropy           | -0.61864215 |
| Train/Entropy_Loss      | 0.000619    |
| Train/Entropy_loss      | 0.000619    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2060863   |
| Train/Loss              | 0.17983429  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17512612  |
| Train/Ratio             | 1.0000262   |
| Train/Return            | 1.6820021   |
| Train/V                 | 1.8571293   |
| Train/Value             | 1.8571293   |
| Train/control_penalty   | 0.40895322  |
| Train/policy_loss       | 0.17512612  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 130 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 129        |
| Time/Actor_Time         | 0.0717     |
| Time/B_Format_Time      | 0.0789     |
| Time/B_Original_Form... | 0.0772     |
| Time/Buffer             | 0.00352    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23187795 |
| Train/Action_magnitu... | 0.51377296 |
| Train/Action_magnitude  | 0.39660662 |
| Train/Action_max        | 0.18138319 |
| Train/Action_std        | 0.13351616 |
| Train/Entropy           | -0.6309649 |
| Train/Entropy_Loss      | 0.000631   |
| Train/Entropy_loss      | 0.000631   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2562801  |
| Train/Loss              | 0.13668336 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13210681 |
| Train/Ratio             | 0.9999918  |
| Train/Return            | 1.6727148  |
| Train/V                 | 1.8048217  |
| Train/Value             | 1.8048217  |
| Train/control_penalty   | 0.39455828 |
| Train/policy_loss       | 0.13210681 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 131 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 130         |
| Time/Actor_Time         | 0.0665      |
| Time/B_Format_Time      | 0.107       |
| Time/B_Original_Form... | 0.0705      |
| Time/Buffer             | 0.00441     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23463492  |
| Train/Action_magnitu... | 0.51903147  |
| Train/Action_magnitude  | 0.40461203  |
| Train/Action_max        | 0.19432817  |
| Train/Action_std        | 0.13478224  |
| Train/Entropy           | -0.6199758  |
| Train/Entropy_Loss      | 0.00062     |
| Train/Entropy_loss      | 0.00062     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2423904   |
| Train/Loss              | 0.104723096 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10002982  |
| Train/Ratio             | 1.0000204   |
| Train/Return            | 1.598114    |
| Train/V                 | 1.6981407   |
| Train/Value             | 1.6981407   |
| Train/control_penalty   | 0.40733004  |
| Train/policy_loss       | 0.10002982  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 132 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 131         |
| Time/Actor_Time         | 0.0729      |
| Time/B_Format_Time      | 0.0703      |
| Time/B_Original_Form... | 0.119       |
| Time/Buffer             | 0.00442     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23834978  |
| Train/Action_magnitu... | 0.5293875   |
| Train/Action_magnitude  | 0.41267234  |
| Train/Action_max        | 0.19902496  |
| Train/Action_std        | 0.1357873   |
| Train/Entropy           | -0.61257875 |
| Train/Entropy_Loss      | 0.000613    |
| Train/Entropy_loss      | 0.000613    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2297627   |
| Train/Loss              | 0.14676084  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14203303  |
| Train/Ratio             | 1.0000107   |
| Train/Return            | 1.4259479   |
| Train/V                 | 1.567967    |
| Train/Value             | 1.567967    |
| Train/control_penalty   | 0.41152436  |
| Train/policy_loss       | 0.14203303  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 133 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 132        |
| Time/Actor_Time         | 0.0736     |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.0996     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23531146 |
| Train/Action_magnitu... | 0.5276955  |
| Train/Action_magnitude  | 0.40954974 |
| Train/Action_max        | 0.20649198 |
| Train/Action_std        | 0.14020374 |
| Train/Entropy           | -0.5834172 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1941589  |
| Train/Loss              | 0.15495536 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.150278   |
| Train/Ratio             | 0.9999604  |
| Train/Return            | 1.6723629  |
| Train/V                 | 1.8226463  |
| Train/Value             | 1.8226463  |
| Train/control_penalty   | 0.4093942  |
| Train/policy_loss       | 0.150278   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 134 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 133        |
| Time/Actor_Time         | 0.0902     |
| Time/B_Format_Time      | 0.106      |
| Time/B_Original_Form... | 0.0796     |
| Time/Buffer             | 0.00313    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23295274 |
| Train/Action_magnitu... | 0.515126   |
| Train/Action_magnitude  | 0.40034577 |
| Train/Action_max        | 0.17664419 |
| Train/Action_std        | 0.13452883 |
| Train/Entropy           | -0.6255805 |
| Train/Entropy_Loss      | 0.000626   |
| Train/Entropy_loss      | 0.000626   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2572007  |
| Train/Loss              | 0.14862889 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14397582 |
| Train/Ratio             | 0.99999726 |
| Train/Return            | 1.510874   |
| Train/V                 | 1.6548452  |
| Train/Value             | 1.6548452  |
| Train/control_penalty   | 0.40274894 |
| Train/policy_loss       | 0.14397582 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 135 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 134         |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0989      |
| Time/B_Original_Form... | 0.068       |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23902842  |
| Train/Action_magnitu... | 0.52623546  |
| Train/Action_magnitude  | 0.41000485  |
| Train/Action_max        | 0.1776755   |
| Train/Action_std        | 0.13592246  |
| Train/Entropy           | -0.61272454 |
| Train/Entropy_Loss      | 0.000613    |
| Train/Entropy_loss      | 0.000613    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2169117   |
| Train/Loss              | 0.12491285  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12021522  |
| Train/Ratio             | 1.0000033   |
| Train/Return            | 1.5843202   |
| Train/V                 | 1.7045379   |
| Train/Value             | 1.7045379   |
| Train/control_penalty   | 0.4084908   |
| Train/policy_loss       | 0.12021522  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0285      |
-----------------------------------------

 ---------------- Iteration 136 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 135        |
| Time/Actor_Time         | 0.0944     |
| Time/B_Format_Time      | 0.0852     |
| Time/B_Original_Form... | 0.0743     |
| Time/Buffer             | 0.00407    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23702715 |
| Train/Action_magnitu... | 0.524668   |
| Train/Action_magnitude  | 0.4081285  |
| Train/Action_max        | 0.19532739 |
| Train/Action_std        | 0.13532932 |
| Train/Entropy           | -0.6149285 |
| Train/Entropy_Loss      | 0.000615   |
| Train/Entropy_loss      | 0.000615   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2302856  |
| Train/Loss              | 0.09402245 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08935082 |
| Train/Ratio             | 1.0000018  |
| Train/Return            | 1.512052   |
| Train/V                 | 1.6014098  |
| Train/Value             | 1.6014098  |
| Train/control_penalty   | 0.4056711  |
| Train/policy_loss       | 0.08935082 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 137 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 136         |
| Time/Actor_Time         | 0.0735      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00372     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.24134038  |
| Train/Action_magnitu... | 0.53092146  |
| Train/Action_magnitude  | 0.4148965   |
| Train/Action_max        | 0.19044958  |
| Train/Action_std        | 0.13496627  |
| Train/Entropy           | -0.6185706  |
| Train/Entropy_Loss      | 0.000619    |
| Train/Entropy_loss      | 0.000619    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2405474   |
| Train/Loss              | 0.12357289  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.118834816 |
| Train/Ratio             | 0.99999917  |
| Train/Return            | 1.4950371   |
| Train/V                 | 1.6138731   |
| Train/Value             | 1.6138731   |
| Train/control_penalty   | 0.41195112  |
| Train/policy_loss       | 0.118834816 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0285      |
-----------------------------------------

 ---------------- Iteration 138 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 137         |
| Time/Actor_Time         | 0.0669      |
| Time/B_Format_Time      | 0.0837      |
| Time/B_Original_Form... | 0.0919      |
| Time/Buffer             | 0.00354     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23980664  |
| Train/Action_magnitu... | 0.5263683   |
| Train/Action_magnitude  | 0.40770563  |
| Train/Action_max        | 0.1884208   |
| Train/Action_std        | 0.13525786  |
| Train/Entropy           | -0.61688983 |
| Train/Entropy_Loss      | 0.000617    |
| Train/Entropy_loss      | 0.000617    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2461615   |
| Train/Loss              | 0.10269945  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09800698  |
| Train/Ratio             | 1.0000118   |
| Train/Return            | 1.5216811   |
| Train/V                 | 1.6196805   |
| Train/Value             | 1.6196805   |
| Train/control_penalty   | 0.40755767  |
| Train/policy_loss       | 0.09800698  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 139 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 138         |
| Time/Actor_Time         | 0.0655      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.066       |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24153921  |
| Train/Action_magnitu... | 0.5401743   |
| Train/Action_magnitude  | 0.4223531   |
| Train/Action_max        | 0.19896968  |
| Train/Action_std        | 0.14261548  |
| Train/Entropy           | -0.56675327 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1027821   |
| Train/Loss              | 0.18613648  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18135631  |
| Train/Ratio             | 0.99998784  |
| Train/Return            | 1.4826767   |
| Train/V                 | 1.6640339   |
| Train/Value             | 1.6640339   |
| Train/control_penalty   | 0.42134288  |
| Train/policy_loss       | 0.18135631  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 140 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 139        |
| Time/Actor_Time         | 0.0665     |
| Time/B_Format_Time      | 0.0702     |
| Time/B_Original_Form... | 0.0755     |
| Time/Buffer             | 0.00279    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23884268 |
| Train/Action_magnitu... | 0.5219858  |
| Train/Action_magnitude  | 0.40273345 |
| Train/Action_max        | 0.18294878 |
| Train/Action_std        | 0.13678174 |
| Train/Entropy           | -0.6037544 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2248011  |
| Train/Loss              | 0.09501853 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09031723 |
| Train/Ratio             | 1.0000027  |
| Train/Return            | 1.6249568  |
| Train/V                 | 1.7152718  |
| Train/Value             | 1.7152718  |
| Train/control_penalty   | 0.4097545  |
| Train/policy_loss       | 0.09031723 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 141 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 140         |
| Time/Actor_Time         | 0.068       |
| Time/B_Format_Time      | 0.0796      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23611946  |
| Train/Action_magnitu... | 0.5142675   |
| Train/Action_magnitude  | 0.40042618  |
| Train/Action_max        | 0.2022647   |
| Train/Action_std        | 0.13330263  |
| Train/Entropy           | -0.63112634 |
| Train/Entropy_Loss      | 0.000631    |
| Train/Entropy_loss      | 0.000631    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2830379   |
| Train/Loss              | 0.16640612  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16176525  |
| Train/Ratio             | 1.0000029   |
| Train/Return            | 1.2545718   |
| Train/V                 | 1.4163411   |
| Train/Value             | 1.4163411   |
| Train/control_penalty   | 0.40097547  |
| Train/policy_loss       | 0.16176525  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01575     |
-----------------------------------------

 ---------------- Iteration 142 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 141         |
| Time/Actor_Time         | 0.0816      |
| Time/B_Format_Time      | 0.0662      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00269     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24229613  |
| Train/Action_magnitu... | 0.5330904   |
| Train/Action_magnitude  | 0.41108185  |
| Train/Action_max        | 0.200989    |
| Train/Action_std        | 0.13759325  |
| Train/Entropy           | -0.6020477  |
| Train/Entropy_Loss      | 0.000602    |
| Train/Entropy_loss      | 0.000602    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1865133   |
| Train/Loss              | 0.07579052  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.071066745 |
| Train/Ratio             | 1.0000149   |
| Train/Return            | 1.9458851   |
| Train/V                 | 2.0169456   |
| Train/Value             | 2.0169456   |
| Train/control_penalty   | 0.41217288  |
| Train/policy_loss       | 0.071066745 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.035       |
-----------------------------------------

 ---------------- Iteration 143 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 142        |
| Time/Actor_Time         | 0.0635     |
| Time/B_Format_Time      | 0.0699     |
| Time/B_Original_Form... | 0.0728     |
| Time/Buffer             | 0.00345    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.232324   |
| Train/Action_magnitu... | 0.51975286 |
| Train/Action_magnitude  | 0.40556258 |
| Train/Action_max        | 0.1872102  |
| Train/Action_std        | 0.13800074 |
| Train/Entropy           | -0.5975178 |
| Train/Entropy_Loss      | 0.000598   |
| Train/Entropy_loss      | 0.000598   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2078015  |
| Train/Loss              | 0.17391802 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16925518 |
| Train/Ratio             | 1.0000038  |
| Train/Return            | 1.437831   |
| Train/V                 | 1.6070733  |
| Train/Value             | 1.6070733  |
| Train/control_penalty   | 0.40653205 |
| Train/policy_loss       | 0.16925518 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 144 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 143         |
| Time/Actor_Time         | 0.0905      |
| Time/B_Format_Time      | 0.0783      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.0047      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24970861  |
| Train/Action_magnitu... | 0.55457145  |
| Train/Action_magnitude  | 0.42581254  |
| Train/Action_max        | 0.22231981  |
| Train/Action_std        | 0.13943839  |
| Train/Entropy           | -0.58893865 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1963757   |
| Train/Loss              | 0.06135393  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.056555722 |
| Train/Ratio             | 1.000023    |
| Train/Return            | 1.7585534   |
| Train/V                 | 1.8151135   |
| Train/Value             | 1.8151135   |
| Train/control_penalty   | 0.42092675  |
| Train/policy_loss       | 0.056555722 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------

 ---------------- Iteration 145 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 144        |
| Time/Actor_Time         | 0.0656     |
| Time/B_Format_Time      | 0.0689     |
| Time/B_Original_Form... | 0.0714     |
| Time/Buffer             | 0.127      |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23181982 |
| Train/Action_magnitu... | 0.518304   |
| Train/Action_magnitude  | 0.40306738 |
| Train/Action_max        | 0.20578839 |
| Train/Action_std        | 0.13919534 |
| Train/Entropy           | -0.5921544 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1910847  |
| Train/Loss              | 0.20927584 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20468466 |
| Train/Ratio             | 1.0000043  |
| Train/Return            | 1.5649747  |
| Train/V                 | 1.7696599  |
| Train/Value             | 1.7696599  |
| Train/control_penalty   | 0.39990276 |
| Train/policy_loss       | 0.20468466 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01825    |
----------------------------------------

 ---------------- Iteration 146 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 145        |
| Time/Actor_Time         | 0.148      |
| Time/B_Format_Time      | 0.138      |
| Time/B_Original_Form... | 0.0672     |
| Time/Buffer             | 0.00522    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24569473 |
| Train/Action_magnitu... | 0.544943   |
| Train/Action_magnitude  | 0.4202483  |
| Train/Action_max        | 0.20707317 |
| Train/Action_std        | 0.14095189 |
| Train/Entropy           | -0.5732351 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1543806  |
| Train/Loss              | 0.09239542 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08766484 |
| Train/Ratio             | 0.9999905  |
| Train/Return            | 1.8754158  |
| Train/V                 | 1.9630818  |
| Train/Value             | 1.9630818  |
| Train/control_penalty   | 0.4157345  |
| Train/policy_loss       | 0.08766484 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 147 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 146        |
| Time/Actor_Time         | 0.0623     |
| Time/B_Format_Time      | 0.0719     |
| Time/B_Original_Form... | 0.07       |
| Time/Buffer             | 0.00423    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22575451 |
| Train/Action_magnitu... | 0.5068146  |
| Train/Action_magnitude  | 0.39246327 |
| Train/Action_max        | 0.19782019 |
| Train/Action_std        | 0.13792782 |
| Train/Entropy           | -0.5971739 |
| Train/Entropy_Loss      | 0.000597   |
| Train/Entropy_loss      | 0.000597   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2309121  |
| Train/Loss              | 0.22795075 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2234016  |
| Train/Ratio             | 1.0000122  |
| Train/Return            | 1.4465121  |
| Train/V                 | 1.6699226  |
| Train/Value             | 1.6699226  |
| Train/control_penalty   | 0.3951972  |
| Train/policy_loss       | 0.2234016  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01625    |
----------------------------------------

 ---------------- Iteration 148 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 147         |
| Time/Actor_Time         | 0.0896      |
| Time/B_Format_Time      | 0.0673      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24587947  |
| Train/Action_magnitu... | 0.5527386   |
| Train/Action_magnitude  | 0.42872506  |
| Train/Action_max        | 0.22131398  |
| Train/Action_std        | 0.14490464  |
| Train/Entropy           | -0.5463741  |
| Train/Entropy_Loss      | 0.000546    |
| Train/Entropy_loss      | 0.000546    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0713503   |
| Train/Loss              | 0.100854695 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0960235   |
| Train/Ratio             | 0.99999124  |
| Train/Return            | 1.693391    |
| Train/V                 | 1.7894189   |
| Train/Value             | 1.7894189   |
| Train/control_penalty   | 0.4284819   |
| Train/policy_loss       | 0.0960235   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 149 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 148         |
| Time/Actor_Time         | 0.0687      |
| Time/B_Format_Time      | 0.0813      |
| Time/B_Original_Form... | 0.109       |
| Time/Buffer             | 0.00485     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23954107  |
| Train/Action_magnitu... | 0.5324267   |
| Train/Action_magnitude  | 0.41332394  |
| Train/Action_max        | 0.20964251  |
| Train/Action_std        | 0.14131258  |
| Train/Entropy           | -0.57237995 |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.15818     |
| Train/Loss              | 0.15833686  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15358758  |
| Train/Ratio             | 1.0000198   |
| Train/Return            | 1.4950125   |
| Train/V                 | 1.6485962   |
| Train/Value             | 1.6485962   |
| Train/control_penalty   | 0.4176902   |
| Train/policy_loss       | 0.15358758  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 150 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 149         |
| Time/Actor_Time         | 0.0716      |
| Time/B_Format_Time      | 0.0705      |
| Time/B_Original_Form... | 0.092       |
| Time/Buffer             | 0.00368     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23414981  |
| Train/Action_magnitu... | 0.5194375   |
| Train/Action_magnitude  | 0.4018651   |
| Train/Action_max        | 0.20423272  |
| Train/Action_std        | 0.13845934  |
| Train/Entropy           | -0.5924198  |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1787639   |
| Train/Loss              | 0.088414304 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08372467  |
| Train/Ratio             | 0.9999922   |
| Train/Return            | 1.4308393   |
| Train/V                 | 1.5145622   |
| Train/Value             | 1.5145622   |
| Train/control_penalty   | 0.40972152  |
| Train/policy_loss       | 0.08372467  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 151 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 150        |
| Time/Actor_Time         | 0.0991     |
| Time/B_Format_Time      | 0.0788     |
| Time/B_Original_Form... | 0.0865     |
| Time/Buffer             | 0.00334    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2248504  |
| Train/Action_magnitu... | 0.5135027  |
| Train/Action_magnitude  | 0.39816138 |
| Train/Action_max        | 0.19805194 |
| Train/Action_std        | 0.1383368  |
| Train/Entropy           | -0.5924749 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1531668  |
| Train/Loss              | 0.19676845 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19220394 |
| Train/Ratio             | 0.9999812  |
| Train/Return            | 1.3704852  |
| Train/V                 | 1.562688   |
| Train/Value             | 1.562688   |
| Train/control_penalty   | 0.3972032  |
| Train/policy_loss       | 0.19220394 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.016      |
----------------------------------------

 ---------------- Iteration 152 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 151         |
| Time/Actor_Time         | 0.0694      |
| Time/B_Format_Time      | 0.0701      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24087113  |
| Train/Action_magnitu... | 0.5402011   |
| Train/Action_magnitude  | 0.41862085  |
| Train/Action_max        | 0.1892565   |
| Train/Action_std        | 0.14247319  |
| Train/Entropy           | -0.55979854 |
| Train/Entropy_Loss      | 0.00056     |
| Train/Entropy_loss      | 0.00056     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1295282   |
| Train/Loss              | 0.11439067  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10964971  |
| Train/Ratio             | 1.0000168   |
| Train/Return            | 1.533078    |
| Train/V                 | 1.6427182   |
| Train/Value             | 1.6427182   |
| Train/control_penalty   | 0.4181159   |
| Train/policy_loss       | 0.10964971  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 153 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 152        |
| Time/Actor_Time         | 0.0717     |
| Time/B_Format_Time      | 0.075      |
| Time/B_Original_Form... | 0.088      |
| Time/Buffer             | 0.00432    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23873077 |
| Train/Action_magnitu... | 0.53548634 |
| Train/Action_magnitude  | 0.4176779  |
| Train/Action_max        | 0.18897744 |
| Train/Action_std        | 0.14120322 |
| Train/Entropy           | -0.5669294 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1625651  |
| Train/Loss              | 0.17336538 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16860102 |
| Train/Ratio             | 0.99998695 |
| Train/Return            | 1.3663926  |
| Train/V                 | 1.534992   |
| Train/Value             | 1.534992   |
| Train/control_penalty   | 0.41974297 |
| Train/policy_loss       | 0.16860102 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 154 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 153         |
| Time/Actor_Time         | 0.0672      |
| Time/B_Format_Time      | 0.108       |
| Time/B_Original_Form... | 0.0778      |
| Time/Buffer             | 0.00772     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24164163  |
| Train/Action_magnitu... | 0.5394902   |
| Train/Action_magnitude  | 0.42148665  |
| Train/Action_max        | 0.17367251  |
| Train/Action_std        | 0.14202975  |
| Train/Entropy           | -0.56591016 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1403401   |
| Train/Loss              | 0.08135928  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.076583676 |
| Train/Ratio             | 0.9999987   |
| Train/Return            | 1.639178    |
| Train/V                 | 1.715752    |
| Train/Value             | 1.715752    |
| Train/control_penalty   | 0.4209694   |
| Train/policy_loss       | 0.076583676 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 155 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 154         |
| Time/Actor_Time         | 0.0651      |
| Time/B_Format_Time      | 0.0897      |
| Time/B_Original_Form... | 0.0904      |
| Time/Buffer             | 0.00828     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24078523  |
| Train/Action_magnitu... | 0.545706    |
| Train/Action_magnitude  | 0.424528    |
| Train/Action_max        | 0.19622217  |
| Train/Action_std        | 0.14078422  |
| Train/Entropy           | -0.5756627  |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1517161   |
| Train/Loss              | 0.09993882  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.095142275 |
| Train/Ratio             | 0.99999416  |
| Train/Return            | 1.5670781   |
| Train/V                 | 1.6622219   |
| Train/Value             | 1.6622219   |
| Train/control_penalty   | 0.42208785  |
| Train/policy_loss       | 0.095142275 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.036       |
-----------------------------------------

 ---------------- Iteration 156 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 155         |
| Time/Actor_Time         | 0.0971      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.151       |
| Time/Buffer             | 0.00385     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23896937  |
| Train/Action_magnitu... | 0.5338482   |
| Train/Action_magnitude  | 0.41632172  |
| Train/Action_max        | 0.19124201  |
| Train/Action_std        | 0.14217517  |
| Train/Entropy           | -0.56396735 |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1166389   |
| Train/Loss              | 0.11858765  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11384212  |
| Train/Ratio             | 0.99999404  |
| Train/Return            | 1.5485574   |
| Train/V                 | 1.6624031   |
| Train/Value             | 1.6624031   |
| Train/control_penalty   | 0.41815668  |
| Train/policy_loss       | 0.11384212  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03375     |
-----------------------------------------

 ---------------- Iteration 157 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 156         |
| Time/Actor_Time         | 0.0644      |
| Time/B_Format_Time      | 0.08        |
| Time/B_Original_Form... | 0.0819      |
| Time/Buffer             | 0.00431     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24823833  |
| Train/Action_magnitu... | 0.5509792   |
| Train/Action_magnitude  | 0.43019482  |
| Train/Action_max        | 0.1999376   |
| Train/Action_std        | 0.14200176  |
| Train/Entropy           | -0.5679961  |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1535122   |
| Train/Loss              | 0.05011183  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.045230076 |
| Train/Ratio             | 1.0000123   |
| Train/Return            | 1.6629968   |
| Train/V                 | 1.7082267   |
| Train/Value             | 1.7082267   |
| Train/control_penalty   | 0.43137592  |
| Train/policy_loss       | 0.045230076 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03975     |
-----------------------------------------

 ---------------- Iteration 158 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 157         |
| Time/Actor_Time         | 0.073       |
| Time/B_Format_Time      | 0.118       |
| Time/B_Original_Form... | 0.158       |
| Time/Buffer             | 0.00485     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24532528  |
| Train/Action_magnitu... | 0.54474175  |
| Train/Action_magnitude  | 0.4240272   |
| Train/Action_max        | 0.17708048  |
| Train/Action_std        | 0.13941695  |
| Train/Entropy           | -0.58439976 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1900337   |
| Train/Loss              | 0.039295364 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.034429394 |
| Train/Ratio             | 1.0000277   |
| Train/Return            | 1.8732897   |
| Train/V                 | 1.9077231   |
| Train/Value             | 1.9077231   |
| Train/control_penalty   | 0.42815703  |
| Train/policy_loss       | 0.034429394 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04775     |
-----------------------------------------

 ---------------- Iteration 159 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 158         |
| Time/Actor_Time         | 0.0677      |
| Time/B_Format_Time      | 0.0793      |
| Time/B_Original_Form... | 0.0815      |
| Time/Buffer             | 0.00815     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2498968   |
| Train/Action_magnitu... | 0.5562975   |
| Train/Action_magnitude  | 0.43694487  |
| Train/Action_max        | 0.18337858  |
| Train/Action_std        | 0.14399935  |
| Train/Entropy           | -0.5484742  |
| Train/Entropy_Loss      | 0.000548    |
| Train/Entropy_loss      | 0.000548    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0644606   |
| Train/Loss              | 0.033744667 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.028871186 |
| Train/Ratio             | 0.9999795   |
| Train/Return            | 1.9800086   |
| Train/V                 | 2.008878    |
| Train/Value             | 2.008878    |
| Train/control_penalty   | 0.4325006   |
| Train/policy_loss       | 0.028871186 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04925     |
-----------------------------------------

 ---------------- Iteration 160 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 159         |
| Time/Actor_Time         | 0.0699      |
| Time/B_Format_Time      | 0.0794      |
| Time/B_Original_Form... | 0.0791      |
| Time/Buffer             | 0.00579     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24155691  |
| Train/Action_magnitu... | 0.5402149   |
| Train/Action_magnitude  | 0.42376205  |
| Train/Action_max        | 0.17569782  |
| Train/Action_std        | 0.1405679   |
| Train/Entropy           | -0.57312864 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1565078   |
| Train/Loss              | 0.06950588  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.064658366 |
| Train/Ratio             | 0.9999973   |
| Train/Return            | 1.9916142   |
| Train/V                 | 2.0562675   |
| Train/Value             | 2.0562675   |
| Train/control_penalty   | 0.427438    |
| Train/policy_loss       | 0.064658366 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.047       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 161 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 160         |
| Time/Actor_Time         | 0.0843      |
| Time/B_Format_Time      | 0.0783      |
| Time/B_Original_Form... | 0.104       |
| Time/Buffer             | 0.00518     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24170616  |
| Train/Action_magnitu... | 0.5418331   |
| Train/Action_magnitude  | 0.42524377  |
| Train/Action_max        | 0.20265701  |
| Train/Action_std        | 0.1431289   |
| Train/Entropy           | -0.55897784 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1249586   |
| Train/Loss              | 0.05685907  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.052059706 |
| Train/Ratio             | 0.9999832   |
| Train/Return            | 1.8671945   |
| Train/V                 | 1.9192723   |
| Train/Value             | 1.9192723   |
| Train/control_penalty   | 0.42403877  |
| Train/policy_loss       | 0.052059706 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0445      |
-----------------------------------------

 ---------------- Iteration 162 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-------------------------------------------
| Itr                     | 161           |
| Time/Actor_Time         | 0.0689        |
| Time/B_Format_Time      | 0.093         |
| Time/B_Original_Form... | 0.0812        |
| Time/Buffer             | 0.00482       |
| Time/Critic_Time        | 0             |
| Train/Action_abs_mean   | 0.24032207    |
| Train/Action_magnitu... | 0.5352237     |
| Train/Action_magnitude  | 0.41852638    |
| Train/Action_max        | 0.19297226    |
| Train/Action_std        | 0.14063986    |
| Train/Entropy           | -0.57786113   |
| Train/Entropy_Loss      | 0.000578      |
| Train/Entropy_loss      | 0.000578      |
| Train/Grad_norm_actor   | 0.0           |
| Train/LogProb           | 1.1683977     |
| Train/Loss              | 0.0011351372  |
| Train/PolicyClip        | 0.0           |
| Train/Policy_loss       | -0.0036182157 |
| Train/Ratio             | 0.99998605    |
| Train/Return            | 1.853237      |
| Train/V                 | 1.849628      |
| Train/Value             | 1.849628      |
| Train/control_penalty   | 0.41754916    |
| Train/policy_loss       | -0.0036182157 |
| Train/recon_loss        | 0.0           |
| train/batch_reward      | 0.04875       |
-------------------------------------------

 ---------------- Iteration 163 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 162        |
| Time/Actor_Time         | 0.0704     |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.104      |
| Time/Buffer             | 0.00467    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24595444 |
| Train/Action_magnitu... | 0.5569276  |
| Train/Action_magnitude  | 0.43705133 |
| Train/Action_max        | 0.19994915 |
| Train/Action_std        | 0.14721368 |
| Train/Entropy           | -0.5298418 |
| Train/Entropy_Loss      | 0.00053    |
| Train/Entropy_loss      | 0.00053    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0229197  |
| Train/Loss              | 0.06172303 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.05681004 |
| Train/Ratio             | 1.0000138  |
| Train/Return            | 1.8619431  |
| Train/V                 | 1.9187522  |
| Train/Value             | 1.9187522  |
| Train/control_penalty   | 0.43831506 |
| Train/policy_loss       | 0.05681004 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.04325    |
----------------------------------------

 ---------------- Iteration 164 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 163         |
| Time/Actor_Time         | 0.0744      |
| Time/B_Format_Time      | 0.0694      |
| Time/B_Original_Form... | 0.0901      |
| Time/Buffer             | 0.00419     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24199966  |
| Train/Action_magnitu... | 0.54860455  |
| Train/Action_magnitude  | 0.4306617   |
| Train/Action_max        | 0.20892707  |
| Train/Action_std        | 0.1432474   |
| Train/Entropy           | -0.5583037  |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1222872   |
| Train/Loss              | 0.095862925 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09105386  |
| Train/Ratio             | 1.000011    |
| Train/Return            | 1.6550589   |
| Train/V                 | 1.7461135   |
| Train/Value             | 1.7461135   |
| Train/control_penalty   | 0.42507666  |
| Train/policy_loss       | 0.09105386  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 165 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 164         |
| Time/Actor_Time         | 0.074       |
| Time/B_Format_Time      | 0.0748      |
| Time/B_Original_Form... | 0.0737      |
| Time/Buffer             | 0.00709     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24015203  |
| Train/Action_magnitu... | 0.53704727  |
| Train/Action_magnitude  | 0.41896644  |
| Train/Action_max        | 0.20201163  |
| Train/Action_std        | 0.14070572  |
| Train/Entropy           | -0.57621324 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1444898   |
| Train/Loss              | 0.07853302  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.073751144 |
| Train/Ratio             | 1.0000309   |
| Train/Return            | 1.5743918   |
| Train/V                 | 1.6481324   |
| Train/Value             | 1.6481324   |
| Train/control_penalty   | 0.42056698  |
| Train/policy_loss       | 0.073751144 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 166 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 165         |
| Time/Actor_Time         | 0.0764      |
| Time/B_Format_Time      | 0.0915      |
| Time/B_Original_Form... | 0.0816      |
| Time/Buffer             | 0.00329     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23645858  |
| Train/Action_magnitu... | 0.52930665  |
| Train/Action_magnitude  | 0.41200736  |
| Train/Action_max        | 0.20546612  |
| Train/Action_std        | 0.14129129  |
| Train/Entropy           | -0.5697357  |
| Train/Entropy_Loss      | 0.00057     |
| Train/Entropy_loss      | 0.00057     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.156258    |
| Train/Loss              | 0.105295524 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10057433  |
| Train/Ratio             | 1.0000018   |
| Train/Return            | 1.5938966   |
| Train/V                 | 1.6944699   |
| Train/Value             | 1.6944699   |
| Train/control_penalty   | 0.41514575  |
| Train/policy_loss       | 0.10057433  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 167 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 166         |
| Time/Actor_Time         | 0.0678      |
| Time/B_Format_Time      | 0.0736      |
| Time/B_Original_Form... | 0.0764      |
| Time/Buffer             | 0.00412     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24371251  |
| Train/Action_magnitu... | 0.54649067  |
| Train/Action_magnitude  | 0.42500252  |
| Train/Action_max        | 0.19696942  |
| Train/Action_std        | 0.14412916  |
| Train/Entropy           | -0.55119586 |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0899061   |
| Train/Loss              | 0.14229207  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13751471  |
| Train/Ratio             | 1.0000231   |
| Train/Return            | 1.6268935   |
| Train/V                 | 1.7644188   |
| Train/Value             | 1.7644188   |
| Train/control_penalty   | 0.42261603  |
| Train/policy_loss       | 0.13751471  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 168 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 167        |
| Time/Actor_Time         | 0.0672     |
| Time/B_Format_Time      | 0.101      |
| Time/B_Original_Form... | 0.0848     |
| Time/Buffer             | 0.00466    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23627183 |
| Train/Action_magnitu... | 0.53091484 |
| Train/Action_magnitude  | 0.41424328 |
| Train/Action_max        | 0.1782145  |
| Train/Action_std        | 0.1443569  |
| Train/Entropy           | -0.5461825 |
| Train/Entropy_Loss      | 0.000546   |
| Train/Entropy_loss      | 0.000546   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0804434  |
| Train/Loss              | 0.09589599 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09113352 |
| Train/Ratio             | 1.0000036  |
| Train/Return            | 1.655907   |
| Train/V                 | 1.747052   |
| Train/Value             | 1.747052   |
| Train/control_penalty   | 0.42162946 |
| Train/policy_loss       | 0.09113352 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 169 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 168        |
| Time/Actor_Time         | 0.0631     |
| Time/B_Format_Time      | 0.0931     |
| Time/B_Original_Form... | 0.0857     |
| Time/Buffer             | 0.00359    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.2501421  |
| Train/Action_magnitu... | 0.5486854  |
| Train/Action_magnitude  | 0.42605424 |
| Train/Action_max        | 0.20772973 |
| Train/Action_std        | 0.1408327  |
| Train/Entropy           | -0.5697823 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.111878   |
| Train/Loss              | 0.18017699 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17533559 |
| Train/Ratio             | 1.0000143  |
| Train/Return            | 1.8245741  |
| Train/V                 | 1.9999095  |
| Train/Value             | 1.9999095  |
| Train/control_penalty   | 0.42716238 |
| Train/policy_loss       | 0.17533559 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 170 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 169         |
| Time/Actor_Time         | 0.0685      |
| Time/B_Format_Time      | 0.0764      |
| Time/B_Original_Form... | 0.0665      |
| Time/Buffer             | 0.0061      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24005328  |
| Train/Action_magnitu... | 0.5354046   |
| Train/Action_magnitude  | 0.4178369   |
| Train/Action_max        | 0.1933148   |
| Train/Action_std        | 0.14062119  |
| Train/Entropy           | -0.57269835 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1633202   |
| Train/Loss              | 0.14938222  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14463142  |
| Train/Ratio             | 1.0000036   |
| Train/Return            | 1.8452525   |
| Train/V                 | 1.9898821   |
| Train/Value             | 1.9898821   |
| Train/control_penalty   | 0.41781083  |
| Train/policy_loss       | 0.14463142  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 171 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 170        |
| Time/Actor_Time         | 0.0664     |
| Time/B_Format_Time      | 0.0748     |
| Time/B_Original_Form... | 0.0748     |
| Time/Buffer             | 0.00776    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2311108  |
| Train/Action_magnitu... | 0.52348566 |
| Train/Action_magnitude  | 0.40619227 |
| Train/Action_max        | 0.17124729 |
| Train/Action_std        | 0.140558   |
| Train/Entropy           | -0.5767782 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1355051  |
| Train/Loss              | 0.15964231 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15501282 |
| Train/Ratio             | 1.0000155  |
| Train/Return            | 1.7187591  |
| Train/V                 | 1.8737617  |
| Train/Value             | 1.8737617  |
| Train/control_penalty   | 0.40527147 |
| Train/policy_loss       | 0.15501282 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 172 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 171        |
| Time/Actor_Time         | 0.0859     |
| Time/B_Format_Time      | 0.0786     |
| Time/B_Original_Form... | 0.127      |
| Time/Buffer             | 0.00403    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23412226 |
| Train/Action_magnitu... | 0.52259254 |
| Train/Action_magnitude  | 0.40722844 |
| Train/Action_max        | 0.18285766 |
| Train/Action_std        | 0.14035732 |
| Train/Entropy           | -0.5763346 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1271945  |
| Train/Loss              | 0.16422619 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15954469 |
| Train/Ratio             | 0.99998814 |
| Train/Return            | 1.751834   |
| Train/V                 | 1.9113773  |
| Train/Value             | 1.9113773  |
| Train/control_penalty   | 0.4105159  |
| Train/policy_loss       | 0.15954469 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 173 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 172        |
| Time/Actor_Time         | 0.0649     |
| Time/B_Format_Time      | 0.0817     |
| Time/B_Original_Form... | 0.0853     |
| Time/Buffer             | 0.00486    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22655864 |
| Train/Action_magnitu... | 0.5052643  |
| Train/Action_magnitude  | 0.39249614 |
| Train/Action_max        | 0.1774933  |
| Train/Action_std        | 0.13648187 |
| Train/Entropy           | -0.6046477 |
| Train/Entropy_Loss      | 0.000605   |
| Train/Entropy_loss      | 0.000605   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2125301  |
| Train/Loss              | 0.17813104 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17358062 |
| Train/Ratio             | 0.9999807  |
| Train/Return            | 1.5477705  |
| Train/V                 | 1.7213534  |
| Train/Value             | 1.7213534  |
| Train/control_penalty   | 0.39457774 |
| Train/policy_loss       | 0.17358062 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01775    |
----------------------------------------

 ---------------- Iteration 174 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 173        |
| Time/Actor_Time         | 0.0803     |
| Time/B_Format_Time      | 0.0738     |
| Time/B_Original_Form... | 0.0807     |
| Time/Buffer             | 0.00396    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2393135  |
| Train/Action_magnitu... | 0.532557   |
| Train/Action_magnitude  | 0.41770074 |
| Train/Action_max        | 0.1866404  |
| Train/Action_std        | 0.1413998  |
| Train/Entropy           | -0.568038  |
| Train/Entropy_Loss      | 0.000568   |
| Train/Entropy_loss      | 0.000568   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1396666  |
| Train/Loss              | 0.14156261 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13679193 |
| Train/Ratio             | 0.99999106 |
| Train/Return            | 1.9842143  |
| Train/V                 | 2.1210058  |
| Train/Value             | 2.1210058  |
| Train/control_penalty   | 0.4202646  |
| Train/policy_loss       | 0.13679193 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0355     |
----------------------------------------

 ---------------- Iteration 175 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 174        |
| Time/Actor_Time         | 0.0748     |
| Time/B_Format_Time      | 0.0717     |
| Time/B_Original_Form... | 0.0721     |
| Time/Buffer             | 0.00313    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23368225 |
| Train/Action_magnitu... | 0.5228654  |
| Train/Action_magnitude  | 0.40984252 |
| Train/Action_max        | 0.18278544 |
| Train/Action_std        | 0.13750458 |
| Train/Entropy           | -0.5963319 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1429094  |
| Train/Loss              | 0.17798051 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17331281 |
| Train/Ratio             | 1.0000045  |
| Train/Return            | 1.6949619  |
| Train/V                 | 1.868275   |
| Train/Value             | 1.868275   |
| Train/control_penalty   | 0.4071376  |
| Train/policy_loss       | 0.17331281 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02375    |
----------------------------------------

 ---------------- Iteration 176 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 175        |
| Time/Actor_Time         | 0.0633     |
| Time/B_Format_Time      | 0.074      |
| Time/B_Original_Form... | 0.0685     |
| Time/Buffer             | 0.00362    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23700324 |
| Train/Action_magnitu... | 0.530441   |
| Train/Action_magnitude  | 0.41482994 |
| Train/Action_max        | 0.17557664 |
| Train/Action_std        | 0.13791095 |
| Train/Entropy           | -0.5933592 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1772093  |
| Train/Loss              | 0.15282997 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14811261 |
| Train/Ratio             | 1.0000087  |
| Train/Return            | 1.8453367  |
| Train/V                 | 1.9934514  |
| Train/Value             | 1.9934514  |
| Train/control_penalty   | 0.41240057 |
| Train/policy_loss       | 0.14811261 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0295     |
----------------------------------------

 ---------------- Iteration 177 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 176         |
| Time/Actor_Time         | 0.0921      |
| Time/B_Format_Time      | 0.123       |
| Time/B_Original_Form... | 0.0704      |
| Time/Buffer             | 0.00354     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22924016  |
| Train/Action_magnitu... | 0.5164616   |
| Train/Action_magnitude  | 0.40458333  |
| Train/Action_max        | 0.16873962  |
| Train/Action_std        | 0.14019708  |
| Train/Entropy           | -0.57791835 |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1522142   |
| Train/Loss              | 0.15538086  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15074958  |
| Train/Ratio             | 1.000012    |
| Train/Return            | 1.7135825   |
| Train/V                 | 1.8643384   |
| Train/Value             | 1.8643384   |
| Train/control_penalty   | 0.4053367   |
| Train/policy_loss       | 0.15074958  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 178 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 177         |
| Time/Actor_Time         | 0.0691      |
| Time/B_Format_Time      | 0.0732      |
| Time/B_Original_Form... | 0.0699      |
| Time/Buffer             | 0.00423     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22996491  |
| Train/Action_magnitu... | 0.5124742   |
| Train/Action_magnitude  | 0.40156853  |
| Train/Action_max        | 0.18448877  |
| Train/Action_std        | 0.1413509   |
| Train/Entropy           | -0.57074314 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.167237    |
| Train/Loss              | 0.1306161   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12601396  |
| Train/Ratio             | 1.0000098   |
| Train/Return            | 1.5649545   |
| Train/V                 | 1.6909668   |
| Train/Value             | 1.6909668   |
| Train/control_penalty   | 0.40313867  |
| Train/policy_loss       | 0.12601396  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 179 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 178        |
| Time/Actor_Time         | 0.0919     |
| Time/B_Format_Time      | 0.067      |
| Time/B_Original_Form... | 0.0801     |
| Time/Buffer             | 0.00381    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24438919 |
| Train/Action_magnitu... | 0.54817873 |
| Train/Action_magnitude  | 0.42824566 |
| Train/Action_max        | 0.19020747 |
| Train/Action_std        | 0.1444956  |
| Train/Entropy           | -0.5483576 |
| Train/Entropy_Loss      | 0.000548   |
| Train/Entropy_loss      | 0.000548   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0923276  |
| Train/Loss              | 0.16758943 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16273147 |
| Train/Ratio             | 0.9999836  |
| Train/Return            | 1.6395421  |
| Train/V                 | 1.802279   |
| Train/Value             | 1.802279   |
| Train/control_penalty   | 0.43095955 |
| Train/policy_loss       | 0.16273147 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 180 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 179        |
| Time/Actor_Time         | 0.0774     |
| Time/B_Format_Time      | 0.0722     |
| Time/B_Original_Form... | 0.0776     |
| Time/Buffer             | 0.00319    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24297056 |
| Train/Action_magnitu... | 0.54151845 |
| Train/Action_magnitude  | 0.42455298 |
| Train/Action_max        | 0.19042405 |
| Train/Action_std        | 0.14296061 |
| Train/Entropy           | -0.5572013 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1163839  |
| Train/Loss              | 0.13935643 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13456458 |
| Train/Ratio             | 0.99999684 |
| Train/Return            | 1.7039893  |
| Train/V                 | 1.8385481  |
| Train/Value             | 1.8385481  |
| Train/control_penalty   | 0.42346546 |
| Train/policy_loss       | 0.13456458 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0305     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 181 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 180         |
| Time/Actor_Time         | 0.0787      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0896      |
| Time/Buffer             | 0.00359     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2422469   |
| Train/Action_magnitu... | 0.5352925   |
| Train/Action_magnitude  | 0.42004243  |
| Train/Action_max        | 0.1949979   |
| Train/Action_std        | 0.14364855  |
| Train/Entropy           | -0.55505216 |
| Train/Entropy_Loss      | 0.000555    |
| Train/Entropy_loss      | 0.000555    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1390872   |
| Train/Loss              | 0.107840076 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.103043914 |
| Train/Ratio             | 1.0000044   |
| Train/Return            | 1.6916556   |
| Train/V                 | 1.7946892   |
| Train/Value             | 1.7946892   |
| Train/control_penalty   | 0.42411074  |
| Train/policy_loss       | 0.103043914 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 182 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 181         |
| Time/Actor_Time         | 0.064       |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0721      |
| Time/Buffer             | 0.00319     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23023544  |
| Train/Action_magnitu... | 0.51914614  |
| Train/Action_magnitude  | 0.40556267  |
| Train/Action_max        | 0.18951935  |
| Train/Action_std        | 0.14125702  |
| Train/Entropy           | -0.57068825 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1403794   |
| Train/Loss              | 0.1268509   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12221709  |
| Train/Ratio             | 0.9999812   |
| Train/Return            | 1.5453572   |
| Train/V                 | 1.667577    |
| Train/Value             | 1.667577    |
| Train/control_penalty   | 0.4063131   |
| Train/policy_loss       | 0.12221709  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 183 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 182         |
| Time/Actor_Time         | 0.0646      |
| Time/B_Format_Time      | 0.069       |
| Time/B_Original_Form... | 0.0976      |
| Time/Buffer             | 0.0038      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23363353  |
| Train/Action_magnitu... | 0.52940917  |
| Train/Action_magnitude  | 0.41494682  |
| Train/Action_max        | 0.20199487  |
| Train/Action_std        | 0.14505728  |
| Train/Entropy           | -0.5456534  |
| Train/Entropy_Loss      | 0.000546    |
| Train/Entropy_loss      | 0.000546    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0957278   |
| Train/Loss              | 0.1024634   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.097740345 |
| Train/Ratio             | 1.0000056   |
| Train/Return            | 1.5691644   |
| Train/V                 | 1.6669035   |
| Train/Value             | 1.6669035   |
| Train/control_penalty   | 0.41774037  |
| Train/policy_loss       | 0.097740345 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03225     |
-----------------------------------------

 ---------------- Iteration 184 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 183        |
| Time/Actor_Time         | 0.0754     |
| Time/B_Format_Time      | 0.0722     |
| Time/B_Original_Form... | 0.086      |
| Time/Buffer             | 0.00694    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2466378  |
| Train/Action_magnitu... | 0.55171806 |
| Train/Action_magnitude  | 0.43173695 |
| Train/Action_max        | 0.21109262 |
| Train/Action_std        | 0.14392947 |
| Train/Entropy           | -0.5527564 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0832925  |
| Train/Loss              | 0.10404178 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09918229 |
| Train/Ratio             | 1.0000019  |
| Train/Return            | 1.5322713  |
| Train/V                 | 1.6314491  |
| Train/Value             | 1.6314491  |
| Train/control_penalty   | 0.4306728  |
| Train/policy_loss       | 0.09918229 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0295     |
----------------------------------------

 ---------------- Iteration 185 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 184         |
| Time/Actor_Time         | 0.0679      |
| Time/B_Format_Time      | 0.079       |
| Time/B_Original_Form... | 0.109       |
| Time/Buffer             | 0.00391     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24221113  |
| Train/Action_magnitu... | 0.5423489   |
| Train/Action_magnitude  | 0.42313826  |
| Train/Action_max        | 0.21697612  |
| Train/Action_std        | 0.14564654  |
| Train/Entropy           | -0.54247874 |
| Train/Entropy_Loss      | 0.000542    |
| Train/Entropy_loss      | 0.000542    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0944014   |
| Train/Loss              | 0.09412415  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08931192  |
| Train/Ratio             | 1.0000181   |
| Train/Return            | 1.6856388   |
| Train/V                 | 1.7749507   |
| Train/Value             | 1.7749507   |
| Train/control_penalty   | 0.4269758   |
| Train/policy_loss       | 0.08931192  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 186 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 185         |
| Time/Actor_Time         | 0.0715      |
| Time/B_Format_Time      | 0.0841      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00363     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24292758  |
| Train/Action_magnitu... | 0.5376174   |
| Train/Action_magnitude  | 0.42023796  |
| Train/Action_max        | 0.20164424  |
| Train/Action_std        | 0.14536813  |
| Train/Entropy           | -0.54265386 |
| Train/Entropy_Loss      | 0.000543    |
| Train/Entropy_loss      | 0.000543    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0985522   |
| Train/Loss              | 0.15777606  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15296076  |
| Train/Ratio             | 0.99998474  |
| Train/Return            | 1.5391877   |
| Train/V                 | 1.6921601   |
| Train/Value             | 1.6921601   |
| Train/control_penalty   | 0.42726356  |
| Train/policy_loss       | 0.15296076  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 187 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 186        |
| Time/Actor_Time         | 0.0648     |
| Time/B_Format_Time      | 0.0687     |
| Time/B_Original_Form... | 0.0697     |
| Time/Buffer             | 0.00347    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23761202 |
| Train/Action_magnitu... | 0.5400416  |
| Train/Action_magnitude  | 0.42257637 |
| Train/Action_max        | 0.19017741 |
| Train/Action_std        | 0.1453925  |
| Train/Entropy           | -0.5421534 |
| Train/Entropy_Loss      | 0.000542   |
| Train/Entropy_loss      | 0.000542   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0422705  |
| Train/Loss              | 0.06968705 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.06495479 |
| Train/Ratio             | 1.0000042  |
| Train/Return            | 1.764118   |
| Train/V                 | 1.82906    |
| Train/Value             | 1.82906    |
| Train/control_penalty   | 0.41901103 |
| Train/policy_loss       | 0.06495479 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 188 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 187        |
| Time/Actor_Time         | 0.0661     |
| Time/B_Format_Time      | 0.129      |
| Time/B_Original_Form... | 0.118      |
| Time/Buffer             | 0.00376    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24169487 |
| Train/Action_magnitu... | 0.54008585 |
| Train/Action_magnitude  | 0.42177132 |
| Train/Action_max        | 0.20090489 |
| Train/Action_std        | 0.1433196  |
| Train/Entropy           | -0.5569523 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1023242  |
| Train/Loss              | 0.0941973  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.0894267  |
| Train/Ratio             | 1.000008   |
| Train/Return            | 1.7282361  |
| Train/V                 | 1.8176563  |
| Train/Value             | 1.8176563  |
| Train/control_penalty   | 0.42136538 |
| Train/policy_loss       | 0.0894267  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 189 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 188         |
| Time/Actor_Time         | 0.0909      |
| Time/B_Format_Time      | 0.0811      |
| Time/B_Original_Form... | 0.0892      |
| Time/Buffer             | 0.00348     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2415683   |
| Train/Action_magnitu... | 0.5411009   |
| Train/Action_magnitude  | 0.4215806   |
| Train/Action_max        | 0.19268125  |
| Train/Action_std        | 0.14235143  |
| Train/Entropy           | -0.5626818  |
| Train/Entropy_Loss      | 0.000563    |
| Train/Entropy_loss      | 0.000563    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1244415   |
| Train/Loss              | 0.107683994 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10294302  |
| Train/Ratio             | 1.0000172   |
| Train/Return            | 1.7573813   |
| Train/V                 | 1.860315    |
| Train/Value             | 1.860315    |
| Train/control_penalty   | 0.41782945  |
| Train/policy_loss       | 0.10294302  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 190 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 189         |
| Time/Actor_Time         | 0.0627      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0807      |
| Time/Buffer             | 0.00351     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24925889  |
| Train/Action_magnitu... | 0.5545775   |
| Train/Action_magnitude  | 0.4325343   |
| Train/Action_max        | 0.22519776  |
| Train/Action_std        | 0.14614004  |
| Train/Entropy           | -0.5396137  |
| Train/Entropy_Loss      | 0.00054     |
| Train/Entropy_loss      | 0.00054     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0700442   |
| Train/Loss              | 0.10334282  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.098493665 |
| Train/Ratio             | 0.9999966   |
| Train/Return            | 1.8591914   |
| Train/V                 | 1.957687    |
| Train/Value             | 1.957687    |
| Train/control_penalty   | 0.4309544   |
| Train/policy_loss       | 0.098493665 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 191 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 190         |
| Time/Actor_Time         | 0.0654      |
| Time/B_Format_Time      | 0.0744      |
| Time/B_Original_Form... | 0.0747      |
| Time/Buffer             | 0.00281     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2370305   |
| Train/Action_magnitu... | 0.529904    |
| Train/Action_magnitude  | 0.4143065   |
| Train/Action_max        | 0.19432634  |
| Train/Action_std        | 0.1425057   |
| Train/Entropy           | -0.56785834 |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1676557   |
| Train/Loss              | 0.10237378  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09768215  |
| Train/Ratio             | 0.9999988   |
| Train/Return            | 1.8512747   |
| Train/V                 | 1.9489689   |
| Train/Value             | 1.9489689   |
| Train/control_penalty   | 0.41237733  |
| Train/policy_loss       | 0.09768215  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03375     |
-----------------------------------------

 ---------------- Iteration 192 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 191         |
| Time/Actor_Time         | 0.0688      |
| Time/B_Format_Time      | 0.0789      |
| Time/B_Original_Form... | 0.0845      |
| Time/Buffer             | 0.00362     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24877106  |
| Train/Action_magnitu... | 0.5565599   |
| Train/Action_magnitude  | 0.43499497  |
| Train/Action_max        | 0.21265864  |
| Train/Action_std        | 0.14358689  |
| Train/Entropy           | -0.55604756 |
| Train/Entropy_Loss      | 0.000556    |
| Train/Entropy_loss      | 0.000556    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1124928   |
| Train/Loss              | 0.16212244  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15723795  |
| Train/Ratio             | 0.99997866  |
| Train/Return            | 1.9235915   |
| Train/V                 | 2.0808349   |
| Train/Value             | 2.0808349   |
| Train/control_penalty   | 0.43284497  |
| Train/policy_loss       | 0.15723795  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 193 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 192         |
| Time/Actor_Time         | 0.133       |
| Time/B_Format_Time      | 0.116       |
| Time/B_Original_Form... | 0.136       |
| Time/Buffer             | 0.00315     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24911197  |
| Train/Action_magnitu... | 0.55351865  |
| Train/Action_magnitude  | 0.43197307  |
| Train/Action_max        | 0.20828159  |
| Train/Action_std        | 0.14262937  |
| Train/Entropy           | -0.56682396 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1455208   |
| Train/Loss              | 0.11943679  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11456623  |
| Train/Ratio             | 0.9999946   |
| Train/Return            | 1.9375553   |
| Train/V                 | 2.0521202   |
| Train/Value             | 2.0521202   |
| Train/control_penalty   | 0.43037358  |
| Train/policy_loss       | 0.11456623  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0345      |
-----------------------------------------

 ---------------- Iteration 194 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 193         |
| Time/Actor_Time         | 0.0707      |
| Time/B_Format_Time      | 0.0776      |
| Time/B_Original_Form... | 0.0761      |
| Time/Buffer             | 0.00353     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24204718  |
| Train/Action_magnitu... | 0.53994817  |
| Train/Action_magnitude  | 0.42119056  |
| Train/Action_max        | 0.20339136  |
| Train/Action_std        | 0.13952506  |
| Train/Entropy           | -0.58839154 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1652114   |
| Train/Loss              | 0.119208306 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11439226  |
| Train/Ratio             | 1.000011    |
| Train/Return            | 1.8778472   |
| Train/V                 | 1.9922413   |
| Train/Value             | 1.9922413   |
| Train/control_penalty   | 0.42276508  |
| Train/policy_loss       | 0.11439226  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------

 ---------------- Iteration 195 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 194        |
| Time/Actor_Time         | 0.0655     |
| Time/B_Format_Time      | 0.0736     |
| Time/B_Original_Form... | 0.0834     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24077159 |
| Train/Action_magnitu... | 0.5353666  |
| Train/Action_magnitude  | 0.41757196 |
| Train/Action_max        | 0.21206263 |
| Train/Action_std        | 0.14107408 |
| Train/Entropy           | -0.5784915 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.142626   |
| Train/Loss              | 0.11287145 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10812733 |
| Train/Ratio             | 1.0000237  |
| Train/Return            | 1.9172636  |
| Train/V                 | 2.025381   |
| Train/Value             | 2.025381   |
| Train/control_penalty   | 0.4165624  |
| Train/policy_loss       | 0.10812733 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 196 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 195         |
| Time/Actor_Time         | 0.0791      |
| Time/B_Format_Time      | 0.0866      |
| Time/B_Original_Form... | 0.0867      |
| Time/Buffer             | 0.00366     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24417219  |
| Train/Action_magnitu... | 0.5455884   |
| Train/Action_magnitude  | 0.42663977  |
| Train/Action_max        | 0.22593342  |
| Train/Action_std        | 0.14220366  |
| Train/Entropy           | -0.5669195  |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1320535   |
| Train/Loss              | 0.10859412  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.103779875 |
| Train/Ratio             | 0.9999902   |
| Train/Return            | 1.8803722   |
| Train/V                 | 1.9841433   |
| Train/Value             | 1.9841433   |
| Train/control_penalty   | 0.42473206  |
| Train/policy_loss       | 0.103779875 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03575     |
-----------------------------------------

 ---------------- Iteration 197 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 196         |
| Time/Actor_Time         | 0.116       |
| Time/B_Format_Time      | 0.081       |
| Time/B_Original_Form... | 0.0901      |
| Time/Buffer             | 0.00508     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25037053  |
| Train/Action_magnitu... | 0.5557332   |
| Train/Action_magnitude  | 0.43142006  |
| Train/Action_max        | 0.2228559   |
| Train/Action_std        | 0.142986    |
| Train/Entropy           | -0.5639358  |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1473056   |
| Train/Loss              | 0.12701125  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.122091815 |
| Train/Ratio             | 0.9999921   |
| Train/Return            | 1.52492     |
| Train/V                 | 1.6470088   |
| Train/Value             | 1.6470088   |
| Train/control_penalty   | 0.43555063  |
| Train/policy_loss       | 0.122091815 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 198 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 197        |
| Time/Actor_Time         | 0.0925     |
| Time/B_Format_Time      | 0.0927     |
| Time/B_Original_Form... | 0.109      |
| Time/Buffer             | 0.00371    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24881515 |
| Train/Action_magnitu... | 0.5513764  |
| Train/Action_magnitude  | 0.4292983  |
| Train/Action_max        | 0.20796485 |
| Train/Action_std        | 0.14595905 |
| Train/Entropy           | -0.543552  |
| Train/Entropy_Loss      | 0.000544   |
| Train/Entropy_loss      | 0.000544   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0998733  |
| Train/Loss              | 0.13808782 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1332357  |
| Train/Ratio             | 1.0000194  |
| Train/Return            | 1.8403044  |
| Train/V                 | 1.9735471  |
| Train/Value             | 1.9735471  |
| Train/control_penalty   | 0.4308577  |
| Train/policy_loss       | 0.1332357  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 199 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 198         |
| Time/Actor_Time         | 0.102       |
| Time/B_Format_Time      | 0.0796      |
| Time/B_Original_Form... | 0.109       |
| Time/Buffer             | 0.00457     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24555838  |
| Train/Action_magnitu... | 0.549084    |
| Train/Action_magnitude  | 0.4283046   |
| Train/Action_max        | 0.23517793  |
| Train/Action_std        | 0.14481807  |
| Train/Entropy           | -0.54988253 |
| Train/Entropy_Loss      | 0.00055     |
| Train/Entropy_loss      | 0.00055     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0803157   |
| Train/Loss              | 0.16475834  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15989834  |
| Train/Ratio             | 1.0000031   |
| Train/Return            | 1.7257831   |
| Train/V                 | 1.8856744   |
| Train/Value             | 1.8856744   |
| Train/control_penalty   | 0.4310121   |
| Train/policy_loss       | 0.15989834  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 200 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 199        |
| Time/Actor_Time         | 0.0731     |
| Time/B_Format_Time      | 0.0772     |
| Time/B_Original_Form... | 0.074      |
| Time/Buffer             | 0.0219     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24717158 |
| Train/Action_magnitu... | 0.5487595  |
| Train/Action_magnitude  | 0.42803827 |
| Train/Action_max        | 0.2051826  |
| Train/Action_std        | 0.14541906 |
| Train/Entropy           | -0.5420687 |
| Train/Entropy_Loss      | 0.000542   |
| Train/Entropy_loss      | 0.000542   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0790647  |
| Train/Loss              | 0.14762245 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14280443 |
| Train/Ratio             | 1.0000087  |
| Train/Return            | 1.7496617  |
| Train/V                 | 1.8924497  |
| Train/Value             | 1.8924497  |
| Train/control_penalty   | 0.42759484 |
| Train/policy_loss       | 0.14280443 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 201 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 200        |
| Time/Actor_Time         | 0.067      |
| Time/B_Format_Time      | 0.0901     |
| Time/B_Original_Form... | 0.0794     |
| Time/Buffer             | 0.00436    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25057888 |
| Train/Action_magnitu... | 0.5559887  |
| Train/Action_magnitude  | 0.4327203  |
| Train/Action_max        | 0.22261281 |
| Train/Action_std        | 0.1442876  |
| Train/Entropy           | -0.5500844 |
| Train/Entropy_Loss      | 0.00055    |
| Train/Entropy_loss      | 0.00055    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.074117   |
| Train/Loss              | 0.1390515  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13418543 |
| Train/Ratio             | 1.0000057  |
| Train/Return            | 1.7407508  |
| Train/V                 | 1.8749348  |
| Train/Value             | 1.8749348  |
| Train/control_penalty   | 0.43159783 |
| Train/policy_loss       | 0.13418543 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 202 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 201         |
| Time/Actor_Time         | 0.0759      |
| Time/B_Format_Time      | 0.0761      |
| Time/B_Original_Form... | 0.0733      |
| Time/Buffer             | 0.00382     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23781577  |
| Train/Action_magnitu... | 0.5349587   |
| Train/Action_magnitude  | 0.417861    |
| Train/Action_max        | 0.18853773  |
| Train/Action_std        | 0.14753406  |
| Train/Entropy           | -0.52558345 |
| Train/Entropy_Loss      | 0.000526    |
| Train/Entropy_loss      | 0.000526    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0460886   |
| Train/Loss              | 0.1319892   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1272334   |
| Train/Ratio             | 0.99999547  |
| Train/Return            | 1.7831582   |
| Train/V                 | 1.9103887   |
| Train/Value             | 1.9103887   |
| Train/control_penalty   | 0.4230221   |
| Train/policy_loss       | 0.1272334   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 203 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 202         |
| Time/Actor_Time         | 0.0732      |
| Time/B_Format_Time      | 0.0862      |
| Time/B_Original_Form... | 0.0757      |
| Time/Buffer             | 0.00692     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23601067  |
| Train/Action_magnitu... | 0.53391093  |
| Train/Action_magnitude  | 0.41808116  |
| Train/Action_max        | 0.19014044  |
| Train/Action_std        | 0.14838074  |
| Train/Entropy           | -0.52183384 |
| Train/Entropy_Loss      | 0.000522    |
| Train/Entropy_loss      | 0.000522    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0426594   |
| Train/Loss              | 0.22606972  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.22136635  |
| Train/Ratio             | 1.000018    |
| Train/Return            | 1.7769371   |
| Train/V                 | 1.9983015   |
| Train/Value             | 1.9983015   |
| Train/control_penalty   | 0.41815326  |
| Train/policy_loss       | 0.22136635  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 204 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 203        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.103      |
| Time/B_Original_Form... | 0.0762     |
| Time/Buffer             | 0.00447    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2375949  |
| Train/Action_magnitu... | 0.53202105 |
| Train/Action_magnitude  | 0.41753006 |
| Train/Action_max        | 0.17140807 |
| Train/Action_std        | 0.14315139 |
| Train/Entropy           | -0.5571519 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1206223  |
| Train/Loss              | 0.21099965 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20619558 |
| Train/Ratio             | 1.0000026  |
| Train/Return            | 1.7867048  |
| Train/V                 | 1.9928819  |
| Train/Value             | 1.9928819  |
| Train/control_penalty   | 0.42469206 |
| Train/policy_loss       | 0.20619558 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 205 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 204        |
| Time/Actor_Time         | 0.0866     |
| Time/B_Format_Time      | 0.0788     |
| Time/B_Original_Form... | 0.089      |
| Time/Buffer             | 0.00301    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22821392 |
| Train/Action_magnitu... | 0.51306283 |
| Train/Action_magnitude  | 0.40008765 |
| Train/Action_max        | 0.18784685 |
| Train/Action_std        | 0.14110388 |
| Train/Entropy           | -0.5701246 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1373608  |
| Train/Loss              | 0.1048748  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10031763 |
| Train/Ratio             | 1.0000032  |
| Train/Return            | 1.615535   |
| Train/V                 | 1.7158438  |
| Train/Value             | 1.7158438  |
| Train/control_penalty   | 0.3987041  |
| Train/policy_loss       | 0.10031763 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 206 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 205         |
| Time/Actor_Time         | 0.0695      |
| Time/B_Format_Time      | 0.124       |
| Time/B_Original_Form... | 0.0751      |
| Time/Buffer             | 0.00391     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23119095  |
| Train/Action_magnitu... | 0.5239353   |
| Train/Action_magnitude  | 0.41180706  |
| Train/Action_max        | 0.17714709  |
| Train/Action_std        | 0.14308889  |
| Train/Entropy           | -0.56070775 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1017853   |
| Train/Loss              | 0.23271425  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.22805156  |
| Train/Ratio             | 0.99995923  |
| Train/Return            | 1.8563147   |
| Train/V                 | 2.0843813   |
| Train/Value             | 2.0843813   |
| Train/control_penalty   | 0.41019946  |
| Train/policy_loss       | 0.22805156  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 207 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 206         |
| Time/Actor_Time         | 0.0725      |
| Time/B_Format_Time      | 0.11        |
| Time/B_Original_Form... | 0.0674      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23193249  |
| Train/Action_magnitu... | 0.52611727  |
| Train/Action_magnitude  | 0.41281006  |
| Train/Action_max        | 0.17018163  |
| Train/Action_std        | 0.14207208  |
| Train/Entropy           | -0.56827134 |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1344963   |
| Train/Loss              | 0.26044214  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.25574854  |
| Train/Ratio             | 1.0000051   |
| Train/Return            | 1.9495701   |
| Train/V                 | 2.205313    |
| Train/Value             | 2.205313    |
| Train/control_penalty   | 0.4125312   |
| Train/policy_loss       | 0.25574854  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02525     |
-----------------------------------------

 ---------------- Iteration 208 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 207         |
| Time/Actor_Time         | 0.0653      |
| Time/B_Format_Time      | 0.068       |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00322     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23919645  |
| Train/Action_magnitu... | 0.53857875  |
| Train/Action_magnitude  | 0.4209983   |
| Train/Action_max        | 0.18593296  |
| Train/Action_std        | 0.14160171  |
| Train/Entropy           | -0.56574136 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1127988   |
| Train/Loss              | 0.25301096  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24821854  |
| Train/Ratio             | 1.0000185   |
| Train/Return            | 2.0067174   |
| Train/V                 | 2.2549262   |
| Train/Value             | 2.2549262   |
| Train/control_penalty   | 0.42266768  |
| Train/policy_loss       | 0.24821854  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 209 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 208         |
| Time/Actor_Time         | 0.0634      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00315     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.23422264  |
| Train/Action_magnitu... | 0.5315636   |
| Train/Action_magnitude  | 0.4179182   |
| Train/Action_max        | 0.17862345  |
| Train/Action_std        | 0.14392555  |
| Train/Entropy           | -0.55569446 |
| Train/Entropy_Loss      | 0.000556    |
| Train/Entropy_loss      | 0.000556    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1247927   |
| Train/Loss              | 0.15457629  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14982131  |
| Train/Ratio             | 0.99998766  |
| Train/Return            | 1.7994827   |
| Train/V                 | 1.9492977   |
| Train/Value             | 1.9492977   |
| Train/control_penalty   | 0.41992876  |
| Train/policy_loss       | 0.14982131  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 210 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 209        |
| Time/Actor_Time         | 0.0705     |
| Time/B_Format_Time      | 0.0752     |
| Time/B_Original_Form... | 0.0783     |
| Time/Buffer             | 0.00369    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2282808  |
| Train/Action_magnitu... | 0.5182918  |
| Train/Action_magnitude  | 0.4040163  |
| Train/Action_max        | 0.19825475 |
| Train/Action_std        | 0.14108588 |
| Train/Entropy           | -0.5716632 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1380796  |
| Train/Loss              | 0.13729122 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1326637  |
| Train/Ratio             | 0.9999874  |
| Train/Return            | 1.6426228  |
| Train/V                 | 1.7752945  |
| Train/Value             | 1.7752945  |
| Train/control_penalty   | 0.40558583 |
| Train/policy_loss       | 0.1326637  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02925    |
----------------------------------------

 ---------------- Iteration 211 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 210         |
| Time/Actor_Time         | 0.0789      |
| Time/B_Format_Time      | 0.0768      |
| Time/B_Original_Form... | 0.0793      |
| Time/Buffer             | 0.00718     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23107189  |
| Train/Action_magnitu... | 0.5223969   |
| Train/Action_magnitude  | 0.40747902  |
| Train/Action_max        | 0.19216031  |
| Train/Action_std        | 0.14289398  |
| Train/Entropy           | -0.56167406 |
| Train/Entropy_Loss      | 0.000562    |
| Train/Entropy_loss      | 0.000562    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.133182    |
| Train/Loss              | 0.13284865  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12823126  |
| Train/Ratio             | 1.0000046   |
| Train/Return            | 1.7090768   |
| Train/V                 | 1.837304    |
| Train/Value             | 1.837304    |
| Train/control_penalty   | 0.4055722   |
| Train/policy_loss       | 0.12823126  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03325     |
-----------------------------------------

 ---------------- Iteration 212 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 211         |
| Time/Actor_Time         | 0.0624      |
| Time/B_Format_Time      | 0.0781      |
| Time/B_Original_Form... | 0.0691      |
| Time/Buffer             | 0.00405     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23948053  |
| Train/Action_magnitu... | 0.53895485  |
| Train/Action_magnitude  | 0.41850793  |
| Train/Action_max        | 0.19999076  |
| Train/Action_std        | 0.14272733  |
| Train/Entropy           | -0.56143916 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1097438   |
| Train/Loss              | 0.16748184  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16271737  |
| Train/Ratio             | 0.9999717   |
| Train/Return            | 1.6841679   |
| Train/V                 | 1.8468875   |
| Train/Value             | 1.8468875   |
| Train/control_penalty   | 0.42030278  |
| Train/policy_loss       | 0.16271737  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 213 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 212         |
| Time/Actor_Time         | 0.0822      |
| Time/B_Format_Time      | 0.0689      |
| Time/B_Original_Form... | 0.0847      |
| Time/Buffer             | 0.0029      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24551623  |
| Train/Action_magnitu... | 0.542962    |
| Train/Action_magnitude  | 0.42013222  |
| Train/Action_max        | 0.20971341  |
| Train/Action_std        | 0.1434983   |
| Train/Entropy           | -0.55840224 |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1070045   |
| Train/Loss              | 0.10496515  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10021662  |
| Train/Ratio             | 1.0000261   |
| Train/Return            | 1.6188338   |
| Train/V                 | 1.7190466   |
| Train/Value             | 1.7190466   |
| Train/control_penalty   | 0.4190135   |
| Train/policy_loss       | 0.10021662  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 214 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 213         |
| Time/Actor_Time         | 0.0668      |
| Time/B_Format_Time      | 0.0983      |
| Time/B_Original_Form... | 0.0699      |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24757935  |
| Train/Action_magnitu... | 0.54866236  |
| Train/Action_magnitude  | 0.42566803  |
| Train/Action_max        | 0.21247698  |
| Train/Action_std        | 0.14562719  |
| Train/Entropy           | -0.54429185 |
| Train/Entropy_Loss      | 0.000544    |
| Train/Entropy_loss      | 0.000544    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1316619   |
| Train/Loss              | 0.14183807  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.137063    |
| Train/Ratio             | 1.0000145   |
| Train/Return            | 1.8297062   |
| Train/V                 | 1.9667645   |
| Train/Value             | 1.9667645   |
| Train/control_penalty   | 0.42307785  |
| Train/policy_loss       | 0.137063    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 215 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 214         |
| Time/Actor_Time         | 0.0863      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24901137  |
| Train/Action_magnitu... | 0.5549849   |
| Train/Action_magnitude  | 0.43160787  |
| Train/Action_max        | 0.20687503  |
| Train/Action_std        | 0.14703678  |
| Train/Entropy           | -0.532477   |
| Train/Entropy_Loss      | 0.000532    |
| Train/Entropy_loss      | 0.000532    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0634773   |
| Train/Loss              | 0.101266645 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09642947  |
| Train/Ratio             | 1.0000066   |
| Train/Return            | 1.839083    |
| Train/V                 | 1.935507    |
| Train/Value             | 1.935507    |
| Train/control_penalty   | 0.4304701   |
| Train/policy_loss       | 0.09642947  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0385      |
-----------------------------------------

 ---------------- Iteration 216 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 215        |
| Time/Actor_Time         | 0.0658     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.0719     |
| Time/Buffer             | 0.00344    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23688431 |
| Train/Action_magnitu... | 0.5220031  |
| Train/Action_magnitude  | 0.40733695 |
| Train/Action_max        | 0.1813476  |
| Train/Action_std        | 0.142278   |
| Train/Entropy           | -0.563855  |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1028062  |
| Train/Loss              | 0.10693783 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10223368 |
| Train/Ratio             | 0.9999977  |
| Train/Return            | 1.9330597  |
| Train/V                 | 2.0352979  |
| Train/Value             | 2.0352979  |
| Train/control_penalty   | 0.41402996 |
| Train/policy_loss       | 0.10223368 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03825    |
----------------------------------------

 ---------------- Iteration 217 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 216        |
| Time/Actor_Time         | 0.0631     |
| Time/B_Format_Time      | 0.0697     |
| Time/B_Original_Form... | 0.0724     |
| Time/Buffer             | 0.00315    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23309961 |
| Train/Action_magnitu... | 0.51790583 |
| Train/Action_magnitude  | 0.40111104 |
| Train/Action_max        | 0.17860273 |
| Train/Action_std        | 0.13833322 |
| Train/Entropy           | -0.5917636 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2004913  |
| Train/Loss              | 0.12885343 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12422046 |
| Train/Ratio             | 0.99999124 |
| Train/Return            | 1.4930214  |
| Train/V                 | 1.6172391  |
| Train/Value             | 1.6172391  |
| Train/control_penalty   | 0.40412    |
| Train/policy_loss       | 0.12422046 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 218 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 217        |
| Time/Actor_Time         | 0.074      |
| Time/B_Format_Time      | 0.0696     |
| Time/B_Original_Form... | 0.0712     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23370384 |
| Train/Action_magnitu... | 0.5183773  |
| Train/Action_magnitude  | 0.40327454 |
| Train/Action_max        | 0.19091062 |
| Train/Action_std        | 0.13923527 |
| Train/Entropy           | -0.5898629 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.198577   |
| Train/Loss              | 0.08120227 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07657143 |
| Train/Ratio             | 0.9999839  |
| Train/Return            | 1.5680139  |
| Train/V                 | 1.6445897  |
| Train/Value             | 1.6445897  |
| Train/control_penalty   | 0.4040981  |
| Train/policy_loss       | 0.07657143 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 219 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 218         |
| Time/Actor_Time         | 0.066       |
| Time/B_Format_Time      | 0.0832      |
| Time/B_Original_Form... | 0.0791      |
| Time/Buffer             | 0.0035      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23937948  |
| Train/Action_magnitu... | 0.5317859   |
| Train/Action_magnitude  | 0.41536844  |
| Train/Action_max        | 0.17839988  |
| Train/Action_std        | 0.14097586  |
| Train/Entropy           | -0.57131255 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1223364   |
| Train/Loss              | 0.14321788  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1384756   |
| Train/Ratio             | 1.0000004   |
| Train/Return            | 1.5055038   |
| Train/V                 | 1.6439762   |
| Train/Value             | 1.6439762   |
| Train/control_penalty   | 0.41709694  |
| Train/policy_loss       | 0.1384756   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 220 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 219         |
| Time/Actor_Time         | 0.0709      |
| Time/B_Format_Time      | 0.0816      |
| Time/B_Original_Form... | 0.0766      |
| Time/Buffer             | 0.0048      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24049911  |
| Train/Action_magnitu... | 0.54072267  |
| Train/Action_magnitude  | 0.4214535   |
| Train/Action_max        | 0.17786989  |
| Train/Action_std        | 0.1416319   |
| Train/Entropy           | -0.56616014 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1194553   |
| Train/Loss              | 0.08716761  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08240841  |
| Train/Ratio             | 1.0000073   |
| Train/Return            | 1.6808342   |
| Train/V                 | 1.7632424   |
| Train/Value             | 1.7632424   |
| Train/control_penalty   | 0.41930395  |
| Train/policy_loss       | 0.08240841  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 221 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 220        |
| Time/Actor_Time         | 0.0695     |
| Time/B_Format_Time      | 0.0707     |
| Time/B_Original_Form... | 0.0727     |
| Time/Buffer             | 0.00378    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24151328 |
| Train/Action_magnitu... | 0.5398079  |
| Train/Action_magnitude  | 0.42149562 |
| Train/Action_max        | 0.18360077 |
| Train/Action_std        | 0.14307576 |
| Train/Entropy           | -0.5551138 |
| Train/Entropy_Loss      | 0.000555   |
| Train/Entropy_loss      | 0.000555   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1099854  |
| Train/Loss              | 0.17143314 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16664433 |
| Train/Ratio             | 0.9999818  |
| Train/Return            | 1.5586301  |
| Train/V                 | 1.7252744  |
| Train/Value             | 1.7252744  |
| Train/control_penalty   | 0.42336908 |
| Train/policy_loss       | 0.16664433 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 222 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 221        |
| Time/Actor_Time         | 0.0643     |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.0726     |
| Time/Buffer             | 0.00458    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24714573 |
| Train/Action_magnitu... | 0.5533974  |
| Train/Action_magnitude  | 0.43331036 |
| Train/Action_max        | 0.19582757 |
| Train/Action_std        | 0.14383563 |
| Train/Entropy           | -0.5529545 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0741878  |
| Train/Loss              | 0.12582368 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12096012 |
| Train/Ratio             | 1.0000131  |
| Train/Return            | 1.5682378  |
| Train/V                 | 1.689207   |
| Train/Value             | 1.689207   |
| Train/control_penalty   | 0.4310608  |
| Train/policy_loss       | 0.12096012 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 223 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 222        |
| Time/Actor_Time         | 0.0767     |
| Time/B_Format_Time      | 0.0705     |
| Time/B_Original_Form... | 0.0822     |
| Time/Buffer             | 0.00324    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24126819 |
| Train/Action_magnitu... | 0.54320574 |
| Train/Action_magnitude  | 0.42192316 |
| Train/Action_max        | 0.22007863 |
| Train/Action_std        | 0.14598109 |
| Train/Entropy           | -0.5379805 |
| Train/Entropy_Loss      | 0.000538   |
| Train/Entropy_loss      | 0.000538   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0822138  |
| Train/Loss              | 0.15469278 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.149974   |
| Train/Ratio             | 0.9999746  |
| Train/Return            | 1.5587988  |
| Train/V                 | 1.708777   |
| Train/Value             | 1.708777   |
| Train/control_penalty   | 0.41808054 |
| Train/policy_loss       | 0.149974   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02325    |
----------------------------------------

 ---------------- Iteration 224 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 223         |
| Time/Actor_Time         | 0.0742      |
| Time/B_Format_Time      | 0.0742      |
| Time/B_Original_Form... | 0.13        |
| Time/Buffer             | 0.00446     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2509274   |
| Train/Action_magnitu... | 0.5565734   |
| Train/Action_magnitude  | 0.4374916   |
| Train/Action_max        | 0.18302189  |
| Train/Action_std        | 0.14294133  |
| Train/Entropy           | -0.55827343 |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1293764   |
| Train/Loss              | 0.12908249  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.124168746 |
| Train/Ratio             | 0.9999996   |
| Train/Return            | 1.5431237   |
| Train/V                 | 1.667294    |
| Train/Value             | 1.667294    |
| Train/control_penalty   | 0.43554628  |
| Train/policy_loss       | 0.124168746 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 225 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 224         |
| Time/Actor_Time         | 0.0645      |
| Time/B_Format_Time      | 0.0772      |
| Time/B_Original_Form... | 0.0763      |
| Time/Buffer             | 0.00435     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24756338  |
| Train/Action_magnitu... | 0.5550385   |
| Train/Action_magnitude  | 0.43439382  |
| Train/Action_max        | 0.1926822   |
| Train/Action_std        | 0.14357252  |
| Train/Entropy           | -0.55313474 |
| Train/Entropy_Loss      | 0.000553    |
| Train/Entropy_loss      | 0.000553    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0934832   |
| Train/Loss              | 0.102530316 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09771063  |
| Train/Ratio             | 0.99998134  |
| Train/Return            | 1.6409763   |
| Train/V                 | 1.7387041   |
| Train/Value             | 1.7387041   |
| Train/control_penalty   | 0.4266556   |
| Train/policy_loss       | 0.09771063  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 226 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 225         |
| Time/Actor_Time         | 0.0772      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.115       |
| Time/Buffer             | 0.00437     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2581852   |
| Train/Action_magnitu... | 0.5741166   |
| Train/Action_magnitude  | 0.45019776  |
| Train/Action_max        | 0.19570981  |
| Train/Action_std        | 0.1415643   |
| Train/Entropy           | -0.56528413 |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.101685    |
| Train/Loss              | 0.11907071  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11403219  |
| Train/Ratio             | 1.0000123   |
| Train/Return            | 1.5533863   |
| Train/V                 | 1.667418    |
| Train/Value             | 1.667418    |
| Train/control_penalty   | 0.4473237   |
| Train/policy_loss       | 0.11403219  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 227 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 226         |
| Time/Actor_Time         | 0.0929      |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.12        |
| Time/Buffer             | 0.00633     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.25844753  |
| Train/Action_magnitu... | 0.57114273  |
| Train/Action_magnitude  | 0.44798988  |
| Train/Action_max        | 0.19716465  |
| Train/Action_std        | 0.14169385  |
| Train/Entropy           | -0.5646648  |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1208987   |
| Train/Loss              | 0.06668336  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.061648857 |
| Train/Ratio             | 0.99999934  |
| Train/Return            | 1.8436186   |
| Train/V                 | 1.90526     |
| Train/Value             | 1.90526     |
| Train/control_penalty   | 0.44698375  |
| Train/policy_loss       | 0.061648857 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04225     |
-----------------------------------------

 ---------------- Iteration 228 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 227        |
| Time/Actor_Time         | 0.0928     |
| Time/B_Format_Time      | 0.0813     |
| Time/B_Original_Form... | 0.0855     |
| Time/Buffer             | 0.00335    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23389931 |
| Train/Action_magnitu... | 0.5224275  |
| Train/Action_magnitude  | 0.40878254 |
| Train/Action_max        | 0.19451672 |
| Train/Action_std        | 0.13349213 |
| Train/Entropy           | -0.6308209 |
| Train/Entropy_Loss      | 0.000631   |
| Train/Entropy_loss      | 0.000631   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2658948  |
| Train/Loss              | 0.08218243 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07747242 |
| Train/Ratio             | 1.0000057  |
| Train/Return            | 1.4647477  |
| Train/V                 | 1.5422232  |
| Train/Value             | 1.5422232  |
| Train/control_penalty   | 0.407919   |
| Train/policy_loss       | 0.07747242 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 229 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 228         |
| Time/Actor_Time         | 0.0705      |
| Time/B_Format_Time      | 0.0702      |
| Time/B_Original_Form... | 0.0838      |
| Time/Buffer             | 0.00421     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24171676  |
| Train/Action_magnitu... | 0.536484    |
| Train/Action_magnitude  | 0.41540417  |
| Train/Action_max        | 0.20377102  |
| Train/Action_std        | 0.1343886   |
| Train/Entropy           | -0.6216152  |
| Train/Entropy_Loss      | 0.000622    |
| Train/Entropy_loss      | 0.000622    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2277312   |
| Train/Loss              | 0.095852196 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09109306  |
| Train/Ratio             | 0.99999404  |
| Train/Return            | 1.5835916   |
| Train/V                 | 1.674689    |
| Train/Value             | 1.674689    |
| Train/control_penalty   | 0.41375136  |
| Train/policy_loss       | 0.09109306  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 230 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 229         |
| Time/Actor_Time         | 0.0747      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0755      |
| Time/Buffer             | 0.00333     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2424691   |
| Train/Action_magnitu... | 0.54423445  |
| Train/Action_magnitude  | 0.4212264   |
| Train/Action_max        | 0.21995792  |
| Train/Action_std        | 0.13745205  |
| Train/Entropy           | -0.59976083 |
| Train/Entropy_Loss      | 0.0006      |
| Train/Entropy_loss      | 0.0006      |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1803576   |
| Train/Loss              | 0.030968755 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.026238518 |
| Train/Ratio             | 1.0000104   |
| Train/Return            | 1.8167913   |
| Train/V                 | 1.8430274   |
| Train/Value             | 1.8430274   |
| Train/control_penalty   | 0.4130476   |
| Train/policy_loss       | 0.026238518 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0365      |
-----------------------------------------

 ---------------- Iteration 231 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 230        |
| Time/Actor_Time         | 0.0687     |
| Time/B_Format_Time      | 0.0686     |
| Time/B_Original_Form... | 0.0711     |
| Time/Buffer             | 0.00589    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2350577  |
| Train/Action_magnitu... | 0.51774555 |
| Train/Action_magnitude  | 0.39941794 |
| Train/Action_max        | 0.22255944 |
| Train/Action_std        | 0.13410549 |
| Train/Entropy           | -0.6244403 |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2635483  |
| Train/Loss              | 0.12842894 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12381621 |
| Train/Ratio             | 1.0000026  |
| Train/Return            | 1.6920711  |
| Train/V                 | 1.8158938  |
| Train/Value             | 1.8158938  |
| Train/control_penalty   | 0.39882818 |
| Train/policy_loss       | 0.12381621 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02525    |
----------------------------------------

 ---------------- Iteration 232 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 231        |
| Time/Actor_Time         | 0.0646     |
| Time/B_Format_Time      | 0.0719     |
| Time/B_Original_Form... | 0.073      |
| Time/Buffer             | 0.00452    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2327296  |
| Train/Action_magnitu... | 0.52716106 |
| Train/Action_magnitude  | 0.40875828 |
| Train/Action_max        | 0.20889506 |
| Train/Action_std        | 0.13864394 |
| Train/Entropy           | -0.5906933 |
| Train/Entropy_Loss      | 0.000591   |
| Train/Entropy_loss      | 0.000591   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1807102  |
| Train/Loss              | 0.13879882 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13418926 |
| Train/Ratio             | 1.0000066  |
| Train/Return            | 1.4896648  |
| Train/V                 | 1.6238598  |
| Train/Value             | 1.6238598  |
| Train/control_penalty   | 0.40188542 |
| Train/policy_loss       | 0.13418926 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 233 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 232         |
| Time/Actor_Time         | 0.0785      |
| Time/B_Format_Time      | 0.076       |
| Time/B_Original_Form... | 0.122       |
| Time/Buffer             | 0.0095      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24962004  |
| Train/Action_magnitu... | 0.5460119   |
| Train/Action_magnitude  | 0.42278638  |
| Train/Action_max        | 0.21508402  |
| Train/Action_std        | 0.1375027   |
| Train/Entropy           | -0.59814477 |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1971794   |
| Train/Loss              | 0.074660964 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.069832146 |
| Train/Ratio             | 0.99998957  |
| Train/Return            | 1.5082136   |
| Train/V                 | 1.5780519   |
| Train/Value             | 1.5780519   |
| Train/control_penalty   | 0.42306727  |
| Train/policy_loss       | 0.069832146 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 234 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 233        |
| Time/Actor_Time         | 0.0653     |
| Time/B_Format_Time      | 0.0696     |
| Time/B_Original_Form... | 0.0982     |
| Time/Buffer             | 0.00298    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23843396 |
| Train/Action_magnitu... | 0.52901036 |
| Train/Action_magnitude  | 0.4094314  |
| Train/Action_max        | 0.20820944 |
| Train/Action_std        | 0.1386089  |
| Train/Entropy           | -0.5934338 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1786997  |
| Train/Loss              | 0.10684753 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10213234 |
| Train/Ratio             | 0.9999913  |
| Train/Return            | 1.4548616  |
| Train/V                 | 1.5569956  |
| Train/Value             | 1.5569956  |
| Train/control_penalty   | 0.41217554 |
| Train/policy_loss       | 0.10213234 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 235 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 234         |
| Time/Actor_Time         | 0.13        |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0737      |
| Time/Buffer             | 0.00496     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23868798  |
| Train/Action_magnitu... | 0.5249114   |
| Train/Action_magnitude  | 0.4082156   |
| Train/Action_max        | 0.20353806  |
| Train/Action_std        | 0.13862851  |
| Train/Entropy           | -0.59061706 |
| Train/Entropy_Loss      | 0.000591    |
| Train/Entropy_loss      | 0.000591    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1549172   |
| Train/Loss              | 0.08439356  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.07971655  |
| Train/Ratio             | 0.99998194  |
| Train/Return            | 1.2958163   |
| Train/V                 | 1.3755356   |
| Train/Value             | 1.3755356   |
| Train/control_penalty   | 0.40863952  |
| Train/policy_loss       | 0.07971655  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 236 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 235         |
| Time/Actor_Time         | 0.0691      |
| Time/B_Format_Time      | 0.0841      |
| Time/B_Original_Form... | 0.0705      |
| Time/Buffer             | 0.00338     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24052335  |
| Train/Action_magnitu... | 0.53821033  |
| Train/Action_magnitude  | 0.41845155  |
| Train/Action_max        | 0.20467411  |
| Train/Action_std        | 0.13951851  |
| Train/Entropy           | -0.5850548  |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1546597   |
| Train/Loss              | 0.0662161   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.061491154 |
| Train/Ratio             | 0.99998844  |
| Train/Return            | 1.5983949   |
| Train/V                 | 1.6598972   |
| Train/Value             | 1.6598972   |
| Train/control_penalty   | 0.4139893   |
| Train/policy_loss       | 0.061491154 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 237 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 236        |
| Time/Actor_Time         | 0.0827     |
| Time/B_Format_Time      | 0.126      |
| Time/B_Original_Form... | 0.0738     |
| Time/Buffer             | 0.0122     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24101464 |
| Train/Action_magnitu... | 0.5308415  |
| Train/Action_magnitude  | 0.41104877 |
| Train/Action_max        | 0.2135803  |
| Train/Action_std        | 0.13868968 |
| Train/Entropy           | -0.589357  |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1978275  |
| Train/Loss              | 0.12739298 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12265552 |
| Train/Ratio             | 1.0000125  |
| Train/Return            | 1.572402   |
| Train/V                 | 1.6950637  |
| Train/Value             | 1.6950637  |
| Train/control_penalty   | 0.41481107 |
| Train/policy_loss       | 0.12265552 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 238 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 237         |
| Time/Actor_Time         | 0.124       |
| Time/B_Format_Time      | 0.0751      |
| Time/B_Original_Form... | 0.085       |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2463436   |
| Train/Action_magnitu... | 0.5388933   |
| Train/Action_magnitude  | 0.4178442   |
| Train/Action_max        | 0.21030934  |
| Train/Action_std        | 0.13955641  |
| Train/Entropy           | -0.5834892  |
| Train/Entropy_Loss      | 0.000583    |
| Train/Entropy_loss      | 0.000583    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1873006   |
| Train/Loss              | 0.111475974 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10667294  |
| Train/Ratio             | 0.99999416  |
| Train/Return            | 1.6939526   |
| Train/V                 | 1.8006237   |
| Train/Value             | 1.8006237   |
| Train/control_penalty   | 0.42195377  |
| Train/policy_loss       | 0.10667294  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 239 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 238        |
| Time/Actor_Time         | 0.071      |
| Time/B_Format_Time      | 0.109      |
| Time/B_Original_Form... | 0.0878     |
| Time/Buffer             | 0.0072     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24396282 |
| Train/Action_magnitu... | 0.5428453  |
| Train/Action_magnitude  | 0.42141056 |
| Train/Action_max        | 0.20799755 |
| Train/Action_std        | 0.13922624 |
| Train/Entropy           | -0.5841404 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1813029  |
| Train/Loss              | 0.08213213 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.0773784  |
| Train/Ratio             | 0.99999833 |
| Train/Return            | 1.6312422  |
| Train/V                 | 1.7086257  |
| Train/Value             | 1.7086257  |
| Train/control_penalty   | 0.41695887 |
| Train/policy_loss       | 0.0773784  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03275    |
----------------------------------------

 ---------------- Iteration 240 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 239         |
| Time/Actor_Time         | 0.107       |
| Time/B_Format_Time      | 0.0899      |
| Time/B_Original_Form... | 0.102       |
| Time/Buffer             | 0.00403     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24396126  |
| Train/Action_magnitu... | 0.5376432   |
| Train/Action_magnitude  | 0.4184183   |
| Train/Action_max        | 0.20443718  |
| Train/Action_std        | 0.13849361  |
| Train/Entropy           | -0.5885601  |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1817155   |
| Train/Loss              | 0.11486118  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.110092424 |
| Train/Ratio             | 1.0         |
| Train/Return            | 1.5672883   |
| Train/V                 | 1.6773771   |
| Train/Value             | 1.6773771   |
| Train/control_penalty   | 0.41802034  |
| Train/policy_loss       | 0.110092424 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 241 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 240        |
| Time/Actor_Time         | 0.0645     |
| Time/B_Format_Time      | 0.103      |
| Time/B_Original_Form... | 0.0676     |
| Time/Buffer             | 0.00468    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24054001 |
| Train/Action_magnitu... | 0.5323463  |
| Train/Action_magnitude  | 0.41159192 |
| Train/Action_max        | 0.21270254 |
| Train/Action_std        | 0.13763626 |
| Train/Entropy           | -0.5957963 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1771119  |
| Train/Loss              | 0.15383705 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14914718 |
| Train/Ratio             | 1.000026   |
| Train/Return            | 1.6422623  |
| Train/V                 | 1.7913958  |
| Train/Value             | 1.7913958  |
| Train/control_penalty   | 0.40940863 |
| Train/policy_loss       | 0.14914718 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 242 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 241         |
| Time/Actor_Time         | 0.0733      |
| Time/B_Format_Time      | 0.0868      |
| Time/B_Original_Form... | 0.0691      |
| Time/Buffer             | 0.00331     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.238833    |
| Train/Action_magnitu... | 0.53077704  |
| Train/Action_magnitude  | 0.4128314   |
| Train/Action_max        | 0.20630121  |
| Train/Action_std        | 0.14093877  |
| Train/Entropy           | -0.57284737 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1482378   |
| Train/Loss              | 0.15997523  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15524857  |
| Train/Ratio             | 0.9999978   |
| Train/Return            | 1.8084356   |
| Train/V                 | 1.963688    |
| Train/Value             | 1.963688    |
| Train/control_penalty   | 0.41538197  |
| Train/policy_loss       | 0.15524857  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 243 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 242         |
| Time/Actor_Time         | 0.0648      |
| Time/B_Format_Time      | 0.0798      |
| Time/B_Original_Form... | 0.0745      |
| Time/Buffer             | 0.00567     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24701434  |
| Train/Action_magnitu... | 0.54496104  |
| Train/Action_magnitude  | 0.42415455  |
| Train/Action_max        | 0.1888244   |
| Train/Action_std        | 0.13806687  |
| Train/Entropy           | -0.5926337  |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1820267   |
| Train/Loss              | 0.099746756 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09493116  |
| Train/Ratio             | 1.0000215   |
| Train/Return            | 1.5979738   |
| Train/V                 | 1.6929007   |
| Train/Value             | 1.6929007   |
| Train/control_penalty   | 0.4222962   |
| Train/policy_loss       | 0.09493116  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------

 ---------------- Iteration 244 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 243         |
| Time/Actor_Time         | 0.0983      |
| Time/B_Format_Time      | 0.0771      |
| Time/B_Original_Form... | 0.0858      |
| Time/Buffer             | 0.00581     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23719592  |
| Train/Action_magnitu... | 0.5272781   |
| Train/Action_magnitude  | 0.411982    |
| Train/Action_max        | 0.20813271  |
| Train/Action_std        | 0.13895316  |
| Train/Entropy           | -0.58842295 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2037089   |
| Train/Loss              | 0.110150516 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10542382  |
| Train/Ratio             | 0.99998903  |
| Train/Return            | 1.7335873   |
| Train/V                 | 1.8390231   |
| Train/Value             | 1.8390231   |
| Train/control_penalty   | 0.41382658  |
| Train/policy_loss       | 0.10542382  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 245 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 244         |
| Time/Actor_Time         | 0.0799      |
| Time/B_Format_Time      | 0.0811      |
| Time/B_Original_Form... | 0.106       |
| Time/Buffer             | 0.0033      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.239368    |
| Train/Action_magnitu... | 0.52544475  |
| Train/Action_magnitude  | 0.41223398  |
| Train/Action_max        | 0.19520655  |
| Train/Action_std        | 0.13986182  |
| Train/Entropy           | -0.58167243 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.174175    |
| Train/Loss              | 0.08771445  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08299921  |
| Train/Ratio             | 0.9999915   |
| Train/Return            | 1.59997     |
| Train/V                 | 1.682972    |
| Train/Value             | 1.682972    |
| Train/control_penalty   | 0.4133571   |
| Train/policy_loss       | 0.08299921  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03225     |
-----------------------------------------

 ---------------- Iteration 246 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 245         |
| Time/Actor_Time         | 0.146       |
| Time/B_Format_Time      | 0.0785      |
| Time/B_Original_Form... | 0.0979      |
| Time/Buffer             | 0.00272     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24059245  |
| Train/Action_magnitu... | 0.5278846   |
| Train/Action_magnitude  | 0.40985206  |
| Train/Action_max        | 0.2132153   |
| Train/Action_std        | 0.13931881  |
| Train/Entropy           | -0.58779657 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.161315    |
| Train/Loss              | 0.13582817  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13111787  |
| Train/Ratio             | 1.0000033   |
| Train/Return            | 1.5757517   |
| Train/V                 | 1.7068716   |
| Train/Value             | 1.7068716   |
| Train/control_penalty   | 0.4122517   |
| Train/policy_loss       | 0.13111787  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 247 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 246         |
| Time/Actor_Time         | 0.0686      |
| Time/B_Format_Time      | 0.0989      |
| Time/B_Original_Form... | 0.115       |
| Time/Buffer             | 0.0028      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24829902  |
| Train/Action_magnitu... | 0.54442054  |
| Train/Action_magnitude  | 0.42244914  |
| Train/Action_max        | 0.2370528   |
| Train/Action_std        | 0.14131919  |
| Train/Entropy           | -0.57067716 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1503257   |
| Train/Loss              | 0.16732912  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16248387  |
| Train/Ratio             | 0.99999905  |
| Train/Return            | 1.5226067   |
| Train/V                 | 1.6851017   |
| Train/Value             | 1.6851017   |
| Train/control_penalty   | 0.42745814  |
| Train/policy_loss       | 0.16248387  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 248 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 247         |
| Time/Actor_Time         | 0.0748      |
| Time/B_Format_Time      | 0.0858      |
| Time/B_Original_Form... | 0.111       |
| Time/Buffer             | 0.00266     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24448656  |
| Train/Action_magnitu... | 0.5413166   |
| Train/Action_magnitude  | 0.41780707  |
| Train/Action_max        | 0.23235346  |
| Train/Action_std        | 0.13918766  |
| Train/Entropy           | -0.58682907 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1961602   |
| Train/Loss              | 0.16081646  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1560324   |
| Train/Ratio             | 0.99999213  |
| Train/Return            | 1.4025105   |
| Train/V                 | 1.5585412   |
| Train/Value             | 1.5585412   |
| Train/control_penalty   | 0.41972426  |
| Train/policy_loss       | 0.1560324   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 249 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 248         |
| Time/Actor_Time         | 0.0666      |
| Time/B_Format_Time      | 0.0757      |
| Time/B_Original_Form... | 0.0733      |
| Time/Buffer             | 0.00837     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24082237  |
| Train/Action_magnitu... | 0.53038454  |
| Train/Action_magnitude  | 0.41010416  |
| Train/Action_max        | 0.22317874  |
| Train/Action_std        | 0.13882674  |
| Train/Entropy           | -0.59322524 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.176414    |
| Train/Loss              | 0.13905384  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1343431   |
| Train/Ratio             | 0.9999885   |
| Train/Return            | 1.5422283   |
| Train/V                 | 1.6765698   |
| Train/Value             | 1.6765698   |
| Train/control_penalty   | 0.41175     |
| Train/policy_loss       | 0.1343431   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 250 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 249        |
| Time/Actor_Time         | 0.083      |
| Time/B_Format_Time      | 0.072      |
| Time/B_Original_Form... | 0.122      |
| Time/Buffer             | 0.00482    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23736294 |
| Train/Action_magnitu... | 0.517911   |
| Train/Action_magnitude  | 0.40077463 |
| Train/Action_max        | 0.19783102 |
| Train/Action_std        | 0.13238317 |
| Train/Entropy           | -0.6345105 |
| Train/Entropy_Loss      | 0.000635   |
| Train/Entropy_loss      | 0.000635   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2907029  |
| Train/Loss              | 0.10521342 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10056965 |
| Train/Ratio             | 1.0000081  |
| Train/Return            | 1.3642194  |
| Train/V                 | 1.4647837  |
| Train/Value             | 1.4647837  |
| Train/control_penalty   | 0.40092573 |
| Train/policy_loss       | 0.10056965 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 251 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 250         |
| Time/Actor_Time         | 0.135       |
| Time/B_Format_Time      | 0.0901      |
| Time/B_Original_Form... | 0.194       |
| Time/Buffer             | 0.00358     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23806833  |
| Train/Action_magnitu... | 0.52160436  |
| Train/Action_magnitude  | 0.40464503  |
| Train/Action_max        | 0.1942399   |
| Train/Action_std        | 0.13324502  |
| Train/Entropy           | -0.6306728  |
| Train/Entropy_Loss      | 0.000631    |
| Train/Entropy_loss      | 0.000631    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2639892   |
| Train/Loss              | 0.10114218  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.096472345 |
| Train/Ratio             | 0.99999046  |
| Train/Return            | 1.4770285   |
| Train/V                 | 1.5734977   |
| Train/Value             | 1.5734977   |
| Train/control_penalty   | 0.40391678  |
| Train/policy_loss       | 0.096472345 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 252 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 251         |
| Time/Actor_Time         | 0.0687      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.071       |
| Time/Buffer             | 0.0033      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23516205  |
| Train/Action_magnitu... | 0.51251817  |
| Train/Action_magnitude  | 0.39615172  |
| Train/Action_max        | 0.17042144  |
| Train/Action_std        | 0.13270402  |
| Train/Entropy           | -0.62961155 |
| Train/Entropy_Loss      | 0.00063     |
| Train/Entropy_loss      | 0.00063     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2684429   |
| Train/Loss              | 0.11508808  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11047839  |
| Train/Ratio             | 1.0000032   |
| Train/Return            | 1.4931505   |
| Train/V                 | 1.6036415   |
| Train/Value             | 1.6036415   |
| Train/control_penalty   | 0.39800853  |
| Train/policy_loss       | 0.11047839  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 253 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 252        |
| Time/Actor_Time         | 0.0699     |
| Time/B_Format_Time      | 0.0719     |
| Time/B_Original_Form... | 0.0713     |
| Time/Buffer             | 0.00277    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23411289 |
| Train/Action_magnitu... | 0.5173339  |
| Train/Action_magnitude  | 0.39955437 |
| Train/Action_max        | 0.18776706 |
| Train/Action_std        | 0.13540165 |
| Train/Entropy           | -0.6099362 |
| Train/Entropy_Loss      | 0.00061    |
| Train/Entropy_loss      | 0.00061    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2091902  |
| Train/Loss              | 0.12604947 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12148031 |
| Train/Ratio             | 1.0000023  |
| Train/Return            | 1.2878208  |
| Train/V                 | 1.4093181  |
| Train/Value             | 1.4093181  |
| Train/control_penalty   | 0.39592287 |
| Train/policy_loss       | 0.12148031 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01975    |
----------------------------------------

 ---------------- Iteration 254 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 253         |
| Time/Actor_Time         | 0.115       |
| Time/B_Format_Time      | 0.128       |
| Time/B_Original_Form... | 0.0785      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23334661  |
| Train/Action_magnitu... | 0.5172503   |
| Train/Action_magnitude  | 0.40030774  |
| Train/Action_max        | 0.20943905  |
| Train/Action_std        | 0.13500784  |
| Train/Entropy           | -0.61583143 |
| Train/Entropy_Loss      | 0.000616    |
| Train/Entropy_loss      | 0.000616    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2216512   |
| Train/Loss              | 0.13545777  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13085866  |
| Train/Ratio             | 1.0000004   |
| Train/Return            | 1.5499529   |
| Train/V                 | 1.6808053   |
| Train/Value             | 1.6808053   |
| Train/control_penalty   | 0.39832684  |
| Train/policy_loss       | 0.13085866  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 255 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 254         |
| Time/Actor_Time         | 0.0647      |
| Time/B_Format_Time      | 0.0748      |
| Time/B_Original_Form... | 0.0682      |
| Time/Buffer             | 0.0033      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2360536   |
| Train/Action_magnitu... | 0.5221381   |
| Train/Action_magnitude  | 0.4053096   |
| Train/Action_max        | 0.18686418  |
| Train/Action_std        | 0.13567723  |
| Train/Entropy           | -0.61351436 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2398441   |
| Train/Loss              | 0.10305544  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09839676  |
| Train/Ratio             | 0.9999942   |
| Train/Return            | 1.2130979   |
| Train/V                 | 1.3115001   |
| Train/Value             | 1.3115001   |
| Train/control_penalty   | 0.40451553  |
| Train/policy_loss       | 0.09839676  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 256 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 255         |
| Time/Actor_Time         | 0.102       |
| Time/B_Format_Time      | 0.0927      |
| Time/B_Original_Form... | 0.115       |
| Time/Buffer             | 0.00362     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23929025  |
| Train/Action_magnitu... | 0.52668214  |
| Train/Action_magnitude  | 0.4084028   |
| Train/Action_max        | 0.21407719  |
| Train/Action_std        | 0.13335249  |
| Train/Entropy           | -0.6297281  |
| Train/Entropy_Loss      | 0.00063     |
| Train/Entropy_loss      | 0.00063     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.235137    |
| Train/Loss              | 0.120463945 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11575625  |
| Train/Ratio             | 1.0000293   |
| Train/Return            | 1.2192049   |
| Train/V                 | 1.3349601   |
| Train/Value             | 1.3349601   |
| Train/control_penalty   | 0.40779668  |
| Train/policy_loss       | 0.11575625  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 257 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 256         |
| Time/Actor_Time         | 0.0634      |
| Time/B_Format_Time      | 0.0682      |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00471     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24203028  |
| Train/Action_magnitu... | 0.52896154  |
| Train/Action_magnitude  | 0.40985584  |
| Train/Action_max        | 0.2047553   |
| Train/Action_std        | 0.13496202  |
| Train/Entropy           | -0.62107575 |
| Train/Entropy_Loss      | 0.000621    |
| Train/Entropy_loss      | 0.000621    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.224958    |
| Train/Loss              | 0.110570975 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.105861574 |
| Train/Ratio             | 0.9999797   |
| Train/Return            | 1.4776022   |
| Train/V                 | 1.5834525   |
| Train/Value             | 1.5834525   |
| Train/control_penalty   | 0.40883258  |
| Train/policy_loss       | 0.105861574 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 258 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 257         |
| Time/Actor_Time         | 0.068       |
| Time/B_Format_Time      | 0.0742      |
| Time/B_Original_Form... | 0.0694      |
| Time/Buffer             | 0.00297     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24219918  |
| Train/Action_magnitu... | 0.53195494  |
| Train/Action_magnitude  | 0.4107142   |
| Train/Action_max        | 0.20633873  |
| Train/Action_std        | 0.13348301  |
| Train/Entropy           | -0.62802225 |
| Train/Entropy_Loss      | 0.000628    |
| Train/Entropy_loss      | 0.000628    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2595569   |
| Train/Loss              | 0.09028164  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.085571244 |
| Train/Ratio             | 0.9999983   |
| Train/Return            | 1.4911135   |
| Train/V                 | 1.5766914   |
| Train/Value             | 1.5766914   |
| Train/control_penalty   | 0.4082373   |
| Train/policy_loss       | 0.085571244 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 259 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 258         |
| Time/Actor_Time         | 0.0654      |
| Time/B_Format_Time      | 0.0969      |
| Time/B_Original_Form... | 0.0678      |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2304769   |
| Train/Action_magnitu... | 0.51792437  |
| Train/Action_magnitude  | 0.40215337  |
| Train/Action_max        | 0.2043185   |
| Train/Action_std        | 0.1381942   |
| Train/Entropy           | -0.59902537 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2023077   |
| Train/Loss              | 0.16817573  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1635795   |
| Train/Ratio             | 1.0000125   |
| Train/Return            | 1.6458974   |
| Train/V                 | 1.8094727   |
| Train/Value             | 1.8094727   |
| Train/control_penalty   | 0.39972058  |
| Train/policy_loss       | 0.1635795   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 260 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 259         |
| Time/Actor_Time         | 0.0961      |
| Time/B_Format_Time      | 0.0839      |
| Time/B_Original_Form... | 0.0664      |
| Time/Buffer             | 0.00402     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23632234  |
| Train/Action_magnitu... | 0.5194099   |
| Train/Action_magnitude  | 0.40171686  |
| Train/Action_max        | 0.20092043  |
| Train/Action_std        | 0.13563363  |
| Train/Entropy           | -0.61420006 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2397461   |
| Train/Loss              | 0.12984562  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12516764  |
| Train/Ratio             | 1.0000069   |
| Train/Return            | 1.5507215   |
| Train/V                 | 1.6758976   |
| Train/Value             | 1.6758976   |
| Train/control_penalty   | 0.4063782   |
| Train/policy_loss       | 0.12516764  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 261 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 260         |
| Time/Actor_Time         | 0.0656      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0698      |
| Time/Buffer             | 0.00254     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23093951  |
| Train/Action_magnitu... | 0.5156446   |
| Train/Action_magnitude  | 0.39840785  |
| Train/Action_max        | 0.19881664  |
| Train/Action_std        | 0.13664265  |
| Train/Entropy           | -0.60814226 |
| Train/Entropy_Loss      | 0.000608    |
| Train/Entropy_loss      | 0.000608    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2160805   |
| Train/Loss              | 0.16205108  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15746307  |
| Train/Ratio             | 0.99999404  |
| Train/Return            | 1.4096369   |
| Train/V                 | 1.5671092   |
| Train/Value             | 1.5671092   |
| Train/control_penalty   | 0.39798695  |
| Train/policy_loss       | 0.15746307  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.016       |
-----------------------------------------

 ---------------- Iteration 262 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 261         |
| Time/Actor_Time         | 0.0643      |
| Time/B_Format_Time      | 0.11        |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00252     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23555396  |
| Train/Action_magnitu... | 0.5196638   |
| Train/Action_magnitude  | 0.4009992   |
| Train/Action_max        | 0.19943723  |
| Train/Action_std        | 0.13663028  |
| Train/Entropy           | -0.60350984 |
| Train/Entropy_Loss      | 0.000604    |
| Train/Entropy_loss      | 0.000604    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2160325   |
| Train/Loss              | 0.11559884  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11100256  |
| Train/Ratio             | 1.0000015   |
| Train/Return            | 1.6926287   |
| Train/V                 | 1.8036319   |
| Train/Value             | 1.8036319   |
| Train/control_penalty   | 0.39927766  |
| Train/policy_loss       | 0.11100256  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 263 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 262        |
| Time/Actor_Time         | 0.0895     |
| Time/B_Format_Time      | 0.0791     |
| Time/B_Original_Form... | 0.0707     |
| Time/Buffer             | 0.00328    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22713855 |
| Train/Action_magnitu... | 0.5007766  |
| Train/Action_magnitude  | 0.3878246  |
| Train/Action_max        | 0.1880814  |
| Train/Action_std        | 0.13407086 |
| Train/Entropy           | -0.6243733 |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2694591  |
| Train/Loss              | 0.20008734 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19556728 |
| Train/Ratio             | 1.0000211  |
| Train/Return            | 1.7919339  |
| Train/V                 | 1.9874953  |
| Train/Value             | 1.9874953  |
| Train/control_penalty   | 0.38956854 |
| Train/policy_loss       | 0.19556728 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 264 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 263         |
| Time/Actor_Time         | 0.0691      |
| Time/B_Format_Time      | 0.0685      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00483     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2368865   |
| Train/Action_magnitu... | 0.53388894  |
| Train/Action_magnitude  | 0.41143498  |
| Train/Action_max        | 0.1949346   |
| Train/Action_std        | 0.14338914  |
| Train/Entropy           | -0.55979985 |
| Train/Entropy_Loss      | 0.00056     |
| Train/Entropy_loss      | 0.00056     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1071074   |
| Train/Loss              | 0.15011768  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1454465   |
| Train/Ratio             | 0.9999735   |
| Train/Return            | 1.8063914   |
| Train/V                 | 1.951864    |
| Train/Value             | 1.951864    |
| Train/control_penalty   | 0.41113862  |
| Train/policy_loss       | 0.1454465   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 265 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 264         |
| Time/Actor_Time         | 0.0666      |
| Time/B_Format_Time      | 0.07        |
| Time/B_Original_Form... | 0.0757      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24089964  |
| Train/Action_magnitu... | 0.5328194   |
| Train/Action_magnitude  | 0.4124846   |
| Train/Action_max        | 0.19125202  |
| Train/Action_std        | 0.13966277  |
| Train/Entropy           | -0.58416283 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1709044   |
| Train/Loss              | 0.1616495   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15690787  |
| Train/Ratio             | 1.0000302   |
| Train/Return            | 1.7580495   |
| Train/V                 | 1.9149507   |
| Train/Value             | 1.9149507   |
| Train/control_penalty   | 0.4157461   |
| Train/policy_loss       | 0.15690787  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 266 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 265        |
| Time/Actor_Time         | 0.0643     |
| Time/B_Format_Time      | 0.0698     |
| Time/B_Original_Form... | 0.0699     |
| Time/Buffer             | 0.00309    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23623195 |
| Train/Action_magnitu... | 0.5272414  |
| Train/Action_magnitude  | 0.4066401  |
| Train/Action_max        | 0.2110236  |
| Train/Action_std        | 0.13798928 |
| Train/Entropy           | -0.5947331 |
| Train/Entropy_Loss      | 0.000595   |
| Train/Entropy_loss      | 0.000595   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1635932  |
| Train/Loss              | 0.14477119 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14013587 |
| Train/Ratio             | 0.99999905 |
| Train/Return            | 1.5335076  |
| Train/V                 | 1.6736398  |
| Train/Value             | 1.6736398  |
| Train/control_penalty   | 0.40405855 |
| Train/policy_loss       | 0.14013587 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 267 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 266         |
| Time/Actor_Time         | 0.0687      |
| Time/B_Format_Time      | 0.0769      |
| Time/B_Original_Form... | 0.0756      |
| Time/Buffer             | 0.00369     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.240406    |
| Train/Action_magnitu... | 0.5346054   |
| Train/Action_magnitude  | 0.41448963  |
| Train/Action_max        | 0.19908242  |
| Train/Action_std        | 0.14208046  |
| Train/Entropy           | -0.56694674 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1365823   |
| Train/Loss              | 0.11789641  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11317194  |
| Train/Ratio             | 0.99998975  |
| Train/Return            | 1.6943405   |
| Train/V                 | 1.807513    |
| Train/Value             | 1.807513    |
| Train/control_penalty   | 0.41575208  |
| Train/policy_loss       | 0.11317194  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03125     |
-----------------------------------------

 ---------------- Iteration 268 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 267        |
| Time/Actor_Time         | 0.104      |
| Time/B_Format_Time      | 0.0785     |
| Time/B_Original_Form... | 0.0844     |
| Time/Buffer             | 0.00717    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.24258736 |
| Train/Action_magnitu... | 0.53719616 |
| Train/Action_magnitude  | 0.41740063 |
| Train/Action_max        | 0.19987094 |
| Train/Action_std        | 0.14301749 |
| Train/Entropy           | -0.5615097 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1416409  |
| Train/Loss              | 0.13306443 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.128297   |
| Train/Ratio             | 1.0000249  |
| Train/Return            | 1.6499598  |
| Train/V                 | 1.7782602  |
| Train/Value             | 1.7782602  |
| Train/control_penalty   | 0.42059267 |
| Train/policy_loss       | 0.128297   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03125    |
----------------------------------------

 ---------------- Iteration 269 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 268         |
| Time/Actor_Time         | 0.07        |
| Time/B_Format_Time      | 0.0773      |
| Time/B_Original_Form... | 0.116       |
| Time/Buffer             | 0.00333     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23139802  |
| Train/Action_magnitu... | 0.51893705  |
| Train/Action_magnitude  | 0.402737    |
| Train/Action_max        | 0.20516004  |
| Train/Action_std        | 0.14064218  |
| Train/Entropy           | -0.57605135 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1644001   |
| Train/Loss              | 0.19979343  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19518125  |
| Train/Ratio             | 1.0000113   |
| Train/Return            | 1.6344094   |
| Train/V                 | 1.8295913   |
| Train/Value             | 1.8295913   |
| Train/control_penalty   | 0.40361252  |
| Train/policy_loss       | 0.19518125  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 270 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 269        |
| Time/Actor_Time         | 0.0751     |
| Time/B_Format_Time      | 0.0869     |
| Time/B_Original_Form... | 0.0805     |
| Time/Buffer             | 0.00386    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24011241 |
| Train/Action_magnitu... | 0.5367733  |
| Train/Action_magnitude  | 0.41629377 |
| Train/Action_max        | 0.20507297 |
| Train/Action_std        | 0.14118509 |
| Train/Entropy           | -0.5775201 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1214969  |
| Train/Loss              | 0.12306797 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11836154 |
| Train/Ratio             | 1.000016   |
| Train/Return            | 1.7177348  |
| Train/V                 | 1.8360966  |
| Train/Value             | 1.8360966  |
| Train/control_penalty   | 0.4128914  |
| Train/policy_loss       | 0.11836154 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03425    |
----------------------------------------

 ---------------- Iteration 271 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 270        |
| Time/Actor_Time         | 0.107      |
| Time/B_Format_Time      | 0.0806     |
| Time/B_Original_Form... | 0.105      |
| Time/Buffer             | 0.00711    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23571396 |
| Train/Action_magnitu... | 0.5347382  |
| Train/Action_magnitude  | 0.41528487 |
| Train/Action_max        | 0.20525122 |
| Train/Action_std        | 0.1448283  |
| Train/Entropy           | -0.5453808 |
| Train/Entropy_Loss      | 0.000545   |
| Train/Entropy_loss      | 0.000545   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0961345  |
| Train/Loss              | 0.15149479 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1468049  |
| Train/Ratio             | 0.9999927  |
| Train/Return            | 1.4911653  |
| Train/V                 | 1.6379772  |
| Train/Value             | 1.6379772  |
| Train/control_penalty   | 0.41445056 |
| Train/policy_loss       | 0.1468049  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 272 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 271         |
| Time/Actor_Time         | 0.0711      |
| Time/B_Format_Time      | 0.0847      |
| Time/B_Original_Form... | 0.0742      |
| Time/Buffer             | 0.00792     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23489386  |
| Train/Action_magnitu... | 0.5268495   |
| Train/Action_magnitude  | 0.4075516   |
| Train/Action_max        | 0.21447806  |
| Train/Action_std        | 0.1405228   |
| Train/Entropy           | -0.57716036 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1540391   |
| Train/Loss              | 0.21201015  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2073427   |
| Train/Ratio             | 0.9999912   |
| Train/Return            | 1.4650818   |
| Train/V                 | 1.6724275   |
| Train/Value             | 1.6724275   |
| Train/control_penalty   | 0.4090273   |
| Train/policy_loss       | 0.2073427   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.014       |
-----------------------------------------

 ---------------- Iteration 273 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 272         |
| Time/Actor_Time         | 0.0909      |
| Time/B_Format_Time      | 0.0917      |
| Time/B_Original_Form... | 0.106       |
| Time/Buffer             | 0.00329     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22394074  |
| Train/Action_magnitu... | 0.5026227   |
| Train/Action_magnitude  | 0.3881946   |
| Train/Action_max        | 0.21494575  |
| Train/Action_std        | 0.1386419   |
| Train/Entropy           | -0.59647566 |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2004805   |
| Train/Loss              | 0.26234084  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.25786936  |
| Train/Ratio             | 1.0000141   |
| Train/Return            | 1.6034331   |
| Train/V                 | 1.8612944   |
| Train/Value             | 1.8612944   |
| Train/control_penalty   | 0.38750178  |
| Train/policy_loss       | 0.25786936  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01125     |
-----------------------------------------

 ---------------- Iteration 274 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 273         |
| Time/Actor_Time         | 0.0778      |
| Time/B_Format_Time      | 0.0853      |
| Time/B_Original_Form... | 0.071       |
| Time/Buffer             | 0.00368     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23762646  |
| Train/Action_magnitu... | 0.539236    |
| Train/Action_magnitude  | 0.4196442   |
| Train/Action_max        | 0.22214752  |
| Train/Action_std        | 0.1433639   |
| Train/Entropy           | -0.56115353 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0885993   |
| Train/Loss              | 0.22274593  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21804634  |
| Train/Ratio             | 1.0000072   |
| Train/Return            | 1.7436543   |
| Train/V                 | 1.9616985   |
| Train/Value             | 1.9616985   |
| Train/control_penalty   | 0.41384348  |
| Train/policy_loss       | 0.21804634  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 275 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 274         |
| Time/Actor_Time         | 0.0805      |
| Time/B_Format_Time      | 0.0748      |
| Time/B_Original_Form... | 0.111       |
| Time/Buffer             | 0.00547     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23649952  |
| Train/Action_magnitu... | 0.5226883   |
| Train/Action_magnitude  | 0.40408164  |
| Train/Action_max        | 0.21855015  |
| Train/Action_std        | 0.13675304  |
| Train/Entropy           | -0.60718286 |
| Train/Entropy_Loss      | 0.000607    |
| Train/Entropy_loss      | 0.000607    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2529148   |
| Train/Loss              | 0.25365248  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24896726  |
| Train/Ratio             | 0.9999978   |
| Train/Return            | 1.8815799   |
| Train/V                 | 2.1305444   |
| Train/Value             | 2.1305444   |
| Train/control_penalty   | 0.40780404  |
| Train/policy_loss       | 0.24896726  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 276 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 275        |
| Time/Actor_Time         | 0.0802     |
| Time/B_Format_Time      | 0.0899     |
| Time/B_Original_Form... | 0.0754     |
| Time/Buffer             | 0.00438    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23511992 |
| Train/Action_magnitu... | 0.5250845  |
| Train/Action_magnitude  | 0.4053035  |
| Train/Action_max        | 0.21081913 |
| Train/Action_std        | 0.13738915 |
| Train/Entropy           | -0.6018867 |
| Train/Entropy_Loss      | 0.000602   |
| Train/Entropy_loss      | 0.000602   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2038256  |
| Train/Loss              | 0.17764275 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17298605 |
| Train/Ratio             | 0.9999786  |
| Train/Return            | 1.6750193  |
| Train/V                 | 1.8480016  |
| Train/Value             | 1.8480016  |
| Train/control_penalty   | 0.40548164 |
| Train/policy_loss       | 0.17298605 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 277 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 276         |
| Time/Actor_Time         | 0.0915      |
| Time/B_Format_Time      | 0.0714      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23458536  |
| Train/Action_magnitu... | 0.52295375  |
| Train/Action_magnitude  | 0.40508825  |
| Train/Action_max        | 0.21201178  |
| Train/Action_std        | 0.13854413  |
| Train/Entropy           | -0.59465075 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.205856    |
| Train/Loss              | 0.17302532  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16839284  |
| Train/Ratio             | 0.9999887   |
| Train/Return            | 1.5893666   |
| Train/V                 | 1.7577566   |
| Train/Value             | 1.7577566   |
| Train/control_penalty   | 0.4037845   |
| Train/policy_loss       | 0.16839284  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 278 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 277        |
| Time/Actor_Time         | 0.0772     |
| Time/B_Format_Time      | 0.0694     |
| Time/B_Original_Form... | 0.0797     |
| Time/Buffer             | 0.00457    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23421116 |
| Train/Action_magnitu... | 0.52681017 |
| Train/Action_magnitude  | 0.40915796 |
| Train/Action_max        | 0.21518764 |
| Train/Action_std        | 0.1423608  |
| Train/Entropy           | -0.5670989 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1278019  |
| Train/Loss              | 0.22036158 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21569496 |
| Train/Ratio             | 0.9999917  |
| Train/Return            | 1.5078958  |
| Train/V                 | 1.7235984  |
| Train/Value             | 1.7235984  |
| Train/control_penalty   | 0.40995148 |
| Train/policy_loss       | 0.21569496 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0175     |
----------------------------------------

 ---------------- Iteration 279 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 278        |
| Time/Actor_Time         | 0.123      |
| Time/B_Format_Time      | 0.106      |
| Time/B_Original_Form... | 0.0737     |
| Time/Buffer             | 0.00646    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24628344 |
| Train/Action_magnitu... | 0.55620116 |
| Train/Action_magnitude  | 0.432457   |
| Train/Action_max        | 0.21330859 |
| Train/Action_std        | 0.144046   |
| Train/Entropy           | -0.5574029 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0958405  |
| Train/Loss              | 0.19034292 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18549249 |
| Train/Ratio             | 1.0000002  |
| Train/Return            | 1.8650906  |
| Train/V                 | 2.0505846  |
| Train/Value             | 2.0505846  |
| Train/control_penalty   | 0.42930296 |
| Train/policy_loss       | 0.18549249 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 280 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 279        |
| Time/Actor_Time         | 0.0643     |
| Time/B_Format_Time      | 0.0673     |
| Time/B_Original_Form... | 0.0752     |
| Time/Buffer             | 0.00313    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24235988 |
| Train/Action_magnitu... | 0.5429557  |
| Train/Action_magnitude  | 0.4237961  |
| Train/Action_max        | 0.19530414 |
| Train/Action_std        | 0.14400658 |
| Train/Entropy           | -0.5524746 |
| Train/Entropy_Loss      | 0.000552   |
| Train/Entropy_loss      | 0.000552   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0889571  |
| Train/Loss              | 0.16160014 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15680312 |
| Train/Ratio             | 0.9999965  |
| Train/Return            | 1.9675432  |
| Train/V                 | 2.124343   |
| Train/Value             | 2.124343   |
| Train/control_penalty   | 0.42445543 |
| Train/policy_loss       | 0.15680312 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0305     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 281 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 280        |
| Time/Actor_Time         | 0.0721     |
| Time/B_Format_Time      | 0.0651     |
| Time/B_Original_Form... | 0.0667     |
| Time/Buffer             | 0.00305    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24957693 |
| Train/Action_magnitu... | 0.5671121  |
| Train/Action_magnitude  | 0.44433126 |
| Train/Action_max        | 0.20231336 |
| Train/Action_std        | 0.14746496 |
| Train/Entropy           | -0.5297072 |
| Train/Entropy_Loss      | 0.00053    |
| Train/Entropy_loss      | 0.00053    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0573732  |
| Train/Loss              | 0.16234778 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15739879 |
| Train/Ratio             | 1.0000044  |
| Train/Return            | 2.1193955  |
| Train/V                 | 2.2768009  |
| Train/Value             | 2.2768009  |
| Train/control_penalty   | 0.44192782 |
| Train/policy_loss       | 0.15739879 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03625    |
----------------------------------------

 ---------------- Iteration 282 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 281        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0727     |
| Time/B_Original_Form... | 0.0722     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23685938 |
| Train/Action_magnitu... | 0.53583705 |
| Train/Action_magnitude  | 0.41966784 |
| Train/Action_max        | 0.20022947 |
| Train/Action_std        | 0.14548676 |
| Train/Entropy           | -0.5411886 |
| Train/Entropy_Loss      | 0.000541   |
| Train/Entropy_loss      | 0.000541   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0917302  |
| Train/Loss              | 0.15333675 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14861777 |
| Train/Ratio             | 0.9999852  |
| Train/Return            | 1.7600126  |
| Train/V                 | 1.9086373  |
| Train/Value             | 1.9086373  |
| Train/control_penalty   | 0.4177786  |
| Train/policy_loss       | 0.14861777 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03125    |
----------------------------------------

 ---------------- Iteration 283 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 282         |
| Time/Actor_Time         | 0.0643      |
| Time/B_Format_Time      | 0.0681      |
| Time/B_Original_Form... | 0.0696      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.24306229  |
| Train/Action_magnitu... | 0.5521843   |
| Train/Action_magnitude  | 0.43006513  |
| Train/Action_max        | 0.21084224  |
| Train/Action_std        | 0.14658405  |
| Train/Entropy           | -0.53571355 |
| Train/Entropy_Loss      | 0.000536    |
| Train/Entropy_loss      | 0.000536    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0712146   |
| Train/Loss              | 0.115275346 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11044108  |
| Train/Ratio             | 0.9999935   |
| Train/Return            | 1.7231404   |
| Train/V                 | 1.8335875   |
| Train/Value             | 1.8335875   |
| Train/control_penalty   | 0.42985526  |
| Train/policy_loss       | 0.11044108  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 284 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 283         |
| Time/Actor_Time         | 0.103       |
| Time/B_Format_Time      | 0.113       |
| Time/B_Original_Form... | 0.0657      |
| Time/Buffer             | 0.00354     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22695518  |
| Train/Action_magnitu... | 0.5127089   |
| Train/Action_magnitude  | 0.39806622  |
| Train/Action_max        | 0.20177133  |
| Train/Action_std        | 0.13959014  |
| Train/Entropy           | -0.58777565 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.214283    |
| Train/Loss              | 0.15511414  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15056379  |
| Train/Ratio             | 0.99998623  |
| Train/Return            | 1.496537    |
| Train/V                 | 1.6470945   |
| Train/Value             | 1.6470945   |
| Train/control_penalty   | 0.39625767  |
| Train/policy_loss       | 0.15056379  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 285 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 284        |
| Time/Actor_Time         | 0.0898     |
| Time/B_Format_Time      | 0.0659     |
| Time/B_Original_Form... | 0.0902     |
| Time/Buffer             | 0.00322    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24587497 |
| Train/Action_magnitu... | 0.55035794 |
| Train/Action_magnitude  | 0.42941344 |
| Train/Action_max        | 0.20133059 |
| Train/Action_std        | 0.14611526 |
| Train/Entropy           | -0.5395904 |
| Train/Entropy_Loss      | 0.00054    |
| Train/Entropy_loss      | 0.00054    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0983787  |
| Train/Loss              | 0.10017536 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09531957 |
| Train/Ratio             | 1.0000287  |
| Train/Return            | 1.7194601  |
| Train/V                 | 1.8147823  |
| Train/Value             | 1.8147823  |
| Train/control_penalty   | 0.4316195  |
| Train/policy_loss       | 0.09531957 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.032      |
----------------------------------------

 ---------------- Iteration 286 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 285         |
| Time/Actor_Time         | 0.0724      |
| Time/B_Format_Time      | 0.0666      |
| Time/B_Original_Form... | 0.0873      |
| Time/Buffer             | 0.0032      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24126132  |
| Train/Action_magnitu... | 0.5448164   |
| Train/Action_magnitude  | 0.42627856  |
| Train/Action_max        | 0.18758889  |
| Train/Action_std        | 0.1470682   |
| Train/Entropy           | -0.53032964 |
| Train/Entropy_Loss      | 0.00053     |
| Train/Entropy_loss      | 0.00053     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0660125   |
| Train/Loss              | 0.08472635  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.07993807  |
| Train/Ratio             | 1.000004    |
| Train/Return            | 1.9279555   |
| Train/V                 | 2.0078917   |
| Train/Value             | 2.0078917   |
| Train/control_penalty   | 0.42579496  |
| Train/policy_loss       | 0.07993807  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0375      |
-----------------------------------------

 ---------------- Iteration 287 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 286         |
| Time/Actor_Time         | 0.0825      |
| Time/B_Format_Time      | 0.0639      |
| Time/B_Original_Form... | 0.0728      |
| Time/Buffer             | 0.00331     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23230302  |
| Train/Action_magnitu... | 0.52800477  |
| Train/Action_magnitude  | 0.4125878   |
| Train/Action_max        | 0.20660299  |
| Train/Action_std        | 0.14208601  |
| Train/Entropy           | -0.5640984  |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1044335   |
| Train/Loss              | 0.115724966 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.111094326 |
| Train/Ratio             | 1.0000203   |
| Train/Return            | 1.7435627   |
| Train/V                 | 1.854658    |
| Train/Value             | 1.854658    |
| Train/control_penalty   | 0.40665403  |
| Train/policy_loss       | 0.111094326 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03575     |
-----------------------------------------

 ---------------- Iteration 288 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 287         |
| Time/Actor_Time         | 0.0739      |
| Time/B_Format_Time      | 0.0635      |
| Time/B_Original_Form... | 0.0877      |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24504396  |
| Train/Action_magnitu... | 0.542161    |
| Train/Action_magnitude  | 0.4209725   |
| Train/Action_max        | 0.20590335  |
| Train/Action_std        | 0.14377469  |
| Train/Entropy           | -0.5506774  |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1272243   |
| Train/Loss              | 0.11191196  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.107136674 |
| Train/Ratio             | 0.99999493  |
| Train/Return            | 1.7430171   |
| Train/V                 | 1.8501526   |
| Train/Value             | 1.8501526   |
| Train/control_penalty   | 0.42246047  |
| Train/policy_loss       | 0.107136674 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03175     |
-----------------------------------------

 ---------------- Iteration 289 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 288        |
| Time/Actor_Time         | 0.0649     |
| Time/B_Format_Time      | 0.0706     |
| Time/B_Original_Form... | 0.0645     |
| Time/Buffer             | 0.00249    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24024116 |
| Train/Action_magnitu... | 0.5406149  |
| Train/Action_magnitude  | 0.41917846 |
| Train/Action_max        | 0.20812427 |
| Train/Action_std        | 0.14621209 |
| Train/Entropy           | -0.5386511 |
| Train/Entropy_Loss      | 0.000539   |
| Train/Entropy_loss      | 0.000539   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0937116  |
| Train/Loss              | 0.17499869 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17025253 |
| Train/Ratio             | 0.9999907  |
| Train/Return            | 1.5979388  |
| Train/V                 | 1.7681948  |
| Train/Value             | 1.7681948  |
| Train/control_penalty   | 0.42075017 |
| Train/policy_loss       | 0.17025253 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 290 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 289         |
| Time/Actor_Time         | 0.0623      |
| Time/B_Format_Time      | 0.0651      |
| Time/B_Original_Form... | 0.0659      |
| Time/Buffer             | 0.00268     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24752012  |
| Train/Action_magnitu... | 0.55751485  |
| Train/Action_magnitude  | 0.43275508  |
| Train/Action_max        | 0.223916    |
| Train/Action_std        | 0.14799479  |
| Train/Entropy           | -0.52469677 |
| Train/Entropy_Loss      | 0.000525    |
| Train/Entropy_loss      | 0.000525    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0293132   |
| Train/Loss              | 0.18770142  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1828679   |
| Train/Ratio             | 0.99996704  |
| Train/Return            | 1.526682    |
| Train/V                 | 1.7095596   |
| Train/Value             | 1.7095596   |
| Train/control_penalty   | 0.43088207  |
| Train/policy_loss       | 0.1828679   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 291 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 290        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0681     |
| Time/B_Original_Form... | 0.0669     |
| Time/Buffer             | 0.00278    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24314792 |
| Train/Action_magnitu... | 0.5428445  |
| Train/Action_magnitude  | 0.41918352 |
| Train/Action_max        | 0.20155771 |
| Train/Action_std        | 0.14562747 |
| Train/Entropy           | -0.5394671 |
| Train/Entropy_Loss      | 0.000539   |
| Train/Entropy_loss      | 0.000539   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0993958  |
| Train/Loss              | 0.21015228 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20538916 |
| Train/Ratio             | 0.99997485 |
| Train/Return            | 1.4833225  |
| Train/V                 | 1.6887157  |
| Train/Value             | 1.6887157  |
| Train/control_penalty   | 0.42236534 |
| Train/policy_loss       | 0.20538916 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01825    |
----------------------------------------

 ---------------- Iteration 292 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 291        |
| Time/Actor_Time         | 0.0661     |
| Time/B_Format_Time      | 0.0674     |
| Time/B_Original_Form... | 0.069      |
| Time/Buffer             | 0.00289    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24081022 |
| Train/Action_magnitu... | 0.5487773  |
| Train/Action_magnitude  | 0.4250467  |
| Train/Action_max        | 0.19994049 |
| Train/Action_std        | 0.14783292 |
| Train/Entropy           | -0.524539  |
| Train/Entropy_Loss      | 0.000525   |
| Train/Entropy_loss      | 0.000525   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.024517   |
| Train/Loss              | 0.14066282 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13596432 |
| Train/Ratio             | 0.9999998  |
| Train/Return            | 1.5965756  |
| Train/V                 | 1.7325444  |
| Train/Value             | 1.7325444  |
| Train/control_penalty   | 0.41739714 |
| Train/policy_loss       | 0.13596432 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02375    |
----------------------------------------

 ---------------- Iteration 293 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 292         |
| Time/Actor_Time         | 0.0632      |
| Time/B_Format_Time      | 0.0813      |
| Time/B_Original_Form... | 0.0623      |
| Time/Buffer             | 0.00297     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24872608  |
| Train/Action_magnitu... | 0.55468744  |
| Train/Action_magnitude  | 0.42981178  |
| Train/Action_max        | 0.21503446  |
| Train/Action_std        | 0.14852045  |
| Train/Entropy           | -0.52028394 |
| Train/Entropy_Loss      | 0.00052     |
| Train/Entropy_loss      | 0.00052     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0550047   |
| Train/Loss              | 0.16269828  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15782163  |
| Train/Ratio             | 1.0000131   |
| Train/Return            | 1.5524185   |
| Train/V                 | 1.7102356   |
| Train/Value             | 1.7102356   |
| Train/control_penalty   | 0.43563685  |
| Train/policy_loss       | 0.15782163  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 294 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 293        |
| Time/Actor_Time         | 0.0631     |
| Time/B_Format_Time      | 0.0685     |
| Time/B_Original_Form... | 0.0661     |
| Time/Buffer             | 0.00341    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23755263 |
| Train/Action_magnitu... | 0.52774143 |
| Train/Action_magnitude  | 0.4090379  |
| Train/Action_max        | 0.21052167 |
| Train/Action_std        | 0.1424473  |
| Train/Entropy           | -0.5662644 |
| Train/Entropy_Loss      | 0.000566   |
| Train/Entropy_loss      | 0.000566   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1269373  |
| Train/Loss              | 0.13132629 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12665722 |
| Train/Ratio             | 1.0000266  |
| Train/Return            | 1.4169279  |
| Train/V                 | 1.543578   |
| Train/Value             | 1.543578   |
| Train/control_penalty   | 0.41028142 |
| Train/policy_loss       | 0.12665722 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 295 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 294         |
| Time/Actor_Time         | 0.064       |
| Time/B_Format_Time      | 0.0757      |
| Time/B_Original_Form... | 0.0644      |
| Time/Buffer             | 0.00298     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24891794  |
| Train/Action_magnitu... | 0.5546588   |
| Train/Action_magnitude  | 0.43053412  |
| Train/Action_max        | 0.22920078  |
| Train/Action_std        | 0.14260809  |
| Train/Entropy           | -0.5651991  |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1276897   |
| Train/Loss              | 0.11775287  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.112936795 |
| Train/Ratio             | 1.0000113   |
| Train/Return            | 1.7189714   |
| Train/V                 | 1.8319029   |
| Train/Value             | 1.8319029   |
| Train/control_penalty   | 0.4250879   |
| Train/policy_loss       | 0.112936795 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 296 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 295         |
| Time/Actor_Time         | 0.0961      |
| Time/B_Format_Time      | 0.0913      |
| Time/B_Original_Form... | 0.0659      |
| Time/Buffer             | 0.00278     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24095465  |
| Train/Action_magnitu... | 0.53878194  |
| Train/Action_magnitude  | 0.41842893  |
| Train/Action_max        | 0.19527608  |
| Train/Action_std        | 0.14682598  |
| Train/Entropy           | -0.53898126 |
| Train/Entropy_Loss      | 0.000539    |
| Train/Entropy_loss      | 0.000539    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.090656    |
| Train/Loss              | 0.19303547  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18829162  |
| Train/Ratio             | 0.999998    |
| Train/Return            | 1.641802    |
| Train/V                 | 1.8300914   |
| Train/Value             | 1.8300914   |
| Train/control_penalty   | 0.42048672  |
| Train/policy_loss       | 0.18829162  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 297 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 296         |
| Time/Actor_Time         | 0.0769      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.073       |
| Time/Buffer             | 0.0029      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2512147   |
| Train/Action_magnitu... | 0.55806357  |
| Train/Action_magnitude  | 0.43152088  |
| Train/Action_max        | 0.21596803  |
| Train/Action_std        | 0.14582756  |
| Train/Entropy           | -0.54007936 |
| Train/Entropy_Loss      | 0.00054     |
| Train/Entropy_loss      | 0.00054     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0869565   |
| Train/Loss              | 0.13965237  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1348116   |
| Train/Ratio             | 1.0000134   |
| Train/Return            | 1.5918692   |
| Train/V                 | 1.7266742   |
| Train/Value             | 1.7266742   |
| Train/control_penalty   | 0.4300705   |
| Train/policy_loss       | 0.1348116   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 298 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 297        |
| Time/Actor_Time         | 0.0624     |
| Time/B_Format_Time      | 0.0664     |
| Time/B_Original_Form... | 0.0669     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24501985 |
| Train/Action_magnitu... | 0.5360071  |
| Train/Action_magnitude  | 0.41399756 |
| Train/Action_max        | 0.1988205  |
| Train/Action_std        | 0.14106064 |
| Train/Entropy           | -0.5732944 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1657659  |
| Train/Loss              | 0.15540028 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1506617  |
| Train/Ratio             | 0.9999869  |
| Train/Return            | 1.4130127  |
| Train/V                 | 1.5636721  |
| Train/Value             | 1.5636721  |
| Train/control_penalty   | 0.4165273  |
| Train/policy_loss       | 0.1506617  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 299 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 298        |
| Time/Actor_Time         | 0.0645     |
| Time/B_Format_Time      | 0.087      |
| Time/B_Original_Form... | 0.0717     |
| Time/Buffer             | 0.00305    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24135095 |
| Train/Action_magnitu... | 0.53651583 |
| Train/Action_magnitude  | 0.41866523 |
| Train/Action_max        | 0.2024591  |
| Train/Action_std        | 0.1437454  |
| Train/Entropy           | -0.55349   |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0675874  |
| Train/Loss              | 0.10550984 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10074589 |
| Train/Ratio             | 0.9999952  |
| Train/Return            | 1.653582   |
| Train/V                 | 1.7543273  |
| Train/Value             | 1.7543273  |
| Train/control_penalty   | 0.4210465  |
| Train/policy_loss       | 0.10074589 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 300 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 299        |
| Time/Actor_Time         | 0.069      |
| Time/B_Format_Time      | 0.0753     |
| Time/B_Original_Form... | 0.0645     |
| Time/Buffer             | 0.00306    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24109165 |
| Train/Action_magnitu... | 0.5394163  |
| Train/Action_magnitude  | 0.4203266  |
| Train/Action_max        | 0.19222963 |
| Train/Action_std        | 0.14545336 |
| Train/Entropy           | -0.5403091 |
| Train/Entropy_Loss      | 0.00054    |
| Train/Entropy_loss      | 0.00054    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.068522   |
| Train/Loss              | 0.18779029 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18303034 |
| Train/Ratio             | 0.9999973  |
| Train/Return            | 1.543207   |
| Train/V                 | 1.7262374  |
| Train/Value             | 1.7262374  |
| Train/control_penalty   | 0.42196342 |
| Train/policy_loss       | 0.18303034 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 301 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 300        |
| Time/Actor_Time         | 0.0813     |
| Time/B_Format_Time      | 0.0655     |
| Time/B_Original_Form... | 0.104      |
| Time/Buffer             | 0.0027     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23209043 |
| Train/Action_magnitu... | 0.52534914 |
| Train/Action_magnitude  | 0.4080722  |
| Train/Action_max        | 0.18576291 |
| Train/Action_std        | 0.14198105 |
| Train/Entropy           | -0.5637375 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1279546  |
| Train/Loss              | 0.15749735 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15287259 |
| Train/Ratio             | 1.0000192  |
| Train/Return            | 1.5626506  |
| Train/V                 | 1.7155192  |
| Train/Value             | 1.7155192  |
| Train/control_penalty   | 0.4061016  |
| Train/policy_loss       | 0.15287259 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 302 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 301        |
| Time/Actor_Time         | 0.0626     |
| Time/B_Format_Time      | 0.0666     |
| Time/B_Original_Form... | 0.0666     |
| Time/Buffer             | 0.0047     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24300177 |
| Train/Action_magnitu... | 0.5485317  |
| Train/Action_magnitude  | 0.42419958 |
| Train/Action_max        | 0.22079545 |
| Train/Action_std        | 0.14558235 |
| Train/Entropy           | -0.5389435 |
| Train/Entropy_Loss      | 0.000539   |
| Train/Entropy_loss      | 0.000539   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.087943   |
| Train/Loss              | 0.19865969 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19390716 |
| Train/Ratio             | 1.0000073  |
| Train/Return            | 1.538444   |
| Train/V                 | 1.7323363  |
| Train/Value             | 1.7323363  |
| Train/control_penalty   | 0.42135894 |
| Train/policy_loss       | 0.19390716 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01875    |
----------------------------------------

 ---------------- Iteration 303 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 302        |
| Time/Actor_Time         | 0.0646     |
| Time/B_Format_Time      | 0.0696     |
| Time/B_Original_Form... | 0.0966     |
| Time/Buffer             | 0.00276    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24360324 |
| Train/Action_magnitu... | 0.5417751  |
| Train/Action_magnitude  | 0.42152604 |
| Train/Action_max        | 0.20550439 |
| Train/Action_std        | 0.14314853 |
| Train/Entropy           | -0.551835  |
| Train/Entropy_Loss      | 0.000552   |
| Train/Entropy_loss      | 0.000552   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1064647  |
| Train/Loss              | 0.17382324 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16902217 |
| Train/Ratio             | 1.0000209  |
| Train/Return            | 1.397287   |
| Train/V                 | 1.5662962  |
| Train/Value             | 1.5662962  |
| Train/control_penalty   | 0.4249223  |
| Train/policy_loss       | 0.16902217 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.016      |
----------------------------------------

 ---------------- Iteration 304 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 303         |
| Time/Actor_Time         | 0.089       |
| Time/B_Format_Time      | 0.0654      |
| Time/B_Original_Form... | 0.0916      |
| Time/Buffer             | 0.00301     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23777983  |
| Train/Action_magnitu... | 0.53880894  |
| Train/Action_magnitude  | 0.4177244   |
| Train/Action_max        | 0.1975287   |
| Train/Action_std        | 0.14216559  |
| Train/Entropy           | -0.56413555 |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1113868   |
| Train/Loss              | 0.17481904  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17009759  |
| Train/Ratio             | 0.99998593  |
| Train/Return            | 1.4418507   |
| Train/V                 | 1.6119488   |
| Train/Value             | 1.6119488   |
| Train/control_penalty   | 0.41573137  |
| Train/policy_loss       | 0.17009759  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0175      |
-----------------------------------------

 ---------------- Iteration 305 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 304        |
| Time/Actor_Time         | 0.0779     |
| Time/B_Format_Time      | 0.0668     |
| Time/B_Original_Form... | 0.0671     |
| Time/Buffer             | 0.0033     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25277922 |
| Train/Action_magnitu... | 0.5623049  |
| Train/Action_magnitude  | 0.43858775 |
| Train/Action_max        | 0.19879043 |
| Train/Action_std        | 0.14502981 |
| Train/Entropy           | -0.5401901 |
| Train/Entropy_Loss      | 0.00054    |
| Train/Entropy_loss      | 0.00054    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0960405  |
| Train/Loss              | 0.13293971 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12805814 |
| Train/Ratio             | 1.0000085  |
| Train/Return            | 1.6168048  |
| Train/V                 | 1.7448593  |
| Train/Value             | 1.7448593  |
| Train/control_penalty   | 0.43413773 |
| Train/policy_loss       | 0.12805814 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 306 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 305        |
| Time/Actor_Time         | 0.0803     |
| Time/B_Format_Time      | 0.0659     |
| Time/B_Original_Form... | 0.0937     |
| Time/Buffer             | 0.0032     |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24277979 |
| Train/Action_magnitu... | 0.5437616  |
| Train/Action_magnitude  | 0.42186633 |
| Train/Action_max        | 0.19454284 |
| Train/Action_std        | 0.14301577 |
| Train/Entropy           | -0.5559786 |
| Train/Entropy_Loss      | 0.000556   |
| Train/Entropy_loss      | 0.000556   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1339175  |
| Train/Loss              | 0.19090517 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18616404 |
| Train/Ratio             | 0.9999996  |
| Train/Return            | 1.5737685  |
| Train/V                 | 1.7599267  |
| Train/Value             | 1.7599267  |
| Train/control_penalty   | 0.41851607 |
| Train/policy_loss       | 0.18616404 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 307 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 306        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.0699     |
| Time/B_Original_Form... | 0.0725     |
| Time/Buffer             | 0.00306    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24826738 |
| Train/Action_magnitu... | 0.54860175 |
| Train/Action_magnitude  | 0.42529768 |
| Train/Action_max        | 0.18828076 |
| Train/Action_std        | 0.14302984 |
| Train/Entropy           | -0.5539381 |
| Train/Entropy_Loss      | 0.000554   |
| Train/Entropy_loss      | 0.000554   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1029375  |
| Train/Loss              | 0.18162884 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17679697 |
| Train/Ratio             | 1.0000007  |
| Train/Return            | 1.5796891  |
| Train/V                 | 1.7564892  |
| Train/Value             | 1.7564892  |
| Train/control_penalty   | 0.42779246 |
| Train/policy_loss       | 0.17679697 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 308 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 307         |
| Time/Actor_Time         | 0.0625      |
| Time/B_Format_Time      | 0.0666      |
| Time/B_Original_Form... | 0.0686      |
| Time/Buffer             | 0.00479     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2495188   |
| Train/Action_magnitu... | 0.5485241   |
| Train/Action_magnitude  | 0.42459965  |
| Train/Action_max        | 0.18019384  |
| Train/Action_std        | 0.14092751  |
| Train/Entropy           | -0.5727551  |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1400452   |
| Train/Loss              | 0.09936115  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.094519176 |
| Train/Ratio             | 0.9999867   |
| Train/Return            | 1.7518923   |
| Train/V                 | 1.8464099   |
| Train/Value             | 1.8464099   |
| Train/control_penalty   | 0.42692226  |
| Train/policy_loss       | 0.094519176 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03225     |
-----------------------------------------

 ---------------- Iteration 309 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 308         |
| Time/Actor_Time         | 0.0728      |
| Time/B_Format_Time      | 0.0626      |
| Time/B_Original_Form... | 0.0731      |
| Time/Buffer             | 0.00257     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24870296  |
| Train/Action_magnitu... | 0.5524318   |
| Train/Action_magnitude  | 0.4279297   |
| Train/Action_max        | 0.19353576  |
| Train/Action_std        | 0.14220013  |
| Train/Entropy           | -0.56056786 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0957805   |
| Train/Loss              | 0.15297449  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14815876  |
| Train/Ratio             | 1.000007    |
| Train/Return            | 1.7228442   |
| Train/V                 | 1.8710017   |
| Train/Value             | 1.8710017   |
| Train/control_penalty   | 0.425516    |
| Train/policy_loss       | 0.14815876  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 310 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 309         |
| Time/Actor_Time         | 0.0782      |
| Time/B_Format_Time      | 0.0774      |
| Time/B_Original_Form... | 0.0667      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25054312  |
| Train/Action_magnitu... | 0.54534507  |
| Train/Action_magnitude  | 0.42203027  |
| Train/Action_max        | 0.19720724  |
| Train/Action_std        | 0.13806081  |
| Train/Entropy           | -0.5928861  |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1968431   |
| Train/Loss              | 0.111734636 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.106876925 |
| Train/Ratio             | 0.9999844   |
| Train/Return            | 1.6001906   |
| Train/V                 | 1.7070642   |
| Train/Value             | 1.7070642   |
| Train/control_penalty   | 0.42648274  |
| Train/policy_loss       | 0.106876925 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 311 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 310         |
| Time/Actor_Time         | 0.0689      |
| Time/B_Format_Time      | 0.0945      |
| Time/B_Original_Form... | 0.0639      |
| Time/Buffer             | 0.00285     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24202742  |
| Train/Action_magnitu... | 0.5363082   |
| Train/Action_magnitude  | 0.4186639   |
| Train/Action_max        | 0.17570694  |
| Train/Action_std        | 0.13853146  |
| Train/Entropy           | -0.5894929  |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1729473   |
| Train/Loss              | 0.11242317  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.107668854 |
| Train/Ratio             | 1.0000134   |
| Train/Return            | 1.6631882   |
| Train/V                 | 1.7708598   |
| Train/Value             | 1.7708598   |
| Train/control_penalty   | 0.41648236  |
| Train/policy_loss       | 0.107668854 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03325     |
-----------------------------------------

 ---------------- Iteration 312 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 311         |
| Time/Actor_Time         | 0.0632      |
| Time/B_Format_Time      | 0.0641      |
| Time/B_Original_Form... | 0.0649      |
| Time/Buffer             | 0.00326     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23585232  |
| Train/Action_magnitu... | 0.52435666  |
| Train/Action_magnitude  | 0.40833104  |
| Train/Action_max        | 0.1935027   |
| Train/Action_std        | 0.13780516  |
| Train/Entropy           | -0.5970397  |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.186791    |
| Train/Loss              | 0.112784274 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10808516  |
| Train/Ratio             | 1.0000056   |
| Train/Return            | 1.6183357   |
| Train/V                 | 1.726418    |
| Train/Value             | 1.726418    |
| Train/control_penalty   | 0.41020757  |
| Train/policy_loss       | 0.10808516  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 313 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 312        |
| Time/Actor_Time         | 0.0658     |
| Time/B_Format_Time      | 0.0955     |
| Time/B_Original_Form... | 0.0638     |
| Time/Buffer             | 0.00333    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23735197 |
| Train/Action_magnitu... | 0.52899724 |
| Train/Action_magnitude  | 0.41125506 |
| Train/Action_max        | 0.19776808 |
| Train/Action_std        | 0.13773556 |
| Train/Entropy           | -0.5962989 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1961884  |
| Train/Loss              | 0.11296375 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10828988 |
| Train/Ratio             | 1.0000108  |
| Train/Return            | 1.696836   |
| Train/V                 | 1.8051374  |
| Train/Value             | 1.8051374  |
| Train/control_penalty   | 0.40775687 |
| Train/policy_loss       | 0.10828988 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03275    |
----------------------------------------

 ---------------- Iteration 314 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 313         |
| Time/Actor_Time         | 0.096       |
| Time/B_Format_Time      | 0.0708      |
| Time/B_Original_Form... | 0.0838      |
| Time/Buffer             | 0.00344     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24119844  |
| Train/Action_magnitu... | 0.53367025  |
| Train/Action_magnitude  | 0.41530615  |
| Train/Action_max        | 0.185727    |
| Train/Action_std        | 0.13943772  |
| Train/Entropy           | -0.58835363 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1710237   |
| Train/Loss              | 0.09528414  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.090557635 |
| Train/Ratio             | 0.9999877   |
| Train/Return            | 1.6723214   |
| Train/V                 | 1.7628776   |
| Train/Value             | 1.7628776   |
| Train/control_penalty   | 0.41381547  |
| Train/policy_loss       | 0.090557635 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03375     |
-----------------------------------------

 ---------------- Iteration 315 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 314         |
| Time/Actor_Time         | 0.0659      |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.0842      |
| Time/Buffer             | 0.00302     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24149215  |
| Train/Action_magnitu... | 0.5358846   |
| Train/Action_magnitude  | 0.414613    |
| Train/Action_max        | 0.20574419  |
| Train/Action_std        | 0.13848577  |
| Train/Entropy           | -0.59470737 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1683494   |
| Train/Loss              | 0.13175695  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12705658  |
| Train/Ratio             | 1.0000052   |
| Train/Return            | 1.7524724   |
| Train/V                 | 1.8795269   |
| Train/Value             | 1.8795269   |
| Train/control_penalty   | 0.41056556  |
| Train/policy_loss       | 0.12705658  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 316 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 315         |
| Time/Actor_Time         | 0.0665      |
| Time/B_Format_Time      | 0.0706      |
| Time/B_Original_Form... | 0.0722      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.24548347  |
| Train/Action_magnitu... | 0.5512629   |
| Train/Action_magnitude  | 0.42907852  |
| Train/Action_max        | 0.21312411  |
| Train/Action_std        | 0.14341985  |
| Train/Entropy           | -0.56079775 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1059514   |
| Train/Loss              | 0.11709107  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11228583  |
| Train/Ratio             | 0.9999838   |
| Train/Return            | 1.5871891   |
| Train/V                 | 1.6994761   |
| Train/Value             | 1.6994761   |
| Train/control_penalty   | 0.42444417  |
| Train/policy_loss       | 0.11228583  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 317 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 316        |
| Time/Actor_Time         | 0.0659     |
| Time/B_Format_Time      | 0.0701     |
| Time/B_Original_Form... | 0.0754     |
| Time/Buffer             | 0.00366    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24479151 |
| Train/Action_magnitu... | 0.5399628  |
| Train/Action_magnitude  | 0.4188638  |
| Train/Action_max        | 0.2136687  |
| Train/Action_std        | 0.14149371 |
| Train/Entropy           | -0.5710426 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1697819  |
| Train/Loss              | 0.14298703 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13818057 |
| Train/Ratio             | 0.9999866  |
| Train/Return            | 1.4561158  |
| Train/V                 | 1.5943002  |
| Train/Value             | 1.5943002  |
| Train/control_penalty   | 0.42354146 |
| Train/policy_loss       | 0.13818057 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 318 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 317        |
| Time/Actor_Time         | 0.0631     |
| Time/B_Format_Time      | 0.0637     |
| Time/B_Original_Form... | 0.0648     |
| Time/Buffer             | 0.00398    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24780081 |
| Train/Action_magnitu... | 0.5538463  |
| Train/Action_magnitude  | 0.42990524 |
| Train/Action_max        | 0.19293492 |
| Train/Action_std        | 0.14159176 |
| Train/Entropy           | -0.5709578 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1327498  |
| Train/Loss              | 0.09671257 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.091869   |
| Train/Ratio             | 1.0000159  |
| Train/Return            | 1.5964994  |
| Train/V                 | 1.6883739  |
| Train/Value             | 1.6883739  |
| Train/control_penalty   | 0.42726082 |
| Train/policy_loss       | 0.091869   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 319 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 318         |
| Time/Actor_Time         | 0.065       |
| Time/B_Format_Time      | 0.1         |
| Time/B_Original_Form... | 0.0651      |
| Time/Buffer             | 0.00331     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24071005  |
| Train/Action_magnitu... | 0.5383991   |
| Train/Action_magnitude  | 0.42034525  |
| Train/Action_max        | 0.20093615  |
| Train/Action_std        | 0.14196585  |
| Train/Entropy           | -0.57026786 |
| Train/Entropy_Loss      | 0.00057     |
| Train/Entropy_loss      | 0.00057     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1434668   |
| Train/Loss              | 0.16245246  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15768494  |
| Train/Ratio             | 0.999993    |
| Train/Return            | 1.3582355   |
| Train/V                 | 1.5159227   |
| Train/Value             | 1.5159227   |
| Train/control_penalty   | 0.41972473  |
| Train/policy_loss       | 0.15768494  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02        |
-----------------------------------------

 ---------------- Iteration 320 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 319         |
| Time/Actor_Time         | 0.0618      |
| Time/B_Format_Time      | 0.0673      |
| Time/B_Original_Form... | 0.0678      |
| Time/Buffer             | 0.00344     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25359955  |
| Train/Action_magnitu... | 0.5655786   |
| Train/Action_magnitude  | 0.44086826  |
| Train/Action_max        | 0.20193893  |
| Train/Action_std        | 0.14544623  |
| Train/Entropy           | -0.5440862  |
| Train/Entropy_Loss      | 0.000544    |
| Train/Entropy_loss      | 0.000544    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0624273   |
| Train/Loss              | 0.10167188  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.096668154 |
| Train/Ratio             | 0.9999938   |
| Train/Return            | 1.427246    |
| Train/V                 | 1.5239198   |
| Train/Value             | 1.5239198   |
| Train/control_penalty   | 0.44596398  |
| Train/policy_loss       | 0.096668154 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 321 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 320        |
| Time/Actor_Time         | 0.0656     |
| Time/B_Format_Time      | 0.0769     |
| Time/B_Original_Form... | 0.0649     |
| Time/Buffer             | 0.00367    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23968999 |
| Train/Action_magnitu... | 0.53126836 |
| Train/Action_magnitude  | 0.4132376  |
| Train/Action_max        | 0.19788091 |
| Train/Action_std        | 0.13954    |
| Train/Entropy           | -0.5868826 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.174721   |
| Train/Loss              | 0.08655517 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08183343 |
| Train/Ratio             | 0.99999976 |
| Train/Return            | 1.2808646  |
| Train/V                 | 1.3627003  |
| Train/Value             | 1.3627003  |
| Train/control_penalty   | 0.41348565 |
| Train/policy_loss       | 0.08183343 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 322 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 321         |
| Time/Actor_Time         | 0.0642      |
| Time/B_Format_Time      | 0.0848      |
| Time/B_Original_Form... | 0.064       |
| Time/Buffer             | 0.00378     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23399244  |
| Train/Action_magnitu... | 0.5230249   |
| Train/Action_magnitude  | 0.40970936  |
| Train/Action_max        | 0.18529992  |
| Train/Action_std        | 0.13978037  |
| Train/Entropy           | -0.5846398  |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1879121   |
| Train/Loss              | 0.03864336  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.033961575 |
| Train/Ratio             | 1.0000032   |
| Train/Return            | 1.3796666   |
| Train/V                 | 1.4136174   |
| Train/Value             | 1.4136174   |
| Train/control_penalty   | 0.40971434  |
| Train/policy_loss       | 0.033961575 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 323 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 322        |
| Time/Actor_Time         | 0.0642     |
| Time/B_Format_Time      | 0.0799     |
| Time/B_Original_Form... | 0.0673     |
| Time/Buffer             | 0.00314    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23089132 |
| Train/Action_magnitu... | 0.5159032  |
| Train/Action_magnitude  | 0.40281242 |
| Train/Action_max        | 0.19570743 |
| Train/Action_std        | 0.1381508  |
| Train/Entropy           | -0.5949624 |
| Train/Entropy_Loss      | 0.000595   |
| Train/Entropy_loss      | 0.000595   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1928697  |
| Train/Loss              | 0.0711719  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.06654054 |
| Train/Ratio             | 0.9999941  |
| Train/Return            | 1.431576   |
| Train/V                 | 1.4981238  |
| Train/Value             | 1.4981238  |
| Train/control_penalty   | 0.40363964 |
| Train/policy_loss       | 0.06654054 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03175    |
----------------------------------------

 ---------------- Iteration 324 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 323        |
| Time/Actor_Time         | 0.065      |
| Time/B_Format_Time      | 0.0912     |
| Time/B_Original_Form... | 0.0634     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.2341059  |
| Train/Action_magnitu... | 0.5199493  |
| Train/Action_magnitude  | 0.40648606 |
| Train/Action_max        | 0.19813235 |
| Train/Action_std        | 0.13904409 |
| Train/Entropy           | -0.5874097 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1774818  |
| Train/Loss              | 0.04581809 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.04117247 |
| Train/Ratio             | 0.99997246 |
| Train/Return            | 1.4897642  |
| Train/V                 | 1.53094    |
| Train/Value             | 1.53094    |
| Train/control_penalty   | 0.40582073 |
| Train/policy_loss       | 0.04117247 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0305     |
----------------------------------------

 ---------------- Iteration 325 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 324         |
| Time/Actor_Time         | 0.0623      |
| Time/B_Format_Time      | 0.0654      |
| Time/B_Original_Form... | 0.0684      |
| Time/Buffer             | 0.0029      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22917414  |
| Train/Action_magnitu... | 0.5173491   |
| Train/Action_magnitude  | 0.40439132  |
| Train/Action_max        | 0.19921756  |
| Train/Action_std        | 0.13774496  |
| Train/Entropy           | -0.59428287 |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1893228   |
| Train/Loss              | 0.056948505 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.052345555 |
| Train/Ratio             | 0.9999984   |
| Train/Return            | 1.4923925   |
| Train/V                 | 1.5447398   |
| Train/Value             | 1.5447398   |
| Train/control_penalty   | 0.40086657  |
| Train/policy_loss       | 0.052345555 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 326 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 325        |
| Time/Actor_Time         | 0.0856     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.0891     |
| Time/Buffer             | 0.00275    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23206869 |
| Train/Action_magnitu... | 0.5136386  |
| Train/Action_magnitude  | 0.3991457  |
| Train/Action_max        | 0.20162609 |
| Train/Action_std        | 0.13631628 |
| Train/Entropy           | -0.6068053 |
| Train/Entropy_Loss      | 0.000607   |
| Train/Entropy_loss      | 0.000607   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1786244  |
| Train/Loss              | 0.13591254 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1313082  |
| Train/Ratio             | 1.0000107  |
| Train/Return            | 1.4928107  |
| Train/V                 | 1.6241194  |
| Train/Value             | 1.6241194  |
| Train/control_penalty   | 0.39975274 |
| Train/policy_loss       | 0.1313082  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 327 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 326        |
| Time/Actor_Time         | 0.0715     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.0798     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23410784 |
| Train/Action_magnitu... | 0.522578   |
| Train/Action_magnitude  | 0.40619376 |
| Train/Action_max        | 0.22094288 |
| Train/Action_std        | 0.1374205  |
| Train/Entropy           | -0.6033082 |
| Train/Entropy_Loss      | 0.000603   |
| Train/Entropy_loss      | 0.000603   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1932197  |
| Train/Loss              | 0.09966655 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09502909 |
| Train/Ratio             | 1.0000058  |
| Train/Return            | 1.6282021  |
| Train/V                 | 1.7232347  |
| Train/Value             | 1.7232347  |
| Train/control_penalty   | 0.4034145  |
| Train/policy_loss       | 0.09502909 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 328 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 327         |
| Time/Actor_Time         | 0.0938      |
| Time/B_Format_Time      | 0.0699      |
| Time/B_Original_Form... | 0.0836      |
| Time/Buffer             | 0.00294     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23416209  |
| Train/Action_magnitu... | 0.5234033   |
| Train/Action_magnitude  | 0.4092152   |
| Train/Action_max        | 0.19339773  |
| Train/Action_std        | 0.13754694  |
| Train/Entropy           | -0.59777    |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1716199   |
| Train/Loss              | 0.11938617  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.114689015 |
| Train/Ratio             | 1.0000372   |
| Train/Return            | 1.723805    |
| Train/V                 | 1.8384826   |
| Train/Value             | 1.8384826   |
| Train/control_penalty   | 0.40993816  |
| Train/policy_loss       | 0.114689015 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 329 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 328         |
| Time/Actor_Time         | 0.0643      |
| Time/B_Format_Time      | 0.0656      |
| Time/B_Original_Form... | 0.0666      |
| Time/Buffer             | 0.00684     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24388672  |
| Train/Action_magnitu... | 0.54648656  |
| Train/Action_magnitude  | 0.4278267   |
| Train/Action_max        | 0.19847319  |
| Train/Action_std        | 0.14354704  |
| Train/Entropy           | -0.5543065  |
| Train/Entropy_Loss      | 0.000554    |
| Train/Entropy_loss      | 0.000554    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0844629   |
| Train/Loss              | 0.115031615 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.110203795 |
| Train/Ratio             | 0.99998564  |
| Train/Return            | 1.8662138   |
| Train/V                 | 1.9764225   |
| Train/Value             | 1.9764225   |
| Train/control_penalty   | 0.4273509   |
| Train/policy_loss       | 0.110203795 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0355      |
-----------------------------------------

 ---------------- Iteration 330 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 329         |
| Time/Actor_Time         | 0.0637      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.065       |
| Time/Buffer             | 0.00886     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21770038  |
| Train/Action_magnitu... | 0.4951454   |
| Train/Action_magnitude  | 0.3858472   |
| Train/Action_max        | 0.1935574   |
| Train/Action_std        | 0.13665791  |
| Train/Entropy           | -0.60671663 |
| Train/Entropy_Loss      | 0.000607    |
| Train/Entropy_loss      | 0.000607    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2462355   |
| Train/Loss              | 0.15250956  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14803943  |
| Train/Ratio             | 1.0000103   |
| Train/Return            | 1.4163281   |
| Train/V                 | 1.5643686   |
| Train/Value             | 1.5643686   |
| Train/control_penalty   | 0.3863404   |
| Train/policy_loss       | 0.14803943  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 331 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 330        |
| Time/Actor_Time         | 0.065      |
| Time/B_Format_Time      | 0.071      |
| Time/B_Original_Form... | 0.0645     |
| Time/Buffer             | 0.00398    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22809292 |
| Train/Action_magnitu... | 0.51891166 |
| Train/Action_magnitude  | 0.40304708 |
| Train/Action_max        | 0.19817558 |
| Train/Action_std        | 0.13860577 |
| Train/Entropy           | -0.5937371 |
| Train/Entropy_Loss      | 0.000594   |
| Train/Entropy_loss      | 0.000594   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1732322  |
| Train/Loss              | 0.15246655 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14787172 |
| Train/Ratio             | 0.9999957  |
| Train/Return            | 1.636241   |
| Train/V                 | 1.7841109  |
| Train/Value             | 1.7841109  |
| Train/control_penalty   | 0.40010977 |
| Train/policy_loss       | 0.14787172 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02525    |
----------------------------------------

 ---------------- Iteration 332 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 331         |
| Time/Actor_Time         | 0.0787      |
| Time/B_Format_Time      | 0.0662      |
| Time/B_Original_Form... | 0.0766      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2372999   |
| Train/Action_magnitu... | 0.5301159   |
| Train/Action_magnitude  | 0.41346553  |
| Train/Action_max        | 0.19407767  |
| Train/Action_std        | 0.14029132  |
| Train/Entropy           | -0.57495826 |
| Train/Entropy_Loss      | 0.000575    |
| Train/Entropy_loss      | 0.000575    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.12717     |
| Train/Loss              | 0.20478497  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20006892  |
| Train/Ratio             | 1.0000116   |
| Train/Return            | 1.54775     |
| Train/V                 | 1.74781     |
| Train/Value             | 1.74781     |
| Train/control_penalty   | 0.41410857  |
| Train/policy_loss       | 0.20006892  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 333 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 332         |
| Time/Actor_Time         | 0.0766      |
| Time/B_Format_Time      | 0.0641      |
| Time/B_Original_Form... | 0.103       |
| Time/Buffer             | 0.00261     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24126792  |
| Train/Action_magnitu... | 0.54523885  |
| Train/Action_magnitude  | 0.422609    |
| Train/Action_max        | 0.21392606  |
| Train/Action_std        | 0.14437734  |
| Train/Entropy           | -0.54913247 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0806855   |
| Train/Loss              | 0.15371583  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14896564  |
| Train/Ratio             | 1.0000225   |
| Train/Return            | 1.4214734   |
| Train/V                 | 1.5704274   |
| Train/Value             | 1.5704274   |
| Train/control_penalty   | 0.42010552  |
| Train/policy_loss       | 0.14896564  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 334 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 333        |
| Time/Actor_Time         | 0.0652     |
| Time/B_Format_Time      | 0.0771     |
| Time/B_Original_Form... | 0.0643     |
| Time/Buffer             | 0.00313    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24285293 |
| Train/Action_magnitu... | 0.53792226 |
| Train/Action_magnitude  | 0.4177702  |
| Train/Action_max        | 0.2120598  |
| Train/Action_std        | 0.13899423 |
| Train/Entropy           | -0.5897466 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1783918  |
| Train/Loss              | 0.17238672 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1675928  |
| Train/Ratio             | 1.0000144  |
| Train/Return            | 1.4811307  |
| Train/V                 | 1.6487237  |
| Train/Value             | 1.6487237  |
| Train/control_penalty   | 0.42041847 |
| Train/policy_loss       | 0.1675928  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02175    |
----------------------------------------

 ---------------- Iteration 335 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 334        |
| Time/Actor_Time         | 0.0668     |
| Time/B_Format_Time      | 0.098      |
| Time/B_Original_Form... | 0.104      |
| Time/Buffer             | 0.00432    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23026691 |
| Train/Action_magnitu... | 0.51377326 |
| Train/Action_magnitude  | 0.39849117 |
| Train/Action_max        | 0.1936594  |
| Train/Action_std        | 0.13772793 |
| Train/Entropy           | -0.5982791 |
| Train/Entropy_Loss      | 0.000598   |
| Train/Entropy_loss      | 0.000598   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1961228  |
| Train/Loss              | 0.12510644 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12050633 |
| Train/Ratio             | 1.0000156  |
| Train/Return            | 1.4514207  |
| Train/V                 | 1.5719243  |
| Train/Value             | 1.5719243  |
| Train/control_penalty   | 0.4001826  |
| Train/policy_loss       | 0.12050633 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 336 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 335        |
| Time/Actor_Time         | 0.0625     |
| Time/B_Format_Time      | 0.0658     |
| Time/B_Original_Form... | 0.0698     |
| Time/Buffer             | 0.00268    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23471236 |
| Train/Action_magnitu... | 0.5199149  |
| Train/Action_magnitude  | 0.40433788 |
| Train/Action_max        | 0.19713578 |
| Train/Action_std        | 0.13788094 |
| Train/Entropy           | -0.5959825 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1859032  |
| Train/Loss              | 0.1555851  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15097296 |
| Train/Ratio             | 1.0000217  |
| Train/Return            | 1.3897622  |
| Train/V                 | 1.5407336  |
| Train/Value             | 1.5407336  |
| Train/control_penalty   | 0.40161481 |
| Train/policy_loss       | 0.15097296 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0175     |
----------------------------------------

 ---------------- Iteration 337 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 336         |
| Time/Actor_Time         | 0.0627      |
| Time/B_Format_Time      | 0.0675      |
| Time/B_Original_Form... | 0.0669      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23972689  |
| Train/Action_magnitu... | 0.5375224   |
| Train/Action_magnitude  | 0.41886234  |
| Train/Action_max        | 0.22091189  |
| Train/Action_std        | 0.13917294  |
| Train/Entropy           | -0.58511084 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1607878   |
| Train/Loss              | 0.19885087  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19412525  |
| Train/Ratio             | 1.0000215   |
| Train/Return            | 1.6804371   |
| Train/V                 | 1.8745587   |
| Train/Value             | 1.8745587   |
| Train/control_penalty   | 0.41405123  |
| Train/policy_loss       | 0.19412525  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 338 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 337        |
| Time/Actor_Time         | 0.0769     |
| Time/B_Format_Time      | 0.0633     |
| Time/B_Original_Form... | 0.0757     |
| Time/Buffer             | 0.00283    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24413148 |
| Train/Action_magnitu... | 0.5442157  |
| Train/Action_magnitude  | 0.4239138  |
| Train/Action_max        | 0.22134987 |
| Train/Action_std        | 0.14064033 |
| Train/Entropy           | -0.5767031 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1731174  |
| Train/Loss              | 0.12576509 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12097752 |
| Train/Ratio             | 1.0000092  |
| Train/Return            | 1.518994   |
| Train/V                 | 1.6399689  |
| Train/Value             | 1.6399689  |
| Train/control_penalty   | 0.42108655 |
| Train/policy_loss       | 0.12097752 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 339 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 338         |
| Time/Actor_Time         | 0.0724      |
| Time/B_Format_Time      | 0.0638      |
| Time/B_Original_Form... | 0.0835      |
| Time/Buffer             | 0.00725     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25745457  |
| Train/Action_magnitu... | 0.56201714  |
| Train/Action_magnitude  | 0.44056395  |
| Train/Action_max        | 0.17377414  |
| Train/Action_std        | 0.14304781  |
| Train/Entropy           | -0.559682   |
| Train/Entropy_Loss      | 0.00056     |
| Train/Entropy_loss      | 0.00056     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1427375   |
| Train/Loss              | 0.085679494 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0807162   |
| Train/Ratio             | 0.9999882   |
| Train/Return            | 1.8234383   |
| Train/V                 | 1.904159    |
| Train/Value             | 1.904159    |
| Train/control_penalty   | 0.44036126  |
| Train/policy_loss       | 0.0807162   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0405      |
-----------------------------------------

 ---------------- Iteration 340 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 339         |
| Time/Actor_Time         | 0.0614      |
| Time/B_Format_Time      | 0.0673      |
| Time/B_Original_Form... | 0.0713      |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23461634  |
| Train/Action_magnitu... | 0.5244315   |
| Train/Action_magnitude  | 0.40770838  |
| Train/Action_max        | 0.18008257  |
| Train/Action_std        | 0.14205253  |
| Train/Entropy           | -0.5681404  |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1493908   |
| Train/Loss              | 0.06399503  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.059367795 |
| Train/Ratio             | 0.9999922   |
| Train/Return            | 1.495615    |
| Train/V                 | 1.5549756   |
| Train/Value             | 1.5549756   |
| Train/control_penalty   | 0.40591     |
| Train/policy_loss       | 0.059367795 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 341 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 340        |
| Time/Actor_Time         | 0.0701     |
| Time/B_Format_Time      | 0.0711     |
| Time/B_Original_Form... | 0.0723     |
| Time/Buffer             | 0.00394    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23661043 |
| Train/Action_magnitu... | 0.52487326 |
| Train/Action_magnitude  | 0.41015184 |
| Train/Action_max        | 0.18140195 |
| Train/Action_std        | 0.14324433 |
| Train/Entropy           | -0.5573657 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1178452  |
| Train/Loss              | 0.09562502 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09094261 |
| Train/Ratio             | 1.0000193  |
| Train/Return            | 1.9257122  |
| Train/V                 | 2.0166552  |
| Train/Value             | 2.0166552  |
| Train/control_penalty   | 0.4125053  |
| Train/policy_loss       | 0.09094261 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.043      |
----------------------------------------

 ---------------- Iteration 342 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 341        |
| Time/Actor_Time         | 0.0723     |
| Time/B_Format_Time      | 0.086      |
| Time/B_Original_Form... | 0.065      |
| Time/Buffer             | 0.00402    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24211365 |
| Train/Action_magnitu... | 0.54859805 |
| Train/Action_magnitude  | 0.42892906 |
| Train/Action_max        | 0.20099247 |
| Train/Action_std        | 0.14551514 |
| Train/Entropy           | -0.5394841 |
| Train/Entropy_Loss      | 0.000539   |
| Train/Entropy_loss      | 0.000539   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0727464  |
| Train/Loss              | 0.13711241 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1323037  |
| Train/Ratio             | 0.99998426 |
| Train/Return            | 1.6415464  |
| Train/V                 | 1.7738507  |
| Train/Value             | 1.7738507  |
| Train/control_penalty   | 0.42692226 |
| Train/policy_loss       | 0.1323037  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 343 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 342        |
| Time/Actor_Time         | 0.0619     |
| Time/B_Format_Time      | 0.0681     |
| Time/B_Original_Form... | 0.0669     |
| Time/Buffer             | 0.00371    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24770331 |
| Train/Action_magnitu... | 0.5504633  |
| Train/Action_magnitude  | 0.43151996 |
| Train/Action_max        | 0.19863375 |
| Train/Action_std        | 0.14421307 |
| Train/Entropy           | -0.5503672 |
| Train/Entropy_Loss      | 0.00055    |
| Train/Entropy_loss      | 0.00055    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0933436  |
| Train/Loss              | 0.12190564 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1169913  |
| Train/Ratio             | 0.9999974  |
| Train/Return            | 1.7252223  |
| Train/V                 | 1.8422133  |
| Train/Value             | 1.8422133  |
| Train/control_penalty   | 0.43639746 |
| Train/policy_loss       | 0.1169913  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03425    |
----------------------------------------

 ---------------- Iteration 344 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 343         |
| Time/Actor_Time         | 0.0624      |
| Time/B_Format_Time      | 0.0679      |
| Time/B_Original_Form... | 0.067       |
| Time/Buffer             | 0.00558     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23792802  |
| Train/Action_magnitu... | 0.53434414  |
| Train/Action_magnitude  | 0.4181069   |
| Train/Action_max        | 0.20692028  |
| Train/Action_std        | 0.14470126  |
| Train/Entropy           | -0.5521448  |
| Train/Entropy_Loss      | 0.000552    |
| Train/Entropy_loss      | 0.000552    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1220902   |
| Train/Loss              | 0.073093794 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.06833106  |
| Train/Ratio             | 1.0000259   |
| Train/Return            | 1.9704664   |
| Train/V                 | 2.0387995   |
| Train/Value             | 2.0387995   |
| Train/control_penalty   | 0.42105806  |
| Train/policy_loss       | 0.06833106  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04875     |
-----------------------------------------

 ---------------- Iteration 345 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 344         |
| Time/Actor_Time         | 0.0807      |
| Time/B_Format_Time      | 0.0689      |
| Time/B_Original_Form... | 0.0846      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2343585   |
| Train/Action_magnitu... | 0.5286114   |
| Train/Action_magnitude  | 0.41550666  |
| Train/Action_max        | 0.19817835  |
| Train/Action_std        | 0.14783736  |
| Train/Entropy           | -0.5244076  |
| Train/Entropy_Loss      | 0.000524    |
| Train/Entropy_loss      | 0.000524    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0498112   |
| Train/Loss              | 0.07713731  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.072485276 |
| Train/Ratio             | 1.0000045   |
| Train/Return            | 1.5894076   |
| Train/V                 | 1.6618949   |
| Train/Value             | 1.6618949   |
| Train/control_penalty   | 0.41276202  |
| Train/policy_loss       | 0.072485276 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 346 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 345         |
| Time/Actor_Time         | 0.0715      |
| Time/B_Format_Time      | 0.062       |
| Time/B_Original_Form... | 0.0817      |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23713757  |
| Train/Action_magnitu... | 0.53262085  |
| Train/Action_magnitude  | 0.41709214  |
| Train/Action_max        | 0.21112941  |
| Train/Action_std        | 0.14286779  |
| Train/Entropy           | -0.56147623 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1281135   |
| Train/Loss              | 0.18577018  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18101537  |
| Train/Ratio             | 1.000009    |
| Train/Return            | 1.76124     |
| Train/V                 | 1.9422541   |
| Train/Value             | 1.9422541   |
| Train/control_penalty   | 0.41933426  |
| Train/policy_loss       | 0.18101537  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0305      |
-----------------------------------------

 ---------------- Iteration 347 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 346         |
| Time/Actor_Time         | 0.066       |
| Time/B_Format_Time      | 0.0666      |
| Time/B_Original_Form... | 0.0687      |
| Time/Buffer             | 0.00426     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23749599  |
| Train/Action_magnitu... | 0.53234434  |
| Train/Action_magnitude  | 0.4160082   |
| Train/Action_max        | 0.20671727  |
| Train/Action_std        | 0.138898    |
| Train/Entropy           | -0.58954436 |
| Train/Entropy_Loss      | 0.00059     |
| Train/Entropy_loss      | 0.00059     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1666206   |
| Train/Loss              | 0.13384834  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12911087  |
| Train/Ratio             | 1.0000023   |
| Train/Return            | 1.6492891   |
| Train/V                 | 1.7783979   |
| Train/Value             | 1.7783979   |
| Train/control_penalty   | 0.414792    |
| Train/policy_loss       | 0.12911087  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0285      |
-----------------------------------------

 ---------------- Iteration 348 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 347         |
| Time/Actor_Time         | 0.0627      |
| Time/B_Format_Time      | 0.0682      |
| Time/B_Original_Form... | 0.0658      |
| Time/Buffer             | 0.00353     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23548123  |
| Train/Action_magnitu... | 0.5215494   |
| Train/Action_magnitude  | 0.40622762  |
| Train/Action_max        | 0.20493107  |
| Train/Action_std        | 0.13869463  |
| Train/Entropy           | -0.59287584 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.191432    |
| Train/Loss              | 0.10507905  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10043924  |
| Train/Ratio             | 0.9999988   |
| Train/Return            | 1.4514823   |
| Train/V                 | 1.5519195   |
| Train/Value             | 1.5519195   |
| Train/control_penalty   | 0.4046931   |
| Train/policy_loss       | 0.10043924  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03125     |
-----------------------------------------

 ---------------- Iteration 349 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 348        |
| Time/Actor_Time         | 0.0637     |
| Time/B_Format_Time      | 0.093      |
| Time/B_Original_Form... | 0.0729     |
| Time/Buffer             | 0.00325    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23095311 |
| Train/Action_magnitu... | 0.51647127 |
| Train/Action_magnitude  | 0.40285116 |
| Train/Action_max        | 0.19950937 |
| Train/Action_std        | 0.13867927 |
| Train/Entropy           | -0.5921618 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1681685  |
| Train/Loss              | 0.15562668 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15100415 |
| Train/Ratio             | 1.0000131  |
| Train/Return            | 1.5776646  |
| Train/V                 | 1.72867    |
| Train/Value             | 1.72867    |
| Train/control_penalty   | 0.40303695 |
| Train/policy_loss       | 0.15100415 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 350 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 349         |
| Time/Actor_Time         | 0.0988      |
| Time/B_Format_Time      | 0.0882      |
| Time/B_Original_Form... | 0.0661      |
| Time/Buffer             | 0.00329     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23663928  |
| Train/Action_magnitu... | 0.5277788   |
| Train/Action_magnitude  | 0.4116441   |
| Train/Action_max        | 0.21019615  |
| Train/Action_std        | 0.14049673  |
| Train/Entropy           | -0.58222455 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1771421   |
| Train/Loss              | 0.094468616 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08977859  |
| Train/Ratio             | 1.0000057   |
| Train/Return            | 1.5096005   |
| Train/V                 | 1.5993717   |
| Train/Value             | 1.5993717   |
| Train/control_penalty   | 0.4107806   |
| Train/policy_loss       | 0.08977859  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 351 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 350         |
| Time/Actor_Time         | 0.0649      |
| Time/B_Format_Time      | 0.0898      |
| Time/B_Original_Form... | 0.117       |
| Time/Buffer             | 0.0039      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23649904  |
| Train/Action_magnitu... | 0.5327024   |
| Train/Action_magnitude  | 0.414301    |
| Train/Action_max        | 0.19629718  |
| Train/Action_std        | 0.14079162  |
| Train/Entropy           | -0.58001214 |
| Train/Entropy_Loss      | 0.00058     |
| Train/Entropy_loss      | 0.00058     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1433768   |
| Train/Loss              | 0.047840875 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.043124698 |
| Train/Ratio             | 0.9999874   |
| Train/Return            | 1.487418    |
| Train/V                 | 1.5305315   |
| Train/Value             | 1.5305315   |
| Train/control_penalty   | 0.41361648  |
| Train/policy_loss       | 0.043124698 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0375      |
-----------------------------------------

 ---------------- Iteration 352 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 351        |
| Time/Actor_Time         | 0.0652     |
| Time/B_Format_Time      | 0.0697     |
| Time/B_Original_Form... | 0.062      |
| Time/Buffer             | 0.0025     |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24062683 |
| Train/Action_magnitu... | 0.5346288  |
| Train/Action_magnitude  | 0.41623417 |
| Train/Action_max        | 0.2156651  |
| Train/Action_std        | 0.14169277 |
| Train/Entropy           | -0.5700779 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1342142  |
| Train/Loss              | 0.1581374  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15338065 |
| Train/Ratio             | 0.9999963  |
| Train/Return            | 1.3693098  |
| Train/V                 | 1.5226858  |
| Train/Value             | 1.5226858  |
| Train/control_penalty   | 0.41866758 |
| Train/policy_loss       | 0.15338065 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.019      |
----------------------------------------

 ---------------- Iteration 353 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 352        |
| Time/Actor_Time         | 0.0619     |
| Time/B_Format_Time      | 0.0663     |
| Time/B_Original_Form... | 0.0662     |
| Time/Buffer             | 0.00366    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2369605  |
| Train/Action_magnitu... | 0.53172785 |
| Train/Action_magnitude  | 0.4124547  |
| Train/Action_max        | 0.21987751 |
| Train/Action_std        | 0.14116292 |
| Train/Entropy           | -0.5799071 |
| Train/Entropy_Loss      | 0.00058    |
| Train/Entropy_loss      | 0.00058    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1619992  |
| Train/Loss              | 0.13435675 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12965006 |
| Train/Ratio             | 1.0000011  |
| Train/Return            | 1.3231276  |
| Train/V                 | 1.4527655  |
| Train/Value             | 1.4527655  |
| Train/control_penalty   | 0.4126787  |
| Train/policy_loss       | 0.12965006 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 354 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 353         |
| Time/Actor_Time         | 0.0879      |
| Time/B_Format_Time      | 0.0628      |
| Time/B_Original_Form... | 0.078       |
| Time/Buffer             | 0.00713     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24299651  |
| Train/Action_magnitu... | 0.54249895  |
| Train/Action_magnitude  | 0.42119855  |
| Train/Action_max        | 0.22425307  |
| Train/Action_std        | 0.13945109  |
| Train/Entropy           | -0.58782506 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1680555   |
| Train/Loss              | 0.10956991  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10482104  |
| Train/Ratio             | 0.9999992   |
| Train/Return            | 1.4411516   |
| Train/V                 | 1.5459787   |
| Train/Value             | 1.5459787   |
| Train/control_penalty   | 0.41610396  |
| Train/policy_loss       | 0.10482104  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 355 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 354         |
| Time/Actor_Time         | 0.101       |
| Time/B_Format_Time      | 0.0922      |
| Time/B_Original_Form... | 0.0646      |
| Time/Buffer             | 0.00393     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23416886  |
| Train/Action_magnitu... | 0.5266276   |
| Train/Action_magnitude  | 0.4132195   |
| Train/Action_max        | 0.18580267  |
| Train/Action_std        | 0.14219475  |
| Train/Entropy           | -0.56523067 |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1235056   |
| Train/Loss              | 0.07400974  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.069289126 |
| Train/Ratio             | 1.0000262   |
| Train/Return            | 1.9316633   |
| Train/V                 | 2.000948    |
| Train/Value             | 2.000948    |
| Train/control_penalty   | 0.41553834  |
| Train/policy_loss       | 0.069289126 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04        |
-----------------------------------------

 ---------------- Iteration 356 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 355         |
| Time/Actor_Time         | 0.089       |
| Time/B_Format_Time      | 0.0638      |
| Time/B_Original_Form... | 0.0856      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2411462   |
| Train/Action_magnitu... | 0.54419386  |
| Train/Action_magnitude  | 0.4277819   |
| Train/Action_max        | 0.20004277  |
| Train/Action_std        | 0.14740027  |
| Train/Entropy           | -0.5304513  |
| Train/Entropy_Loss      | 0.00053     |
| Train/Entropy_loss      | 0.00053     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0714563   |
| Train/Loss              | 0.09913872  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.094302736 |
| Train/Ratio             | 0.99998593  |
| Train/Return            | 1.8725493   |
| Train/V                 | 1.9668436   |
| Train/Value             | 1.9668436   |
| Train/control_penalty   | 0.43055356  |
| Train/policy_loss       | 0.094302736 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 357 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 356         |
| Time/Actor_Time         | 0.101       |
| Time/B_Format_Time      | 0.0772      |
| Time/B_Original_Form... | 0.0643      |
| Time/Buffer             | 0.00288     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24504709  |
| Train/Action_magnitu... | 0.5428229   |
| Train/Action_magnitude  | 0.4234485   |
| Train/Action_max        | 0.22311243  |
| Train/Action_std        | 0.14305949  |
| Train/Entropy           | -0.55558664 |
| Train/Entropy_Loss      | 0.000556    |
| Train/Entropy_loss      | 0.000556    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.130124    |
| Train/Loss              | 0.17409477  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16926672  |
| Train/Ratio             | 1.0000072   |
| Train/Return            | 1.673732    |
| Train/V                 | 1.8429856   |
| Train/Value             | 1.8429856   |
| Train/control_penalty   | 0.4272456   |
| Train/policy_loss       | 0.16926672  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 358 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 357         |
| Time/Actor_Time         | 0.115       |
| Time/B_Format_Time      | 0.114       |
| Time/B_Original_Form... | 0.0755      |
| Time/Buffer             | 0.00339     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23729897  |
| Train/Action_magnitu... | 0.5315855   |
| Train/Action_magnitude  | 0.41548482  |
| Train/Action_max        | 0.2011325   |
| Train/Action_std        | 0.14391999  |
| Train/Entropy           | -0.55332214 |
| Train/Entropy_Loss      | 0.000553    |
| Train/Entropy_loss      | 0.000553    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.110251    |
| Train/Loss              | 0.1495337   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14477427  |
| Train/Ratio             | 1.0000039   |
| Train/Return            | 1.6979245   |
| Train/V                 | 1.8427017   |
| Train/Value             | 1.8427017   |
| Train/control_penalty   | 0.4206104   |
| Train/policy_loss       | 0.14477427  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 359 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 358         |
| Time/Actor_Time         | 0.0984      |
| Time/B_Format_Time      | 0.0688      |
| Time/B_Original_Form... | 0.0665      |
| Time/Buffer             | 0.00387     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24193922  |
| Train/Action_magnitu... | 0.5396151   |
| Train/Action_magnitude  | 0.419632    |
| Train/Action_max        | 0.19680469  |
| Train/Action_std        | 0.14335753  |
| Train/Entropy           | -0.5595321  |
| Train/Entropy_Loss      | 0.00056     |
| Train/Entropy_loss      | 0.00056     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1178887   |
| Train/Loss              | 0.055736497 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.05093517  |
| Train/Ratio             | 1.0000037   |
| Train/Return            | 2.0164583   |
| Train/V                 | 2.0673828   |
| Train/Value             | 2.0673828   |
| Train/control_penalty   | 0.42417938  |
| Train/policy_loss       | 0.05093517  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.045       |
-----------------------------------------

 ---------------- Iteration 360 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 359        |
| Time/Actor_Time         | 0.0722     |
| Time/B_Format_Time      | 0.067      |
| Time/B_Original_Form... | 0.0892     |
| Time/Buffer             | 0.00312    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24639267 |
| Train/Action_magnitu... | 0.546635   |
| Train/Action_magnitude  | 0.42514747 |
| Train/Action_max        | 0.20401198 |
| Train/Action_std        | 0.14316471 |
| Train/Entropy           | -0.5587829 |
| Train/Entropy_Loss      | 0.000559   |
| Train/Entropy_loss      | 0.000559   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1410842  |
| Train/Loss              | 0.0674931  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.06268769 |
| Train/Ratio             | 0.9999995  |
| Train/Return            | 1.8207879  |
| Train/V                 | 1.8834802  |
| Train/Value             | 1.8834802  |
| Train/control_penalty   | 0.4246632  |
| Train/policy_loss       | 0.06268769 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0365     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 361 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 360        |
| Time/Actor_Time         | 0.0648     |
| Time/B_Format_Time      | 0.127      |
| Time/B_Original_Form... | 0.137      |
| Time/Buffer             | 0.00268    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.25222087 |
| Train/Action_magnitu... | 0.5664336  |
| Train/Action_magnitude  | 0.44007728 |
| Train/Action_max        | 0.20684268 |
| Train/Action_std        | 0.14710711 |
| Train/Entropy           | -0.5266819 |
| Train/Entropy_Loss      | 0.000527   |
| Train/Entropy_loss      | 0.000527   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0557078  |
| Train/Loss              | 0.15129516 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14638966 |
| Train/Ratio             | 1.0000083  |
| Train/Return            | 1.8775362  |
| Train/V                 | 2.0239272  |
| Train/Value             | 2.0239272  |
| Train/control_penalty   | 0.43788093 |
| Train/policy_loss       | 0.14638966 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.031      |
----------------------------------------

 ---------------- Iteration 362 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 361         |
| Time/Actor_Time         | 0.0918      |
| Time/B_Format_Time      | 0.0707      |
| Time/B_Original_Form... | 0.0696      |
| Time/Buffer             | 0.00359     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23988864  |
| Train/Action_magnitu... | 0.54021     |
| Train/Action_magnitude  | 0.4218762   |
| Train/Action_max        | 0.18550906  |
| Train/Action_std        | 0.14494377  |
| Train/Entropy           | -0.5470088  |
| Train/Entropy_Loss      | 0.000547    |
| Train/Entropy_loss      | 0.000547    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1037405   |
| Train/Loss              | 0.099015765 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0942505   |
| Train/Ratio             | 0.99999243  |
| Train/Return            | 1.9358926   |
| Train/V                 | 2.0301437   |
| Train/Value             | 2.0301437   |
| Train/control_penalty   | 0.42182574  |
| Train/policy_loss       | 0.0942505   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03825     |
-----------------------------------------

 ---------------- Iteration 363 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 362         |
| Time/Actor_Time         | 0.0642      |
| Time/B_Format_Time      | 0.0742      |
| Time/B_Original_Form... | 0.0656      |
| Time/Buffer             | 0.00576     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23382019  |
| Train/Action_magnitu... | 0.5177961   |
| Train/Action_magnitude  | 0.40218097  |
| Train/Action_max        | 0.19815251  |
| Train/Action_std        | 0.13866366  |
| Train/Entropy           | -0.59589154 |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1706154   |
| Train/Loss              | 0.134588    |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12994584  |
| Train/Ratio             | 0.99998033  |
| Train/Return            | 1.7900836   |
| Train/V                 | 1.9200405   |
| Train/Value             | 1.9200405   |
| Train/control_penalty   | 0.40462664  |
| Train/policy_loss       | 0.12994584  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 364 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 363         |
| Time/Actor_Time         | 0.0668      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00313     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24139895  |
| Train/Action_magnitu... | 0.5351189   |
| Train/Action_magnitude  | 0.41470948  |
| Train/Action_max        | 0.20823267  |
| Train/Action_std        | 0.13963655  |
| Train/Entropy           | -0.58612615 |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1914176   |
| Train/Loss              | 0.15748255  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15272851  |
| Train/Ratio             | 1.0000126   |
| Train/Return            | 1.9622383   |
| Train/V                 | 2.1149576   |
| Train/Value             | 2.1149576   |
| Train/control_penalty   | 0.41679212  |
| Train/policy_loss       | 0.15272851  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 365 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 364         |
| Time/Actor_Time         | 0.0649      |
| Time/B_Format_Time      | 0.0631      |
| Time/B_Original_Form... | 0.0657      |
| Time/Buffer             | 0.00311     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2348673   |
| Train/Action_magnitu... | 0.52533704  |
| Train/Action_magnitude  | 0.4043405   |
| Train/Action_max        | 0.18620038  |
| Train/Action_std        | 0.14049692  |
| Train/Entropy           | -0.57696897 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1924994   |
| Train/Loss              | 0.22630179  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.221658    |
| Train/Ratio             | 1.0000252   |
| Train/Return            | 1.7952385   |
| Train/V                 | 2.0168803   |
| Train/Value             | 2.0168803   |
| Train/control_penalty   | 0.4066814   |
| Train/policy_loss       | 0.221658    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 366 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 365        |
| Time/Actor_Time         | 0.0635     |
| Time/B_Format_Time      | 0.067      |
| Time/B_Original_Form... | 0.0681     |
| Time/Buffer             | 0.00307    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22730647 |
| Train/Action_magnitu... | 0.5193247  |
| Train/Action_magnitude  | 0.40242124 |
| Train/Action_max        | 0.19502793 |
| Train/Action_std        | 0.14296196 |
| Train/Entropy           | -0.5618256 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1222613  |
| Train/Loss              | 0.15009414 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14552741 |
| Train/Ratio             | 0.9999926  |
| Train/Return            | 1.5527549  |
| Train/V                 | 1.6982834  |
| Train/Value             | 1.6982834  |
| Train/control_penalty   | 0.4004905  |
| Train/policy_loss       | 0.14552741 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 367 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 366        |
| Time/Actor_Time         | 0.0961     |
| Time/B_Format_Time      | 0.0637     |
| Time/B_Original_Form... | 0.0879     |
| Time/Buffer             | 0.00341    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22661296 |
| Train/Action_magnitu... | 0.5060489  |
| Train/Action_magnitude  | 0.3933492  |
| Train/Action_max        | 0.19227737 |
| Train/Action_std        | 0.13849527 |
| Train/Entropy           | -0.5963771 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1738408  |
| Train/Loss              | 0.13869284 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1341331  |
| Train/Ratio             | 1.0000033  |
| Train/Return            | 1.6979711  |
| Train/V                 | 1.8321084  |
| Train/Value             | 1.8321084  |
| Train/control_penalty   | 0.39633715 |
| Train/policy_loss       | 0.1341331  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 368 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 367         |
| Time/Actor_Time         | 0.0629      |
| Time/B_Format_Time      | 0.0668      |
| Time/B_Original_Form... | 0.0664      |
| Time/Buffer             | 0.00293     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23665273  |
| Train/Action_magnitu... | 0.525121    |
| Train/Action_magnitude  | 0.40552002  |
| Train/Action_max        | 0.20679751  |
| Train/Action_std        | 0.13536404  |
| Train/Entropy           | -0.61377627 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.231375    |
| Train/Loss              | 0.17173634  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16709426  |
| Train/Ratio             | 0.99998826  |
| Train/Return            | 1.9945334   |
| Train/V                 | 2.1616426   |
| Train/Value             | 2.1616426   |
| Train/control_penalty   | 0.40283105  |
| Train/policy_loss       | 0.16709426  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 369 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 368        |
| Time/Actor_Time         | 0.0644     |
| Time/B_Format_Time      | 0.0674     |
| Time/B_Original_Form... | 0.0655     |
| Time/Buffer             | 0.00298    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2358848  |
| Train/Action_magnitu... | 0.5313539  |
| Train/Action_magnitude  | 0.41274115 |
| Train/Action_max        | 0.20799808 |
| Train/Action_std        | 0.14323427 |
| Train/Entropy           | -0.5639691 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0920687  |
| Train/Loss              | 0.14608207 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14141671 |
| Train/Ratio             | 0.9999804  |
| Train/Return            | 1.7684044  |
| Train/V                 | 1.909823   |
| Train/Value             | 1.909823   |
| Train/control_penalty   | 0.41013947 |
| Train/policy_loss       | 0.14141671 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------

 ---------------- Iteration 370 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 369        |
| Time/Actor_Time         | 0.072      |
| Time/B_Format_Time      | 0.0633     |
| Time/B_Original_Form... | 0.107      |
| Time/Buffer             | 0.00254    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22758026 |
| Train/Action_magnitu... | 0.5128247  |
| Train/Action_magnitude  | 0.396456   |
| Train/Action_max        | 0.19959018 |
| Train/Action_std        | 0.13909476 |
| Train/Entropy           | -0.5883231 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1612442  |
| Train/Loss              | 0.15156657 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14706223 |
| Train/Ratio             | 1.0000075  |
| Train/Return            | 1.6353858  |
| Train/V                 | 1.7824439  |
| Train/Value             | 1.7824439  |
| Train/control_penalty   | 0.3916016  |
| Train/policy_loss       | 0.14706223 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 371 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 370         |
| Time/Actor_Time         | 0.0824      |
| Time/B_Format_Time      | 0.0633      |
| Time/B_Original_Form... | 0.0837      |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22675085  |
| Train/Action_magnitu... | 0.5173015   |
| Train/Action_magnitude  | 0.39999652  |
| Train/Action_max        | 0.20121083  |
| Train/Action_std        | 0.1421948   |
| Train/Entropy           | -0.56845605 |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1521709   |
| Train/Loss              | 0.18990166  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.185352    |
| Train/Ratio             | 0.99999833  |
| Train/Return            | 1.5490775   |
| Train/V                 | 1.7344303   |
| Train/Value             | 1.7344303   |
| Train/control_penalty   | 0.3981223   |
| Train/policy_loss       | 0.185352    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 372 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 371        |
| Time/Actor_Time         | 0.09       |
| Time/B_Format_Time      | 0.0964     |
| Time/B_Original_Form... | 0.239      |
| Time/Buffer             | 0.00342    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23816325 |
| Train/Action_magnitu... | 0.53363764 |
| Train/Action_magnitude  | 0.41384378 |
| Train/Action_max        | 0.20741636 |
| Train/Action_std        | 0.14018764 |
| Train/Entropy           | -0.5803066 |
| Train/Entropy_Loss      | 0.00058    |
| Train/Entropy_loss      | 0.00058    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1746947  |
| Train/Loss              | 0.1337485  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12903482 |
| Train/Ratio             | 0.99999654 |
| Train/Return            | 1.8540076  |
| Train/V                 | 1.9830371  |
| Train/Value             | 1.9830371  |
| Train/control_penalty   | 0.4133377  |
| Train/policy_loss       | 0.12903482 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 373 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 372        |
| Time/Actor_Time         | 0.0627     |
| Time/B_Format_Time      | 0.0659     |
| Time/B_Original_Form... | 0.0649     |
| Time/Buffer             | 0.00484    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23281921 |
| Train/Action_magnitu... | 0.5242368  |
| Train/Action_magnitude  | 0.4047435  |
| Train/Action_max        | 0.2037598  |
| Train/Action_std        | 0.14075238 |
| Train/Entropy           | -0.5782608 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1393687  |
| Train/Loss              | 0.18410662 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17947617 |
| Train/Ratio             | 0.9999969  |
| Train/Return            | 1.8589236  |
| Train/V                 | 2.0383928  |
| Train/Value             | 2.0383928  |
| Train/control_penalty   | 0.40521857 |
| Train/policy_loss       | 0.17947617 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 374 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 373         |
| Time/Actor_Time         | 0.0801      |
| Time/B_Format_Time      | 0.0631      |
| Time/B_Original_Form... | 0.0884      |
| Time/Buffer             | 0.00275     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22989798  |
| Train/Action_magnitu... | 0.5104213   |
| Train/Action_magnitude  | 0.3949115   |
| Train/Action_max        | 0.18206833  |
| Train/Action_std        | 0.1343429   |
| Train/Entropy           | -0.62157446 |
| Train/Entropy_Loss      | 0.000622    |
| Train/Entropy_loss      | 0.000622    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2229811   |
| Train/Loss              | 0.19145876  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1868831   |
| Train/Ratio             | 0.9999793   |
| Train/Return            | 1.7883705   |
| Train/V                 | 1.9752519   |
| Train/Value             | 1.9752519   |
| Train/control_penalty   | 0.3954083   |
| Train/policy_loss       | 0.1868831   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 375 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 374        |
| Time/Actor_Time         | 0.0905     |
| Time/B_Format_Time      | 0.108      |
| Time/B_Original_Form... | 0.0658     |
| Time/Buffer             | 0.00359    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24136755 |
| Train/Action_magnitu... | 0.53679115 |
| Train/Action_magnitude  | 0.41679028 |
| Train/Action_max        | 0.19745058 |
| Train/Action_std        | 0.1419646  |
| Train/Entropy           | -0.5679672 |
| Train/Entropy_Loss      | 0.000568   |
| Train/Entropy_loss      | 0.000568   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1453909  |
| Train/Loss              | 0.10567611 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10095456 |
| Train/Ratio             | 1.0000027  |
| Train/Return            | 1.7224019  |
| Train/V                 | 1.8233647  |
| Train/Value             | 1.8233647  |
| Train/control_penalty   | 0.415358   |
| Train/policy_loss       | 0.10095456 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02925    |
----------------------------------------

 ---------------- Iteration 376 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 375        |
| Time/Actor_Time         | 0.0935     |
| Time/B_Format_Time      | 0.0831     |
| Time/B_Original_Form... | 0.0634     |
| Time/Buffer             | 0.00305    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24154007 |
| Train/Action_magnitu... | 0.5378327  |
| Train/Action_magnitude  | 0.41687098 |
| Train/Action_max        | 0.21048962 |
| Train/Action_std        | 0.14268601 |
| Train/Entropy           | -0.5625498 |
| Train/Entropy_Loss      | 0.000563   |
| Train/Entropy_loss      | 0.000563   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1294004  |
| Train/Loss              | 0.16880287 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16405778 |
| Train/Ratio             | 1.0000274  |
| Train/Return            | 1.5838966  |
| Train/V                 | 1.7479539  |
| Train/Value             | 1.7479539  |
| Train/control_penalty   | 0.4182553  |
| Train/policy_loss       | 0.16405778 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02525    |
----------------------------------------

 ---------------- Iteration 377 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 376        |
| Time/Actor_Time         | 0.0646     |
| Time/B_Format_Time      | 0.0636     |
| Time/B_Original_Form... | 0.0811     |
| Time/Buffer             | 0.00291    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24289097 |
| Train/Action_magnitu... | 0.5422263  |
| Train/Action_magnitude  | 0.41838324 |
| Train/Action_max        | 0.21568814 |
| Train/Action_std        | 0.14100577 |
| Train/Entropy           | -0.5763033 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1533701  |
| Train/Loss              | 0.14844054 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14368547 |
| Train/Ratio             | 1.0000125  |
| Train/Return            | 1.7009767  |
| Train/V                 | 1.8446583  |
| Train/Value             | 1.8446583  |
| Train/control_penalty   | 0.41787693 |
| Train/policy_loss       | 0.14368547 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 378 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 377        |
| Time/Actor_Time         | 0.0626     |
| Time/B_Format_Time      | 0.0664     |
| Time/B_Original_Form... | 0.065      |
| Time/Buffer             | 0.00206    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22644833 |
| Train/Action_magnitu... | 0.51947594 |
| Train/Action_magnitude  | 0.40335354 |
| Train/Action_max        | 0.19416578 |
| Train/Action_std        | 0.14314432 |
| Train/Entropy           | -0.5626134 |
| Train/Entropy_Loss      | 0.000563   |
| Train/Entropy_loss      | 0.000563   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1313928  |
| Train/Loss              | 0.18095465 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17635772 |
| Train/Ratio             | 1.0000141  |
| Train/Return            | 1.8986126  |
| Train/V                 | 2.0749643  |
| Train/Value             | 2.0749643  |
| Train/control_penalty   | 0.4034328  |
| Train/policy_loss       | 0.17635772 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0185     |
----------------------------------------

 ---------------- Iteration 379 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 378         |
| Time/Actor_Time         | 0.0629      |
| Time/B_Format_Time      | 0.0681      |
| Time/B_Original_Form... | 0.067       |
| Time/Buffer             | 0.00362     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23720624  |
| Train/Action_magnitu... | 0.52434397  |
| Train/Action_magnitude  | 0.4060168   |
| Train/Action_max        | 0.20526488  |
| Train/Action_std        | 0.13732691  |
| Train/Entropy           | -0.60249823 |
| Train/Entropy_Loss      | 0.000602    |
| Train/Entropy_loss      | 0.000602    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2183139   |
| Train/Loss              | 0.20600176  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20133273  |
| Train/Ratio             | 0.9999955   |
| Train/Return            | 1.7890759   |
| Train/V                 | 1.9904023   |
| Train/Value             | 1.9904023   |
| Train/control_penalty   | 0.4066521   |
| Train/policy_loss       | 0.20133273  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.032       |
-----------------------------------------

 ---------------- Iteration 380 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 379         |
| Time/Actor_Time         | 0.0613      |
| Time/B_Format_Time      | 0.0646      |
| Time/B_Original_Form... | 0.0677      |
| Time/Buffer             | 0.00254     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23573111  |
| Train/Action_magnitu... | 0.52718604  |
| Train/Action_magnitude  | 0.40844792  |
| Train/Action_max        | 0.19386584  |
| Train/Action_std        | 0.13888842  |
| Train/Entropy           | -0.5873794  |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1707622   |
| Train/Loss              | 0.10180643  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.097180784 |
| Train/Ratio             | 0.99999243  |
| Train/Return            | 1.8828498   |
| Train/V                 | 1.9800214   |
| Train/Value             | 1.9800214   |
| Train/control_penalty   | 0.40382653  |
| Train/policy_loss       | 0.097180784 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 381 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 380         |
| Time/Actor_Time         | 0.062       |
| Time/B_Format_Time      | 0.0655      |
| Time/B_Original_Form... | 0.0674      |
| Time/Buffer             | 0.00271     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23755713  |
| Train/Action_magnitu... | 0.5339345   |
| Train/Action_magnitude  | 0.41413048  |
| Train/Action_max        | 0.20420295  |
| Train/Action_std        | 0.14038369  |
| Train/Entropy           | -0.57561344 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1477579   |
| Train/Loss              | 0.20850942  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20381808  |
| Train/Ratio             | 0.99997056  |
| Train/Return            | 1.7137268   |
| Train/V                 | 1.9175575   |
| Train/Value             | 1.9175575   |
| Train/control_penalty   | 0.41157207  |
| Train/policy_loss       | 0.20381808  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 382 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 381        |
| Time/Actor_Time         | 0.064      |
| Time/B_Format_Time      | 0.0739     |
| Time/B_Original_Form... | 0.072      |
| Time/Buffer             | 0.00308    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25397563 |
| Train/Action_magnitu... | 0.5621278  |
| Train/Action_magnitude  | 0.4370438  |
| Train/Action_max        | 0.22542568 |
| Train/Action_std        | 0.14433925 |
| Train/Entropy           | -0.5469682 |
| Train/Entropy_Loss      | 0.000547   |
| Train/Entropy_loss      | 0.000547   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0801731  |
| Train/Loss              | 0.1539589  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1490775  |
| Train/Ratio             | 0.9999855  |
| Train/Return            | 1.8114089  |
| Train/V                 | 1.9604961  |
| Train/Value             | 1.9604961  |
| Train/control_penalty   | 0.4334434  |
| Train/policy_loss       | 0.1490775  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.031      |
----------------------------------------

 ---------------- Iteration 383 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 382        |
| Time/Actor_Time         | 0.0667     |
| Time/B_Format_Time      | 0.107      |
| Time/B_Original_Form... | 0.0624     |
| Time/Buffer             | 0.003      |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24308813 |
| Train/Action_magnitu... | 0.53911304 |
| Train/Action_magnitude  | 0.41740572 |
| Train/Action_max        | 0.202323   |
| Train/Action_std        | 0.14007428 |
| Train/Entropy           | -0.5806124 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1565869  |
| Train/Loss              | 0.0916562  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08686383 |
| Train/Ratio             | 0.9999925  |
| Train/Return            | 1.8308051  |
| Train/V                 | 1.9176697  |
| Train/Value             | 1.9176697  |
| Train/control_penalty   | 0.4211761  |
| Train/policy_loss       | 0.08686383 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 384 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 383        |
| Time/Actor_Time         | 0.0817     |
| Time/B_Format_Time      | 0.0934     |
| Time/B_Original_Form... | 0.0629     |
| Time/Buffer             | 0.00315    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2325358  |
| Train/Action_magnitu... | 0.52016014 |
| Train/Action_magnitude  | 0.4059722  |
| Train/Action_max        | 0.18276298 |
| Train/Action_std        | 0.14006333 |
| Train/Entropy           | -0.5782737 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1604344  |
| Train/Loss              | 0.15896228 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1543085  |
| Train/Ratio             | 1.0000035  |
| Train/Return            | 1.7179387  |
| Train/V                 | 1.8722452  |
| Train/Value             | 1.8722452  |
| Train/control_penalty   | 0.40755048 |
| Train/policy_loss       | 0.1543085  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 385 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 384        |
| Time/Actor_Time         | 0.0714     |
| Time/B_Format_Time      | 0.0861     |
| Time/B_Original_Form... | 0.0628     |
| Time/Buffer             | 0.00323    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.2459772  |
| Train/Action_magnitu... | 0.54506373 |
| Train/Action_magnitude  | 0.42461878 |
| Train/Action_max        | 0.2142246  |
| Train/Action_std        | 0.14358    |
| Train/Entropy           | -0.5559115 |
| Train/Entropy_Loss      | 0.000556   |
| Train/Entropy_loss      | 0.000556   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.104556   |
| Train/Loss              | 0.144397   |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1396095  |
| Train/Ratio             | 0.9999944  |
| Train/Return            | 1.8452715  |
| Train/V                 | 1.984886   |
| Train/Value             | 1.984886   |
| Train/control_penalty   | 0.42315826 |
| Train/policy_loss       | 0.1396095  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 386 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 385         |
| Time/Actor_Time         | 0.0648      |
| Time/B_Format_Time      | 0.0837      |
| Time/B_Original_Form... | 0.0764      |
| Time/Buffer             | 0.00323     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24231954  |
| Train/Action_magnitu... | 0.5425315   |
| Train/Action_magnitude  | 0.42294824  |
| Train/Action_max        | 0.18934208  |
| Train/Action_std        | 0.14323741  |
| Train/Entropy           | -0.55415297 |
| Train/Entropy_Loss      | 0.000554    |
| Train/Entropy_loss      | 0.000554    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1348999   |
| Train/Loss              | 0.12826972  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12350708  |
| Train/Ratio             | 1.0000274   |
| Train/Return            | 1.7787699   |
| Train/V                 | 1.902277    |
| Train/Value             | 1.902277    |
| Train/control_penalty   | 0.42084786  |
| Train/policy_loss       | 0.12350708  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 387 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 386         |
| Time/Actor_Time         | 0.0647      |
| Time/B_Format_Time      | 0.091       |
| Time/B_Original_Form... | 0.0722      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22930637  |
| Train/Action_magnitu... | 0.5216355   |
| Train/Action_magnitude  | 0.40777266  |
| Train/Action_max        | 0.19258033  |
| Train/Action_std        | 0.14281638  |
| Train/Entropy           | -0.56078196 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1393175   |
| Train/Loss              | 0.12719178  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12259473  |
| Train/Ratio             | 0.99997646  |
| Train/Return            | 1.7005235   |
| Train/V                 | 1.8231287   |
| Train/Value             | 1.8231287   |
| Train/control_penalty   | 0.4036269   |
| Train/policy_loss       | 0.12259473  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 388 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 387         |
| Time/Actor_Time         | 0.062       |
| Time/B_Format_Time      | 0.0656      |
| Time/B_Original_Form... | 0.0661      |
| Time/Buffer             | 0.00235     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24507572  |
| Train/Action_magnitu... | 0.5436259   |
| Train/Action_magnitude  | 0.4230459   |
| Train/Action_max        | 0.21476544  |
| Train/Action_std        | 0.14301945  |
| Train/Entropy           | -0.55934954 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1133478   |
| Train/Loss              | 0.16923517  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16443042  |
| Train/Ratio             | 0.9999976   |
| Train/Return            | 1.7768921   |
| Train/V                 | 1.9413271   |
| Train/Value             | 1.9413271   |
| Train/control_penalty   | 0.42454016  |
| Train/policy_loss       | 0.16443042  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 389 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 388        |
| Time/Actor_Time         | 0.0719     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.0751     |
| Time/Buffer             | 0.00263    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22674365 |
| Train/Action_magnitu... | 0.5147371  |
| Train/Action_magnitude  | 0.40444258 |
| Train/Action_max        | 0.17813303 |
| Train/Action_std        | 0.14076966 |
| Train/Entropy           | -0.5748563 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.129698   |
| Train/Loss              | 0.19573604 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1911265  |
| Train/Ratio             | 0.9999664  |
| Train/Return            | 1.845947   |
| Train/V                 | 2.0370822  |
| Train/Value             | 2.0370822  |
| Train/control_penalty   | 0.40346766 |
| Train/policy_loss       | 0.1911265  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 390 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 389         |
| Time/Actor_Time         | 0.0632      |
| Time/B_Format_Time      | 0.0639      |
| Time/B_Original_Form... | 0.0683      |
| Time/Buffer             | 0.00302     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2397027   |
| Train/Action_magnitu... | 0.5350008   |
| Train/Action_magnitude  | 0.4165609   |
| Train/Action_max        | 0.19077031  |
| Train/Action_std        | 0.1407721   |
| Train/Entropy           | -0.57122487 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.141879    |
| Train/Loss              | 0.15293424  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14820474  |
| Train/Ratio             | 0.9999855   |
| Train/Return            | 1.759691    |
| Train/V                 | 1.9078943   |
| Train/Value             | 1.9078943   |
| Train/control_penalty   | 0.41582745  |
| Train/policy_loss       | 0.14820474  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 391 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 390         |
| Time/Actor_Time         | 0.0619      |
| Time/B_Format_Time      | 0.066       |
| Time/B_Original_Form... | 0.0652      |
| Time/Buffer             | 0.00319     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23601532  |
| Train/Action_magnitu... | 0.5338415   |
| Train/Action_magnitude  | 0.41836855  |
| Train/Action_max        | 0.16120616  |
| Train/Action_std        | 0.13955146  |
| Train/Entropy           | -0.58017915 |
| Train/Entropy_Loss      | 0.00058     |
| Train/Entropy_loss      | 0.00058     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1471912   |
| Train/Loss              | 0.18683776  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18207467  |
| Train/Ratio             | 1.0000333   |
| Train/Return            | 1.8246372   |
| Train/V                 | 2.0067005   |
| Train/Value             | 2.0067005   |
| Train/control_penalty   | 0.41829142  |
| Train/policy_loss       | 0.18207467  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 392 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 391        |
| Time/Actor_Time         | 0.0621     |
| Time/B_Format_Time      | 0.0697     |
| Time/B_Original_Form... | 0.0686     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2487615  |
| Train/Action_magnitu... | 0.55828303 |
| Train/Action_magnitude  | 0.4391095  |
| Train/Action_max        | 0.17653699 |
| Train/Action_std        | 0.140111   |
| Train/Entropy           | -0.5733244 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1374031  |
| Train/Loss              | 0.2552973  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.25034148 |
| Train/Ratio             | 0.9999852  |
| Train/Return            | 2.2671006  |
| Train/V                 | 2.517455   |
| Train/Value             | 2.517455   |
| Train/control_penalty   | 0.43824986 |
| Train/policy_loss       | 0.25034148 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 393 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 392        |
| Time/Actor_Time         | 0.0648     |
| Time/B_Format_Time      | 0.108      |
| Time/B_Original_Form... | 0.0667     |
| Time/Buffer             | 0.00611    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23135066 |
| Train/Action_magnitu... | 0.5220626  |
| Train/Action_magnitude  | 0.4070724  |
| Train/Action_max        | 0.17718561 |
| Train/Action_std        | 0.13837859 |
| Train/Entropy           | -0.5903793 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2077776  |
| Train/Loss              | 0.24732916 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.24266697 |
| Train/Ratio             | 0.9999793  |
| Train/Return            | 1.8290505  |
| Train/V                 | 2.0717137  |
| Train/Value             | 2.0717137  |
| Train/control_penalty   | 0.40718022 |
| Train/policy_loss       | 0.24266697 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 394 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 393        |
| Time/Actor_Time         | 0.0796     |
| Time/B_Format_Time      | 0.0748     |
| Time/B_Original_Form... | 0.064      |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23545423 |
| Train/Action_magnitu... | 0.5265899  |
| Train/Action_magnitude  | 0.4120521  |
| Train/Action_max        | 0.19034763 |
| Train/Action_std        | 0.14019035 |
| Train/Entropy           | -0.5783329 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.153875   |
| Train/Loss              | 0.15899172 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15427817 |
| Train/Ratio             | 0.9999893  |
| Train/Return            | 1.7415413  |
| Train/V                 | 1.8958215  |
| Train/Value             | 1.8958215  |
| Train/control_penalty   | 0.41352275 |
| Train/policy_loss       | 0.15427817 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0305     |
----------------------------------------

 ---------------- Iteration 395 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 394        |
| Time/Actor_Time         | 0.0654     |
| Time/B_Format_Time      | 0.0689     |
| Time/B_Original_Form... | 0.0639     |
| Time/Buffer             | 0.00345    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23113298 |
| Train/Action_magnitu... | 0.5236632  |
| Train/Action_magnitude  | 0.4087878  |
| Train/Action_max        | 0.16997083 |
| Train/Action_std        | 0.14123124 |
| Train/Entropy           | -0.5723987 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1398853  |
| Train/Loss              | 0.20477608 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20012926 |
| Train/Ratio             | 1.0000142  |
| Train/Return            | 1.9968418  |
| Train/V                 | 2.1969595  |
| Train/Value             | 2.1969595  |
| Train/control_penalty   | 0.40744177 |
| Train/policy_loss       | 0.20012926 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 396 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 395        |
| Time/Actor_Time         | 0.062      |
| Time/B_Format_Time      | 0.065      |
| Time/B_Original_Form... | 0.0648     |
| Time/Buffer             | 0.00274    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24550706 |
| Train/Action_magnitu... | 0.5498215  |
| Train/Action_magnitude  | 0.43382663 |
| Train/Action_max        | 0.18814346 |
| Train/Action_std        | 0.14035359 |
| Train/Entropy           | -0.5748631 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1354897  |
| Train/Loss              | 0.2805773  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2756661  |
| Train/Ratio             | 0.9999957  |
| Train/Return            | 2.2331207  |
| Train/V                 | 2.5087917  |
| Train/Value             | 2.5087917  |
| Train/control_penalty   | 0.43363476 |
| Train/policy_loss       | 0.2756661  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 397 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 396        |
| Time/Actor_Time         | 0.0653     |
| Time/B_Format_Time      | 0.0936     |
| Time/B_Original_Form... | 0.0637     |
| Time/Buffer             | 0.003      |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24842991 |
| Train/Action_magnitu... | 0.55605704 |
| Train/Action_magnitude  | 0.4370705  |
| Train/Action_max        | 0.17176594 |
| Train/Action_std        | 0.14117181 |
| Train/Entropy           | -0.5719256 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.129573   |
| Train/Loss              | 0.3009477  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2960298  |
| Train/Ratio             | 1.0000085  |
| Train/Return            | 2.193044   |
| Train/V                 | 2.4890683  |
| Train/Value             | 2.4890683  |
| Train/control_penalty   | 0.43459502 |
| Train/policy_loss       | 0.2960298  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 398 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 397        |
| Time/Actor_Time         | 0.0813     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.0651     |
| Time/Buffer             | 0.00346    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23445146 |
| Train/Action_magnitu... | 0.527899   |
| Train/Action_magnitude  | 0.4134737  |
| Train/Action_max        | 0.17130826 |
| Train/Action_std        | 0.13706468 |
| Train/Entropy           | -0.6013359 |
| Train/Entropy_Loss      | 0.000601   |
| Train/Entropy_loss      | 0.000601   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2084259  |
| Train/Loss              | 0.2282427  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22346844 |
| Train/Ratio             | 0.99998033 |
| Train/Return            | 1.9330895  |
| Train/V                 | 2.156564   |
| Train/Value             | 2.156564   |
| Train/control_penalty   | 0.4172914  |
| Train/policy_loss       | 0.22346844 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02725    |
----------------------------------------

 ---------------- Iteration 399 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 398        |
| Time/Actor_Time         | 0.071      |
| Time/B_Format_Time      | 0.0627     |
| Time/B_Original_Form... | 0.0742     |
| Time/Buffer             | 0.00575    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24778754 |
| Train/Action_magnitu... | 0.5571639  |
| Train/Action_magnitude  | 0.43879327 |
| Train/Action_max        | 0.15459524 |
| Train/Action_std        | 0.13947716 |
| Train/Entropy           | -0.5825652 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1624073  |
| Train/Loss              | 0.32896698 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.32398787 |
| Train/Ratio             | 1.00001    |
| Train/Return            | 2.1016705  |
| Train/V                 | 2.425666   |
| Train/Value             | 2.425666   |
| Train/control_penalty   | 0.43965235 |
| Train/policy_loss       | 0.32398787 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.019      |
----------------------------------------

 ---------------- Iteration 400 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 399        |
| Time/Actor_Time         | 0.0642     |
| Time/B_Format_Time      | 0.0632     |
| Time/B_Original_Form... | 0.065      |
| Time/Buffer             | 0.0137     |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23284249 |
| Train/Action_magnitu... | 0.5289794  |
| Train/Action_magnitude  | 0.4186693  |
| Train/Action_max        | 0.13958046 |
| Train/Action_std        | 0.13934322 |
| Train/Entropy           | -0.5852381 |
| Train/Entropy_Loss      | 0.000585   |
| Train/Entropy_loss      | 0.000585   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1952256  |
| Train/Loss              | 0.35222265 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.34745738 |
| Train/Ratio             | 0.9999939  |
| Train/Return            | 2.0988064  |
| Train/V                 | 2.446266   |
| Train/Value             | 2.446266   |
| Train/control_penalty   | 0.41800568 |
| Train/policy_loss       | 0.34745738 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.021      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 401 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 400        |
| Time/Actor_Time         | 0.0694     |
| Time/B_Format_Time      | 0.0684     |
| Time/B_Original_Form... | 0.0689     |
| Time/Buffer             | 0.00364    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23853473 |
| Train/Action_magnitu... | 0.53740335 |
| Train/Action_magnitude  | 0.4227933  |
| Train/Action_max        | 0.17937927 |
| Train/Action_std        | 0.14246467 |
| Train/Entropy           | -0.563349  |
| Train/Entropy_Loss      | 0.000563   |
| Train/Entropy_loss      | 0.000563   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1286758  |
| Train/Loss              | 0.24385278 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.23904495 |
| Train/Ratio             | 0.99997807 |
| Train/Return            | 1.7244201  |
| Train/V                 | 1.9634786  |
| Train/Value             | 1.9634786  |
| Train/control_penalty   | 0.42444733 |
| Train/policy_loss       | 0.23904495 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 402 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 401        |
| Time/Actor_Time         | 0.0707     |
| Time/B_Format_Time      | 0.0893     |
| Time/B_Original_Form... | 0.0654     |
| Time/Buffer             | 0.00329    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24075204 |
| Train/Action_magnitu... | 0.5377076  |
| Train/Action_magnitude  | 0.42387888 |
| Train/Action_max        | 0.18029158 |
| Train/Action_std        | 0.14032154 |
| Train/Entropy           | -0.5817563 |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1780283  |
| Train/Loss              | 0.3000088  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.29520038 |
| Train/Ratio             | 0.999976   |
| Train/Return            | 2.2244322  |
| Train/V                 | 2.519644   |
| Train/Value             | 2.519644   |
| Train/control_penalty   | 0.42266646 |
| Train/policy_loss       | 0.29520038 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02175    |
----------------------------------------

 ---------------- Iteration 403 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 402         |
| Time/Actor_Time         | 0.0794      |
| Time/B_Format_Time      | 0.0656      |
| Time/B_Original_Form... | 0.0876      |
| Time/Buffer             | 0.00308     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23144849  |
| Train/Action_magnitu... | 0.521405    |
| Train/Action_magnitude  | 0.4090728   |
| Train/Action_max        | 0.18086278  |
| Train/Action_std        | 0.13965623  |
| Train/Entropy           | -0.58624744 |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1627458   |
| Train/Loss              | 0.26404768  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.25934213  |
| Train/Ratio             | 0.99999565  |
| Train/Return            | 1.8269745   |
| Train/V                 | 2.0863333   |
| Train/Value             | 2.0863333   |
| Train/control_penalty   | 0.4119314   |
| Train/policy_loss       | 0.25934213  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 404 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 403        |
| Time/Actor_Time         | 0.065      |
| Time/B_Format_Time      | 0.067      |
| Time/B_Original_Form... | 0.114      |
| Time/Buffer             | 0.00268    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23769893 |
| Train/Action_magnitu... | 0.5357362  |
| Train/Action_magnitude  | 0.4193098  |
| Train/Action_max        | 0.20384341 |
| Train/Action_std        | 0.14353159 |
| Train/Entropy           | -0.5601131 |
| Train/Entropy_Loss      | 0.00056    |
| Train/Entropy_loss      | 0.00056    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1108227  |
| Train/Loss              | 0.24585389 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.24106944 |
| Train/Ratio             | 1.0000045  |
| Train/Return            | 2.03581    |
| Train/V                 | 2.27688    |
| Train/Value             | 2.27688    |
| Train/control_penalty   | 0.42243236 |
| Train/policy_loss       | 0.24106944 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01975    |
----------------------------------------

 ---------------- Iteration 405 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 404         |
| Time/Actor_Time         | 0.0621      |
| Time/B_Format_Time      | 0.0692      |
| Time/B_Original_Form... | 0.0673      |
| Time/Buffer             | 0.00313     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21975602  |
| Train/Action_magnitu... | 0.50149596  |
| Train/Action_magnitude  | 0.38909125  |
| Train/Action_max        | 0.19646455  |
| Train/Action_std        | 0.1363412   |
| Train/Entropy           | -0.61374855 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2431267   |
| Train/Loss              | 0.23694971  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.23249581  |
| Train/Ratio             | 0.9999991   |
| Train/Return            | 1.6467147   |
| Train/V                 | 1.8792238   |
| Train/Value             | 1.8792238   |
| Train/control_penalty   | 0.38401428  |
| Train/policy_loss       | 0.23249581  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 406 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 405         |
| Time/Actor_Time         | 0.0637      |
| Time/B_Format_Time      | 0.068       |
| Time/B_Original_Form... | 0.0659      |
| Time/Buffer             | 0.00275     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23761246  |
| Train/Action_magnitu... | 0.529808    |
| Train/Action_magnitude  | 0.41113222  |
| Train/Action_max        | 0.20335765  |
| Train/Action_std        | 0.13992895  |
| Train/Entropy           | -0.58840686 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1598935   |
| Train/Loss              | 0.2512827   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24655204  |
| Train/Ratio             | 0.99999195  |
| Train/Return            | 1.7935374   |
| Train/V                 | 2.040084    |
| Train/Value             | 2.040084    |
| Train/control_penalty   | 0.41422695  |
| Train/policy_loss       | 0.24655204  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.018       |
-----------------------------------------

 ---------------- Iteration 407 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 406         |
| Time/Actor_Time         | 0.0642      |
| Time/B_Format_Time      | 0.0774      |
| Time/B_Original_Form... | 0.0646      |
| Time/Buffer             | 0.0023      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21925794  |
| Train/Action_magnitu... | 0.50044686  |
| Train/Action_magnitude  | 0.38948792  |
| Train/Action_max        | 0.18947689  |
| Train/Action_std        | 0.13413161  |
| Train/Entropy           | -0.62830454 |
| Train/Entropy_Loss      | 0.000628    |
| Train/Entropy_loss      | 0.000628    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.24621     |
| Train/Loss              | 0.2588923   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.25439265  |
| Train/Ratio             | 0.9999909   |
| Train/Return            | 1.6012974   |
| Train/V                 | 1.8557004   |
| Train/Value             | 1.8557004   |
| Train/control_penalty   | 0.38713443  |
| Train/policy_loss       | 0.25439265  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01675     |
-----------------------------------------

 ---------------- Iteration 408 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 407        |
| Time/Actor_Time         | 0.0735     |
| Time/B_Format_Time      | 0.0629     |
| Time/B_Original_Form... | 0.0702     |
| Time/Buffer             | 0.00285    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23442098 |
| Train/Action_magnitu... | 0.51659405 |
| Train/Action_magnitude  | 0.400535   |
| Train/Action_max        | 0.20694944 |
| Train/Action_std        | 0.13425313 |
| Train/Entropy           | -0.6264126 |
| Train/Entropy_Loss      | 0.000626   |
| Train/Entropy_loss      | 0.000626   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2606304  |
| Train/Loss              | 0.15108562 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1464584  |
| Train/Ratio             | 0.99999267 |
| Train/Return            | 1.5680405  |
| Train/V                 | 1.714494   |
| Train/Value             | 1.714494   |
| Train/control_penalty   | 0.40007964 |
| Train/policy_loss       | 0.1464584  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 409 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 408         |
| Time/Actor_Time         | 0.0654      |
| Time/B_Format_Time      | 0.066       |
| Time/B_Original_Form... | 0.0651      |
| Time/Buffer             | 0.00233     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22430351  |
| Train/Action_magnitu... | 0.49866644  |
| Train/Action_magnitude  | 0.38608566  |
| Train/Action_max        | 0.19972393  |
| Train/Action_std        | 0.13489653  |
| Train/Entropy           | -0.62210715 |
| Train/Entropy_Loss      | 0.000622    |
| Train/Entropy_loss      | 0.000622    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2628405   |
| Train/Loss              | 0.20037094  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1958961   |
| Train/Ratio             | 0.9999909   |
| Train/Return            | 1.4944063   |
| Train/V                 | 1.6903082   |
| Train/Value             | 1.6903082   |
| Train/control_penalty   | 0.3852726   |
| Train/policy_loss       | 0.1958961   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0165      |
-----------------------------------------

 ---------------- Iteration 410 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 409        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0859     |
| Time/B_Original_Form... | 0.076      |
| Time/Buffer             | 0.00286    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2348823  |
| Train/Action_magnitu... | 0.52710855 |
| Train/Action_magnitude  | 0.4123632  |
| Train/Action_max        | 0.18641467 |
| Train/Action_std        | 0.14309445 |
| Train/Entropy           | -0.5622442 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1008384  |
| Train/Loss              | 0.17081366 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1661134  |
| Train/Ratio             | 1.0000019  |
| Train/Return            | 1.7345456  |
| Train/V                 | 1.9006641  |
| Train/Value             | 1.9006641  |
| Train/control_penalty   | 0.41380098 |
| Train/policy_loss       | 0.1661134  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 411 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 410        |
| Time/Actor_Time         | 0.0796     |
| Time/B_Format_Time      | 0.0718     |
| Time/B_Original_Form... | 0.0678     |
| Time/Buffer             | 0.00589    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23332857 |
| Train/Action_magnitu... | 0.51635236 |
| Train/Action_magnitude  | 0.40182236 |
| Train/Action_max        | 0.18475477 |
| Train/Action_std        | 0.13717477 |
| Train/Entropy           | -0.6025577 |
| Train/Entropy_Loss      | 0.000603   |
| Train/Entropy_loss      | 0.000603   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2271035  |
| Train/Loss              | 0.18130934 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17669636 |
| Train/Ratio             | 1.0000254  |
| Train/Return            | 1.5765162  |
| Train/V                 | 1.7531974  |
| Train/Value             | 1.7531974  |
| Train/control_penalty   | 0.4010424  |
| Train/policy_loss       | 0.17669636 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 412 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 411        |
| Time/Actor_Time         | 0.0641     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.0632     |
| Time/Buffer             | 0.00268    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22969638 |
| Train/Action_magnitu... | 0.51259345 |
| Train/Action_magnitude  | 0.3962793  |
| Train/Action_max        | 0.19930188 |
| Train/Action_std        | 0.13370389 |
| Train/Entropy           | -0.6278415 |
| Train/Entropy_Loss      | 0.000628   |
| Train/Entropy_loss      | 0.000628   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2420349  |
| Train/Loss              | 0.19108139 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18654586 |
| Train/Ratio             | 1.0000086  |
| Train/Return            | 1.514111   |
| Train/V                 | 1.700655   |
| Train/Value             | 1.700655   |
| Train/control_penalty   | 0.3907677  |
| Train/policy_loss       | 0.18654586 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.017      |
----------------------------------------

 ---------------- Iteration 413 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 412         |
| Time/Actor_Time         | 0.0689      |
| Time/B_Format_Time      | 0.0638      |
| Time/B_Original_Form... | 0.0962      |
| Time/Buffer             | 0.00282     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23001811  |
| Train/Action_magnitu... | 0.50956875  |
| Train/Action_magnitude  | 0.3950701   |
| Train/Action_max        | 0.19536474  |
| Train/Action_std        | 0.13581589  |
| Train/Entropy           | -0.61204773 |
| Train/Entropy_Loss      | 0.000612    |
| Train/Entropy_loss      | 0.000612    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2164973   |
| Train/Loss              | 0.23700486  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.23245747  |
| Train/Ratio             | 1.0000105   |
| Train/Return            | 1.6674404   |
| Train/V                 | 1.8998953   |
| Train/Value             | 1.8998953   |
| Train/control_penalty   | 0.39353356  |
| Train/policy_loss       | 0.23245747  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01475     |
-----------------------------------------

 ---------------- Iteration 414 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 413        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0719     |
| Time/B_Original_Form... | 0.0642     |
| Time/Buffer             | 0.0029     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22513096 |
| Train/Action_magnitu... | 0.51267755 |
| Train/Action_magnitude  | 0.3991596  |
| Train/Action_max        | 0.1879642  |
| Train/Action_std        | 0.13923727 |
| Train/Entropy           | -0.590479  |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1800007  |
| Train/Loss              | 0.19530493 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1907797  |
| Train/Ratio             | 0.9999818  |
| Train/Return            | 1.8200963  |
| Train/V                 | 2.010871   |
| Train/Value             | 2.010871   |
| Train/control_penalty   | 0.39347485 |
| Train/policy_loss       | 0.1907797  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02375    |
----------------------------------------

 ---------------- Iteration 415 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 414         |
| Time/Actor_Time         | 0.0719      |
| Time/B_Format_Time      | 0.0639      |
| Time/B_Original_Form... | 0.0704      |
| Time/Buffer             | 0.00281     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22862777  |
| Train/Action_magnitu... | 0.50893784  |
| Train/Action_magnitude  | 0.3968959   |
| Train/Action_max        | 0.19048359  |
| Train/Action_std        | 0.13594845  |
| Train/Entropy           | -0.60997117 |
| Train/Entropy_Loss      | 0.00061     |
| Train/Entropy_loss      | 0.00061     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2113093   |
| Train/Loss              | 0.23698816  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2323856   |
| Train/Ratio             | 1.0000037   |
| Train/Return            | 1.6771929   |
| Train/V                 | 1.9095724   |
| Train/Value             | 1.9095724   |
| Train/control_penalty   | 0.39925867  |
| Train/policy_loss       | 0.2323856   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 416 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 415         |
| Time/Actor_Time         | 0.0665      |
| Time/B_Format_Time      | 0.069       |
| Time/B_Original_Form... | 0.0732      |
| Time/Buffer             | 0.00351     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23384188  |
| Train/Action_magnitu... | 0.5190423   |
| Train/Action_magnitude  | 0.40517265  |
| Train/Action_max        | 0.20555764  |
| Train/Action_std        | 0.13859735  |
| Train/Entropy           | -0.59248704 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1576438   |
| Train/Loss              | 0.18737371  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18271251  |
| Train/Ratio             | 1.0000303   |
| Train/Return            | 1.7321248   |
| Train/V                 | 1.9148387   |
| Train/Value             | 1.9148387   |
| Train/control_penalty   | 0.4068713   |
| Train/policy_loss       | 0.18271251  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 417 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 416        |
| Time/Actor_Time         | 0.0622     |
| Time/B_Format_Time      | 0.0689     |
| Time/B_Original_Form... | 0.0654     |
| Time/Buffer             | 0.00321    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22831728 |
| Train/Action_magnitu... | 0.51140416 |
| Train/Action_magnitude  | 0.3978747  |
| Train/Action_max        | 0.18828584 |
| Train/Action_std        | 0.13872316 |
| Train/Entropy           | -0.5945657 |
| Train/Entropy_Loss      | 0.000595   |
| Train/Entropy_loss      | 0.000595   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2044336  |
| Train/Loss              | 0.1922047  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18763614 |
| Train/Ratio             | 1.0000114  |
| Train/Return            | 1.672076   |
| Train/V                 | 1.8597103  |
| Train/Value             | 1.8597103  |
| Train/control_penalty   | 0.3973984  |
| Train/policy_loss       | 0.18763614 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01925    |
----------------------------------------

 ---------------- Iteration 418 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 417         |
| Time/Actor_Time         | 0.0689      |
| Time/B_Format_Time      | 0.0684      |
| Time/B_Original_Form... | 0.072       |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23089305  |
| Train/Action_magnitu... | 0.5156329   |
| Train/Action_magnitude  | 0.40173772  |
| Train/Action_max        | 0.18765353  |
| Train/Action_std        | 0.14029253  |
| Train/Entropy           | -0.58172476 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1895726   |
| Train/Loss              | 0.18732528  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18269177  |
| Train/Ratio             | 1.0000105   |
| Train/Return            | 1.7838806   |
| Train/V                 | 1.9665686   |
| Train/Value             | 1.9665686   |
| Train/control_penalty   | 0.40517882  |
| Train/policy_loss       | 0.18269177  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 419 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 418        |
| Time/Actor_Time         | 0.0747     |
| Time/B_Format_Time      | 0.0849     |
| Time/B_Original_Form... | 0.0713     |
| Time/Buffer             | 0.00346    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23908308 |
| Train/Action_magnitu... | 0.54269075 |
| Train/Action_magnitude  | 0.4238755  |
| Train/Action_max        | 0.19408938 |
| Train/Action_std        | 0.14320654 |
| Train/Entropy           | -0.5618959 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1210234  |
| Train/Loss              | 0.22103594 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21623923 |
| Train/Ratio             | 1.000032   |
| Train/Return            | 1.7481929  |
| Train/V                 | 1.9644221  |
| Train/Value             | 1.9644221  |
| Train/control_penalty   | 0.42348242 |
| Train/policy_loss       | 0.21623923 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 420 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 419         |
| Time/Actor_Time         | 0.112       |
| Time/B_Format_Time      | 0.0634      |
| Time/B_Original_Form... | 0.0709      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24455518  |
| Train/Action_magnitu... | 0.5460049   |
| Train/Action_magnitude  | 0.42907587  |
| Train/Action_max        | 0.19769771  |
| Train/Action_std        | 0.14272362  |
| Train/Entropy           | -0.56064326 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1299407   |
| Train/Loss              | 0.1154338   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.110540055 |
| Train/Ratio             | 1.0000007   |
| Train/Return            | 1.639603    |
| Train/V                 | 1.7501494   |
| Train/Value             | 1.7501494   |
| Train/control_penalty   | 0.4333101   |
| Train/policy_loss       | 0.110540055 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 421 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 420         |
| Time/Actor_Time         | 0.0647      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0645      |
| Time/Buffer             | 0.00334     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24602911  |
| Train/Action_magnitu... | 0.5508353   |
| Train/Action_magnitude  | 0.42706195  |
| Train/Action_max        | 0.21638536  |
| Train/Action_std        | 0.14086938  |
| Train/Entropy           | -0.57675064 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1453335   |
| Train/Loss              | 0.099049784 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09423789  |
| Train/Ratio             | 0.9999979   |
| Train/Return            | 1.79964     |
| Train/V                 | 1.8938693   |
| Train/Value             | 1.8938693   |
| Train/control_penalty   | 0.4235152   |
| Train/policy_loss       | 0.09423789  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 422 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 421        |
| Time/Actor_Time         | 0.0646     |
| Time/B_Format_Time      | 0.0854     |
| Time/B_Original_Form... | 0.0645     |
| Time/Buffer             | 0.00283    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24553522 |
| Train/Action_magnitu... | 0.5455514  |
| Train/Action_magnitude  | 0.42508656 |
| Train/Action_max        | 0.21186477 |
| Train/Action_std        | 0.14344533 |
| Train/Entropy           | -0.5581386 |
| Train/Entropy_Loss      | 0.000558   |
| Train/Entropy_loss      | 0.000558   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1228027  |
| Train/Loss              | 0.15912534 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15427808 |
| Train/Ratio             | 0.99997824 |
| Train/Return            | 1.6400167  |
| Train/V                 | 1.7942948  |
| Train/Value             | 1.7942948  |
| Train/control_penalty   | 0.42891148 |
| Train/policy_loss       | 0.15427808 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 423 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 422        |
| Time/Actor_Time         | 0.0638     |
| Time/B_Format_Time      | 0.0722     |
| Time/B_Original_Form... | 0.0645     |
| Time/Buffer             | 0.00283    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24533227 |
| Train/Action_magnitu... | 0.5435873  |
| Train/Action_magnitude  | 0.42569906 |
| Train/Action_max        | 0.18252724 |
| Train/Action_std        | 0.14295068 |
| Train/Entropy           | -0.5569121 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1092424  |
| Train/Loss              | 0.16780795 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16294572 |
| Train/Ratio             | 1.0000091  |
| Train/Return            | 1.791886   |
| Train/V                 | 1.9548343  |
| Train/Value             | 1.9548343  |
| Train/control_penalty   | 0.43053162 |
| Train/policy_loss       | 0.16294572 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 424 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 423        |
| Time/Actor_Time         | 0.0638     |
| Time/B_Format_Time      | 0.0803     |
| Time/B_Original_Form... | 0.112      |
| Time/Buffer             | 0.00264    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24722464 |
| Train/Action_magnitu... | 0.550952   |
| Train/Action_magnitude  | 0.42865416 |
| Train/Action_max        | 0.21163882 |
| Train/Action_std        | 0.14104162 |
| Train/Entropy           | -0.5710893 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1362396  |
| Train/Loss              | 0.1604987  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15568805 |
| Train/Ratio             | 1.0000168  |
| Train/Return            | 1.7977374  |
| Train/V                 | 1.9534345  |
| Train/Value             | 1.9534345  |
| Train/control_penalty   | 0.42395654 |
| Train/policy_loss       | 0.15568805 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 425 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 424         |
| Time/Actor_Time         | 0.0621      |
| Time/B_Format_Time      | 0.068       |
| Time/B_Original_Form... | 0.0657      |
| Time/Buffer             | 0.00498     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22831716  |
| Train/Action_magnitu... | 0.51950806  |
| Train/Action_magnitude  | 0.40468132  |
| Train/Action_max        | 0.21094024  |
| Train/Action_std        | 0.1410318   |
| Train/Entropy           | -0.57535404 |
| Train/Entropy_Loss      | 0.000575    |
| Train/Entropy_loss      | 0.000575    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1697309   |
| Train/Loss              | 0.15836559  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15378027  |
| Train/Ratio             | 0.9999848   |
| Train/Return            | 1.5124556   |
| Train/V                 | 1.6662285   |
| Train/Value             | 1.6662285   |
| Train/control_penalty   | 0.4009975   |
| Train/policy_loss       | 0.15378027  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 426 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 425        |
| Time/Actor_Time         | 0.0622     |
| Time/B_Format_Time      | 0.0659     |
| Time/B_Original_Form... | 0.0653     |
| Time/Buffer             | 0.00286    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23849596 |
| Train/Action_magnitu... | 0.53353614 |
| Train/Action_magnitude  | 0.41542622 |
| Train/Action_max        | 0.20544012 |
| Train/Action_std        | 0.13841406 |
| Train/Entropy           | -0.5899068 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1836846  |
| Train/Loss              | 0.14722142 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14250593 |
| Train/Ratio             | 1.000002   |
| Train/Return            | 1.6094393  |
| Train/V                 | 1.7519282  |
| Train/Value             | 1.7519282  |
| Train/control_penalty   | 0.4125582  |
| Train/policy_loss       | 0.14250593 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 427 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 426         |
| Time/Actor_Time         | 0.0637      |
| Time/B_Format_Time      | 0.0758      |
| Time/B_Original_Form... | 0.0667      |
| Time/Buffer             | 0.00327     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24087137  |
| Train/Action_magnitu... | 0.5331703   |
| Train/Action_magnitude  | 0.41686133  |
| Train/Action_max        | 0.20356564  |
| Train/Action_std        | 0.14166322  |
| Train/Entropy           | -0.56644803 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1560926   |
| Train/Loss              | 0.1297357   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12500083  |
| Train/Ratio             | 0.99997926  |
| Train/Return            | 1.4578366   |
| Train/V                 | 1.5828447   |
| Train/Value             | 1.5828447   |
| Train/control_penalty   | 0.41684037  |
| Train/policy_loss       | 0.12500083  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 428 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 427        |
| Time/Actor_Time         | 0.0717     |
| Time/B_Format_Time      | 0.0632     |
| Time/B_Original_Form... | 0.0896     |
| Time/Buffer             | 0.00282    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22256225 |
| Train/Action_magnitu... | 0.5094706  |
| Train/Action_magnitude  | 0.39467618 |
| Train/Action_max        | 0.19581336 |
| Train/Action_std        | 0.13896078 |
| Train/Entropy           | -0.5875669 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1313335  |
| Train/Loss              | 0.16190916 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15741596 |
| Train/Ratio             | 0.999993   |
| Train/Return            | 1.3726382  |
| Train/V                 | 1.5300565  |
| Train/Value             | 1.5300565  |
| Train/control_penalty   | 0.39056444 |
| Train/policy_loss       | 0.15741596 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0155     |
----------------------------------------

 ---------------- Iteration 429 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 428        |
| Time/Actor_Time         | 0.0706     |
| Time/B_Format_Time      | 0.0642     |
| Time/B_Original_Form... | 0.0833     |
| Time/Buffer             | 0.00347    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23620918 |
| Train/Action_magnitu... | 0.52693915 |
| Train/Action_magnitude  | 0.40929592 |
| Train/Action_max        | 0.19119684 |
| Train/Action_std        | 0.1386655  |
| Train/Entropy           | -0.5880478 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1844776  |
| Train/Loss              | 0.10061459 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09593074 |
| Train/Ratio             | 0.999983   |
| Train/Return            | 1.5531346  |
| Train/V                 | 1.6490844  |
| Train/Value             | 1.6490844  |
| Train/control_penalty   | 0.4095806  |
| Train/policy_loss       | 0.09593074 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03125    |
----------------------------------------

 ---------------- Iteration 430 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 429         |
| Time/Actor_Time         | 0.115       |
| Time/B_Format_Time      | 0.111       |
| Time/B_Original_Form... | 0.064       |
| Time/Buffer             | 0.00429     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23358145  |
| Train/Action_magnitu... | 0.5310579   |
| Train/Action_magnitude  | 0.4158509   |
| Train/Action_max        | 0.1853654   |
| Train/Action_std        | 0.14037205  |
| Train/Entropy           | -0.5775188  |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1546973   |
| Train/Loss              | 0.08675185  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.082052626 |
| Train/Ratio             | 1.0000063   |
| Train/Return            | 1.5740615   |
| Train/V                 | 1.6561104   |
| Train/Value             | 1.6561104   |
| Train/control_penalty   | 0.4121705   |
| Train/policy_loss       | 0.082052626 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 431 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 430         |
| Time/Actor_Time         | 0.0786      |
| Time/B_Format_Time      | 0.0641      |
| Time/B_Original_Form... | 0.0738      |
| Time/Buffer             | 0.00679     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23249485  |
| Train/Action_magnitu... | 0.5217205   |
| Train/Action_magnitude  | 0.40470466  |
| Train/Action_max        | 0.18385586  |
| Train/Action_std        | 0.14071222  |
| Train/Entropy           | -0.57425165 |
| Train/Entropy_Loss      | 0.000574    |
| Train/Entropy_loss      | 0.000574    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1691895   |
| Train/Loss              | 0.13678509  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13216892  |
| Train/Ratio             | 0.99999857  |
| Train/Return            | 1.6552947   |
| Train/V                 | 1.7874669   |
| Train/Value             | 1.7874669   |
| Train/control_penalty   | 0.404192    |
| Train/policy_loss       | 0.13216892  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 432 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 431         |
| Time/Actor_Time         | 0.0621      |
| Time/B_Format_Time      | 0.0671      |
| Time/B_Original_Form... | 0.0658      |
| Time/Buffer             | 0.00339     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23383325  |
| Train/Action_magnitu... | 0.5250979   |
| Train/Action_magnitude  | 0.40794685  |
| Train/Action_max        | 0.18960296  |
| Train/Action_std        | 0.14009845  |
| Train/Entropy           | -0.58268183 |
| Train/Entropy_Loss      | 0.000583    |
| Train/Entropy_loss      | 0.000583    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1662676   |
| Train/Loss              | 0.15861574  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1539286   |
| Train/Ratio             | 0.9999859   |
| Train/Return            | 1.6755238   |
| Train/V                 | 1.829435    |
| Train/Value             | 1.829435    |
| Train/control_penalty   | 0.4104458   |
| Train/policy_loss       | 0.1539286   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 433 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 432         |
| Time/Actor_Time         | 0.0778      |
| Time/B_Format_Time      | 0.0841      |
| Time/B_Original_Form... | 0.0625      |
| Time/Buffer             | 0.00253     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23519649  |
| Train/Action_magnitu... | 0.52338326  |
| Train/Action_magnitude  | 0.40497276  |
| Train/Action_max        | 0.1970313   |
| Train/Action_std        | 0.14057024  |
| Train/Entropy           | -0.57584983 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.160528    |
| Train/Loss              | 0.18089773  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1762488   |
| Train/Ratio             | 0.99999535  |
| Train/Return            | 1.8129799   |
| Train/V                 | 1.989216    |
| Train/Value             | 1.989216    |
| Train/control_penalty   | 0.40730712  |
| Train/policy_loss       | 0.1762488   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 434 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 433         |
| Time/Actor_Time         | 0.0644      |
| Time/B_Format_Time      | 0.0865      |
| Time/B_Original_Form... | 0.09        |
| Time/Buffer             | 0.00311     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22547512  |
| Train/Action_magnitu... | 0.5080324   |
| Train/Action_magnitude  | 0.39111307  |
| Train/Action_max        | 0.19728503  |
| Train/Action_std        | 0.13799192  |
| Train/Entropy           | -0.59566724 |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1926173   |
| Train/Loss              | 0.19801348  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19354987  |
| Train/Ratio             | 0.9999908   |
| Train/Return            | 1.5269375   |
| Train/V                 | 1.7204849   |
| Train/Value             | 1.7204849   |
| Train/control_penalty   | 0.3867946   |
| Train/policy_loss       | 0.19354987  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01725     |
-----------------------------------------

 ---------------- Iteration 435 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 434        |
| Time/Actor_Time         | 0.0638     |
| Time/B_Format_Time      | 0.0826     |
| Time/B_Original_Form... | 0.0629     |
| Time/Buffer             | 0.00314    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24368848 |
| Train/Action_magnitu... | 0.5386751  |
| Train/Action_magnitude  | 0.41595063 |
| Train/Action_max        | 0.19204962 |
| Train/Action_std        | 0.13830568 |
| Train/Entropy           | -0.588651  |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1784236  |
| Train/Loss              | 0.15759009 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15284178 |
| Train/Ratio             | 0.9999973  |
| Train/Return            | 1.8027122  |
| Train/V                 | 1.9555553  |
| Train/Value             | 1.9555553  |
| Train/control_penalty   | 0.4159659  |
| Train/policy_loss       | 0.15284178 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 436 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 435        |
| Time/Actor_Time         | 0.0622     |
| Time/B_Format_Time      | 0.0645     |
| Time/B_Original_Form... | 0.071      |
| Time/Buffer             | 0.00503    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24186513 |
| Train/Action_magnitu... | 0.5385006  |
| Train/Action_magnitude  | 0.4160708  |
| Train/Action_max        | 0.20385097 |
| Train/Action_std        | 0.14011784 |
| Train/Entropy           | -0.578313  |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1325135  |
| Train/Loss              | 0.1110106  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10629776 |
| Train/Ratio             | 0.99999446 |
| Train/Return            | 1.6786408  |
| Train/V                 | 1.7849408  |
| Train/Value             | 1.7849408  |
| Train/control_penalty   | 0.41345263 |
| Train/policy_loss       | 0.10629776 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 437 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 436        |
| Time/Actor_Time         | 0.0676     |
| Time/B_Format_Time      | 0.0627     |
| Time/B_Original_Form... | 0.0833     |
| Time/Buffer             | 0.00426    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24324097 |
| Train/Action_magnitu... | 0.5376383  |
| Train/Action_magnitude  | 0.41703567 |
| Train/Action_max        | 0.21208318 |
| Train/Action_std        | 0.13787304 |
| Train/Entropy           | -0.5956134 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.21126    |
| Train/Loss              | 0.15900704 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15425868 |
| Train/Ratio             | 1.0000108  |
| Train/Return            | 1.8147265  |
| Train/V                 | 1.9689856  |
| Train/Value             | 1.9689856  |
| Train/control_penalty   | 0.41527477 |
| Train/policy_loss       | 0.15425868 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 438 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 437        |
| Time/Actor_Time         | 0.0643     |
| Time/B_Format_Time      | 0.0797     |
| Time/B_Original_Form... | 0.076      |
| Time/Buffer             | 0.00531    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23672466 |
| Train/Action_magnitu... | 0.5246569  |
| Train/Action_magnitude  | 0.40591913 |
| Train/Action_max        | 0.20142998 |
| Train/Action_std        | 0.13661063 |
| Train/Entropy           | -0.6054591 |
| Train/Entropy_Loss      | 0.000605   |
| Train/Entropy_loss      | 0.000605   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2014633  |
| Train/Loss              | 0.15495983 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15031344 |
| Train/Ratio             | 0.99999523 |
| Train/Return            | 1.7554481  |
| Train/V                 | 1.9057789  |
| Train/Value             | 1.9057789  |
| Train/control_penalty   | 0.404093   |
| Train/policy_loss       | 0.15031344 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 439 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 438         |
| Time/Actor_Time         | 0.0766      |
| Time/B_Format_Time      | 0.0631      |
| Time/B_Original_Form... | 0.0778      |
| Time/Buffer             | 0.00298     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24283746  |
| Train/Action_magnitu... | 0.54226714  |
| Train/Action_magnitude  | 0.42051575  |
| Train/Action_max        | 0.2092463   |
| Train/Action_std        | 0.14029913  |
| Train/Entropy           | -0.58063173 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1553193   |
| Train/Loss              | 0.16126548  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15648001  |
| Train/Ratio             | 1.0000035   |
| Train/Return            | 1.9011112   |
| Train/V                 | 2.0575845   |
| Train/Value             | 2.0575845   |
| Train/control_penalty   | 0.42048234  |
| Train/policy_loss       | 0.15648001  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 440 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 439         |
| Time/Actor_Time         | 0.0877      |
| Time/B_Format_Time      | 0.0641      |
| Time/B_Original_Form... | 0.0818      |
| Time/Buffer             | 0.00325     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23990965  |
| Train/Action_magnitu... | 0.5312759   |
| Train/Action_magnitude  | 0.41016904  |
| Train/Action_max        | 0.20852026  |
| Train/Action_std        | 0.13515216  |
| Train/Entropy           | -0.61872196 |
| Train/Entropy_Loss      | 0.000619    |
| Train/Entropy_loss      | 0.000619    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2231182   |
| Train/Loss              | 0.14626104  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14153433  |
| Train/Ratio             | 1.0000267   |
| Train/Return            | 1.7490965   |
| Train/V                 | 1.8906211   |
| Train/Value             | 1.8906211   |
| Train/control_penalty   | 0.4107985   |
| Train/policy_loss       | 0.14153433  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 441 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 440        |
| Time/Actor_Time         | 0.069      |
| Time/B_Format_Time      | 0.0695     |
| Time/B_Original_Form... | 0.0709     |
| Time/Buffer             | 0.00296    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23570281 |
| Train/Action_magnitu... | 0.5272765  |
| Train/Action_magnitude  | 0.4072013  |
| Train/Action_max        | 0.21973348 |
| Train/Action_std        | 0.13710146 |
| Train/Entropy           | -0.6038514 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2190163  |
| Train/Loss              | 0.1837118  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17903912 |
| Train/Ratio             | 0.99998266 |
| Train/Return            | 1.7267045  |
| Train/V                 | 1.90575    |
| Train/Value             | 1.90575    |
| Train/control_penalty   | 0.40688166 |
| Train/policy_loss       | 0.17903912 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 442 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 441         |
| Time/Actor_Time         | 0.0647      |
| Time/B_Format_Time      | 0.0703      |
| Time/B_Original_Form... | 0.0782      |
| Time/Buffer             | 0.00429     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24128488  |
| Train/Action_magnitu... | 0.53291994  |
| Train/Action_magnitude  | 0.4117024   |
| Train/Action_max        | 0.21018398  |
| Train/Action_std        | 0.13835716  |
| Train/Entropy           | -0.59640723 |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2076292   |
| Train/Loss              | 0.14954701  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14482674  |
| Train/Ratio             | 1.0000044   |
| Train/Return            | 1.72963     |
| Train/V                 | 1.874456    |
| Train/Value             | 1.874456    |
| Train/control_penalty   | 0.41238672  |
| Train/policy_loss       | 0.14482674  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03175     |
-----------------------------------------

 ---------------- Iteration 443 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 442        |
| Time/Actor_Time         | 0.0667     |
| Time/B_Format_Time      | 0.0677     |
| Time/B_Original_Form... | 0.0672     |
| Time/Buffer             | 0.00276    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22986384 |
| Train/Action_magnitu... | 0.50987506 |
| Train/Action_magnitude  | 0.39408028 |
| Train/Action_max        | 0.21328452 |
| Train/Action_std        | 0.13742973 |
| Train/Entropy           | -0.6011608 |
| Train/Entropy_Loss      | 0.000601   |
| Train/Entropy_loss      | 0.000601   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2033247  |
| Train/Loss              | 0.1903811  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18581763 |
| Train/Ratio             | 1.0000155  |
| Train/Return            | 1.634493   |
| Train/V                 | 1.8202989  |
| Train/Value             | 1.8202989  |
| Train/control_penalty   | 0.39623132 |
| Train/policy_loss       | 0.18581763 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 444 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 443        |
| Time/Actor_Time         | 0.0789     |
| Time/B_Format_Time      | 0.0644     |
| Time/B_Original_Form... | 0.0937     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24762738 |
| Train/Action_magnitu... | 0.5442384  |
| Train/Action_magnitude  | 0.42213607 |
| Train/Action_max        | 0.20942193 |
| Train/Action_std        | 0.13952565 |
| Train/Entropy           | -0.5868505 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1946262  |
| Train/Loss              | 0.16274294 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15791245 |
| Train/Ratio             | 1.0000304  |
| Train/Return            | 1.7468606  |
| Train/V                 | 1.9047638  |
| Train/Value             | 1.9047638  |
| Train/control_penalty   | 0.4243637  |
| Train/policy_loss       | 0.15791245 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03175    |
----------------------------------------

 ---------------- Iteration 445 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 444        |
| Time/Actor_Time         | 0.062      |
| Time/B_Format_Time      | 0.0649     |
| Time/B_Original_Form... | 0.0662     |
| Time/Buffer             | 0.00432    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22901542 |
| Train/Action_magnitu... | 0.5139398  |
| Train/Action_magnitude  | 0.39922926 |
| Train/Action_max        | 0.21949936 |
| Train/Action_std        | 0.14249131 |
| Train/Entropy           | -0.5657271 |
| Train/Entropy_Loss      | 0.000566   |
| Train/Entropy_loss      | 0.000566   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1061703  |
| Train/Loss              | 0.16005623 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15547471 |
| Train/Ratio             | 1.0000196  |
| Train/Return            | 1.5782443  |
| Train/V                 | 1.733705   |
| Train/Value             | 1.733705   |
| Train/control_penalty   | 0.40158108 |
| Train/policy_loss       | 0.15547471 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 446 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 445        |
| Time/Actor_Time         | 0.0639     |
| Time/B_Format_Time      | 0.0752     |
| Time/B_Original_Form... | 0.0839     |
| Time/Buffer             | 0.00544    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2438484  |
| Train/Action_magnitu... | 0.537532   |
| Train/Action_magnitude  | 0.4167934  |
| Train/Action_max        | 0.22239451 |
| Train/Action_std        | 0.14180219 |
| Train/Entropy           | -0.569019  |
| Train/Entropy_Loss      | 0.000569   |
| Train/Entropy_loss      | 0.000569   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1501651  |
| Train/Loss              | 0.14238966 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13759845 |
| Train/Ratio             | 1.0000055  |
| Train/Return            | 1.5828899  |
| Train/V                 | 1.7204951  |
| Train/Value             | 1.7204951  |
| Train/control_penalty   | 0.42221832 |
| Train/policy_loss       | 0.13759845 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 447 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 446         |
| Time/Actor_Time         | 0.0639      |
| Time/B_Format_Time      | 0.0735      |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24783924  |
| Train/Action_magnitu... | 0.5548941   |
| Train/Action_magnitude  | 0.43146828  |
| Train/Action_max        | 0.2269389   |
| Train/Action_std        | 0.14495547  |
| Train/Entropy           | -0.54646343 |
| Train/Entropy_Loss      | 0.000546    |
| Train/Entropy_loss      | 0.000546    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1069763   |
| Train/Loss              | 0.13248229  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12764697  |
| Train/Ratio             | 0.9999762   |
| Train/Return            | 1.5989473   |
| Train/V                 | 1.7265993   |
| Train/Value             | 1.7265993   |
| Train/control_penalty   | 0.4288849   |
| Train/policy_loss       | 0.12764697  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 448 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 447         |
| Time/Actor_Time         | 0.0742      |
| Time/B_Format_Time      | 0.0641      |
| Time/B_Original_Form... | 0.0761      |
| Time/Buffer             | 0.0033      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24298461  |
| Train/Action_magnitu... | 0.53743297  |
| Train/Action_magnitude  | 0.41650778  |
| Train/Action_max        | 0.22316608  |
| Train/Action_std        | 0.1385335   |
| Train/Entropy           | -0.59316707 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1907502   |
| Train/Loss              | 0.08040746  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.07565519  |
| Train/Ratio             | 1.0000165   |
| Train/Return            | 1.6896272   |
| Train/V                 | 1.7652795   |
| Train/Value             | 1.7652795   |
| Train/control_penalty   | 0.4159102   |
| Train/policy_loss       | 0.07565519  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------

 ---------------- Iteration 449 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 448         |
| Time/Actor_Time         | 0.0621      |
| Time/B_Format_Time      | 0.0669      |
| Time/B_Original_Form... | 0.0661      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23423253  |
| Train/Action_magnitu... | 0.5163499   |
| Train/Action_magnitude  | 0.40061247  |
| Train/Action_max        | 0.19987811  |
| Train/Action_std        | 0.13629918  |
| Train/Entropy           | -0.60660994 |
| Train/Entropy_Loss      | 0.000607    |
| Train/Entropy_loss      | 0.000607    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2142087   |
| Train/Loss              | 0.12758431  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1229254   |
| Train/Ratio             | 1.0000117   |
| Train/Return            | 1.6370984   |
| Train/V                 | 1.7600254   |
| Train/Value             | 1.7600254   |
| Train/control_penalty   | 0.405229    |
| Train/policy_loss       | 0.1229254   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 450 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 449         |
| Time/Actor_Time         | 0.0617      |
| Time/B_Format_Time      | 0.0648      |
| Time/B_Original_Form... | 0.0655      |
| Time/Buffer             | 0.00419     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23940119  |
| Train/Action_magnitu... | 0.5371909   |
| Train/Action_magnitude  | 0.41903055  |
| Train/Action_max        | 0.20662175  |
| Train/Action_std        | 0.14322084  |
| Train/Entropy           | -0.55848485 |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1298137   |
| Train/Loss              | 0.10042615  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09566119  |
| Train/Ratio             | 1.0000343   |
| Train/Return            | 1.7292246   |
| Train/V                 | 1.8248795   |
| Train/Value             | 1.8248795   |
| Train/control_penalty   | 0.4206472   |
| Train/policy_loss       | 0.09566119  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 451 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 450         |
| Time/Actor_Time         | 0.0707      |
| Time/B_Format_Time      | 0.0664      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00403     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23040517  |
| Train/Action_magnitu... | 0.51713467  |
| Train/Action_magnitude  | 0.4034149   |
| Train/Action_max        | 0.18380219  |
| Train/Action_std        | 0.13926812  |
| Train/Entropy           | -0.5868815  |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.173761    |
| Train/Loss              | 0.090265796 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08567355  |
| Train/Ratio             | 1.0000184   |
| Train/Return            | 1.5972488   |
| Train/V                 | 1.68291     |
| Train/Value             | 1.68291     |
| Train/control_penalty   | 0.40053645  |
| Train/policy_loss       | 0.08567355  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 452 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 451         |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0781      |
| Time/Buffer             | 0.00317     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23483813  |
| Train/Action_magnitu... | 0.52366745  |
| Train/Action_magnitude  | 0.41185224  |
| Train/Action_max        | 0.18257546  |
| Train/Action_std        | 0.14026336  |
| Train/Entropy           | -0.5785913  |
| Train/Entropy_Loss      | 0.000579    |
| Train/Entropy_loss      | 0.000579    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.159439    |
| Train/Loss              | 0.11656188  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.111859635 |
| Train/Ratio             | 0.9999875   |
| Train/Return            | 1.9465142   |
| Train/V                 | 2.0583723   |
| Train/Value             | 2.0583723   |
| Train/control_penalty   | 0.41236567  |
| Train/policy_loss       | 0.111859635 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0395      |
-----------------------------------------

 ---------------- Iteration 453 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 452        |
| Time/Actor_Time         | 0.0859     |
| Time/B_Format_Time      | 0.0629     |
| Time/B_Original_Form... | 0.0744     |
| Time/Buffer             | 0.00722    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24020976 |
| Train/Action_magnitu... | 0.5358669  |
| Train/Action_magnitude  | 0.41938826 |
| Train/Action_max        | 0.20299952 |
| Train/Action_std        | 0.1441167  |
| Train/Entropy           | -0.5505136 |
| Train/Entropy_Loss      | 0.000551   |
| Train/Entropy_loss      | 0.000551   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1152285  |
| Train/Loss              | 0.11129687 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10653543 |
| Train/Ratio             | 1.000015   |
| Train/Return            | 1.654637   |
| Train/V                 | 1.7611624  |
| Train/Value             | 1.7611624  |
| Train/control_penalty   | 0.42109233 |
| Train/policy_loss       | 0.10653543 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 454 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 453        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.0843     |
| Time/Buffer             | 0.00246    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24123754 |
| Train/Action_magnitu... | 0.5404486  |
| Train/Action_magnitude  | 0.42298746 |
| Train/Action_max        | 0.20923492 |
| Train/Action_std        | 0.14328896 |
| Train/Entropy           | -0.5568214 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0940887  |
| Train/Loss              | 0.13412447 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1293038  |
| Train/Ratio             | 0.999984   |
| Train/Return            | 1.6826785  |
| Train/V                 | 1.8119761  |
| Train/Value             | 1.8119761  |
| Train/control_penalty   | 0.42638522 |
| Train/policy_loss       | 0.1293038  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 455 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 454         |
| Time/Actor_Time         | 0.0678      |
| Time/B_Format_Time      | 0.0792      |
| Time/B_Original_Form... | 0.0645      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24526002  |
| Train/Action_magnitu... | 0.550037    |
| Train/Action_magnitude  | 0.43046594  |
| Train/Action_max        | 0.1920652   |
| Train/Action_std        | 0.14823581  |
| Train/Entropy           | -0.52330047 |
| Train/Entropy_Loss      | 0.000523    |
| Train/Entropy_loss      | 0.000523    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0157939   |
| Train/Loss              | 0.1484864   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14362548  |
| Train/Ratio             | 1.0000186   |
| Train/Return            | 1.8374045   |
| Train/V                 | 1.9810283   |
| Train/Value             | 1.9810283   |
| Train/control_penalty   | 0.43376178  |
| Train/policy_loss       | 0.14362548  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0305      |
-----------------------------------------

 ---------------- Iteration 456 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 455        |
| Time/Actor_Time         | 0.0634     |
| Time/B_Format_Time      | 0.0651     |
| Time/B_Original_Form... | 0.0652     |
| Time/Buffer             | 0.00309    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22897488 |
| Train/Action_magnitu... | 0.5183011  |
| Train/Action_magnitude  | 0.40243188 |
| Train/Action_max        | 0.2100582  |
| Train/Action_std        | 0.14054832 |
| Train/Entropy           | -0.580789  |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1723579  |
| Train/Loss              | 0.16164394 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15703319 |
| Train/Ratio             | 0.9999932  |
| Train/Return            | 1.5454824  |
| Train/V                 | 1.7025242  |
| Train/Value             | 1.7025242  |
| Train/control_penalty   | 0.40299577 |
| Train/policy_loss       | 0.15703319 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02025    |
----------------------------------------

 ---------------- Iteration 457 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 456         |
| Time/Actor_Time         | 0.104       |
| Time/B_Format_Time      | 0.0985      |
| Time/B_Original_Form... | 0.0653      |
| Time/Buffer             | 0.00287     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2348505   |
| Train/Action_magnitu... | 0.52647644  |
| Train/Action_magnitude  | 0.4087263   |
| Train/Action_max        | 0.20873001  |
| Train/Action_std        | 0.14137654  |
| Train/Entropy           | -0.57841235 |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1463269   |
| Train/Loss              | 0.15881889  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15414     |
| Train/Ratio             | 1.0000103   |
| Train/Return            | 1.6298784   |
| Train/V                 | 1.7840099   |
| Train/Value             | 1.7840099   |
| Train/control_penalty   | 0.41004664  |
| Train/policy_loss       | 0.15414     |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02525     |
-----------------------------------------

 ---------------- Iteration 458 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 457         |
| Time/Actor_Time         | 0.064       |
| Time/B_Format_Time      | 0.111       |
| Time/B_Original_Form... | 0.0726      |
| Time/Buffer             | 0.00305     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2345025   |
| Train/Action_magnitu... | 0.5219229   |
| Train/Action_magnitude  | 0.40787336  |
| Train/Action_max        | 0.19119748  |
| Train/Action_std        | 0.13881354  |
| Train/Entropy           | -0.59183705 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.203616    |
| Train/Loss              | 0.14365537  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13901865  |
| Train/Ratio             | 0.999969    |
| Train/Return            | 1.6226647   |
| Train/V                 | 1.7616807   |
| Train/Value             | 1.7616807   |
| Train/control_penalty   | 0.4044882   |
| Train/policy_loss       | 0.13901865  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 459 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 458        |
| Time/Actor_Time         | 0.0689     |
| Time/B_Format_Time      | 0.0629     |
| Time/B_Original_Form... | 0.0803     |
| Time/Buffer             | 0.00689    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2380956  |
| Train/Action_magnitu... | 0.5312697  |
| Train/Action_magnitude  | 0.41382298 |
| Train/Action_max        | 0.19206116 |
| Train/Action_std        | 0.1419925  |
| Train/Entropy           | -0.5666874 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1327925  |
| Train/Loss              | 0.13587385 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13116111 |
| Train/Ratio             | 0.99999243 |
| Train/Return            | 1.681988   |
| Train/V                 | 1.8131539  |
| Train/Value             | 1.8131539  |
| Train/control_penalty   | 0.41460535 |
| Train/policy_loss       | 0.13116111 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 460 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 459         |
| Time/Actor_Time         | 0.065       |
| Time/B_Format_Time      | 0.0779      |
| Time/B_Original_Form... | 0.0743      |
| Time/Buffer             | 0.0032      |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23925252  |
| Train/Action_magnitu... | 0.5333247   |
| Train/Action_magnitude  | 0.4178897   |
| Train/Action_max        | 0.19823904  |
| Train/Action_std        | 0.14135683  |
| Train/Entropy           | -0.5719267  |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1314688   |
| Train/Loss              | 0.0914183   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.086731404 |
| Train/Ratio             | 0.9999949   |
| Train/Return            | 1.5421994   |
| Train/V                 | 1.6289331   |
| Train/Value             | 1.6289331   |
| Train/control_penalty   | 0.41149685  |
| Train/policy_loss       | 0.086731404 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.032       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 461 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 460         |
| Time/Actor_Time         | 0.0854      |
| Time/B_Format_Time      | 0.0771      |
| Time/B_Original_Form... | 0.0668      |
| Time/Buffer             | 0.00334     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24331044  |
| Train/Action_magnitu... | 0.54594857  |
| Train/Action_magnitude  | 0.42694232  |
| Train/Action_max        | 0.19147192  |
| Train/Action_std        | 0.14266205  |
| Train/Entropy           | -0.5598904  |
| Train/Entropy_Loss      | 0.00056     |
| Train/Entropy_loss      | 0.00056     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.126565    |
| Train/Loss              | 0.084835574 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08002797  |
| Train/Ratio             | 1.0000008   |
| Train/Return            | 1.7846808   |
| Train/V                 | 1.8647025   |
| Train/Value             | 1.8647025   |
| Train/control_penalty   | 0.4247716   |
| Train/policy_loss       | 0.08002797  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.036       |
-----------------------------------------

 ---------------- Iteration 462 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 461         |
| Time/Actor_Time         | 0.0681      |
| Time/B_Format_Time      | 0.0734      |
| Time/B_Original_Form... | 0.0919      |
| Time/Buffer             | 0.00293     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24162275  |
| Train/Action_magnitu... | 0.5387259   |
| Train/Action_magnitude  | 0.4218415   |
| Train/Action_max        | 0.19305877  |
| Train/Action_std        | 0.14106032  |
| Train/Entropy           | -0.57246846 |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1356918   |
| Train/Loss              | 0.1321268   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12739564  |
| Train/Ratio             | 0.99997807  |
| Train/Return            | 1.9529916   |
| Train/V                 | 2.080393    |
| Train/Value             | 2.080393    |
| Train/control_penalty   | 0.4158672   |
| Train/policy_loss       | 0.12739564  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.036       |
-----------------------------------------

 ---------------- Iteration 463 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 462        |
| Time/Actor_Time         | 0.0642     |
| Time/B_Format_Time      | 0.0915     |
| Time/B_Original_Form... | 0.0663     |
| Time/Buffer             | 0.00299    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.2351702  |
| Train/Action_magnitu... | 0.526288   |
| Train/Action_magnitude  | 0.41279772 |
| Train/Action_max        | 0.18329251 |
| Train/Action_std        | 0.14088061 |
| Train/Entropy           | -0.5720022 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1692994  |
| Train/Loss              | 0.16821168 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16348214 |
| Train/Ratio             | 1.0000045  |
| Train/Return            | 1.92375    |
| Train/V                 | 2.087227   |
| Train/Value             | 2.087227   |
| Train/control_penalty   | 0.41575474 |
| Train/policy_loss       | 0.16348214 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 464 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 463        |
| Time/Actor_Time         | 0.0964     |
| Time/B_Format_Time      | 0.0674     |
| Time/B_Original_Form... | 0.0718     |
| Time/Buffer             | 0.00671    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22635119 |
| Train/Action_magnitu... | 0.5171326  |
| Train/Action_magnitude  | 0.40554968 |
| Train/Action_max        | 0.17546485 |
| Train/Action_std        | 0.13894765 |
| Train/Entropy           | -0.5877332 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1727148  |
| Train/Loss              | 0.2684701  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2638421  |
| Train/Ratio             | 1.0000105  |
| Train/Return            | 1.9511847  |
| Train/V                 | 2.2150252  |
| Train/Value             | 2.2150252  |
| Train/control_penalty   | 0.40402806 |
| Train/policy_loss       | 0.2638421  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02375    |
----------------------------------------

 ---------------- Iteration 465 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 464         |
| Time/Actor_Time         | 0.063       |
| Time/B_Format_Time      | 0.0679      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.0101      |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.24409373  |
| Train/Action_magnitu... | 0.5478167   |
| Train/Action_magnitude  | 0.4284344   |
| Train/Action_max        | 0.17261042  |
| Train/Action_std        | 0.13985549  |
| Train/Entropy           | -0.57936645 |
| Train/Entropy_Loss      | 0.000579    |
| Train/Entropy_loss      | 0.000579    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1625979   |
| Train/Loss              | 0.21414006  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20927031  |
| Train/Ratio             | 0.9999853   |
| Train/Return            | 1.8907487   |
| Train/V                 | 2.1000288   |
| Train/Value             | 2.1000288   |
| Train/control_penalty   | 0.4290375   |
| Train/policy_loss       | 0.20927031  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 466 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 465         |
| Time/Actor_Time         | 0.0646      |
| Time/B_Format_Time      | 0.0669      |
| Time/B_Original_Form... | 0.0664      |
| Time/Buffer             | 0.00364     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22996138  |
| Train/Action_magnitu... | 0.5190738   |
| Train/Action_magnitude  | 0.40589014  |
| Train/Action_max        | 0.16100605  |
| Train/Action_std        | 0.13654627  |
| Train/Entropy           | -0.60096747 |
| Train/Entropy_Loss      | 0.000601    |
| Train/Entropy_loss      | 0.000601    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1821612   |
| Train/Loss              | 0.273151    |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.26851344  |
| Train/Ratio             | 1.0000142   |
| Train/Return            | 1.9791448   |
| Train/V                 | 2.247644    |
| Train/Value             | 2.247644    |
| Train/control_penalty   | 0.4036617   |
| Train/policy_loss       | 0.26851344  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 467 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 466        |
| Time/Actor_Time         | 0.064      |
| Time/B_Format_Time      | 0.0699     |
| Time/B_Original_Form... | 0.0682     |
| Time/Buffer             | 0.00323    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23127234 |
| Train/Action_magnitu... | 0.5195308  |
| Train/Action_magnitude  | 0.40566957 |
| Train/Action_max        | 0.19860682 |
| Train/Action_std        | 0.13990557 |
| Train/Entropy           | -0.5853828 |
| Train/Entropy_Loss      | 0.000585   |
| Train/Entropy_loss      | 0.000585   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1510285  |
| Train/Loss              | 0.1525837  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14791875 |
| Train/Ratio             | 1.0000067  |
| Train/Return            | 1.8518565  |
| Train/V                 | 1.9997774  |
| Train/Value             | 1.9997774  |
| Train/control_penalty   | 0.40795746 |
| Train/policy_loss       | 0.14791875 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 468 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 467         |
| Time/Actor_Time         | 0.0653      |
| Time/B_Format_Time      | 0.0674      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00633     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23630655  |
| Train/Action_magnitu... | 0.52005553  |
| Train/Action_magnitude  | 0.40696064  |
| Train/Action_max        | 0.17527726  |
| Train/Action_std        | 0.1376202   |
| Train/Entropy           | -0.5958542  |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2182882   |
| Train/Loss              | 0.02708017  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.022360582 |
| Train/Ratio             | 0.99998754  |
| Train/Return            | 1.9446357   |
| Train/V                 | 1.967       |
| Train/Value             | 1.967       |
| Train/control_penalty   | 0.4123734   |
| Train/policy_loss       | 0.022360582 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.047       |
-----------------------------------------

 ---------------- Iteration 469 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 468        |
| Time/Actor_Time         | 0.0624     |
| Time/B_Format_Time      | 0.0707     |
| Time/B_Original_Form... | 0.0715     |
| Time/Buffer             | 0.00308    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2269443  |
| Train/Action_magnitu... | 0.50402236 |
| Train/Action_magnitude  | 0.3914908  |
| Train/Action_max        | 0.1831503  |
| Train/Action_std        | 0.1341884  |
| Train/Entropy           | -0.623794  |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2476287  |
| Train/Loss              | 0.17631608 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1717625  |
| Train/Ratio             | 0.9999913  |
| Train/Return            | 2.070624   |
| Train/V                 | 2.2423933  |
| Train/Value             | 2.2423933  |
| Train/control_penalty   | 0.39297965 |
| Train/policy_loss       | 0.1717625  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------

 ---------------- Iteration 470 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 469         |
| Time/Actor_Time         | 0.0624      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0696      |
| Time/Buffer             | 0.00469     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23527722  |
| Train/Action_magnitu... | 0.52808917  |
| Train/Action_magnitude  | 0.41350663  |
| Train/Action_max        | 0.17091179  |
| Train/Action_std        | 0.13689673  |
| Train/Entropy           | -0.59922135 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1827506   |
| Train/Loss              | 0.27677152  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.27206185  |
| Train/Ratio             | 0.99999636  |
| Train/Return            | 2.3324003   |
| Train/V                 | 2.6044664   |
| Train/Value             | 2.6044664   |
| Train/control_penalty   | 0.41104132  |
| Train/policy_loss       | 0.27206185  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 471 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 470         |
| Time/Actor_Time         | 0.0674      |
| Time/B_Format_Time      | 0.0624      |
| Time/B_Original_Form... | 0.0819      |
| Time/Buffer             | 0.00261     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23040849  |
| Train/Action_magnitu... | 0.51503104  |
| Train/Action_magnitude  | 0.4011724   |
| Train/Action_max        | 0.1650891   |
| Train/Action_std        | 0.13298516  |
| Train/Entropy           | -0.63074356 |
| Train/Entropy_Loss      | 0.000631    |
| Train/Entropy_loss      | 0.000631    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.25594     |
| Train/Loss              | 0.24548458  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2408588   |
| Train/Ratio             | 1.0000074   |
| Train/Return            | 2.3288412   |
| Train/V                 | 2.5697114   |
| Train/Value             | 2.5697114   |
| Train/control_penalty   | 0.39950404  |
| Train/policy_loss       | 0.2408588   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 472 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 471        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0916     |
| Time/B_Original_Form... | 0.0987     |
| Time/Buffer             | 0.00321    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22948505 |
| Train/Action_magnitu... | 0.5110367  |
| Train/Action_magnitude  | 0.39846027 |
| Train/Action_max        | 0.16607949 |
| Train/Action_std        | 0.1318374  |
| Train/Entropy           | -0.6373536 |
| Train/Entropy_Loss      | 0.000637   |
| Train/Entropy_loss      | 0.000637   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2676765  |
| Train/Loss              | 0.20879766 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20416549 |
| Train/Ratio             | 1.0000029  |
| Train/Return            | 1.9396067  |
| Train/V                 | 2.1437714  |
| Train/Value             | 2.1437714  |
| Train/control_penalty   | 0.3994825  |
| Train/policy_loss       | 0.20416549 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 473 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 472        |
| Time/Actor_Time         | 0.0769     |
| Time/B_Format_Time      | 0.0816     |
| Time/B_Original_Form... | 0.0694     |
| Time/Buffer             | 0.00279    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23670392 |
| Train/Action_magnitu... | 0.5286079  |
| Train/Action_magnitude  | 0.41360107 |
| Train/Action_max        | 0.1578804  |
| Train/Action_std        | 0.13574585 |
| Train/Entropy           | -0.6118087 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2045796  |
| Train/Loss              | 0.3347344  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.32998982 |
| Train/Ratio             | 0.99999523 |
| Train/Return            | 2.255968   |
| Train/V                 | 2.585975   |
| Train/Value             | 2.585975   |
| Train/control_penalty   | 0.4132785  |
| Train/policy_loss       | 0.32998982 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 474 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 473         |
| Time/Actor_Time         | 0.0626      |
| Time/B_Format_Time      | 0.0678      |
| Time/B_Original_Form... | 0.069       |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22759736  |
| Train/Action_magnitu... | 0.51619077  |
| Train/Action_magnitude  | 0.40580758  |
| Train/Action_max        | 0.16508926  |
| Train/Action_std        | 0.13938375  |
| Train/Entropy           | -0.58688754 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1337882   |
| Train/Loss              | 0.24344687  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.23880276  |
| Train/Ratio             | 1.000002    |
| Train/Return            | 1.9665223   |
| Train/V                 | 2.2053323   |
| Train/Value             | 2.2053323   |
| Train/control_penalty   | 0.40572277  |
| Train/policy_loss       | 0.23880276  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 475 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 474        |
| Time/Actor_Time         | 0.0616     |
| Time/B_Format_Time      | 0.0677     |
| Time/B_Original_Form... | 0.0682     |
| Time/Buffer             | 0.00286    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22635785 |
| Train/Action_magnitu... | 0.5091108  |
| Train/Action_magnitude  | 0.39713347 |
| Train/Action_max        | 0.16369222 |
| Train/Action_std        | 0.13614658 |
| Train/Entropy           | -0.6105793 |
| Train/Entropy_Loss      | 0.000611   |
| Train/Entropy_loss      | 0.000611   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2099062  |
| Train/Loss              | 0.19254781 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1880052  |
| Train/Ratio             | 0.99998295 |
| Train/Return            | 1.7895894  |
| Train/V                 | 1.9775984  |
| Train/Value             | 1.9775984  |
| Train/control_penalty   | 0.39320478 |
| Train/policy_loss       | 0.1880052  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 476 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 475        |
| Time/Actor_Time         | 0.0633     |
| Time/B_Format_Time      | 0.0842     |
| Time/B_Original_Form... | 0.0673     |
| Time/Buffer             | 0.00502    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.2272898  |
| Train/Action_magnitu... | 0.5197398  |
| Train/Action_magnitude  | 0.40458727 |
| Train/Action_max        | 0.17515671 |
| Train/Action_std        | 0.13883266 |
| Train/Entropy           | -0.5886244 |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1533176  |
| Train/Loss              | 0.169634   |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16506353 |
| Train/Ratio             | 1.0000321  |
| Train/Return            | 1.6953645  |
| Train/V                 | 1.8604093  |
| Train/Value             | 1.8604093  |
| Train/control_penalty   | 0.39818415 |
| Train/policy_loss       | 0.16506353 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 477 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 476         |
| Time/Actor_Time         | 0.0891      |
| Time/B_Format_Time      | 0.0632      |
| Time/B_Original_Form... | 0.0984      |
| Time/Buffer             | 0.00278     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2194665   |
| Train/Action_magnitu... | 0.49581137  |
| Train/Action_magnitude  | 0.38416785  |
| Train/Action_max        | 0.18849635  |
| Train/Action_std        | 0.13276961  |
| Train/Entropy           | -0.63229793 |
| Train/Entropy_Loss      | 0.000632    |
| Train/Entropy_loss      | 0.000632    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2616749   |
| Train/Loss              | 0.22740088  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2229564   |
| Train/Ratio             | 0.9999798   |
| Train/Return            | 1.9105499   |
| Train/V                 | 2.1335099   |
| Train/Value             | 2.1335099   |
| Train/control_penalty   | 0.38121855  |
| Train/policy_loss       | 0.2229564   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 478 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 477        |
| Time/Actor_Time         | 0.111      |
| Time/B_Format_Time      | 0.0716     |
| Time/B_Original_Form... | 0.0775     |
| Time/Buffer             | 0.00753    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24088089 |
| Train/Action_magnitu... | 0.53398204 |
| Train/Action_magnitude  | 0.41522422 |
| Train/Action_max        | 0.19315958 |
| Train/Action_std        | 0.13595209 |
| Train/Entropy           | -0.6081524 |
| Train/Entropy_Loss      | 0.000608   |
| Train/Entropy_loss      | 0.000608   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2001787  |
| Train/Loss              | 0.2105713  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20581372 |
| Train/Ratio             | 1.0000048  |
| Train/Return            | 1.9273751  |
| Train/V                 | 2.1331875  |
| Train/Value             | 2.1331875  |
| Train/control_penalty   | 0.4149432  |
| Train/policy_loss       | 0.20581372 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.034      |
----------------------------------------

 ---------------- Iteration 479 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 478         |
| Time/Actor_Time         | 0.0829      |
| Time/B_Format_Time      | 0.0634      |
| Time/B_Original_Form... | 0.078       |
| Time/Buffer             | 0.00303     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22036561  |
| Train/Action_magnitu... | 0.49120906  |
| Train/Action_magnitude  | 0.38211438  |
| Train/Action_max        | 0.16221976  |
| Train/Action_std        | 0.13313416  |
| Train/Entropy           | -0.63330525 |
| Train/Entropy_Loss      | 0.000633    |
| Train/Entropy_loss      | 0.000633    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2501185   |
| Train/Loss              | 0.10679848  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10231213  |
| Train/Ratio             | 0.9999583   |
| Train/Return            | 1.7599627   |
| Train/V                 | 1.862293    |
| Train/Value             | 1.862293    |
| Train/control_penalty   | 0.3853036   |
| Train/policy_loss       | 0.10231213  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 480 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 479         |
| Time/Actor_Time         | 0.073       |
| Time/B_Format_Time      | 0.0643      |
| Time/B_Original_Form... | 0.0975      |
| Time/Buffer             | 0.00303     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22971192  |
| Train/Action_magnitu... | 0.5138961   |
| Train/Action_magnitude  | 0.39961553  |
| Train/Action_max        | 0.19086066  |
| Train/Action_std        | 0.13643186  |
| Train/Entropy           | -0.60808766 |
| Train/Entropy_Loss      | 0.000608    |
| Train/Entropy_loss      | 0.000608    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2025305   |
| Train/Loss              | 0.19725078  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19260968  |
| Train/Ratio             | 0.999985    |
| Train/Return            | 2.0158036   |
| Train/V                 | 2.2084062   |
| Train/Value             | 2.2084062   |
| Train/control_penalty   | 0.40330115  |
| Train/policy_loss       | 0.19260968  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 481 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 480        |
| Time/Actor_Time         | 0.0692     |
| Time/B_Format_Time      | 0.105      |
| Time/B_Original_Form... | 0.102      |
| Time/Buffer             | 0.00234    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2273624  |
| Train/Action_magnitu... | 0.50418717 |
| Train/Action_magnitude  | 0.39193326 |
| Train/Action_max        | 0.18470761 |
| Train/Action_std        | 0.1353144  |
| Train/Entropy           | -0.6177194 |
| Train/Entropy_Loss      | 0.000618   |
| Train/Entropy_loss      | 0.000618   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2219055  |
| Train/Loss              | 0.19312343 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18855178 |
| Train/Ratio             | 0.9999846  |
| Train/Return            | 1.7326281  |
| Train/V                 | 1.9211977  |
| Train/Value             | 1.9211977  |
| Train/control_penalty   | 0.39539337 |
| Train/policy_loss       | 0.18855178 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 482 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 481        |
| Time/Actor_Time         | 0.0644     |
| Time/B_Format_Time      | 0.108      |
| Time/B_Original_Form... | 0.0786     |
| Time/Buffer             | 0.00567    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23315278 |
| Train/Action_magnitu... | 0.51893586 |
| Train/Action_magnitude  | 0.40132672 |
| Train/Action_max        | 0.18506797 |
| Train/Action_std        | 0.13910745 |
| Train/Entropy           | -0.5887833 |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.155501   |
| Train/Loss              | 0.13559946 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1309892  |
| Train/Ratio             | 1.0000175  |
| Train/Return            | 1.7467418  |
| Train/V                 | 1.877725   |
| Train/Value             | 1.877725   |
| Train/control_penalty   | 0.40214798 |
| Train/policy_loss       | 0.1309892  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0295     |
----------------------------------------

 ---------------- Iteration 483 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 482         |
| Time/Actor_Time         | 0.0845      |
| Time/B_Format_Time      | 0.0705      |
| Time/B_Original_Form... | 0.0733      |
| Time/Buffer             | 0.00299     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23982802  |
| Train/Action_magnitu... | 0.5316813   |
| Train/Action_magnitude  | 0.4125019   |
| Train/Action_max        | 0.20018919  |
| Train/Action_std        | 0.13847241  |
| Train/Entropy           | -0.59269273 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1913638   |
| Train/Loss              | 0.15871167  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15401614  |
| Train/Ratio             | 1.0000093   |
| Train/Return            | 1.6620378   |
| Train/V                 | 1.8160459   |
| Train/Value             | 1.8160459   |
| Train/control_penalty   | 0.41028368  |
| Train/policy_loss       | 0.15401614  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 484 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 483        |
| Time/Actor_Time         | 0.0946     |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0703     |
| Time/Buffer             | 0.00704    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23043793 |
| Train/Action_magnitu... | 0.51779383 |
| Train/Action_magnitude  | 0.4008477  |
| Train/Action_max        | 0.19630346 |
| Train/Action_std        | 0.13586111 |
| Train/Entropy           | -0.6098584 |
| Train/Entropy_Loss      | 0.00061    |
| Train/Entropy_loss      | 0.00061    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2264143  |
| Train/Loss              | 0.16829291 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16368653 |
| Train/Ratio             | 0.9999881  |
| Train/Return            | 1.6740422  |
| Train/V                 | 1.8377278  |
| Train/Value             | 1.8377278  |
| Train/control_penalty   | 0.3996524  |
| Train/policy_loss       | 0.16368653 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 485 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 484        |
| Time/Actor_Time         | 0.0626     |
| Time/B_Format_Time      | 0.0679     |
| Time/B_Original_Form... | 0.0689     |
| Time/Buffer             | 0.00393    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23928219 |
| Train/Action_magnitu... | 0.54422194 |
| Train/Action_magnitude  | 0.42508298 |
| Train/Action_max        | 0.19700974 |
| Train/Action_std        | 0.14588758 |
| Train/Entropy           | -0.5405765 |
| Train/Entropy_Loss      | 0.000541   |
| Train/Entropy_loss      | 0.000541   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0342209  |
| Train/Loss              | 0.14735596 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14261727 |
| Train/Ratio             | 1.0000192  |
| Train/Return            | 2.0293815  |
| Train/V                 | 2.1719897  |
| Train/Value             | 2.1719897  |
| Train/control_penalty   | 0.41981196 |
| Train/policy_loss       | 0.14261727 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03525    |
----------------------------------------

 ---------------- Iteration 486 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 485        |
| Time/Actor_Time         | 0.0628     |
| Time/B_Format_Time      | 0.0654     |
| Time/B_Original_Form... | 0.0687     |
| Time/Buffer             | 0.00314    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23297568 |
| Train/Action_magnitu... | 0.52750605 |
| Train/Action_magnitude  | 0.41135776 |
| Train/Action_max        | 0.17959982 |
| Train/Action_std        | 0.14227198 |
| Train/Entropy           | -0.5670173 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1439147  |
| Train/Loss              | 0.16010095 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15546194 |
| Train/Ratio             | 0.9999916  |
| Train/Return            | 1.9727943  |
| Train/V                 | 2.128254   |
| Train/Value             | 2.128254   |
| Train/control_penalty   | 0.40719998 |
| Train/policy_loss       | 0.15546194 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03275    |
----------------------------------------

 ---------------- Iteration 487 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 486         |
| Time/Actor_Time         | 0.0643      |
| Time/B_Format_Time      | 0.0779      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.006       |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22603971  |
| Train/Action_magnitu... | 0.505199    |
| Train/Action_magnitude  | 0.3905452   |
| Train/Action_max        | 0.1924583   |
| Train/Action_std        | 0.13755086  |
| Train/Entropy           | -0.603279   |
| Train/Entropy_Loss      | 0.000603    |
| Train/Entropy_loss      | 0.000603    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2153134   |
| Train/Loss              | 0.0991003   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.094541736 |
| Train/Ratio             | 0.99999964  |
| Train/Return            | 1.8566608   |
| Train/V                 | 1.9512116   |
| Train/Value             | 1.9512116   |
| Train/control_penalty   | 0.3955281   |
| Train/policy_loss       | 0.094541736 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 488 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 487        |
| Time/Actor_Time         | 0.0813     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.071      |
| Time/Buffer             | 0.0031     |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.22259787 |
| Train/Action_magnitu... | 0.5063353  |
| Train/Action_magnitude  | 0.39426354 |
| Train/Action_max        | 0.18651138 |
| Train/Action_std        | 0.13714637 |
| Train/Entropy           | -0.6050626 |
| Train/Entropy_Loss      | 0.000605   |
| Train/Entropy_loss      | 0.000605   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1937046  |
| Train/Loss              | 0.18738376 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18284261 |
| Train/Ratio             | 1.0000254  |
| Train/Return            | 1.899642   |
| Train/V                 | 2.082468   |
| Train/Value             | 2.082468   |
| Train/control_penalty   | 0.39360824 |
| Train/policy_loss       | 0.18284261 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 489 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 488        |
| Time/Actor_Time         | 0.0816     |
| Time/B_Format_Time      | 0.11       |
| Time/B_Original_Form... | 0.0629     |
| Time/Buffer             | 0.00325    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22670145 |
| Train/Action_magnitu... | 0.51466906 |
| Train/Action_magnitude  | 0.4006686  |
| Train/Action_max        | 0.19618487 |
| Train/Action_std        | 0.13993663 |
| Train/Entropy           | -0.5866265 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1862851  |
| Train/Loss              | 0.18589169 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1813348  |
| Train/Ratio             | 0.9999992  |
| Train/Return            | 1.6626586  |
| Train/V                 | 1.8440027  |
| Train/Value             | 1.8440027  |
| Train/control_penalty   | 0.3970271  |
| Train/policy_loss       | 0.1813348  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02325    |
----------------------------------------

 ---------------- Iteration 490 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 489         |
| Time/Actor_Time         | 0.0714      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0726      |
| Time/Buffer             | 0.00303     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23516344  |
| Train/Action_magnitu... | 0.5308609   |
| Train/Action_magnitude  | 0.41275626  |
| Train/Action_max        | 0.20351021  |
| Train/Action_std        | 0.14305437  |
| Train/Entropy           | -0.56412554 |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1247879   |
| Train/Loss              | 0.10746905  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10276059  |
| Train/Ratio             | 1.0000012   |
| Train/Return            | 1.828704    |
| Train/V                 | 1.9314787   |
| Train/Value             | 1.9314787   |
| Train/control_penalty   | 0.41443348  |
| Train/policy_loss       | 0.10276059  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------

 ---------------- Iteration 491 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 490         |
| Time/Actor_Time         | 0.0621      |
| Time/B_Format_Time      | 0.0644      |
| Time/B_Original_Form... | 0.0636      |
| Time/Buffer             | 0.0029      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24019888  |
| Train/Action_magnitu... | 0.53079194  |
| Train/Action_magnitude  | 0.41029143  |
| Train/Action_max        | 0.20598114  |
| Train/Action_std        | 0.13984473  |
| Train/Entropy           | -0.58163506 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1552641   |
| Train/Loss              | 0.19703077  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1923349   |
| Train/Ratio             | 1.0000097   |
| Train/Return            | 1.4817535   |
| Train/V                 | 1.6740825   |
| Train/Value             | 1.6740825   |
| Train/control_penalty   | 0.4114226   |
| Train/policy_loss       | 0.1923349   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 492 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 491        |
| Time/Actor_Time         | 0.0701     |
| Time/B_Format_Time      | 0.0699     |
| Time/B_Original_Form... | 0.0716     |
| Time/Buffer             | 0.00252    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22412404 |
| Train/Action_magnitu... | 0.5129855  |
| Train/Action_magnitude  | 0.39918944 |
| Train/Action_max        | 0.20550226 |
| Train/Action_std        | 0.14032131 |
| Train/Entropy           | -0.5810062 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1665676  |
| Train/Loss              | 0.13005635 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12549558 |
| Train/Ratio             | 1.0000252  |
| Train/Return            | 1.5276048  |
| Train/V                 | 1.6530923  |
| Train/Value             | 1.6530923  |
| Train/control_penalty   | 0.39797643 |
| Train/policy_loss       | 0.12549558 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 493 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 492         |
| Time/Actor_Time         | 0.0652      |
| Time/B_Format_Time      | 0.0742      |
| Time/B_Original_Form... | 0.107       |
| Time/Buffer             | 0.00352     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2343169   |
| Train/Action_magnitu... | 0.5414464   |
| Train/Action_magnitude  | 0.4223257   |
| Train/Action_max        | 0.20095374  |
| Train/Action_std        | 0.1446163   |
| Train/Entropy           | -0.55230725 |
| Train/Entropy_Loss      | 0.000552    |
| Train/Entropy_loss      | 0.000552    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0711198   |
| Train/Loss              | 0.11315559  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10841561  |
| Train/Ratio             | 0.99999     |
| Train/Return            | 1.6926773   |
| Train/V                 | 1.8011053   |
| Train/Value             | 1.8011053   |
| Train/control_penalty   | 0.4187674   |
| Train/policy_loss       | 0.10841561  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0315      |
-----------------------------------------

 ---------------- Iteration 494 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 493        |
| Time/Actor_Time         | 0.0622     |
| Time/B_Format_Time      | 0.0653     |
| Time/B_Original_Form... | 0.0647     |
| Time/Buffer             | 0.00579    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23387028 |
| Train/Action_magnitu... | 0.5311905  |
| Train/Action_magnitude  | 0.41300574 |
| Train/Action_max        | 0.19923113 |
| Train/Action_std        | 0.1432782  |
| Train/Entropy           | -0.5563315 |
| Train/Entropy_Loss      | 0.000556   |
| Train/Entropy_loss      | 0.000556   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1106675  |
| Train/Loss              | 0.1881546  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18347764 |
| Train/Ratio             | 1.0000055  |
| Train/Return            | 1.6186509  |
| Train/V                 | 1.8021203  |
| Train/Value             | 1.8021203  |
| Train/control_penalty   | 0.41206124 |
| Train/policy_loss       | 0.18347764 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 495 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 494        |
| Time/Actor_Time         | 0.065      |
| Time/B_Format_Time      | 0.0679     |
| Time/B_Original_Form... | 0.11       |
| Time/Buffer             | 0.00272    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23480928 |
| Train/Action_magnitu... | 0.52848184 |
| Train/Action_magnitude  | 0.41058555 |
| Train/Action_max        | 0.19953401 |
| Train/Action_std        | 0.1418223  |
| Train/Entropy           | -0.5698512 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1401829  |
| Train/Loss              | 0.13063061 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12595767 |
| Train/Ratio             | 0.9999987  |
| Train/Return            | 1.5576261  |
| Train/V                 | 1.6835824  |
| Train/Value             | 1.6835824  |
| Train/control_penalty   | 0.410309   |
| Train/policy_loss       | 0.12595767 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 496 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 495         |
| Time/Actor_Time         | 0.0639      |
| Time/B_Format_Time      | 0.106       |
| Time/B_Original_Form... | 0.101       |
| Time/Buffer             | 0.00225     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23129691  |
| Train/Action_magnitu... | 0.5184852   |
| Train/Action_magnitude  | 0.4014582   |
| Train/Action_max        | 0.2041798   |
| Train/Action_std        | 0.14074649  |
| Train/Entropy           | -0.57782286 |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1704226   |
| Train/Loss              | 0.1783881   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17381392  |
| Train/Ratio             | 0.9999957   |
| Train/Return            | 1.5798663   |
| Train/V                 | 1.7536885   |
| Train/Value             | 1.7536885   |
| Train/control_penalty   | 0.39963537  |
| Train/policy_loss       | 0.17381392  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 497 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 496        |
| Time/Actor_Time         | 0.0744     |
| Time/B_Format_Time      | 0.0647     |
| Time/B_Original_Form... | 0.0728     |
| Time/Buffer             | 0.00297    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22984505 |
| Train/Action_magnitu... | 0.5161791  |
| Train/Action_magnitude  | 0.40132213 |
| Train/Action_max        | 0.198311   |
| Train/Action_std        | 0.14023294 |
| Train/Entropy           | -0.5794765 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1647527  |
| Train/Loss              | 0.17484023 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17022423 |
| Train/Ratio             | 0.9999897  |
| Train/Return            | 1.6341839  |
| Train/V                 | 1.8044066  |
| Train/Value             | 1.8044066  |
| Train/control_penalty   | 0.40365168 |
| Train/policy_loss       | 0.17022423 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 498 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 497         |
| Time/Actor_Time         | 0.064       |
| Time/B_Format_Time      | 0.088       |
| Time/B_Original_Form... | 0.0631      |
| Time/Buffer             | 0.00251     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23872815  |
| Train/Action_magnitu... | 0.5384143   |
| Train/Action_magnitude  | 0.41951612  |
| Train/Action_max        | 0.20490517  |
| Train/Action_std        | 0.14108653  |
| Train/Entropy           | -0.57387525 |
| Train/Entropy_Loss      | 0.000574    |
| Train/Entropy_loss      | 0.000574    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1282481   |
| Train/Loss              | 0.20109846  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19639902  |
| Train/Ratio             | 1.0000062   |
| Train/Return            | 1.6927232   |
| Train/V                 | 1.8891172   |
| Train/Value             | 1.8891172   |
| Train/control_penalty   | 0.41255695  |
| Train/policy_loss       | 0.19639902  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 499 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 498        |
| Time/Actor_Time         | 0.0637     |
| Time/B_Format_Time      | 0.0649     |
| Time/B_Original_Form... | 0.0653     |
| Time/Buffer             | 0.00566    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.2309365  |
| Train/Action_magnitu... | 0.51687884 |
| Train/Action_magnitude  | 0.4010048  |
| Train/Action_max        | 0.19057575 |
| Train/Action_std        | 0.14051281 |
| Train/Entropy           | -0.5749791 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.148793   |
| Train/Loss              | 0.1436376  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13904375 |
| Train/Ratio             | 0.99998534 |
| Train/Return            | 1.5610697  |
| Train/V                 | 1.7001061  |
| Train/Value             | 1.7001061  |
| Train/control_penalty   | 0.4018874  |
| Train/policy_loss       | 0.13904375 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 500 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 499        |
| Time/Actor_Time         | 0.0627     |
| Time/B_Format_Time      | 0.0661     |
| Time/B_Original_Form... | 0.0655     |
| Time/Buffer             | 0.00264    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22654878 |
| Train/Action_magnitu... | 0.51173824 |
| Train/Action_magnitude  | 0.3940244  |
| Train/Action_max        | 0.20736523 |
| Train/Action_std        | 0.13709973 |
| Train/Entropy           | -0.6020856 |
| Train/Entropy_Loss      | 0.000602   |
| Train/Entropy_loss      | 0.000602   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2056522  |
| Train/Loss              | 0.16520363 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16068523 |
| Train/Ratio             | 0.9999802  |
| Train/Return            | 1.4336592  |
| Train/V                 | 1.594353   |
| Train/Value             | 1.594353   |
| Train/control_penalty   | 0.39163283 |
| Train/policy_loss       | 0.16068523 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.018      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 501 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 500        |
| Time/Actor_Time         | 0.0635     |
| Time/B_Format_Time      | 0.0677     |
| Time/B_Original_Form... | 0.0657     |
| Time/Buffer             | 0.00577    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23306489 |
| Train/Action_magnitu... | 0.5181312  |
| Train/Action_magnitude  | 0.4030873  |
| Train/Action_max        | 0.18776779 |
| Train/Action_std        | 0.13816558 |
| Train/Entropy           | -0.5933007 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1826485  |
| Train/Loss              | 0.16007516 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15548874 |
| Train/Ratio             | 1.0000159  |
| Train/Return            | 1.5192611  |
| Train/V                 | 1.6747433  |
| Train/Value             | 1.6747433  |
| Train/control_penalty   | 0.3993102  |
| Train/policy_loss       | 0.15548874 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 502 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 501        |
| Time/Actor_Time         | 0.0641     |
| Time/B_Format_Time      | 0.102      |
| Time/B_Original_Form... | 0.0749     |
| Time/Buffer             | 0.00272    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23099934 |
| Train/Action_magnitu... | 0.512988   |
| Train/Action_magnitude  | 0.39516988 |
| Train/Action_max        | 0.18842998 |
| Train/Action_std        | 0.13527314 |
| Train/Entropy           | -0.612489  |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2248628  |
| Train/Loss              | 0.18299028 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17843108 |
| Train/Ratio             | 1.0000101  |
| Train/Return            | 1.3291043  |
| Train/V                 | 1.5075346  |
| Train/Value             | 1.5075346  |
| Train/control_penalty   | 0.39467156 |
| Train/policy_loss       | 0.17843108 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01675    |
----------------------------------------

 ---------------- Iteration 503 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 502        |
| Time/Actor_Time         | 0.106      |
| Time/B_Format_Time      | 0.0647     |
| Time/B_Original_Form... | 0.0822     |
| Time/Buffer             | 0.00292    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2318377  |
| Train/Action_magnitu... | 0.51567155 |
| Train/Action_magnitude  | 0.39735234 |
| Train/Action_max        | 0.2136561  |
| Train/Action_std        | 0.1347963  |
| Train/Entropy           | -0.6199183 |
| Train/Entropy_Loss      | 0.00062    |
| Train/Entropy_loss      | 0.00062    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2367635  |
| Train/Loss              | 0.1673222  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16271596 |
| Train/Ratio             | 0.9999944  |
| Train/Return            | 1.3010685  |
| Train/V                 | 1.4637935  |
| Train/Value             | 1.4637935  |
| Train/control_penalty   | 0.39863288 |
| Train/policy_loss       | 0.16271596 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.016      |
----------------------------------------

 ---------------- Iteration 504 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 503         |
| Time/Actor_Time         | 0.0644      |
| Time/B_Format_Time      | 0.0737      |
| Time/B_Original_Form... | 0.0731      |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2379203   |
| Train/Action_magnitu... | 0.5262333   |
| Train/Action_magnitude  | 0.40865535  |
| Train/Action_max        | 0.19435538  |
| Train/Action_std        | 0.13717438  |
| Train/Entropy           | -0.59894854 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2042538   |
| Train/Loss              | 0.19845732  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19374576  |
| Train/Ratio             | 1.0000162   |
| Train/Return            | 1.5210701   |
| Train/V                 | 1.7148131   |
| Train/Value             | 1.7148131   |
| Train/control_penalty   | 0.4112605   |
| Train/policy_loss       | 0.19374576  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 505 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 504         |
| Time/Actor_Time         | 0.086       |
| Time/B_Format_Time      | 0.0634      |
| Time/B_Original_Form... | 0.0899      |
| Time/Buffer             | 0.00295     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23341818  |
| Train/Action_magnitu... | 0.51261294  |
| Train/Action_magnitude  | 0.39547914  |
| Train/Action_max        | 0.193302    |
| Train/Action_std        | 0.1371096   |
| Train/Entropy           | -0.60039926 |
| Train/Entropy_Loss      | 0.0006      |
| Train/Entropy_loss      | 0.0006      |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1894774   |
| Train/Loss              | 0.19566897  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1910653   |
| Train/Ratio             | 1.0000085   |
| Train/Return            | 1.475082    |
| Train/V                 | 1.6661497   |
| Train/Value             | 1.6661497   |
| Train/control_penalty   | 0.40032727  |
| Train/policy_loss       | 0.1910653   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0155      |
-----------------------------------------

 ---------------- Iteration 506 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 505        |
| Time/Actor_Time         | 0.0634     |
| Time/B_Format_Time      | 0.0789     |
| Time/B_Original_Form... | 0.104      |
| Time/Buffer             | 0.00268    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23220567 |
| Train/Action_magnitu... | 0.51181513 |
| Train/Action_magnitude  | 0.39535618 |
| Train/Action_max        | 0.19018064 |
| Train/Action_std        | 0.13535693 |
| Train/Entropy           | -0.6144438 |
| Train/Entropy_Loss      | 0.000614   |
| Train/Entropy_loss      | 0.000614   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2416866  |
| Train/Loss              | 0.19455938 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18994065 |
| Train/Ratio             | 1.000008   |
| Train/Return            | 1.5134053  |
| Train/V                 | 1.7033508  |
| Train/Value             | 1.7033508  |
| Train/control_penalty   | 0.40042797 |
| Train/policy_loss       | 0.18994065 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0185     |
----------------------------------------

 ---------------- Iteration 507 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 506         |
| Time/Actor_Time         | 0.0626      |
| Time/B_Format_Time      | 0.0663      |
| Time/B_Original_Form... | 0.0654      |
| Time/Buffer             | 0.00614     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23160553  |
| Train/Action_magnitu... | 0.51653314  |
| Train/Action_magnitude  | 0.39772487  |
| Train/Action_max        | 0.19485568  |
| Train/Action_std        | 0.13592775  |
| Train/Entropy           | -0.61158717 |
| Train/Entropy_Loss      | 0.000612    |
| Train/Entropy_loss      | 0.000612    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2204853   |
| Train/Loss              | 0.22744289  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2228741   |
| Train/Ratio             | 0.9999915   |
| Train/Return            | 1.5054772   |
| Train/V                 | 1.728359    |
| Train/Value             | 1.728359    |
| Train/control_penalty   | 0.39571965  |
| Train/policy_loss       | 0.2228741   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01525     |
-----------------------------------------

 ---------------- Iteration 508 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 507        |
| Time/Actor_Time         | 0.0782     |
| Time/B_Format_Time      | 0.0726     |
| Time/B_Original_Form... | 0.0687     |
| Time/Buffer             | 0.00267    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22385922 |
| Train/Action_magnitu... | 0.49355024 |
| Train/Action_magnitude  | 0.38177612 |
| Train/Action_max        | 0.18432625 |
| Train/Action_std        | 0.13094738 |
| Train/Entropy           | -0.6473598 |
| Train/Entropy_Loss      | 0.000647   |
| Train/Entropy_loss      | 0.000647   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.3077073  |
| Train/Loss              | 0.18586458 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18140054 |
| Train/Ratio             | 1.0000169  |
| Train/Return            | 1.529261   |
| Train/V                 | 1.7106537  |
| Train/Value             | 1.7106537  |
| Train/control_penalty   | 0.38166833 |
| Train/policy_loss       | 0.18140054 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0165     |
----------------------------------------

 ---------------- Iteration 509 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 508        |
| Time/Actor_Time         | 0.084      |
| Time/B_Format_Time      | 0.0675     |
| Time/B_Original_Form... | 0.0745     |
| Time/Buffer             | 0.00316    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23950735 |
| Train/Action_magnitu... | 0.53555965 |
| Train/Action_magnitude  | 0.4147     |
| Train/Action_max        | 0.20368832 |
| Train/Action_std        | 0.14318572 |
| Train/Entropy           | -0.5621541 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.129141   |
| Train/Loss              | 0.23740734 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.23270506 |
| Train/Ratio             | 1.0000001  |
| Train/Return            | 1.5837512  |
| Train/V                 | 1.8164557  |
| Train/Value             | 1.8164557  |
| Train/control_penalty   | 0.41401303 |
| Train/policy_loss       | 0.23270506 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.016      |
----------------------------------------

 ---------------- Iteration 510 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 509        |
| Time/Actor_Time         | 0.0641     |
| Time/B_Format_Time      | 0.104      |
| Time/B_Original_Form... | 0.0743     |
| Time/Buffer             | 0.00304    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.25449848 |
| Train/Action_magnitu... | 0.5583266  |
| Train/Action_magnitude  | 0.4334241  |
| Train/Action_max        | 0.19806896 |
| Train/Action_std        | 0.14435665 |
| Train/Entropy           | -0.5463062 |
| Train/Entropy_Loss      | 0.000546   |
| Train/Entropy_loss      | 0.000546   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0956824  |
| Train/Loss              | 0.1488319  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14397281 |
| Train/Ratio             | 1.0000217  |
| Train/Return            | 1.7360723  |
| Train/V                 | 1.88004    |
| Train/Value             | 1.88004    |
| Train/control_penalty   | 0.431278   |
| Train/policy_loss       | 0.14397281 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 511 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 510        |
| Time/Actor_Time         | 0.0808     |
| Time/B_Format_Time      | 0.0894     |
| Time/B_Original_Form... | 0.0687     |
| Time/Buffer             | 0.00317    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24992031 |
| Train/Action_magnitu... | 0.55164313 |
| Train/Action_magnitude  | 0.43138462 |
| Train/Action_max        | 0.18969098 |
| Train/Action_std        | 0.1420874  |
| Train/Entropy           | -0.5668912 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1321087  |
| Train/Loss              | 0.14176397 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13690087 |
| Train/Ratio             | 0.99999547 |
| Train/Return            | 1.607809   |
| Train/V                 | 1.744713   |
| Train/Value             | 1.744713   |
| Train/control_penalty   | 0.4296211  |
| Train/policy_loss       | 0.13690087 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 512 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 511         |
| Time/Actor_Time         | 0.0637      |
| Time/B_Format_Time      | 0.0674      |
| Time/B_Original_Form... | 0.0647      |
| Time/Buffer             | 0.00292     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2423187   |
| Train/Action_magnitu... | 0.5415027   |
| Train/Action_magnitude  | 0.4222992   |
| Train/Action_max        | 0.20330423  |
| Train/Action_std        | 0.1443515   |
| Train/Entropy           | -0.54876524 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1045146   |
| Train/Loss              | 0.18066132  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17589812  |
| Train/Ratio             | 0.9999797   |
| Train/Return            | 1.5653949   |
| Train/V                 | 1.7412918   |
| Train/Value             | 1.7412918   |
| Train/control_penalty   | 0.42144328  |
| Train/policy_loss       | 0.17589812  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01775     |
-----------------------------------------

 ---------------- Iteration 513 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 512        |
| Time/Actor_Time         | 0.0972     |
| Time/B_Format_Time      | 0.064      |
| Time/B_Original_Form... | 0.0819     |
| Time/Buffer             | 0.00316    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24360599 |
| Train/Action_magnitu... | 0.5459409  |
| Train/Action_magnitude  | 0.4236294  |
| Train/Action_max        | 0.19268344 |
| Train/Action_std        | 0.14306396 |
| Train/Entropy           | -0.5573535 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1017282  |
| Train/Loss              | 0.16163628 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1568471  |
| Train/Ratio             | 0.99998283 |
| Train/Return            | 1.6320102  |
| Train/V                 | 1.7888658  |
| Train/Value             | 1.7888658  |
| Train/control_penalty   | 0.42318252 |
| Train/policy_loss       | 0.1568471  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 514 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 513        |
| Time/Actor_Time         | 0.0692     |
| Time/B_Format_Time      | 0.0632     |
| Time/B_Original_Form... | 0.0742     |
| Time/Buffer             | 0.00283    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23487143 |
| Train/Action_magnitu... | 0.52690244 |
| Train/Action_magnitude  | 0.41149327 |
| Train/Action_max        | 0.19808525 |
| Train/Action_std        | 0.14057258 |
| Train/Entropy           | -0.5772016 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1476468  |
| Train/Loss              | 0.15224665 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14755973 |
| Train/Ratio             | 1.0000017  |
| Train/Return            | 1.6476367  |
| Train/V                 | 1.7951945  |
| Train/Value             | 1.7951945  |
| Train/control_penalty   | 0.41097188 |
| Train/policy_loss       | 0.14755973 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 515 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 514         |
| Time/Actor_Time         | 0.0644      |
| Time/B_Format_Time      | 0.0792      |
| Time/B_Original_Form... | 0.104       |
| Time/Buffer             | 0.00331     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24358496  |
| Train/Action_magnitu... | 0.5374864   |
| Train/Action_magnitude  | 0.4179445   |
| Train/Action_max        | 0.19627514  |
| Train/Action_std        | 0.13981666  |
| Train/Entropy           | -0.58537394 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.161048    |
| Train/Loss              | 0.21675138  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21199071  |
| Train/Ratio             | 0.9999554   |
| Train/Return            | 1.6829115   |
| Train/V                 | 1.8949077   |
| Train/Value             | 1.8949077   |
| Train/control_penalty   | 0.41752923  |
| Train/policy_loss       | 0.21199071  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 516 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 515         |
| Time/Actor_Time         | 0.0697      |
| Time/B_Format_Time      | 0.0628      |
| Time/B_Original_Form... | 0.0623      |
| Time/Buffer             | 0.00315     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24357095  |
| Train/Action_magnitu... | 0.5376853   |
| Train/Action_magnitude  | 0.417536    |
| Train/Action_max        | 0.20264892  |
| Train/Action_std        | 0.13828063  |
| Train/Entropy           | -0.59301984 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1872063   |
| Train/Loss              | 0.16141038  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15663117  |
| Train/Ratio             | 0.9999913   |
| Train/Return            | 1.6666251   |
| Train/V                 | 1.8232663   |
| Train/Value             | 1.8232663   |
| Train/control_penalty   | 0.41861898  |
| Train/policy_loss       | 0.15663117  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 517 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 516        |
| Time/Actor_Time         | 0.0627     |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.0734     |
| Time/Buffer             | 0.00344    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24612795 |
| Train/Action_magnitu... | 0.5463773  |
| Train/Action_magnitude  | 0.4259418  |
| Train/Action_max        | 0.1867099  |
| Train/Action_std        | 0.14038724 |
| Train/Entropy           | -0.5806533 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1615976  |
| Train/Loss              | 0.14522229 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14040546 |
| Train/Ratio             | 0.9999723  |
| Train/Return            | 1.6561309  |
| Train/V                 | 1.7965559  |
| Train/Value             | 1.7965559  |
| Train/control_penalty   | 0.42361718 |
| Train/policy_loss       | 0.14040546 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 518 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 517        |
| Time/Actor_Time         | 0.0662     |
| Time/B_Format_Time      | 0.0698     |
| Time/B_Original_Form... | 0.0711     |
| Time/Buffer             | 0.00313    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24076034 |
| Train/Action_magnitu... | 0.5363946  |
| Train/Action_magnitude  | 0.41621706 |
| Train/Action_max        | 0.19948083 |
| Train/Action_std        | 0.14069545 |
| Train/Entropy           | -0.5773217 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.144637   |
| Train/Loss              | 0.13097602 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12623169 |
| Train/Ratio             | 0.9999975  |
| Train/Return            | 1.6805323  |
| Train/V                 | 1.8067732  |
| Train/Value             | 1.8067732  |
| Train/control_penalty   | 0.41670173 |
| Train/policy_loss       | 0.12623169 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 519 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 518        |
| Time/Actor_Time         | 0.0662     |
| Time/B_Format_Time      | 0.0703     |
| Time/B_Original_Form... | 0.0681     |
| Time/Buffer             | 0.00463    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24339047 |
| Train/Action_magnitu... | 0.54552346 |
| Train/Action_magnitude  | 0.4254633  |
| Train/Action_max        | 0.21454544 |
| Train/Action_std        | 0.14241467 |
| Train/Entropy           | -0.5611964 |
| Train/Entropy_Loss      | 0.000561   |
| Train/Entropy_loss      | 0.000561   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1124682  |
| Train/Loss              | 0.23263194 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22785886 |
| Train/Ratio             | 1.0000119  |
| Train/Return            | 1.7012167  |
| Train/V                 | 1.929065   |
| Train/Value             | 1.929065   |
| Train/control_penalty   | 0.42118895 |
| Train/policy_loss       | 0.22785886 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 520 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 519        |
| Time/Actor_Time         | 0.0678     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.0638     |
| Time/Buffer             | 0.00319    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2483196  |
| Train/Action_magnitu... | 0.5521759  |
| Train/Action_magnitude  | 0.42992854 |
| Train/Action_max        | 0.1925102  |
| Train/Action_std        | 0.14309591 |
| Train/Entropy           | -0.5565929 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1173494  |
| Train/Loss              | 0.09875479 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09393429 |
| Train/Ratio             | 1.0000175  |
| Train/Return            | 1.7032746  |
| Train/V                 | 1.7972121  |
| Train/Value             | 1.7972121  |
| Train/control_penalty   | 0.42639068 |
| Train/policy_loss       | 0.09393429 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 521 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 520         |
| Time/Actor_Time         | 0.0634      |
| Time/B_Format_Time      | 0.0676      |
| Time/B_Original_Form... | 0.0658      |
| Time/Buffer             | 0.00516     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23919119  |
| Train/Action_magnitu... | 0.53089195  |
| Train/Action_magnitude  | 0.41216224  |
| Train/Action_max        | 0.19992682  |
| Train/Action_std        | 0.1394573   |
| Train/Entropy           | -0.58595145 |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.170993    |
| Train/Loss              | 0.21928096  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21458781  |
| Train/Ratio             | 1.000011    |
| Train/Return            | 1.7614735   |
| Train/V                 | 1.9760551   |
| Train/Value             | 1.9760551   |
| Train/control_penalty   | 0.4107192   |
| Train/policy_loss       | 0.21458781  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 522 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 521         |
| Time/Actor_Time         | 0.064       |
| Time/B_Format_Time      | 0.0772      |
| Time/B_Original_Form... | 0.0648      |
| Time/Buffer             | 0.00256     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24237269  |
| Train/Action_magnitu... | 0.5347144   |
| Train/Action_magnitude  | 0.41779363  |
| Train/Action_max        | 0.19084947  |
| Train/Action_std        | 0.14040443  |
| Train/Entropy           | -0.57832247 |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1583313   |
| Train/Loss              | 0.17770608  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17292911  |
| Train/Ratio             | 0.9999742   |
| Train/Return            | 1.566206    |
| Train/V                 | 1.7391436   |
| Train/Value             | 1.7391436   |
| Train/control_penalty   | 0.41986474  |
| Train/policy_loss       | 0.17292911  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 523 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 522        |
| Time/Actor_Time         | 0.0634     |
| Time/B_Format_Time      | 0.0706     |
| Time/B_Original_Form... | 0.0629     |
| Time/Buffer             | 0.00256    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23152535 |
| Train/Action_magnitu... | 0.5208323  |
| Train/Action_magnitude  | 0.40651438 |
| Train/Action_max        | 0.19920503 |
| Train/Action_std        | 0.14041735 |
| Train/Entropy           | -0.5810708 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1656123  |
| Train/Loss              | 0.14099953 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13638073 |
| Train/Ratio             | 0.9999961  |
| Train/Return            | 1.486967   |
| Train/V                 | 1.6233423  |
| Train/Value             | 1.6233423  |
| Train/control_penalty   | 0.4037727  |
| Train/policy_loss       | 0.13638073 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 524 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 523        |
| Time/Actor_Time         | 0.0619     |
| Time/B_Format_Time      | 0.0681     |
| Time/B_Original_Form... | 0.0707     |
| Time/Buffer             | 0.00408    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23875979 |
| Train/Action_magnitu... | 0.52933764 |
| Train/Action_magnitude  | 0.41370985 |
| Train/Action_max        | 0.18813884 |
| Train/Action_std        | 0.13941886 |
| Train/Entropy           | -0.5835357 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1458704  |
| Train/Loss              | 0.109502   |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10479179 |
| Train/Ratio             | 0.9999834  |
| Train/Return            | 1.5635602  |
| Train/V                 | 1.6683557  |
| Train/Value             | 1.6683557  |
| Train/control_penalty   | 0.41266778 |
| Train/policy_loss       | 0.10479179 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03175    |
----------------------------------------

 ---------------- Iteration 525 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 524        |
| Time/Actor_Time         | 0.0643     |
| Time/B_Format_Time      | 0.0862     |
| Time/B_Original_Form... | 0.0784     |
| Time/Buffer             | 0.00296    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24019362 |
| Train/Action_magnitu... | 0.53506166 |
| Train/Action_magnitude  | 0.4156011  |
| Train/Action_max        | 0.19558902 |
| Train/Action_std        | 0.14148355 |
| Train/Entropy           | -0.5677241 |
| Train/Entropy_Loss      | 0.000568   |
| Train/Entropy_loss      | 0.000568   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1529573  |
| Train/Loss              | 0.12984383 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12513196 |
| Train/Ratio             | 0.9999999  |
| Train/Return            | 1.5463474  |
| Train/V                 | 1.6714846  |
| Train/Value             | 1.6714846  |
| Train/control_penalty   | 0.41441453 |
| Train/policy_loss       | 0.12513196 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 526 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 525         |
| Time/Actor_Time         | 0.0707      |
| Time/B_Format_Time      | 0.103       |
| Time/B_Original_Form... | 0.0637      |
| Time/Buffer             | 0.0032      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23641096  |
| Train/Action_magnitu... | 0.5286875   |
| Train/Action_magnitude  | 0.41141993  |
| Train/Action_max        | 0.19309276  |
| Train/Action_std        | 0.14007239  |
| Train/Entropy           | -0.57880884 |
| Train/Entropy_Loss      | 0.000579    |
| Train/Entropy_loss      | 0.000579    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1337544   |
| Train/Loss              | 0.14834483  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14365341  |
| Train/Ratio             | 1.0000099   |
| Train/Return            | 1.5562472   |
| Train/V                 | 1.6999038   |
| Train/Value             | 1.6999038   |
| Train/control_penalty   | 0.4112618   |
| Train/policy_loss       | 0.14365341  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 527 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 526         |
| Time/Actor_Time         | 0.0642      |
| Time/B_Format_Time      | 0.0871      |
| Time/B_Original_Form... | 0.0954      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22629535  |
| Train/Action_magnitu... | 0.51464635  |
| Train/Action_magnitude  | 0.40236133  |
| Train/Action_max        | 0.1799847   |
| Train/Action_std        | 0.13994549  |
| Train/Entropy           | -0.58057976 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.158376    |
| Train/Loss              | 0.24688022  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24226828  |
| Train/Ratio             | 1.0000064   |
| Train/Return            | 1.8774582   |
| Train/V                 | 2.1197262   |
| Train/Value             | 2.1197262   |
| Train/control_penalty   | 0.40313628  |
| Train/policy_loss       | 0.24226828  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02125     |
-----------------------------------------

 ---------------- Iteration 528 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 527         |
| Time/Actor_Time         | 0.0907      |
| Time/B_Format_Time      | 0.0666      |
| Time/B_Original_Form... | 0.0715      |
| Time/Buffer             | 0.00275     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2303638   |
| Train/Action_magnitu... | 0.51715523  |
| Train/Action_magnitude  | 0.40527752  |
| Train/Action_max        | 0.16444424  |
| Train/Action_std        | 0.1389981   |
| Train/Entropy           | -0.58676106 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1994957   |
| Train/Loss              | 0.28253284  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.27790755  |
| Train/Ratio             | 1.0000149   |
| Train/Return            | 2.0559795   |
| Train/V                 | 2.3338628   |
| Train/Value             | 2.3338628   |
| Train/control_penalty   | 0.40385553  |
| Train/policy_loss       | 0.27790755  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 529 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 528        |
| Time/Actor_Time         | 0.0639     |
| Time/B_Format_Time      | 0.0686     |
| Time/B_Original_Form... | 0.0677     |
| Time/Buffer             | 0.00258    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23143536 |
| Train/Action_magnitu... | 0.5205347  |
| Train/Action_magnitude  | 0.4087052  |
| Train/Action_max        | 0.16715991 |
| Train/Action_std        | 0.13954657 |
| Train/Entropy           | -0.5838513 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1792616  |
| Train/Loss              | 0.29345432 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2888023  |
| Train/Ratio             | 0.99998635 |
| Train/Return            | 1.9731457  |
| Train/V                 | 2.2619727  |
| Train/Value             | 2.2619727  |
| Train/control_penalty   | 0.4068166  |
| Train/policy_loss       | 0.2888023  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------

 ---------------- Iteration 530 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 529        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.072      |
| Time/B_Original_Form... | 0.0653     |
| Time/Buffer             | 0.00336    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23632441 |
| Train/Action_magnitu... | 0.5308181  |
| Train/Action_magnitude  | 0.41506186 |
| Train/Action_max        | 0.18360116 |
| Train/Action_std        | 0.14183106 |
| Train/Entropy           | -0.5653126 |
| Train/Entropy_Loss      | 0.000565   |
| Train/Entropy_loss      | 0.000565   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1206442  |
| Train/Loss              | 0.21413396 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20940779 |
| Train/Ratio             | 0.9999976  |
| Train/Return            | 1.8809658  |
| Train/V                 | 2.0903795  |
| Train/Value             | 2.0903795  |
| Train/control_penalty   | 0.41608608 |
| Train/policy_loss       | 0.20940779 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 531 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 530        |
| Time/Actor_Time         | 0.0636     |
| Time/B_Format_Time      | 0.0872     |
| Time/B_Original_Form... | 0.0637     |
| Time/Buffer             | 0.0032     |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.227463   |
| Train/Action_magnitu... | 0.5163302  |
| Train/Action_magnitude  | 0.40431967 |
| Train/Action_max        | 0.18525946 |
| Train/Action_std        | 0.14306132 |
| Train/Entropy           | -0.5543796 |
| Train/Entropy_Loss      | 0.000554   |
| Train/Entropy_loss      | 0.000554   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.105494   |
| Train/Loss              | 0.21475172 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21015614 |
| Train/Ratio             | 0.9999917  |
| Train/Return            | 1.7697744  |
| Train/V                 | 1.9799432  |
| Train/Value             | 1.9799432  |
| Train/control_penalty   | 0.40411997 |
| Train/policy_loss       | 0.21015614 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 532 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 531        |
| Time/Actor_Time         | 0.0715     |
| Time/B_Format_Time      | 0.0673     |
| Time/B_Original_Form... | 0.0801     |
| Time/Buffer             | 0.00311    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23739159 |
| Train/Action_magnitu... | 0.53379625 |
| Train/Action_magnitude  | 0.4160998  |
| Train/Action_max        | 0.18389732 |
| Train/Action_std        | 0.1450959  |
| Train/Entropy           | -0.5410457 |
| Train/Entropy_Loss      | 0.000541   |
| Train/Entropy_loss      | 0.000541   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0839199  |
| Train/Loss              | 0.18530864 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18059452 |
| Train/Ratio             | 0.99999917 |
| Train/Return            | 1.931246   |
| Train/V                 | 2.1118453  |
| Train/Value             | 2.1118453  |
| Train/control_penalty   | 0.41730696 |
| Train/policy_loss       | 0.18059452 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 533 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 532        |
| Time/Actor_Time         | 0.0642     |
| Time/B_Format_Time      | 0.088      |
| Time/B_Original_Form... | 0.0648     |
| Time/Buffer             | 0.0038     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24005984 |
| Train/Action_magnitu... | 0.53642285 |
| Train/Action_magnitude  | 0.41950727 |
| Train/Action_max        | 0.18830997 |
| Train/Action_std        | 0.1421382  |
| Train/Entropy           | -0.5632049 |
| Train/Entropy_Loss      | 0.000563   |
| Train/Entropy_loss      | 0.000563   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1206654  |
| Train/Loss              | 0.17622961 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17147812 |
| Train/Ratio             | 0.99998474 |
| Train/Return            | 2.0755796  |
| Train/V                 | 2.2470536  |
| Train/Value             | 2.2470536  |
| Train/control_penalty   | 0.41882804 |
| Train/policy_loss       | 0.17147812 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03625    |
----------------------------------------

 ---------------- Iteration 534 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 533        |
| Time/Actor_Time         | 0.0702     |
| Time/B_Format_Time      | 0.0813     |
| Time/B_Original_Form... | 0.0682     |
| Time/Buffer             | 0.00268    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22921564 |
| Train/Action_magnitu... | 0.5151872  |
| Train/Action_magnitude  | 0.40268138 |
| Train/Action_max        | 0.15438503 |
| Train/Action_std        | 0.13868238 |
| Train/Entropy           | -0.5847614 |
| Train/Entropy_Loss      | 0.000585   |
| Train/Entropy_loss      | 0.000585   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1615394  |
| Train/Loss              | 0.21496631 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21037678 |
| Train/Ratio             | 1.000019   |
| Train/Return            | 1.9544587  |
| Train/V                 | 2.1648276  |
| Train/Value             | 2.1648276  |
| Train/control_penalty   | 0.4004758  |
| Train/policy_loss       | 0.21037678 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 535 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 534        |
| Time/Actor_Time         | 0.0627     |
| Time/B_Format_Time      | 0.0682     |
| Time/B_Original_Form... | 0.0663     |
| Time/Buffer             | 0.0033     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24802531 |
| Train/Action_magnitu... | 0.5528728  |
| Train/Action_magnitude  | 0.43161353 |
| Train/Action_max        | 0.16849823 |
| Train/Action_std        | 0.14718686 |
| Train/Entropy           | -0.5293505 |
| Train/Entropy_Loss      | 0.000529   |
| Train/Entropy_loss      | 0.000529   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0604339  |
| Train/Loss              | 0.23143505 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2265308  |
| Train/Ratio             | 0.9999984  |
| Train/Return            | 1.9833096  |
| Train/V                 | 2.209828   |
| Train/Value             | 2.209828   |
| Train/control_penalty   | 0.4374887  |
| Train/policy_loss       | 0.2265308  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 536 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 535        |
| Time/Actor_Time         | 0.0773     |
| Time/B_Format_Time      | 0.0645     |
| Time/B_Original_Form... | 0.0747     |
| Time/Buffer             | 0.0035     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22625165 |
| Train/Action_magnitu... | 0.5197003  |
| Train/Action_magnitude  | 0.40908253 |
| Train/Action_max        | 0.16764417 |
| Train/Action_std        | 0.14444391 |
| Train/Entropy           | -0.5484795 |
| Train/Entropy_Loss      | 0.000548   |
| Train/Entropy_loss      | 0.000548   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1055863  |
| Train/Loss              | 0.29969126 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.29508248 |
| Train/Ratio             | 0.99999166 |
| Train/Return            | 2.222568   |
| Train/V                 | 2.5176466  |
| Train/Value             | 2.5176466  |
| Train/control_penalty   | 0.40602946 |
| Train/policy_loss       | 0.29508248 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 537 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 536        |
| Time/Actor_Time         | 0.0701     |
| Time/B_Format_Time      | 0.0648     |
| Time/B_Original_Form... | 0.0814     |
| Time/Buffer             | 0.00366    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23411293 |
| Train/Action_magnitu... | 0.5335195  |
| Train/Action_magnitude  | 0.4183812  |
| Train/Action_max        | 0.18682061 |
| Train/Action_std        | 0.14343329 |
| Train/Entropy           | -0.5561701 |
| Train/Entropy_Loss      | 0.000556   |
| Train/Entropy_loss      | 0.000556   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1152095  |
| Train/Loss              | 0.28027904 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.27554786 |
| Train/Ratio             | 0.9999934  |
| Train/Return            | 2.050433   |
| Train/V                 | 2.325992   |
| Train/Value             | 2.325992   |
| Train/control_penalty   | 0.41750157 |
| Train/policy_loss       | 0.27554786 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 538 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 537        |
| Time/Actor_Time         | 0.0817     |
| Time/B_Format_Time      | 0.0641     |
| Time/B_Original_Form... | 0.0886     |
| Time/Buffer             | 0.0027     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24454875 |
| Train/Action_magnitu... | 0.5479458  |
| Train/Action_magnitude  | 0.4282443  |
| Train/Action_max        | 0.1918592  |
| Train/Action_std        | 0.14236109 |
| Train/Entropy           | -0.5619391 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1260448  |
| Train/Loss              | 0.21478453 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20993009 |
| Train/Ratio             | 1.0000107  |
| Train/Return            | 1.9991481  |
| Train/V                 | 2.2090738  |
| Train/Value             | 2.2090738  |
| Train/control_penalty   | 0.42924994 |
| Train/policy_loss       | 0.20993009 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 539 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 538         |
| Time/Actor_Time         | 0.0639      |
| Time/B_Format_Time      | 0.0656      |
| Time/B_Original_Form... | 0.0666      |
| Time/Buffer             | 0.00266     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22705121  |
| Train/Action_magnitu... | 0.51388997  |
| Train/Action_magnitude  | 0.4022621   |
| Train/Action_max        | 0.176918    |
| Train/Action_std        | 0.13928959  |
| Train/Entropy           | -0.58579415 |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1732328   |
| Train/Loss              | 0.3070416   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.3024153   |
| Train/Ratio             | 1.000005    |
| Train/Return            | 2.2738233   |
| Train/V                 | 2.5762324   |
| Train/Value             | 2.5762324   |
| Train/control_penalty   | 0.40404764  |
| Train/policy_loss       | 0.3024153   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 540 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 539         |
| Time/Actor_Time         | 0.0836      |
| Time/B_Format_Time      | 0.101       |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.00258     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23857333  |
| Train/Action_magnitu... | 0.5315086   |
| Train/Action_magnitude  | 0.4136668   |
| Train/Action_max        | 0.18993087  |
| Train/Action_std        | 0.14074005  |
| Train/Entropy           | -0.57482964 |
| Train/Entropy_Loss      | 0.000575    |
| Train/Entropy_loss      | 0.000575    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1491328   |
| Train/Loss              | 0.25127497  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24653564  |
| Train/Ratio             | 0.99999785  |
| Train/Return            | 1.942498    |
| Train/V                 | 2.1890264   |
| Train/Value             | 2.1890264   |
| Train/control_penalty   | 0.41644946  |
| Train/policy_loss       | 0.24653564  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 541 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 540         |
| Time/Actor_Time         | 0.0737      |
| Time/B_Format_Time      | 0.0642      |
| Time/B_Original_Form... | 0.0747      |
| Time/Buffer             | 0.00338     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24047986  |
| Train/Action_magnitu... | 0.5343537   |
| Train/Action_magnitude  | 0.41549164  |
| Train/Action_max        | 0.18728022  |
| Train/Action_std        | 0.13916999  |
| Train/Entropy           | -0.58270204 |
| Train/Entropy_Loss      | 0.000583    |
| Train/Entropy_loss      | 0.000583    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1820276   |
| Train/Loss              | 0.23396175  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2292508   |
| Train/Ratio             | 1.0000067   |
| Train/Return            | 2.0413678   |
| Train/V                 | 2.2706149   |
| Train/Value             | 2.2706149   |
| Train/control_penalty   | 0.41282544  |
| Train/policy_loss       | 0.2292508   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 542 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 541         |
| Time/Actor_Time         | 0.0721      |
| Time/B_Format_Time      | 0.0694      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23567973  |
| Train/Action_magnitu... | 0.53111416  |
| Train/Action_magnitude  | 0.41372412  |
| Train/Action_max        | 0.17430209  |
| Train/Action_std        | 0.14024054  |
| Train/Entropy           | -0.57614934 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1622014   |
| Train/Loss              | 0.340582    |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.3358586   |
| Train/Ratio             | 1.0000011   |
| Train/Return            | 1.9772838   |
| Train/V                 | 2.313143    |
| Train/Value             | 2.313143    |
| Train/control_penalty   | 0.4147268   |
| Train/policy_loss       | 0.3358586   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02125     |
-----------------------------------------

 ---------------- Iteration 543 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 542         |
| Time/Actor_Time         | 0.0627      |
| Time/B_Format_Time      | 0.0633      |
| Time/B_Original_Form... | 0.0667      |
| Time/Buffer             | 0.00305     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23810688  |
| Train/Action_magnitu... | 0.5345144   |
| Train/Action_magnitude  | 0.4164293   |
| Train/Action_max        | 0.1946031   |
| Train/Action_std        | 0.14195317  |
| Train/Entropy           | -0.56587577 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1497647   |
| Train/Loss              | 0.15699896  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15226467  |
| Train/Ratio             | 1.0000066   |
| Train/Return            | 1.6443062   |
| Train/V                 | 1.7965702   |
| Train/Value             | 1.7965702   |
| Train/control_penalty   | 0.41684186  |
| Train/policy_loss       | 0.15226467  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 544 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 543         |
| Time/Actor_Time         | 0.0695      |
| Time/B_Format_Time      | 0.0678      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00488     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.24351273  |
| Train/Action_magnitu... | 0.5400416   |
| Train/Action_magnitude  | 0.42031842  |
| Train/Action_max        | 0.20851341  |
| Train/Action_std        | 0.14118321  |
| Train/Entropy           | -0.57039434 |
| Train/Entropy_Loss      | 0.00057     |
| Train/Entropy_loss      | 0.00057     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1249151   |
| Train/Loss              | 0.20382439  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19902316  |
| Train/Ratio             | 0.9999694   |
| Train/Return            | 1.7231721   |
| Train/V                 | 1.9222044   |
| Train/Value             | 1.9222044   |
| Train/control_penalty   | 0.42308223  |
| Train/policy_loss       | 0.19902316  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 545 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 544         |
| Time/Actor_Time         | 0.0626      |
| Time/B_Format_Time      | 0.0667      |
| Time/B_Original_Form... | 0.0634      |
| Time/Buffer             | 0.00299     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2414825   |
| Train/Action_magnitu... | 0.53971744  |
| Train/Action_magnitude  | 0.4211078   |
| Train/Action_max        | 0.209123    |
| Train/Action_std        | 0.14469424  |
| Train/Entropy           | -0.54744154 |
| Train/Entropy_Loss      | 0.000547    |
| Train/Entropy_loss      | 0.000547    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0837283   |
| Train/Loss              | 0.20595282  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20116709  |
| Train/Ratio             | 0.9999782   |
| Train/Return            | 1.634656    |
| Train/V                 | 1.8358346   |
| Train/Value             | 1.8358346   |
| Train/control_penalty   | 0.4238288   |
| Train/policy_loss       | 0.20116709  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 546 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 545        |
| Time/Actor_Time         | 0.0643     |
| Time/B_Format_Time      | 0.0823     |
| Time/B_Original_Form... | 0.0666     |
| Time/Buffer             | 0.00259    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24227405 |
| Train/Action_magnitu... | 0.53585243 |
| Train/Action_magnitude  | 0.41568372 |
| Train/Action_max        | 0.21275046 |
| Train/Action_std        | 0.13985085 |
| Train/Entropy           | -0.5791826 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1512437  |
| Train/Loss              | 0.19572297 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19098319 |
| Train/Ratio             | 1.000007   |
| Train/Return            | 1.7027771  |
| Train/V                 | 1.8937616  |
| Train/Value             | 1.8937616  |
| Train/control_penalty   | 0.41606036 |
| Train/policy_loss       | 0.19098319 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 547 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 546        |
| Time/Actor_Time         | 0.0647     |
| Time/B_Format_Time      | 0.0826     |
| Time/B_Original_Form... | 0.0638     |
| Time/Buffer             | 0.00298    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23198013 |
| Train/Action_magnitu... | 0.52303547 |
| Train/Action_magnitude  | 0.4074707  |
| Train/Action_max        | 0.19034751 |
| Train/Action_std        | 0.14220062 |
| Train/Entropy           | -0.5645635 |
| Train/Entropy_Loss      | 0.000565   |
| Train/Entropy_loss      | 0.000565   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1254122  |
| Train/Loss              | 0.18903162 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18438257 |
| Train/Ratio             | 0.9999985  |
| Train/Return            | 1.6816623  |
| Train/V                 | 1.866045   |
| Train/Value             | 1.866045   |
| Train/control_penalty   | 0.40844768 |
| Train/policy_loss       | 0.18438257 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02325    |
----------------------------------------

 ---------------- Iteration 548 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 547        |
| Time/Actor_Time         | 0.071      |
| Time/B_Format_Time      | 0.0726     |
| Time/B_Original_Form... | 0.0672     |
| Time/Buffer             | 0.00247    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23002747 |
| Train/Action_magnitu... | 0.5192711  |
| Train/Action_magnitude  | 0.40153828 |
| Train/Action_max        | 0.19153473 |
| Train/Action_std        | 0.13943735 |
| Train/Entropy           | -0.5868219 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1967225  |
| Train/Loss              | 0.14996013 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14537404 |
| Train/Ratio             | 0.9999881  |
| Train/Return            | 1.5550802  |
| Train/V                 | 1.7004545  |
| Train/Value             | 1.7004545  |
| Train/control_penalty   | 0.3999265  |
| Train/policy_loss       | 0.14537404 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 549 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 548        |
| Time/Actor_Time         | 0.0649     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.081      |
| Time/Buffer             | 0.00316    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22982484 |
| Train/Action_magnitu... | 0.51483    |
| Train/Action_magnitude  | 0.40150088 |
| Train/Action_max        | 0.20359133 |
| Train/Action_std        | 0.13969043 |
| Train/Entropy           | -0.5842518 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1646109  |
| Train/Loss              | 0.19223961 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18764322 |
| Train/Ratio             | 0.9999905  |
| Train/Return            | 1.6842986  |
| Train/V                 | 1.871945   |
| Train/Value             | 1.871945   |
| Train/control_penalty   | 0.40121564 |
| Train/policy_loss       | 0.18764322 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 550 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 549        |
| Time/Actor_Time         | 0.0634     |
| Time/B_Format_Time      | 0.0669     |
| Time/B_Original_Form... | 0.0673     |
| Time/Buffer             | 0.00273    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2370218  |
| Train/Action_magnitu... | 0.5258724  |
| Train/Action_magnitude  | 0.40734082 |
| Train/Action_max        | 0.21243477 |
| Train/Action_std        | 0.13914713 |
| Train/Entropy           | -0.5892836 |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1785431  |
| Train/Loss              | 0.1835168  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17885315 |
| Train/Ratio             | 1.0000027  |
| Train/Return            | 1.501089   |
| Train/V                 | 1.67994    |
| Train/Value             | 1.67994    |
| Train/control_penalty   | 0.40743658 |
| Train/policy_loss       | 0.17885315 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.019      |
----------------------------------------

 ---------------- Iteration 551 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 550        |
| Time/Actor_Time         | 0.0725     |
| Time/B_Format_Time      | 0.0622     |
| Time/B_Original_Form... | 0.0712     |
| Time/Buffer             | 0.0026     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24358048 |
| Train/Action_magnitu... | 0.546982   |
| Train/Action_magnitude  | 0.4259297  |
| Train/Action_max        | 0.22423945 |
| Train/Action_std        | 0.14368705 |
| Train/Entropy           | -0.553775  |
| Train/Entropy_Loss      | 0.000554   |
| Train/Entropy_loss      | 0.000554   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0919536  |
| Train/Loss              | 0.13669643 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1319383  |
| Train/Ratio             | 0.9999954  |
| Train/Return            | 1.4766712  |
| Train/V                 | 1.6086084  |
| Train/Value             | 1.6086084  |
| Train/control_penalty   | 0.420436   |
| Train/policy_loss       | 0.1319383  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 552 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 551         |
| Time/Actor_Time         | 0.0623      |
| Time/B_Format_Time      | 0.0661      |
| Time/B_Original_Form... | 0.0657      |
| Time/Buffer             | 0.00525     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.24117346  |
| Train/Action_magnitu... | 0.54260623  |
| Train/Action_magnitude  | 0.42109534  |
| Train/Action_max        | 0.23071155  |
| Train/Action_std        | 0.14303674  |
| Train/Entropy           | -0.56320184 |
| Train/Entropy_Loss      | 0.000563    |
| Train/Entropy_loss      | 0.000563    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1033946   |
| Train/Loss              | 0.16914149  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16438976  |
| Train/Ratio             | 0.99999887  |
| Train/Return            | 1.5816793   |
| Train/V                 | 1.7460738   |
| Train/Value             | 1.7460738   |
| Train/control_penalty   | 0.41885206  |
| Train/policy_loss       | 0.16438976  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 553 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 552         |
| Time/Actor_Time         | 0.0618      |
| Time/B_Format_Time      | 0.0681      |
| Time/B_Original_Form... | 0.0684      |
| Time/Buffer             | 0.00325     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24303994  |
| Train/Action_magnitu... | 0.54848236  |
| Train/Action_magnitude  | 0.42910916  |
| Train/Action_max        | 0.20502245  |
| Train/Action_std        | 0.144922    |
| Train/Entropy           | -0.54722464 |
| Train/Entropy_Loss      | 0.000547    |
| Train/Entropy_loss      | 0.000547    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0671465   |
| Train/Loss              | 0.096192256 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09137492  |
| Train/Ratio             | 1.000011    |
| Train/Return            | 1.673745    |
| Train/V                 | 1.7651188   |
| Train/Value             | 1.7651188   |
| Train/control_penalty   | 0.42701128  |
| Train/policy_loss       | 0.09137492  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03625     |
-----------------------------------------

 ---------------- Iteration 554 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 553         |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0677      |
| Time/B_Original_Form... | 0.0808      |
| Time/Buffer             | 0.00379     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23799896  |
| Train/Action_magnitu... | 0.5404742   |
| Train/Action_magnitude  | 0.42159662  |
| Train/Action_max        | 0.2070304   |
| Train/Action_std        | 0.1483587   |
| Train/Entropy           | -0.52353865 |
| Train/Entropy_Loss      | 0.000524    |
| Train/Entropy_loss      | 0.000524    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0299891   |
| Train/Loss              | 0.1535414   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14881846  |
| Train/Ratio             | 1.0000122   |
| Train/Return            | 1.5427632   |
| Train/V                 | 1.69158     |
| Train/Value             | 1.69158     |
| Train/control_penalty   | 0.4199402   |
| Train/policy_loss       | 0.14881846  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 555 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 554        |
| Time/Actor_Time         | 0.0635     |
| Time/B_Format_Time      | 0.0915     |
| Time/B_Original_Form... | 0.0642     |
| Time/Buffer             | 0.00701    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23681518 |
| Train/Action_magnitu... | 0.5286516  |
| Train/Action_magnitude  | 0.41136828 |
| Train/Action_max        | 0.21250854 |
| Train/Action_std        | 0.14151722 |
| Train/Entropy           | -0.5702677 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1414816  |
| Train/Loss              | 0.12284618 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11816134 |
| Train/Ratio             | 0.9999724  |
| Train/Return            | 1.6159962  |
| Train/V                 | 1.7341654  |
| Train/Value             | 1.7341654  |
| Train/control_penalty   | 0.41145718 |
| Train/policy_loss       | 0.11816134 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0295     |
----------------------------------------

 ---------------- Iteration 556 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 555         |
| Time/Actor_Time         | 0.075       |
| Time/B_Format_Time      | 0.0683      |
| Time/B_Original_Form... | 0.0825      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23986875  |
| Train/Action_magnitu... | 0.5413569   |
| Train/Action_magnitude  | 0.4212432   |
| Train/Action_max        | 0.20299534  |
| Train/Action_std        | 0.14734977  |
| Train/Entropy           | -0.53033483 |
| Train/Entropy_Loss      | 0.00053     |
| Train/Entropy_loss      | 0.00053     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.060167    |
| Train/Loss              | 0.17405689  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16928065  |
| Train/Ratio             | 1.0000187   |
| Train/Return            | 1.4497725   |
| Train/V                 | 1.6190499   |
| Train/Value             | 1.6190499   |
| Train/control_penalty   | 0.42459083  |
| Train/policy_loss       | 0.16928065  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 557 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 556        |
| Time/Actor_Time         | 0.0735     |
| Time/B_Format_Time      | 0.0657     |
| Time/B_Original_Form... | 0.0733     |
| Time/Buffer             | 0.00314    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23999545 |
| Train/Action_magnitu... | 0.543524   |
| Train/Action_magnitude  | 0.42246103 |
| Train/Action_max        | 0.21256922 |
| Train/Action_std        | 0.1412622  |
| Train/Entropy           | -0.5723873 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1367979  |
| Train/Loss              | 0.13097294 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12621462 |
| Train/Ratio             | 1.0000015  |
| Train/Return            | 1.5949731  |
| Train/V                 | 1.7211756  |
| Train/Value             | 1.7211756  |
| Train/control_penalty   | 0.41859326 |
| Train/policy_loss       | 0.12621462 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 558 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 557        |
| Time/Actor_Time         | 0.0946     |
| Time/B_Format_Time      | 0.0634     |
| Time/B_Original_Form... | 0.0706     |
| Time/Buffer             | 0.00345    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23256724 |
| Train/Action_magnitu... | 0.5238674  |
| Train/Action_magnitude  | 0.4093082  |
| Train/Action_max        | 0.19486791 |
| Train/Action_std        | 0.14250197 |
| Train/Entropy           | -0.563697  |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1232431  |
| Train/Loss              | 0.13393098 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12926076 |
| Train/Ratio             | 0.9999993  |
| Train/Return            | 1.6159505  |
| Train/V                 | 1.7452091  |
| Train/Value             | 1.7452091  |
| Train/control_penalty   | 0.41065288 |
| Train/policy_loss       | 0.12926076 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.032      |
----------------------------------------

 ---------------- Iteration 559 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 558         |
| Time/Actor_Time         | 0.0638      |
| Time/B_Format_Time      | 0.0682      |
| Time/B_Original_Form... | 0.0702      |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24765351  |
| Train/Action_magnitu... | 0.5544611   |
| Train/Action_magnitude  | 0.43200445  |
| Train/Action_max        | 0.22799836  |
| Train/Action_std        | 0.14364877  |
| Train/Entropy           | -0.55632514 |
| Train/Entropy_Loss      | 0.000556    |
| Train/Entropy_loss      | 0.000556    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1338617   |
| Train/Loss              | 0.09171935  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08685784  |
| Train/Ratio             | 0.9999583   |
| Train/Return            | 1.8778485   |
| Train/V                 | 1.9647083   |
| Train/Value             | 1.9647083   |
| Train/control_penalty   | 0.4305181   |
| Train/policy_loss       | 0.08685784  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03925     |
-----------------------------------------

 ---------------- Iteration 560 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 559        |
| Time/Actor_Time         | 0.0763     |
| Time/B_Format_Time      | 0.0641     |
| Time/B_Original_Form... | 0.0717     |
| Time/Buffer             | 0.00284    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23145066 |
| Train/Action_magnitu... | 0.5290636  |
| Train/Action_magnitude  | 0.41153178 |
| Train/Action_max        | 0.2088507  |
| Train/Action_std        | 0.14443842 |
| Train/Entropy           | -0.5533692 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0976613  |
| Train/Loss              | 0.1359759  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13130896 |
| Train/Ratio             | 1.0000206  |
| Train/Return            | 1.5149186  |
| Train/V                 | 1.6462276  |
| Train/Value             | 1.6462276  |
| Train/control_penalty   | 0.4113569  |
| Train/policy_loss       | 0.13130896 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 561 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 560        |
| Time/Actor_Time         | 0.0897     |
| Time/B_Format_Time      | 0.0646     |
| Time/B_Original_Form... | 0.0893     |
| Time/Buffer             | 0.00333    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24122818 |
| Train/Action_magnitu... | 0.54115874 |
| Train/Action_magnitude  | 0.4209269  |
| Train/Action_max        | 0.23082049 |
| Train/Action_std        | 0.14372015 |
| Train/Entropy           | -0.5578731 |
| Train/Entropy_Loss      | 0.000558   |
| Train/Entropy_loss      | 0.000558   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1087763  |
| Train/Loss              | 0.13090415 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12614933 |
| Train/Ratio             | 1.0000119  |
| Train/Return            | 1.6550384  |
| Train/V                 | 1.7811884  |
| Train/Value             | 1.7811884  |
| Train/control_penalty   | 0.41969553 |
| Train/policy_loss       | 0.12614933 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03225    |
----------------------------------------

 ---------------- Iteration 562 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 561        |
| Time/Actor_Time         | 0.0699     |
| Time/B_Format_Time      | 0.0763     |
| Time/B_Original_Form... | 0.0644     |
| Time/Buffer             | 0.00397    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23111896 |
| Train/Action_magnitu... | 0.5242676  |
| Train/Action_magnitude  | 0.4089501  |
| Train/Action_max        | 0.20409383 |
| Train/Action_std        | 0.14134832 |
| Train/Entropy           | -0.5737554 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1389514  |
| Train/Loss              | 0.17418817 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16952088 |
| Train/Ratio             | 1.0000281  |
| Train/Return            | 1.3886094  |
| Train/V                 | 1.5581311  |
| Train/Value             | 1.5581311  |
| Train/control_penalty   | 0.40935308 |
| Train/policy_loss       | 0.16952088 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 563 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 562        |
| Time/Actor_Time         | 0.0822     |
| Time/B_Format_Time      | 0.0656     |
| Time/B_Original_Form... | 0.0918     |
| Time/Buffer             | 0.00357    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.238814   |
| Train/Action_magnitu... | 0.53205365 |
| Train/Action_magnitude  | 0.41586363 |
| Train/Action_max        | 0.20301321 |
| Train/Action_std        | 0.14337738 |
| Train/Entropy           | -0.5580077 |
| Train/Entropy_Loss      | 0.000558   |
| Train/Entropy_loss      | 0.000558   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1422832  |
| Train/Loss              | 0.12110098 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11638937 |
| Train/Ratio             | 0.99999493 |
| Train/Return            | 1.8033047  |
| Train/V                 | 1.9196941  |
| Train/Value             | 1.9196941  |
| Train/control_penalty   | 0.41535956 |
| Train/policy_loss       | 0.11638937 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.038      |
----------------------------------------

 ---------------- Iteration 564 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 563        |
| Time/Actor_Time         | 0.0635     |
| Time/B_Format_Time      | 0.0741     |
| Time/B_Original_Form... | 0.0969     |
| Time/Buffer             | 0.00315    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22650602 |
| Train/Action_magnitu... | 0.51483136 |
| Train/Action_magnitude  | 0.40247363 |
| Train/Action_max        | 0.20005882 |
| Train/Action_std        | 0.14346561 |
| Train/Entropy           | -0.559551  |
| Train/Entropy_Loss      | 0.00056    |
| Train/Entropy_loss      | 0.00056    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1379491  |
| Train/Loss              | 0.0846149  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08003281 |
| Train/Ratio             | 1.0000267  |
| Train/Return            | 1.5460721  |
| Train/V                 | 1.6261027  |
| Train/Value             | 1.6261027  |
| Train/control_penalty   | 0.40225422 |
| Train/policy_loss       | 0.08003281 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 565 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 564         |
| Time/Actor_Time         | 0.0655      |
| Time/B_Format_Time      | 0.0825      |
| Time/B_Original_Form... | 0.0711      |
| Time/Buffer             | 0.00308     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24179684  |
| Train/Action_magnitu... | 0.5446476   |
| Train/Action_magnitude  | 0.42769605  |
| Train/Action_max        | 0.21948189  |
| Train/Action_std        | 0.1448721   |
| Train/Entropy           | -0.55089957 |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1036999   |
| Train/Loss              | 0.17632553  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17149508  |
| Train/Ratio             | 0.99998087  |
| Train/Return            | 1.8110808   |
| Train/V                 | 1.9825947   |
| Train/Value             | 1.9825947   |
| Train/control_penalty   | 0.4279558   |
| Train/policy_loss       | 0.17149508  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 566 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 565        |
| Time/Actor_Time         | 0.0628     |
| Time/B_Format_Time      | 0.0668     |
| Time/B_Original_Form... | 0.0662     |
| Time/Buffer             | 0.00464    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23120596 |
| Train/Action_magnitu... | 0.5206316  |
| Train/Action_magnitude  | 0.40604872 |
| Train/Action_max        | 0.21105103 |
| Train/Action_std        | 0.14061806 |
| Train/Entropy           | -0.5837738 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1681668  |
| Train/Loss              | 0.13918047 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13450025 |
| Train/Ratio             | 0.99996877 |
| Train/Return            | 1.625472   |
| Train/V                 | 1.7599827  |
| Train/Value             | 1.7599827  |
| Train/control_penalty   | 0.40964487 |
| Train/policy_loss       | 0.13450025 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 567 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 566        |
| Time/Actor_Time         | 0.0634     |
| Time/B_Format_Time      | 0.0635     |
| Time/B_Original_Form... | 0.0644     |
| Time/Buffer             | 0.00341    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23700319 |
| Train/Action_magnitu... | 0.53145003 |
| Train/Action_magnitude  | 0.4139118  |
| Train/Action_max        | 0.2012108  |
| Train/Action_std        | 0.14085491 |
| Train/Entropy           | -0.5790272 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1812863  |
| Train/Loss              | 0.16164362 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15692705 |
| Train/Ratio             | 0.99998486 |
| Train/Return            | 1.5225644  |
| Train/V                 | 1.6794958  |
| Train/Value             | 1.6794958  |
| Train/control_penalty   | 0.41375452 |
| Train/policy_loss       | 0.15692705 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 568 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 567        |
| Time/Actor_Time         | 0.0639     |
| Time/B_Format_Time      | 0.0624     |
| Time/B_Original_Form... | 0.0623     |
| Time/Buffer             | 0.0026     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23153985 |
| Train/Action_magnitu... | 0.52095515 |
| Train/Action_magnitude  | 0.4055695  |
| Train/Action_max        | 0.2137655  |
| Train/Action_std        | 0.14027096 |
| Train/Entropy           | -0.581936  |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1641983  |
| Train/Loss              | 0.10899047 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10440453 |
| Train/Ratio             | 1.0000193  |
| Train/Return            | 1.6476363  |
| Train/V                 | 1.7520382  |
| Train/Value             | 1.7520382  |
| Train/control_penalty   | 0.40040004 |
| Train/policy_loss       | 0.10440453 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 569 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 568         |
| Time/Actor_Time         | 0.0635      |
| Time/B_Format_Time      | 0.0634      |
| Time/B_Original_Form... | 0.0626      |
| Time/Buffer             | 0.00324     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2365785   |
| Train/Action_magnitu... | 0.53157014  |
| Train/Action_magnitude  | 0.41348228  |
| Train/Action_max        | 0.20733696  |
| Train/Action_std        | 0.14317325  |
| Train/Entropy           | -0.56244457 |
| Train/Entropy_Loss      | 0.000562    |
| Train/Entropy_loss      | 0.000562    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.113918    |
| Train/Loss              | 0.13950193  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13481496  |
| Train/Ratio             | 1.0000013   |
| Train/Return            | 1.5609505   |
| Train/V                 | 1.695765    |
| Train/Value             | 1.695765    |
| Train/control_penalty   | 0.41245225  |
| Train/policy_loss       | 0.13481496  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 570 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 569         |
| Time/Actor_Time         | 0.0616      |
| Time/B_Format_Time      | 0.0608      |
| Time/B_Original_Form... | 0.0615      |
| Time/Buffer             | 0.00268     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22956756  |
| Train/Action_magnitu... | 0.5192824   |
| Train/Action_magnitude  | 0.40573174  |
| Train/Action_max        | 0.20723544  |
| Train/Action_std        | 0.14393738  |
| Train/Entropy           | -0.55378544 |
| Train/Entropy_Loss      | 0.000554    |
| Train/Entropy_loss      | 0.000554    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1314697   |
| Train/Loss              | 0.15290102  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1482694   |
| Train/Ratio             | 1.0000027   |
| Train/Return            | 1.6027982   |
| Train/V                 | 1.7510601   |
| Train/Value             | 1.7510601   |
| Train/control_penalty   | 0.4077843   |
| Train/policy_loss       | 0.1482694   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 571 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 570        |
| Time/Actor_Time         | 0.062      |
| Time/B_Format_Time      | 0.0628     |
| Time/B_Original_Form... | 0.0631     |
| Time/Buffer             | 0.00288    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23284574 |
| Train/Action_magnitu... | 0.5209059  |
| Train/Action_magnitude  | 0.4073706  |
| Train/Action_max        | 0.1956251  |
| Train/Action_std        | 0.14137065 |
| Train/Entropy           | -0.5743166 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1531179  |
| Train/Loss              | 0.16474342 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16006495 |
| Train/Ratio             | 1.0000029  |
| Train/Return            | 1.6779132  |
| Train/V                 | 1.837977   |
| Train/Value             | 1.837977   |
| Train/control_penalty   | 0.410416   |
| Train/policy_loss       | 0.16006495 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 572 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 571        |
| Time/Actor_Time         | 0.0627     |
| Time/B_Format_Time      | 0.0617     |
| Time/B_Original_Form... | 0.0631     |
| Time/Buffer             | 0.00243    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22121668 |
| Train/Action_magnitu... | 0.5042466  |
| Train/Action_magnitude  | 0.3930356  |
| Train/Action_max        | 0.19998972 |
| Train/Action_std        | 0.13914932 |
| Train/Entropy           | -0.5933529 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1723338  |
| Train/Loss              | 0.14199485 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13749555 |
| Train/Ratio             | 1.0000175  |
| Train/Return            | 1.3949904  |
| Train/V                 | 1.5324866  |
| Train/Value             | 1.5324866  |
| Train/control_penalty   | 0.39059502 |
| Train/policy_loss       | 0.13749555 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 573 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 572         |
| Time/Actor_Time         | 0.0639      |
| Time/B_Format_Time      | 0.065       |
| Time/B_Original_Form... | 0.0638      |
| Time/Buffer             | 0.00335     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23747699  |
| Train/Action_magnitu... | 0.5378646   |
| Train/Action_magnitude  | 0.41863537  |
| Train/Action_max        | 0.21600327  |
| Train/Action_std        | 0.1451525   |
| Train/Entropy           | -0.54948205 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0917771   |
| Train/Loss              | 0.12113099  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11639813  |
| Train/Ratio             | 0.9999939   |
| Train/Return            | 1.525391    |
| Train/V                 | 1.6417863   |
| Train/Value             | 1.6417863   |
| Train/control_penalty   | 0.41833746  |
| Train/policy_loss       | 0.11639813  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 574 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 573        |
| Time/Actor_Time         | 0.0634     |
| Time/B_Format_Time      | 0.0618     |
| Time/B_Original_Form... | 0.0615     |
| Time/Buffer             | 0.00526    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23683502 |
| Train/Action_magnitu... | 0.53681934 |
| Train/Action_magnitude  | 0.4211862  |
| Train/Action_max        | 0.20271954 |
| Train/Action_std        | 0.1448777  |
| Train/Entropy           | -0.5446041 |
| Train/Entropy_Loss      | 0.000545   |
| Train/Entropy_loss      | 0.000545   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0834593  |
| Train/Loss              | 0.15650508 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15177205 |
| Train/Ratio             | 1.0000085  |
| Train/Return            | 1.6650312  |
| Train/V                 | 1.8168043  |
| Train/Value             | 1.8168043  |
| Train/control_penalty   | 0.4188414  |
| Train/policy_loss       | 0.15177205 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 575 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 574        |
| Time/Actor_Time         | 0.0616     |
| Time/B_Format_Time      | 0.0632     |
| Time/B_Original_Form... | 0.0634     |
| Time/Buffer             | 0.00242    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22322659 |
| Train/Action_magnitu... | 0.5100932  |
| Train/Action_magnitude  | 0.39785093 |
| Train/Action_max        | 0.19530326 |
| Train/Action_std        | 0.14357969 |
| Train/Entropy           | -0.5568455 |
| Train/Entropy_Loss      | 0.000557   |
| Train/Entropy_loss      | 0.000557   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1171373  |
| Train/Loss              | 0.17783223 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17331618 |
| Train/Ratio             | 0.9999839  |
| Train/Return            | 1.596721   |
| Train/V                 | 1.7700348  |
| Train/Value             | 1.7700348  |
| Train/control_penalty   | 0.3959212  |
| Train/policy_loss       | 0.17331618 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 576 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 575         |
| Time/Actor_Time         | 0.0626      |
| Time/B_Format_Time      | 0.0611      |
| Time/B_Original_Form... | 0.0609      |
| Time/Buffer             | 0.00262     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22487985  |
| Train/Action_magnitu... | 0.50518703  |
| Train/Action_magnitude  | 0.39486086  |
| Train/Action_max        | 0.19667374  |
| Train/Action_std        | 0.13940725  |
| Train/Entropy           | -0.59013444 |
| Train/Entropy_Loss      | 0.00059     |
| Train/Entropy_loss      | 0.00059     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1475431   |
| Train/Loss              | 0.17302416  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16850668  |
| Train/Ratio             | 1.0000236   |
| Train/Return            | 1.6563233   |
| Train/V                 | 1.8248156   |
| Train/Value             | 1.8248156   |
| Train/control_penalty   | 0.39273447  |
| Train/policy_loss       | 0.16850668  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 577 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 576        |
| Time/Actor_Time         | 0.0626     |
| Time/B_Format_Time      | 0.0607     |
| Time/B_Original_Form... | 0.0609     |
| Time/Buffer             | 0.00255    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23725101 |
| Train/Action_magnitu... | 0.5437546  |
| Train/Action_magnitude  | 0.4242453  |
| Train/Action_max        | 0.20962903 |
| Train/Action_std        | 0.14749086 |
| Train/Entropy           | -0.5270902 |
| Train/Entropy_Loss      | 0.000527   |
| Train/Entropy_loss      | 0.000527   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.036803   |
| Train/Loss              | 0.17636544 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17164047 |
| Train/Ratio             | 0.9999949  |
| Train/Return            | 1.5639021  |
| Train/V                 | 1.7355494  |
| Train/Value             | 1.7355494  |
| Train/control_penalty   | 0.41978845 |
| Train/policy_loss       | 0.17164047 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02025    |
----------------------------------------

 ---------------- Iteration 578 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 577         |
| Time/Actor_Time         | 0.0632      |
| Time/B_Format_Time      | 0.0627      |
| Time/B_Original_Form... | 0.0631      |
| Time/Buffer             | 0.00333     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22768827  |
| Train/Action_magnitu... | 0.5143062   |
| Train/Action_magnitude  | 0.40154174  |
| Train/Action_max        | 0.19961338  |
| Train/Action_std        | 0.1408833   |
| Train/Entropy           | -0.57304144 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1459594   |
| Train/Loss              | 0.14917041  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14458857  |
| Train/Ratio             | 1.0000001   |
| Train/Return            | 1.4330987   |
| Train/V                 | 1.5776774   |
| Train/Value             | 1.5776774   |
| Train/control_penalty   | 0.40087977  |
| Train/policy_loss       | 0.14458857  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 579 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 578         |
| Time/Actor_Time         | 0.0635      |
| Time/B_Format_Time      | 0.0621      |
| Time/B_Original_Form... | 0.0633      |
| Time/Buffer             | 0.00285     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22567454  |
| Train/Action_magnitu... | 0.5054966   |
| Train/Action_magnitude  | 0.39489844  |
| Train/Action_max        | 0.1881746   |
| Train/Action_std        | 0.1391073   |
| Train/Entropy           | -0.58923656 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1973472   |
| Train/Loss              | 0.11586853  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11129611  |
| Train/Ratio             | 0.9999882   |
| Train/Return            | 1.5466801   |
| Train/V                 | 1.6579859   |
| Train/Value             | 1.6579859   |
| Train/control_penalty   | 0.39831838  |
| Train/policy_loss       | 0.11129611  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------

 ---------------- Iteration 580 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 579        |
| Time/Actor_Time         | 0.063      |
| Time/B_Format_Time      | 0.0624     |
| Time/B_Original_Form... | 0.0648     |
| Time/Buffer             | 0.00296    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23578444 |
| Train/Action_magnitu... | 0.5293066  |
| Train/Action_magnitude  | 0.41056383 |
| Train/Action_max        | 0.20820484 |
| Train/Action_std        | 0.14392354 |
| Train/Entropy           | -0.5537193 |
| Train/Entropy_Loss      | 0.000554   |
| Train/Entropy_loss      | 0.000554   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0969253  |
| Train/Loss              | 0.18449853 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17982909 |
| Train/Ratio             | 0.9999849  |
| Train/Return            | 1.6607207  |
| Train/V                 | 1.8405532  |
| Train/Value             | 1.8405532  |
| Train/control_penalty   | 0.41157252 |
| Train/policy_loss       | 0.17982909 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 581 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 580         |
| Time/Actor_Time         | 0.0625      |
| Time/B_Format_Time      | 0.0623      |
| Time/B_Original_Form... | 0.064       |
| Time/Buffer             | 0.00302     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23556462  |
| Train/Action_magnitu... | 0.52573794  |
| Train/Action_magnitude  | 0.40986636  |
| Train/Action_max        | 0.20633921  |
| Train/Action_std        | 0.14375834  |
| Train/Entropy           | -0.54989016 |
| Train/Entropy_Loss      | 0.00055     |
| Train/Entropy_loss      | 0.00055     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1089231   |
| Train/Loss              | 0.14161654  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13693361  |
| Train/Ratio             | 1.0000147   |
| Train/Return            | 1.555125    |
| Train/V                 | 1.6920513   |
| Train/Value             | 1.6920513   |
| Train/control_penalty   | 0.41330364  |
| Train/policy_loss       | 0.13693361  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 582 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 581        |
| Time/Actor_Time         | 0.0637     |
| Time/B_Format_Time      | 0.0618     |
| Time/B_Original_Form... | 0.0621     |
| Time/Buffer             | 0.00274    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22392811 |
| Train/Action_magnitu... | 0.5040089  |
| Train/Action_magnitude  | 0.3926997  |
| Train/Action_max        | 0.17946151 |
| Train/Action_std        | 0.14076096 |
| Train/Entropy           | -0.576989  |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1197592  |
| Train/Loss              | 0.1518602  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14731671 |
| Train/Ratio             | 1.0000048  |
| Train/Return            | 1.5265554  |
| Train/V                 | 1.6738694  |
| Train/Value             | 1.6738694  |
| Train/control_penalty   | 0.39665127 |
| Train/policy_loss       | 0.14731671 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 583 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 582        |
| Time/Actor_Time         | 0.063      |
| Time/B_Format_Time      | 0.0635     |
| Time/B_Original_Form... | 0.0637     |
| Time/Buffer             | 0.00304    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22645938 |
| Train/Action_magnitu... | 0.5138175  |
| Train/Action_magnitude  | 0.40062279 |
| Train/Action_max        | 0.19230567 |
| Train/Action_std        | 0.14010684 |
| Train/Entropy           | -0.5793227 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1508802  |
| Train/Loss              | 0.15785013 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15327008 |
| Train/Ratio             | 0.9999703  |
| Train/Return            | 1.5797627  |
| Train/V                 | 1.7330426  |
| Train/Value             | 1.7330426  |
| Train/control_penalty   | 0.40007195 |
| Train/policy_loss       | 0.15327008 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 584 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 583         |
| Time/Actor_Time         | 0.0631      |
| Time/B_Format_Time      | 0.0628      |
| Time/B_Original_Form... | 0.0647      |
| Time/Buffer             | 0.00315     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22828397  |
| Train/Action_magnitu... | 0.51577497  |
| Train/Action_magnitude  | 0.4021374   |
| Train/Action_max        | 0.19910258  |
| Train/Action_std        | 0.14338219  |
| Train/Entropy           | -0.55772066 |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1051638   |
| Train/Loss              | 0.13857086  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13395678  |
| Train/Ratio             | 0.99999255  |
| Train/Return            | 1.6287094   |
| Train/V                 | 1.7626619   |
| Train/Value             | 1.7626619   |
| Train/control_penalty   | 0.40563583  |
| Train/policy_loss       | 0.13395678  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 585 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 584        |
| Time/Actor_Time         | 0.0628     |
| Time/B_Format_Time      | 0.0627     |
| Time/B_Original_Form... | 0.0649     |
| Time/Buffer             | 0.0031     |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.22856382 |
| Train/Action_magnitu... | 0.5207131  |
| Train/Action_magnitude  | 0.40683758 |
| Train/Action_max        | 0.1994635  |
| Train/Action_std        | 0.14277379 |
| Train/Entropy           | -0.5619845 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1189641  |
| Train/Loss              | 0.15677994 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15217797 |
| Train/Ratio             | 1.0000113  |
| Train/Return            | 1.6899146  |
| Train/V                 | 1.8420967  |
| Train/Value             | 1.8420967  |
| Train/control_penalty   | 0.40399855 |
| Train/policy_loss       | 0.15217797 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 586 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 585         |
| Time/Actor_Time         | 0.0628      |
| Time/B_Format_Time      | 0.0615      |
| Time/B_Original_Form... | 0.0616      |
| Time/Buffer             | 0.00273     |
| Time/Critic_Time        | 7.15e-07    |
| Train/Action_abs_mean   | 0.23036624  |
| Train/Action_magnitu... | 0.51512706  |
| Train/Action_magnitude  | 0.39994702  |
| Train/Action_max        | 0.19929916  |
| Train/Action_std        | 0.14012891  |
| Train/Entropy           | -0.58022314 |
| Train/Entropy_Loss      | 0.00058     |
| Train/Entropy_loss      | 0.00058     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1392139   |
| Train/Loss              | 0.19253351  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18797286  |
| Train/Ratio             | 0.9999851   |
| Train/Return            | 1.6287721   |
| Train/V                 | 1.8167392   |
| Train/Value             | 1.8167392   |
| Train/control_penalty   | 0.39804336  |
| Train/policy_loss       | 0.18797286  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 587 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 586        |
| Time/Actor_Time         | 0.0627     |
| Time/B_Format_Time      | 0.0628     |
| Time/B_Original_Form... | 0.0645     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22604346 |
| Train/Action_magnitu... | 0.5138114  |
| Train/Action_magnitude  | 0.40000886 |
| Train/Action_max        | 0.1934581  |
| Train/Action_std        | 0.13912411 |
| Train/Entropy           | -0.5903218 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1871432  |
| Train/Loss              | 0.16701409 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16244815 |
| Train/Ratio             | 0.99999636 |
| Train/Return            | 1.6572434  |
| Train/V                 | 1.819702   |
| Train/Value             | 1.819702   |
| Train/control_penalty   | 0.39756197 |
| Train/policy_loss       | 0.16244815 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 588 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 587        |
| Time/Actor_Time         | 0.0633     |
| Time/B_Format_Time      | 0.0614     |
| Time/B_Original_Form... | 0.0644     |
| Time/Buffer             | 0.00325    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22221927 |
| Train/Action_magnitu... | 0.50231147 |
| Train/Action_magnitude  | 0.39017388 |
| Train/Action_max        | 0.20600156 |
| Train/Action_std        | 0.14047928 |
| Train/Entropy           | -0.577782  |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1660647  |
| Train/Loss              | 0.16859289 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16411026 |
| Train/Ratio             | 1.0000025  |
| Train/Return            | 1.5334938  |
| Train/V                 | 1.6975925  |
| Train/Value             | 1.6975925  |
| Train/control_penalty   | 0.3904847  |
| Train/policy_loss       | 0.16411026 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 589 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 588         |
| Time/Actor_Time         | 0.0636      |
| Time/B_Format_Time      | 0.0642      |
| Time/B_Original_Form... | 0.0638      |
| Time/Buffer             | 0.00552     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.21665424  |
| Train/Action_magnitu... | 0.48923215  |
| Train/Action_magnitude  | 0.3807076   |
| Train/Action_max        | 0.1883795   |
| Train/Action_std        | 0.13758029  |
| Train/Entropy           | -0.59660006 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1922827   |
| Train/Loss              | 0.17436042  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16995344  |
| Train/Ratio             | 0.99997216  |
| Train/Return            | 1.6781765   |
| Train/V                 | 1.848137    |
| Train/Value             | 1.848137    |
| Train/control_penalty   | 0.38103938  |
| Train/policy_loss       | 0.16995344  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 590 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 589        |
| Time/Actor_Time         | 0.0624     |
| Time/B_Format_Time      | 0.0616     |
| Time/B_Original_Form... | 0.0626     |
| Time/Buffer             | 0.00277    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22072919 |
| Train/Action_magnitu... | 0.50165784 |
| Train/Action_magnitude  | 0.38962838 |
| Train/Action_max        | 0.18521182 |
| Train/Action_std        | 0.13812773 |
| Train/Entropy           | -0.597906  |
| Train/Entropy_Loss      | 0.000598   |
| Train/Entropy_loss      | 0.000598   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1849643  |
| Train/Loss              | 0.15986773 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15534769 |
| Train/Ratio             | 0.99998266 |
| Train/Return            | 1.575058   |
| Train/V                 | 1.7304094  |
| Train/Value             | 1.7304094  |
| Train/control_penalty   | 0.39221337 |
| Train/policy_loss       | 0.15534769 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 591 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 590         |
| Time/Actor_Time         | 0.0627      |
| Time/B_Format_Time      | 0.0613      |
| Time/B_Original_Form... | 0.0644      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2289625   |
| Train/Action_magnitu... | 0.5181461   |
| Train/Action_magnitude  | 0.4039688   |
| Train/Action_max        | 0.18910605  |
| Train/Action_std        | 0.13947856  |
| Train/Entropy           | -0.58140737 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.151486    |
| Train/Loss              | 0.14338267  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13876815  |
| Train/Ratio             | 1.0000036   |
| Train/Return            | 1.8984324   |
| Train/V                 | 2.037201    |
| Train/Value             | 2.037201    |
| Train/control_penalty   | 0.40331087  |
| Train/policy_loss       | 0.13876815  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03525     |
-----------------------------------------

 ---------------- Iteration 592 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 591         |
| Time/Actor_Time         | 0.0628      |
| Time/B_Format_Time      | 0.0623      |
| Time/B_Original_Form... | 0.0644      |
| Time/Buffer             | 0.0031      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22015405  |
| Train/Action_magnitu... | 0.5019828   |
| Train/Action_magnitude  | 0.38985676  |
| Train/Action_max        | 0.184454    |
| Train/Action_std        | 0.13833447  |
| Train/Entropy           | -0.59675914 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1915598   |
| Train/Loss              | 0.09866523  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0941877   |
| Train/Ratio             | 1.0000291   |
| Train/Return            | 1.5951494   |
| Train/V                 | 1.689333    |
| Train/Value             | 1.689333    |
| Train/control_penalty   | 0.38807687  |
| Train/policy_loss       | 0.0941877   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03225     |
-----------------------------------------

 ---------------- Iteration 593 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 592        |
| Time/Actor_Time         | 0.063      |
| Time/B_Format_Time      | 0.0632     |
| Time/B_Original_Form... | 0.0628     |
| Time/Buffer             | 0.0031     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21622895 |
| Train/Action_magnitu... | 0.4870149  |
| Train/Action_magnitude  | 0.37733775 |
| Train/Action_max        | 0.18628262 |
| Train/Action_std        | 0.13623227 |
| Train/Entropy           | -0.6087997 |
| Train/Entropy_Loss      | 0.000609   |
| Train/Entropy_loss      | 0.000609   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2178671  |
| Train/Loss              | 0.15304792 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14865825 |
| Train/Ratio             | 1.0000007  |
| Train/Return            | 1.4569291  |
| Train/V                 | 1.6055899  |
| Train/Value             | 1.6055899  |
| Train/control_penalty   | 0.37808737 |
| Train/policy_loss       | 0.14865825 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02       |
----------------------------------------

 ---------------- Iteration 594 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 593        |
| Time/Actor_Time         | 0.0629     |
| Time/B_Format_Time      | 0.0627     |
| Time/B_Original_Form... | 0.0652     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22035865 |
| Train/Action_magnitu... | 0.49184167 |
| Train/Action_magnitude  | 0.3824954  |
| Train/Action_max        | 0.1764446  |
| Train/Action_std        | 0.13623375 |
| Train/Entropy           | -0.6067233 |
| Train/Entropy_Loss      | 0.000607   |
| Train/Entropy_loss      | 0.000607   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1955152  |
| Train/Loss              | 0.16261181 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15816353 |
| Train/Ratio             | 0.9999627  |
| Train/Return            | 1.5930903  |
| Train/V                 | 1.7512673  |
| Train/Value             | 1.7512673  |
| Train/control_penalty   | 0.38415477 |
| Train/policy_loss       | 0.15816353 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 595 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 594        |
| Time/Actor_Time         | 0.063      |
| Time/B_Format_Time      | 0.0625     |
| Time/B_Original_Form... | 0.0641     |
| Time/Buffer             | 0.00272    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21874458 |
| Train/Action_magnitu... | 0.48698977 |
| Train/Action_magnitude  | 0.37805942 |
| Train/Action_max        | 0.19815628 |
| Train/Action_std        | 0.13533552 |
| Train/Entropy           | -0.6171162 |
| Train/Entropy_Loss      | 0.000617   |
| Train/Entropy_loss      | 0.000617   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2378623  |
| Train/Loss              | 0.1962736  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19179915 |
| Train/Ratio             | 0.9999999  |
| Train/Return            | 1.6615847  |
| Train/V                 | 1.853382   |
| Train/Value             | 1.853382   |
| Train/control_penalty   | 0.38573223 |
| Train/policy_loss       | 0.19179915 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 596 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 595         |
| Time/Actor_Time         | 0.0638      |
| Time/B_Format_Time      | 0.0623      |
| Time/B_Original_Form... | 0.0626      |
| Time/Buffer             | 0.00266     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22279093  |
| Train/Action_magnitu... | 0.49888816  |
| Train/Action_magnitude  | 0.38863397  |
| Train/Action_max        | 0.19147962  |
| Train/Action_std        | 0.13986382  |
| Train/Entropy           | -0.58072954 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1604909   |
| Train/Loss              | 0.10294896  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09848128  |
| Train/Ratio             | 1.0000068   |
| Train/Return            | 1.6293402   |
| Train/V                 | 1.7278193   |
| Train/Value             | 1.7278193   |
| Train/control_penalty   | 0.3886952   |
| Train/policy_loss       | 0.09848128  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03175     |
-----------------------------------------

 ---------------- Iteration 597 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 596        |
| Time/Actor_Time         | 0.0624     |
| Time/B_Format_Time      | 0.0621     |
| Time/B_Original_Form... | 0.0626     |
| Time/Buffer             | 0.00284    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22174339 |
| Train/Action_magnitu... | 0.49969512 |
| Train/Action_magnitude  | 0.38782334 |
| Train/Action_max        | 0.19623013 |
| Train/Action_std        | 0.13462931 |
| Train/Entropy           | -0.6238444 |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2549897  |
| Train/Loss              | 0.12085585 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11641261 |
| Train/Ratio             | 0.99997646 |
| Train/Return            | 1.5141863  |
| Train/V                 | 1.6306087  |
| Train/Value             | 1.6306087  |
| Train/control_penalty   | 0.3819396  |
| Train/policy_loss       | 0.11641261 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 598 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 597        |
| Time/Actor_Time         | 0.0843     |
| Time/B_Format_Time      | 0.0816     |
| Time/B_Original_Form... | 0.085      |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23328203 |
| Train/Action_magnitu... | 0.52211183 |
| Train/Action_magnitude  | 0.40458676 |
| Train/Action_max        | 0.20367804 |
| Train/Action_std        | 0.14074989 |
| Train/Entropy           | -0.5758115 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1372094  |
| Train/Loss              | 0.12356271 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11894877 |
| Train/Ratio             | 0.9999697  |
| Train/Return            | 1.6492324  |
| Train/V                 | 1.768188   |
| Train/Value             | 1.768188   |
| Train/control_penalty   | 0.40381286 |
| Train/policy_loss       | 0.11894877 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 599 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 598        |
| Time/Actor_Time         | 0.0814     |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0773     |
| Time/Buffer             | 0.00442    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21411021 |
| Train/Action_magnitu... | 0.48411167 |
| Train/Action_magnitude  | 0.37506047 |
| Train/Action_max        | 0.19103545 |
| Train/Action_std        | 0.13378802 |
| Train/Entropy           | -0.6296479 |
| Train/Entropy_Loss      | 0.00063    |
| Train/Entropy_loss      | 0.00063    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2482356  |
| Train/Loss              | 0.15719917 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15281186 |
| Train/Ratio             | 1.0000026  |
| Train/Return            | 1.4819204  |
| Train/V                 | 1.6347269  |
| Train/Value             | 1.6347269  |
| Train/control_penalty   | 0.37576663 |
| Train/policy_loss       | 0.15281186 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01975    |
----------------------------------------

 ---------------- Iteration 600 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 599        |
| Time/Actor_Time         | 0.0845     |
| Time/B_Format_Time      | 0.0745     |
| Time/B_Original_Form... | 0.0782     |
| Time/Buffer             | 0.0131     |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23325908 |
| Train/Action_magnitu... | 0.5261425  |
| Train/Action_magnitude  | 0.4087915  |
| Train/Action_max        | 0.19887276 |
| Train/Action_std        | 0.14163846 |
| Train/Entropy           | -0.5730515 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1099509  |
| Train/Loss              | 0.15595728 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15129592 |
| Train/Ratio             | 0.9999868  |
| Train/Return            | 1.6683624  |
| Train/V                 | 1.819666   |
| Train/Value             | 1.819666   |
| Train/control_penalty   | 0.40883052 |
| Train/policy_loss       | 0.15129592 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 601 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 600        |
| Time/Actor_Time         | 0.159      |
| Time/B_Format_Time      | 0.0816     |
| Time/B_Original_Form... | 0.0823     |
| Time/Buffer             | 0.00449    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23791544 |
| Train/Action_magnitu... | 0.5352684  |
| Train/Action_magnitude  | 0.41465107 |
| Train/Action_max        | 0.2048943  |
| Train/Action_std        | 0.13988034 |
| Train/Entropy           | -0.5807815 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1568645  |
| Train/Loss              | 0.12819782 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12350518 |
| Train/Ratio             | 1.0000111  |
| Train/Return            | 1.6309963  |
| Train/V                 | 1.7545011  |
| Train/Value             | 1.7545011  |
| Train/control_penalty   | 0.41118562 |
| Train/policy_loss       | 0.12350518 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03425    |
----------------------------------------

 ---------------- Iteration 602 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 601        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0789     |
| Time/B_Original_Form... | 0.0789     |
| Time/Buffer             | 0.00341    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22228147 |
| Train/Action_magnitu... | 0.49791095 |
| Train/Action_magnitude  | 0.38712394 |
| Train/Action_max        | 0.1879501  |
| Train/Action_std        | 0.13833776 |
| Train/Entropy           | -0.5923234 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1951345  |
| Train/Loss              | 0.09628426 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09180298 |
| Train/Ratio             | 0.99999607 |
| Train/Return            | 1.4747412  |
| Train/V                 | 1.5665379  |
| Train/Value             | 1.5665379  |
| Train/control_penalty   | 0.3888968  |
| Train/policy_loss       | 0.09180298 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 603 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 602        |
| Time/Actor_Time         | 0.0765     |
| Time/B_Format_Time      | 0.0746     |
| Time/B_Original_Form... | 0.0746     |
| Time/Buffer             | 0.00324    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22702298 |
| Train/Action_magnitu... | 0.51365757 |
| Train/Action_magnitude  | 0.39600033 |
| Train/Action_max        | 0.21946292 |
| Train/Action_std        | 0.13767074 |
| Train/Entropy           | -0.5989584 |
| Train/Entropy_Loss      | 0.000599   |
| Train/Entropy_loss      | 0.000599   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1817648  |
| Train/Loss              | 0.16696925 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16244686 |
| Train/Ratio             | 0.9999892  |
| Train/Return            | 1.3506306  |
| Train/V                 | 1.5130774  |
| Train/Value             | 1.5130774  |
| Train/control_penalty   | 0.39234415 |
| Train/policy_loss       | 0.16244686 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0175     |
----------------------------------------

 ---------------- Iteration 604 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 603         |
| Time/Actor_Time         | 0.0751      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.0746      |
| Time/Buffer             | 0.00583     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2350305   |
| Train/Action_magnitu... | 0.520755    |
| Train/Action_magnitude  | 0.4040267   |
| Train/Action_max        | 0.19630358  |
| Train/Action_std        | 0.13649127  |
| Train/Entropy           | -0.6075821  |
| Train/Entropy_Loss      | 0.000608    |
| Train/Entropy_loss      | 0.000608    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2127732   |
| Train/Loss              | 0.12962547  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.124955766 |
| Train/Ratio             | 1.0000266   |
| Train/Return            | 1.6216515   |
| Train/V                 | 1.7465962   |
| Train/Value             | 1.7465962   |
| Train/control_penalty   | 0.40621114  |
| Train/policy_loss       | 0.124955766 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 605 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 604        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0779     |
| Time/B_Original_Form... | 0.0749     |
| Time/Buffer             | 0.00345    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22938555 |
| Train/Action_magnitu... | 0.5147923  |
| Train/Action_magnitude  | 0.4006108  |
| Train/Action_max        | 0.1896097  |
| Train/Action_std        | 0.13807526 |
| Train/Entropy           | -0.5965583 |
| Train/Entropy_Loss      | 0.000597   |
| Train/Entropy_loss      | 0.000597   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1845282  |
| Train/Loss              | 0.13290837 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12833539 |
| Train/Ratio             | 1.0000198  |
| Train/Return            | 1.5051118  |
| Train/V                 | 1.6334511  |
| Train/Value             | 1.6334511  |
| Train/control_penalty   | 0.39764318 |
| Train/policy_loss       | 0.12833539 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 606 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 605        |
| Time/Actor_Time         | 0.0767     |
| Time/B_Format_Time      | 0.0713     |
| Time/B_Original_Form... | 0.0734     |
| Time/Buffer             | 0.00517    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2325569  |
| Train/Action_magnitu... | 0.522594   |
| Train/Action_magnitude  | 0.40762675 |
| Train/Action_max        | 0.19676185 |
| Train/Action_std        | 0.14262252 |
| Train/Entropy           | -0.5642361 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0992825  |
| Train/Loss              | 0.14068225 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1360547  |
| Train/Ratio             | 0.9999958  |
| Train/Return            | 1.703745   |
| Train/V                 | 1.8398004  |
| Train/Value             | 1.8398004  |
| Train/control_penalty   | 0.40633214 |
| Train/policy_loss       | 0.1360547  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 607 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 606         |
| Time/Actor_Time         | 0.0748      |
| Time/B_Format_Time      | 0.0781      |
| Time/B_Original_Form... | 0.0818      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24086417  |
| Train/Action_magnitu... | 0.53699815  |
| Train/Action_magnitude  | 0.41873872  |
| Train/Action_max        | 0.19860289  |
| Train/Action_std        | 0.14009516  |
| Train/Entropy           | -0.58122903 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1487921   |
| Train/Loss              | 0.096480675 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09175391  |
| Train/Ratio             | 1.0000032   |
| Train/Return            | 1.7067806   |
| Train/V                 | 1.7985384   |
| Train/Value             | 1.7985384   |
| Train/control_penalty   | 0.41455394  |
| Train/policy_loss       | 0.09175391  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 608 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 607         |
| Time/Actor_Time         | 0.0758      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0779      |
| Time/Buffer             | 0.00306     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23611337  |
| Train/Action_magnitu... | 0.529647    |
| Train/Action_magnitude  | 0.41161188  |
| Train/Action_max        | 0.18777753  |
| Train/Action_std        | 0.14050522  |
| Train/Entropy           | -0.57969403 |
| Train/Entropy_Loss      | 0.00058     |
| Train/Entropy_loss      | 0.00058     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1459265   |
| Train/Loss              | 0.10673848  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10203401  |
| Train/Ratio             | 0.99998534  |
| Train/Return            | 1.6865761   |
| Train/V                 | 1.7886176   |
| Train/Value             | 1.7886176   |
| Train/control_penalty   | 0.41247723  |
| Train/policy_loss       | 0.10203401  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 609 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 608        |
| Time/Actor_Time         | 0.0786     |
| Time/B_Format_Time      | 0.0832     |
| Time/B_Original_Form... | 0.0853     |
| Time/Buffer             | 0.00561    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24091181 |
| Train/Action_magnitu... | 0.52930665 |
| Train/Action_magnitude  | 0.4109559  |
| Train/Action_max        | 0.22032532 |
| Train/Action_std        | 0.14107233 |
| Train/Entropy           | -0.57377   |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1473624  |
| Train/Loss              | 0.15194057 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14721519 |
| Train/Ratio             | 1.0000012  |
| Train/Return            | 1.571376   |
| Train/V                 | 1.718593   |
| Train/Value             | 1.718593   |
| Train/control_penalty   | 0.41516092 |
| Train/policy_loss       | 0.14721519 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 610 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 609         |
| Time/Actor_Time         | 0.0764      |
| Time/B_Format_Time      | 0.077       |
| Time/B_Original_Form... | 0.0752      |
| Time/Buffer             | 0.00452     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24184686  |
| Train/Action_magnitu... | 0.5385665   |
| Train/Action_magnitude  | 0.41971242  |
| Train/Action_max        | 0.18974681  |
| Train/Action_std        | 0.1419134   |
| Train/Entropy           | -0.5678543  |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.123018    |
| Train/Loss              | 0.066181615 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.061386485 |
| Train/Ratio             | 1.0000144   |
| Train/Return            | 1.6776571   |
| Train/V                 | 1.739038    |
| Train/Value             | 1.739038    |
| Train/control_penalty   | 0.42272767  |
| Train/policy_loss       | 0.061386485 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03775     |
-----------------------------------------

 ---------------- Iteration 611 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 610         |
| Time/Actor_Time         | 0.0816      |
| Time/B_Format_Time      | 0.0751      |
| Time/B_Original_Form... | 0.0779      |
| Time/Buffer             | 0.0041      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22840136  |
| Train/Action_magnitu... | 0.516817    |
| Train/Action_magnitude  | 0.40345696  |
| Train/Action_max        | 0.19339651  |
| Train/Action_std        | 0.14244086  |
| Train/Entropy           | -0.56329036 |
| Train/Entropy_Loss      | 0.000563    |
| Train/Entropy_loss      | 0.000563    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.159304    |
| Train/Loss              | 0.16463451  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16003084  |
| Train/Ratio             | 0.99999493  |
| Train/Return            | 1.632325    |
| Train/V                 | 1.7923584   |
| Train/Value             | 1.7923584   |
| Train/control_penalty   | 0.40403733  |
| Train/policy_loss       | 0.16003084  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 612 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 611        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0754     |
| Time/B_Original_Form... | 0.077      |
| Time/Buffer             | 0.00307    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24403422 |
| Train/Action_magnitu... | 0.54286253 |
| Train/Action_magnitude  | 0.4234734  |
| Train/Action_max        | 0.18463783 |
| Train/Action_std        | 0.1417662  |
| Train/Entropy           | -0.5641415 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1557288  |
| Train/Loss              | 0.11327857 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10847794 |
| Train/Ratio             | 1.0000037  |
| Train/Return            | 1.6822697  |
| Train/V                 | 1.7907373  |
| Train/Value             | 1.7907373  |
| Train/control_penalty   | 0.42364824 |
| Train/policy_loss       | 0.10847794 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03375    |
----------------------------------------

 ---------------- Iteration 613 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 612        |
| Time/Actor_Time         | 0.0781     |
| Time/B_Format_Time      | 0.0721     |
| Time/B_Original_Form... | 0.0749     |
| Time/Buffer             | 0.0045     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2516686  |
| Train/Action_magnitu... | 0.5589303  |
| Train/Action_magnitude  | 0.43598038 |
| Train/Action_max        | 0.20209515 |
| Train/Action_std        | 0.1458522  |
| Train/Entropy           | -0.5407955 |
| Train/Entropy_Loss      | 0.000541   |
| Train/Entropy_loss      | 0.000541   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0760971  |
| Train/Loss              | 0.14208306 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1372053  |
| Train/Ratio             | 0.99999255 |
| Train/Return            | 1.9592881  |
| Train/V                 | 2.096482   |
| Train/Value             | 2.096482   |
| Train/control_penalty   | 0.43369654 |
| Train/policy_loss       | 0.1372053  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03225    |
----------------------------------------

 ---------------- Iteration 614 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 613         |
| Time/Actor_Time         | 0.0825      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.0742      |
| Time/Buffer             | 0.00467     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23316371  |
| Train/Action_magnitu... | 0.5340125   |
| Train/Action_magnitude  | 0.41426754  |
| Train/Action_max        | 0.21253632  |
| Train/Action_std        | 0.14456868  |
| Train/Entropy           | -0.55034924 |
| Train/Entropy_Loss      | 0.00055     |
| Train/Entropy_loss      | 0.00055     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1143092   |
| Train/Loss              | 0.22204378  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21740328  |
| Train/Ratio             | 1.0000169   |
| Train/Return            | 1.7542107   |
| Train/V                 | 1.9716158   |
| Train/Value             | 1.9716158   |
| Train/control_penalty   | 0.40901598  |
| Train/policy_loss       | 0.21740328  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 615 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 614        |
| Time/Actor_Time         | 0.0759     |
| Time/B_Format_Time      | 0.0778     |
| Time/B_Original_Form... | 0.0777     |
| Time/Buffer             | 0.00727    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2513301  |
| Train/Action_magnitu... | 0.55895746 |
| Train/Action_magnitude  | 0.43787375 |
| Train/Action_max        | 0.18723032 |
| Train/Action_std        | 0.14469667 |
| Train/Entropy           | -0.5442976 |
| Train/Entropy_Loss      | 0.000544   |
| Train/Entropy_loss      | 0.000544   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.075487   |
| Train/Loss              | 0.15417483 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1492529  |
| Train/Ratio             | 0.999999   |
| Train/Return            | 1.9577373  |
| Train/V                 | 2.1069832  |
| Train/Value             | 2.1069832  |
| Train/control_penalty   | 0.43776354 |
| Train/policy_loss       | 0.1492529  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03775    |
----------------------------------------

 ---------------- Iteration 616 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 615        |
| Time/Actor_Time         | 0.0771     |
| Time/B_Format_Time      | 0.0707     |
| Time/B_Original_Form... | 0.076      |
| Time/Buffer             | 0.0035     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24623021 |
| Train/Action_magnitu... | 0.55265576 |
| Train/Action_magnitude  | 0.4294303  |
| Train/Action_max        | 0.22933027 |
| Train/Action_std        | 0.14578934 |
| Train/Entropy           | -0.5379032 |
| Train/Entropy_Loss      | 0.000538   |
| Train/Entropy_loss      | 0.000538   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0521743  |
| Train/Loss              | 0.14128575 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13647863 |
| Train/Ratio             | 1.0000137  |
| Train/Return            | 1.5907397  |
| Train/V                 | 1.727213   |
| Train/Value             | 1.727213   |
| Train/control_penalty   | 0.4269214  |
| Train/policy_loss       | 0.13647863 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 617 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 616         |
| Time/Actor_Time         | 0.0754      |
| Time/B_Format_Time      | 0.0776      |
| Time/B_Original_Form... | 0.0759      |
| Time/Buffer             | 0.00362     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23540136  |
| Train/Action_magnitu... | 0.523854    |
| Train/Action_magnitude  | 0.409133    |
| Train/Action_max        | 0.18904942  |
| Train/Action_std        | 0.1417102   |
| Train/Entropy           | -0.56579393 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.131948    |
| Train/Loss              | 0.12919857  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1245038   |
| Train/Ratio             | 0.99998796  |
| Train/Return            | 1.5390875   |
| Train/V                 | 1.6635954   |
| Train/Value             | 1.6635954   |
| Train/control_penalty   | 0.41289783  |
| Train/policy_loss       | 0.1245038   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 618 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 617         |
| Time/Actor_Time         | 0.0779      |
| Time/B_Format_Time      | 0.0815      |
| Time/B_Original_Form... | 0.0834      |
| Time/Buffer             | 0.00405     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23517065  |
| Train/Action_magnitu... | 0.5338307   |
| Train/Action_magnitude  | 0.41805592  |
| Train/Action_max        | 0.18164444  |
| Train/Action_std        | 0.14508493  |
| Train/Entropy           | -0.54035604 |
| Train/Entropy_Loss      | 0.00054     |
| Train/Entropy_loss      | 0.00054     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0776799   |
| Train/Loss              | 0.15727602  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15255307  |
| Train/Ratio             | 0.9999942   |
| Train/Return            | 1.5518267   |
| Train/V                 | 1.7043877   |
| Train/Value             | 1.7043877   |
| Train/control_penalty   | 0.41825923  |
| Train/policy_loss       | 0.15255307  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 619 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 618        |
| Time/Actor_Time         | 0.0763     |
| Time/B_Format_Time      | 0.08       |
| Time/B_Original_Form... | 0.0813     |
| Time/Buffer             | 0.00616    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24161147 |
| Train/Action_magnitu... | 0.5391791  |
| Train/Action_magnitude  | 0.42145997 |
| Train/Action_max        | 0.18587372 |
| Train/Action_std        | 0.14187162 |
| Train/Entropy           | -0.5643737 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.128968   |
| Train/Loss              | 0.15977535 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15499483 |
| Train/Ratio             | 0.9999977  |
| Train/Return            | 1.8273071  |
| Train/V                 | 1.98231    |
| Train/Value             | 1.98231    |
| Train/control_penalty   | 0.4216153  |
| Train/policy_loss       | 0.15499483 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------

 ---------------- Iteration 620 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 619        |
| Time/Actor_Time         | 0.0777     |
| Time/B_Format_Time      | 0.0777     |
| Time/B_Original_Form... | 0.0775     |
| Time/Buffer             | 0.00399    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23346482 |
| Train/Action_magnitu... | 0.5261185  |
| Train/Action_magnitude  | 0.40921295 |
| Train/Action_max        | 0.1857591  |
| Train/Action_std        | 0.14085709 |
| Train/Entropy           | -0.5738185 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1237996  |
| Train/Loss              | 0.18925844 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18459491 |
| Train/Ratio             | 0.9999838  |
| Train/Return            | 1.8706925  |
| Train/V                 | 2.0552986  |
| Train/Value             | 2.0552986  |
| Train/control_penalty   | 0.40897068 |
| Train/policy_loss       | 0.18459491 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 621 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 620        |
| Time/Actor_Time         | 0.0744     |
| Time/B_Format_Time      | 0.0767     |
| Time/B_Original_Form... | 0.0755     |
| Time/Buffer             | 0.00354    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23706345 |
| Train/Action_magnitu... | 0.52329916 |
| Train/Action_magnitude  | 0.40654278 |
| Train/Action_max        | 0.1834041  |
| Train/Action_std        | 0.13745539 |
| Train/Entropy           | -0.5986266 |
| Train/Entropy_Loss      | 0.000599   |
| Train/Entropy_loss      | 0.000599   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1799307  |
| Train/Loss              | 0.17654273 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17186576 |
| Train/Ratio             | 0.99999857 |
| Train/Return            | 1.8462609  |
| Train/V                 | 2.0181377  |
| Train/Value             | 2.0181377  |
| Train/control_penalty   | 0.40783504 |
| Train/policy_loss       | 0.17186576 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 622 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 621        |
| Time/Actor_Time         | 0.0769     |
| Time/B_Format_Time      | 0.0785     |
| Time/B_Original_Form... | 0.0807     |
| Time/Buffer             | 0.00327    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2300974  |
| Train/Action_magnitu... | 0.5174357  |
| Train/Action_magnitude  | 0.4025114  |
| Train/Action_max        | 0.19461434 |
| Train/Action_std        | 0.13928811 |
| Train/Entropy           | -0.5815039 |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1381549  |
| Train/Loss              | 0.235006   |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.23039205 |
| Train/Ratio             | 0.99999356 |
| Train/Return            | 1.5410861  |
| Train/V                 | 1.77148    |
| Train/Value             | 1.77148    |
| Train/control_penalty   | 0.40324485 |
| Train/policy_loss       | 0.23039205 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.019      |
----------------------------------------

 ---------------- Iteration 623 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 622        |
| Time/Actor_Time         | 0.0764     |
| Time/B_Format_Time      | 0.0785     |
| Time/B_Original_Form... | 0.0818     |
| Time/Buffer             | 0.00495    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25090018 |
| Train/Action_magnitu... | 0.5567874  |
| Train/Action_magnitude  | 0.432646   |
| Train/Action_max        | 0.20702301 |
| Train/Action_std        | 0.14081517 |
| Train/Entropy           | -0.5700828 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1220559  |
| Train/Loss              | 0.16104136 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15617357 |
| Train/Ratio             | 1.0000024  |
| Train/Return            | 1.7217201  |
| Train/V                 | 1.8778981  |
| Train/Value             | 1.8778981  |
| Train/control_penalty   | 0.42977047 |
| Train/policy_loss       | 0.15617357 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03375    |
----------------------------------------

 ---------------- Iteration 624 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 623         |
| Time/Actor_Time         | 0.0804      |
| Time/B_Format_Time      | 0.0845      |
| Time/B_Original_Form... | 0.0846      |
| Time/Buffer             | 0.00359     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24166511  |
| Train/Action_magnitu... | 0.53353584  |
| Train/Action_magnitude  | 0.41324845  |
| Train/Action_max        | 0.18192512  |
| Train/Action_std        | 0.13937375  |
| Train/Entropy           | -0.58300054 |
| Train/Entropy_Loss      | 0.000583    |
| Train/Entropy_loss      | 0.000583    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1769416   |
| Train/Loss              | 0.09506083  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09027839  |
| Train/Ratio             | 1.0000087   |
| Train/Return            | 1.6140959   |
| Train/V                 | 1.7043753   |
| Train/Value             | 1.7043753   |
| Train/control_penalty   | 0.41994482  |
| Train/policy_loss       | 0.09027839  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 625 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 624         |
| Time/Actor_Time         | 0.074       |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00405     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2408765   |
| Train/Action_magnitu... | 0.5434638   |
| Train/Action_magnitude  | 0.42032233  |
| Train/Action_max        | 0.19638143  |
| Train/Action_std        | 0.13898385  |
| Train/Entropy           | -0.58554614 |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1645513   |
| Train/Loss              | 0.14732237  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14258072  |
| Train/Ratio             | 0.99999046  |
| Train/Return            | 1.5484239   |
| Train/V                 | 1.6910093   |
| Train/Value             | 1.6910093   |
| Train/control_penalty   | 0.41561148  |
| Train/policy_loss       | 0.14258072  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0195      |
-----------------------------------------

 ---------------- Iteration 626 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 625        |
| Time/Actor_Time         | 0.0734     |
| Time/B_Format_Time      | 0.074      |
| Time/B_Original_Form... | 0.0745     |
| Time/Buffer             | 0.00698    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24157892 |
| Train/Action_magnitu... | 0.53439105 |
| Train/Action_magnitude  | 0.41449475 |
| Train/Action_max        | 0.19627297 |
| Train/Action_std        | 0.13672525 |
| Train/Entropy           | -0.6024058 |
| Train/Entropy_Loss      | 0.000602   |
| Train/Entropy_loss      | 0.000602   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.218338   |
| Train/Loss              | 0.15248135 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14771372 |
| Train/Ratio             | 0.99998945 |
| Train/Return            | 1.7905427  |
| Train/V                 | 1.9382548  |
| Train/Value             | 1.9382548  |
| Train/control_penalty   | 0.41652134 |
| Train/policy_loss       | 0.14771372 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 627 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 626         |
| Time/Actor_Time         | 0.0838      |
| Time/B_Format_Time      | 0.0869      |
| Time/B_Original_Form... | 0.0896      |
| Time/Buffer             | 0.00454     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23697048  |
| Train/Action_magnitu... | 0.5249655   |
| Train/Action_magnitude  | 0.4078495   |
| Train/Action_max        | 0.18914211  |
| Train/Action_std        | 0.13543314  |
| Train/Entropy           | -0.61381614 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.243765    |
| Train/Loss              | 0.15678817  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15206127  |
| Train/Ratio             | 0.99999475  |
| Train/Return            | 1.6826683   |
| Train/V                 | 1.8347304   |
| Train/Value             | 1.8347304   |
| Train/control_penalty   | 0.41130775  |
| Train/policy_loss       | 0.15206127  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 628 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 627         |
| Time/Actor_Time         | 0.079       |
| Time/B_Format_Time      | 0.0779      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00475     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23899776  |
| Train/Action_magnitu... | 0.5258846   |
| Train/Action_magnitude  | 0.40892267  |
| Train/Action_max        | 0.17840889  |
| Train/Action_std        | 0.13751349  |
| Train/Entropy           | -0.59690034 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2215375   |
| Train/Loss              | 0.15507711  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15035869  |
| Train/Ratio             | 1.0000132   |
| Train/Return            | 1.5947574   |
| Train/V                 | 1.7451199   |
| Train/Value             | 1.7451199   |
| Train/control_penalty   | 0.41215226  |
| Train/policy_loss       | 0.15035869  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 629 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 628        |
| Time/Actor_Time         | 0.0756     |
| Time/B_Format_Time      | 0.0716     |
| Time/B_Original_Form... | 0.075      |
| Time/Buffer             | 0.00365    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22736304 |
| Train/Action_magnitu... | 0.50586677 |
| Train/Action_magnitude  | 0.39312115 |
| Train/Action_max        | 0.19131595 |
| Train/Action_std        | 0.13350806 |
| Train/Entropy           | -0.6276392 |
| Train/Entropy_Loss      | 0.000628   |
| Train/Entropy_loss      | 0.000628   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2439766  |
| Train/Loss              | 0.14688098 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1423618  |
| Train/Ratio             | 1.000009   |
| Train/Return            | 1.3869925  |
| Train/V                 | 1.5293511  |
| Train/Value             | 1.5293511  |
| Train/control_penalty   | 0.38915476 |
| Train/policy_loss       | 0.1423618  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------

 ---------------- Iteration 630 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 629        |
| Time/Actor_Time         | 0.0753     |
| Time/B_Format_Time      | 0.0768     |
| Time/B_Original_Form... | 0.0742     |
| Time/Buffer             | 0.00362    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22764577 |
| Train/Action_magnitu... | 0.5036459  |
| Train/Action_magnitude  | 0.38864264 |
| Train/Action_max        | 0.18659292 |
| Train/Action_std        | 0.13194478 |
| Train/Entropy           | -0.6407804 |
| Train/Entropy_Loss      | 0.000641   |
| Train/Entropy_loss      | 0.000641   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2939891  |
| Train/Loss              | 0.13684723 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13233453 |
| Train/Ratio             | 0.9999932  |
| Train/Return            | 1.4047253  |
| Train/V                 | 1.5370569  |
| Train/Value             | 1.5370569  |
| Train/control_penalty   | 0.38719225 |
| Train/policy_loss       | 0.13233453 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02       |
----------------------------------------

 ---------------- Iteration 631 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 630        |
| Time/Actor_Time         | 0.0743     |
| Time/B_Format_Time      | 0.0752     |
| Time/B_Original_Form... | 0.0726     |
| Time/Buffer             | 0.00354    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.21900132 |
| Train/Action_magnitu... | 0.49020675 |
| Train/Action_magnitude  | 0.3810174  |
| Train/Action_max        | 0.1731682  |
| Train/Action_std        | 0.13250239 |
| Train/Entropy           | -0.6344081 |
| Train/Entropy_Loss      | 0.000634   |
| Train/Entropy_loss      | 0.000634   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2962246  |
| Train/Loss              | 0.15391861 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.149447   |
| Train/Ratio             | 1.0000252  |
| Train/Return            | 1.4073827  |
| Train/V                 | 1.5568098  |
| Train/Value             | 1.5568098  |
| Train/control_penalty   | 0.3837212  |
| Train/policy_loss       | 0.149447   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 632 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 631        |
| Time/Actor_Time         | 0.0732     |
| Time/B_Format_Time      | 0.0739     |
| Time/B_Original_Form... | 0.0737     |
| Time/Buffer             | 0.00412    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22396515 |
| Train/Action_magnitu... | 0.4959367  |
| Train/Action_magnitude  | 0.3851904  |
| Train/Action_max        | 0.19047971 |
| Train/Action_std        | 0.13092011 |
| Train/Entropy           | -0.6481191 |
| Train/Entropy_Loss      | 0.000648   |
| Train/Entropy_loss      | 0.000648   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2977735  |
| Train/Loss              | 0.17809123 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17359054 |
| Train/Ratio             | 1.0000187  |
| Train/Return            | 1.4734554  |
| Train/V                 | 1.6470354  |
| Train/Value             | 1.6470354  |
| Train/control_penalty   | 0.38525623 |
| Train/policy_loss       | 0.17359054 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02       |
----------------------------------------

 ---------------- Iteration 633 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 632        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0764     |
| Time/B_Original_Form... | 0.0758     |
| Time/Buffer             | 0.00418    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22989307 |
| Train/Action_magnitu... | 0.51195115 |
| Train/Action_magnitude  | 0.39837036 |
| Train/Action_max        | 0.1918336  |
| Train/Action_std        | 0.13442197 |
| Train/Entropy           | -0.6228256 |
| Train/Entropy_Loss      | 0.000623   |
| Train/Entropy_loss      | 0.000623   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2228327  |
| Train/Loss              | 0.10200963 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09737663 |
| Train/Ratio             | 0.99999046 |
| Train/Return            | 1.434502   |
| Train/V                 | 1.5318811  |
| Train/Value             | 1.5318811  |
| Train/control_penalty   | 0.40101758 |
| Train/policy_loss       | 0.09737663 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 634 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 633         |
| Time/Actor_Time         | 0.0735      |
| Time/B_Format_Time      | 0.069       |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.0042      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22580704  |
| Train/Action_magnitu... | 0.50716037  |
| Train/Action_magnitude  | 0.39505354  |
| Train/Action_max        | 0.18905021  |
| Train/Action_std        | 0.13413572  |
| Train/Entropy           | -0.62478524 |
| Train/Entropy_Loss      | 0.000625    |
| Train/Entropy_loss      | 0.000625    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.225124    |
| Train/Loss              | 0.12998137  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12545073  |
| Train/Ratio             | 0.9999883   |
| Train/Return            | 1.4283158   |
| Train/V                 | 1.5537623   |
| Train/Value             | 1.5537623   |
| Train/control_penalty   | 0.39058518  |
| Train/policy_loss       | 0.12545073  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------

 ---------------- Iteration 635 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 634        |
| Time/Actor_Time         | 0.0732     |
| Time/B_Format_Time      | 0.0746     |
| Time/B_Original_Form... | 0.0783     |
| Time/Buffer             | 0.00814    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24145849 |
| Train/Action_magnitu... | 0.5293078  |
| Train/Action_magnitude  | 0.41315895 |
| Train/Action_max        | 0.21283324 |
| Train/Action_std        | 0.13821456 |
| Train/Entropy           | -0.5972291 |
| Train/Entropy_Loss      | 0.000597   |
| Train/Entropy_loss      | 0.000597   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2205054  |
| Train/Loss              | 0.15740223 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15262687 |
| Train/Ratio             | 0.9999931  |
| Train/Return            | 1.3387817  |
| Train/V                 | 1.4914073  |
| Train/Value             | 1.4914073  |
| Train/control_penalty   | 0.4178137  |
| Train/policy_loss       | 0.15262687 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.015      |
----------------------------------------

 ---------------- Iteration 636 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 635        |
| Time/Actor_Time         | 0.0767     |
| Time/B_Format_Time      | 0.074      |
| Time/B_Original_Form... | 0.0767     |
| Time/Buffer             | 0.00333    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23176438 |
| Train/Action_magnitu... | 0.52089196 |
| Train/Action_magnitude  | 0.40543106 |
| Train/Action_max        | 0.19390136 |
| Train/Action_std        | 0.13635227 |
| Train/Entropy           | -0.6117145 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2263293  |
| Train/Loss              | 0.19532223 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19070801 |
| Train/Ratio             | 1.0000268  |
| Train/Return            | 1.3440053  |
| Train/V                 | 1.5347074  |
| Train/Value             | 1.5347074  |
| Train/control_penalty   | 0.40025046 |
| Train/policy_loss       | 0.19070801 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.017      |
----------------------------------------

 ---------------- Iteration 637 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 636        |
| Time/Actor_Time         | 0.0775     |
| Time/B_Format_Time      | 0.0768     |
| Time/B_Original_Form... | 0.0778     |
| Time/Buffer             | 0.00455    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21576628 |
| Train/Action_magnitu... | 0.4894364  |
| Train/Action_magnitude  | 0.3800947  |
| Train/Action_max        | 0.18991172 |
| Train/Action_std        | 0.1345904  |
| Train/Entropy           | -0.6238932 |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2444023  |
| Train/Loss              | 0.12467437 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12028515 |
| Train/Ratio             | 0.9999935  |
| Train/Return            | 1.2689372  |
| Train/V                 | 1.389223   |
| Train/Value             | 1.389223   |
| Train/control_penalty   | 0.37653214 |
| Train/policy_loss       | 0.12028515 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.017      |
----------------------------------------

 ---------------- Iteration 638 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 637        |
| Time/Actor_Time         | 0.0754     |
| Time/B_Format_Time      | 0.0781     |
| Time/B_Original_Form... | 0.077      |
| Time/Buffer             | 0.00315    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23542991 |
| Train/Action_magnitu... | 0.5233762  |
| Train/Action_magnitude  | 0.40779877 |
| Train/Action_max        | 0.18965976 |
| Train/Action_std        | 0.1364577  |
| Train/Entropy           | -0.6070026 |
| Train/Entropy_Loss      | 0.000607   |
| Train/Entropy_loss      | 0.000607   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1998911  |
| Train/Loss              | 0.13903356 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13435301 |
| Train/Ratio             | 1.0000032  |
| Train/Return            | 1.5008874  |
| Train/V                 | 1.6352419  |
| Train/Value             | 1.6352419  |
| Train/control_penalty   | 0.40735382 |
| Train/policy_loss       | 0.13435301 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01975    |
----------------------------------------

 ---------------- Iteration 639 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 638        |
| Time/Actor_Time         | 0.0752     |
| Time/B_Format_Time      | 0.0727     |
| Time/B_Original_Form... | 0.0743     |
| Time/Buffer             | 0.00323    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22240774 |
| Train/Action_magnitu... | 0.4939967  |
| Train/Action_magnitude  | 0.3853778  |
| Train/Action_max        | 0.18627588 |
| Train/Action_std        | 0.13386498 |
| Train/Entropy           | -0.6292885 |
| Train/Entropy_Loss      | 0.000629   |
| Train/Entropy_loss      | 0.000629   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2616365  |
| Train/Loss              | 0.17295037 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16845858 |
| Train/Ratio             | 1.0000194  |
| Train/Return            | 1.4217422  |
| Train/V                 | 1.5902036  |
| Train/Value             | 1.5902036  |
| Train/control_penalty   | 0.38625053 |
| Train/policy_loss       | 0.16845858 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.017      |
----------------------------------------

 ---------------- Iteration 640 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 639         |
| Time/Actor_Time         | 0.0752      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0749      |
| Time/Buffer             | 0.00414     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23322736  |
| Train/Action_magnitu... | 0.5182021   |
| Train/Action_magnitude  | 0.4033436   |
| Train/Action_max        | 0.18360353  |
| Train/Action_std        | 0.13681673  |
| Train/Entropy           | -0.60747534 |
| Train/Entropy_Loss      | 0.000607    |
| Train/Entropy_loss      | 0.000607    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2124594   |
| Train/Loss              | 0.1433446   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13868548  |
| Train/Ratio             | 0.99999756  |
| Train/Return            | 1.5037217   |
| Train/V                 | 1.6424134   |
| Train/Value             | 1.6424134   |
| Train/control_penalty   | 0.4051646   |
| Train/policy_loss       | 0.13868548  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 641 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 640        |
| Time/Actor_Time         | 0.0736     |
| Time/B_Format_Time      | 0.0717     |
| Time/B_Original_Form... | 0.0756     |
| Time/Buffer             | 0.00446    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23469555 |
| Train/Action_magnitu... | 0.520437   |
| Train/Action_magnitude  | 0.405784   |
| Train/Action_max        | 0.20078987 |
| Train/Action_std        | 0.13742402 |
| Train/Entropy           | -0.6022772 |
| Train/Entropy_Loss      | 0.000602   |
| Train/Entropy_loss      | 0.000602   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2210103  |
| Train/Loss              | 0.14032376 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13565274 |
| Train/Ratio             | 0.9999851  |
| Train/Return            | 1.5999686  |
| Train/V                 | 1.7356188  |
| Train/Value             | 1.7356188  |
| Train/control_penalty   | 0.40687487 |
| Train/policy_loss       | 0.13565274 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 642 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 641         |
| Time/Actor_Time         | 0.073       |
| Time/B_Format_Time      | 0.0742      |
| Time/B_Original_Form... | 0.0769      |
| Time/Buffer             | 0.00402     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23999375  |
| Train/Action_magnitu... | 0.5315594   |
| Train/Action_magnitude  | 0.41497153  |
| Train/Action_max        | 0.1933219   |
| Train/Action_std        | 0.13795684  |
| Train/Entropy           | -0.59370464 |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.207799    |
| Train/Loss              | 0.11811109  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11335053  |
| Train/Ratio             | 0.9999997   |
| Train/Return            | 1.5689842   |
| Train/V                 | 1.6823297   |
| Train/Value             | 1.6823297   |
| Train/control_penalty   | 0.4166848   |
| Train/policy_loss       | 0.11335053  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.023       |
-----------------------------------------

 ---------------- Iteration 643 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 642         |
| Time/Actor_Time         | 0.0726      |
| Time/B_Format_Time      | 0.0698      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00705     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23635136  |
| Train/Action_magnitu... | 0.52703     |
| Train/Action_magnitude  | 0.4083541   |
| Train/Action_max        | 0.19839856  |
| Train/Action_std        | 0.13932462  |
| Train/Entropy           | -0.58925366 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2046742   |
| Train/Loss              | 0.12172328  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.117067836 |
| Train/Ratio             | 1.000013    |
| Train/Return            | 1.7628022   |
| Train/V                 | 1.8798696   |
| Train/Value             | 1.8798696   |
| Train/control_penalty   | 0.40661907  |
| Train/policy_loss       | 0.117067836 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 644 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 643         |
| Time/Actor_Time         | 0.0725      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.0712      |
| Time/Buffer             | 0.00446     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24214368  |
| Train/Action_magnitu... | 0.53249854  |
| Train/Action_magnitude  | 0.41520295  |
| Train/Action_max        | 0.19713715  |
| Train/Action_std        | 0.13560908  |
| Train/Entropy           | -0.61015517 |
| Train/Entropy_Loss      | 0.00061     |
| Train/Entropy_loss      | 0.00061     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2097363   |
| Train/Loss              | 0.15834516  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15357314  |
| Train/Ratio             | 1.0000049   |
| Train/Return            | 1.6872816   |
| Train/V                 | 1.8408452   |
| Train/Value             | 1.8408452   |
| Train/control_penalty   | 0.41618705  |
| Train/policy_loss       | 0.15357314  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 645 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 644        |
| Time/Actor_Time         | 0.0824     |
| Time/B_Format_Time      | 0.085      |
| Time/B_Original_Form... | 0.0852     |
| Time/Buffer             | 0.00434    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23897658 |
| Train/Action_magnitu... | 0.5364897  |
| Train/Action_magnitude  | 0.41974753 |
| Train/Action_max        | 0.19399334 |
| Train/Action_std        | 0.14137582 |
| Train/Entropy           | -0.5702188 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1338084  |
| Train/Loss              | 0.16596936 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1612268  |
| Train/Ratio             | 1.0000042  |
| Train/Return            | 1.672248   |
| Train/V                 | 1.8334756  |
| Train/Value             | 1.8334756  |
| Train/control_penalty   | 0.41723368 |
| Train/policy_loss       | 0.1612268  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 646 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 645        |
| Time/Actor_Time         | 0.0726     |
| Time/B_Format_Time      | 0.0736     |
| Time/B_Original_Form... | 0.0728     |
| Time/Buffer             | 0.00347    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23104724 |
| Train/Action_magnitu... | 0.5249696  |
| Train/Action_magnitude  | 0.40743822 |
| Train/Action_max        | 0.21408764 |
| Train/Action_std        | 0.13941458 |
| Train/Entropy           | -0.5888785 |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1606023  |
| Train/Loss              | 0.11160554 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.106985   |
| Train/Ratio             | 1.0000069  |
| Train/Return            | 1.4931723  |
| Train/V                 | 1.6001501  |
| Train/Value             | 1.6001501  |
| Train/control_penalty   | 0.4031655  |
| Train/policy_loss       | 0.106985   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 647 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 646        |
| Time/Actor_Time         | 0.0793     |
| Time/B_Format_Time      | 0.0787     |
| Time/B_Original_Form... | 0.079      |
| Time/Buffer             | 0.00384    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2447219  |
| Train/Action_magnitu... | 0.5382504  |
| Train/Action_magnitude  | 0.4200221  |
| Train/Action_max        | 0.18474437 |
| Train/Action_std        | 0.13787259 |
| Train/Entropy           | -0.5962779 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1890731  |
| Train/Loss              | 0.15997063 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15514123 |
| Train/Ratio             | 0.99999744 |
| Train/Return            | 1.693152   |
| Train/V                 | 1.8482983  |
| Train/Value             | 1.8482983  |
| Train/control_penalty   | 0.42331064 |
| Train/policy_loss       | 0.15514123 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 648 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 647        |
| Time/Actor_Time         | 0.0824     |
| Time/B_Format_Time      | 0.0862     |
| Time/B_Original_Form... | 0.0889     |
| Time/Buffer             | 0.00418    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2305989  |
| Train/Action_magnitu... | 0.5261884  |
| Train/Action_magnitude  | 0.41015166 |
| Train/Action_max        | 0.19941531 |
| Train/Action_std        | 0.13948885 |
| Train/Entropy           | -0.5846385 |
| Train/Entropy_Loss      | 0.000585   |
| Train/Entropy_loss      | 0.000585   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.170878   |
| Train/Loss              | 0.1176862  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11303571 |
| Train/Ratio             | 0.9999907  |
| Train/Return            | 1.6876175  |
| Train/V                 | 1.8006536  |
| Train/Value             | 1.8006536  |
| Train/control_penalty   | 0.4065849  |
| Train/policy_loss       | 0.11303571 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 649 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 648        |
| Time/Actor_Time         | 0.0738     |
| Time/B_Format_Time      | 0.0743     |
| Time/B_Original_Form... | 0.0774     |
| Time/Buffer             | 0.00305    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24034841 |
| Train/Action_magnitu... | 0.5321289  |
| Train/Action_magnitude  | 0.41109997 |
| Train/Action_max        | 0.20870937 |
| Train/Action_std        | 0.13994232 |
| Train/Entropy           | -0.5830381 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1808803  |
| Train/Loss              | 0.17168796 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16695932 |
| Train/Ratio             | 1.0000014  |
| Train/Return            | 1.7605702  |
| Train/V                 | 1.9275168  |
| Train/Value             | 1.9275168  |
| Train/control_penalty   | 0.41456017 |
| Train/policy_loss       | 0.16695932 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 650 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 649        |
| Time/Actor_Time         | 0.0748     |
| Time/B_Format_Time      | 0.0778     |
| Time/B_Original_Form... | 0.0748     |
| Time/Buffer             | 0.00451    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2399065  |
| Train/Action_magnitu... | 0.53828895 |
| Train/Action_magnitude  | 0.42032462 |
| Train/Action_max        | 0.19010259 |
| Train/Action_std        | 0.14069845 |
| Train/Entropy           | -0.5721488 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1235795  |
| Train/Loss              | 0.17281657 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16805446 |
| Train/Ratio             | 0.9999736  |
| Train/Return            | 1.8211931  |
| Train/V                 | 1.9892511  |
| Train/Value             | 1.9892511  |
| Train/control_penalty   | 0.41899738 |
| Train/policy_loss       | 0.16805446 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 651 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 650        |
| Time/Actor_Time         | 0.0725     |
| Time/B_Format_Time      | 0.0706     |
| Time/B_Original_Form... | 0.0739     |
| Time/Buffer             | 0.00368    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22317912 |
| Train/Action_magnitu... | 0.5008183  |
| Train/Action_magnitude  | 0.38831466 |
| Train/Action_max        | 0.20215353 |
| Train/Action_std        | 0.13587138 |
| Train/Entropy           | -0.614014  |
| Train/Entropy_Loss      | 0.000614   |
| Train/Entropy_loss      | 0.000614   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2508526  |
| Train/Loss              | 0.15993513 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1554458  |
| Train/Ratio             | 1.000004   |
| Train/Return            | 1.725355   |
| Train/V                 | 1.8808023  |
| Train/Value             | 1.8808023  |
| Train/control_penalty   | 0.38753086 |
| Train/policy_loss       | 0.1554458  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 652 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 651        |
| Time/Actor_Time         | 0.0722     |
| Time/B_Format_Time      | 0.0756     |
| Time/B_Original_Form... | 0.0736     |
| Time/Buffer             | 0.00344    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23250297 |
| Train/Action_magnitu... | 0.51476043 |
| Train/Action_magnitude  | 0.39929995 |
| Train/Action_max        | 0.19235688 |
| Train/Action_std        | 0.134845   |
| Train/Entropy           | -0.6176846 |
| Train/Entropy_Loss      | 0.000618   |
| Train/Entropy_loss      | 0.000618   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2264473  |
| Train/Loss              | 0.16096374 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15635535 |
| Train/Ratio             | 0.9999791  |
| Train/Return            | 1.7842573  |
| Train/V                 | 1.9406191  |
| Train/Value             | 1.9406191  |
| Train/control_penalty   | 0.39907062 |
| Train/policy_loss       | 0.15635535 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 653 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 652        |
| Time/Actor_Time         | 0.0746     |
| Time/B_Format_Time      | 0.0722     |
| Time/B_Original_Form... | 0.0744     |
| Time/Buffer             | 0.00333    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23591459 |
| Train/Action_magnitu... | 0.5257992  |
| Train/Action_magnitude  | 0.40849385 |
| Train/Action_max        | 0.19299744 |
| Train/Action_std        | 0.13724397 |
| Train/Entropy           | -0.5992077 |
| Train/Entropy_Loss      | 0.000599   |
| Train/Entropy_loss      | 0.000599   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2112011  |
| Train/Loss              | 0.18387273 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17919315 |
| Train/Ratio             | 0.99999315 |
| Train/Return            | 1.8486686  |
| Train/V                 | 2.0278711  |
| Train/Value             | 2.0278711  |
| Train/control_penalty   | 0.40803695 |
| Train/policy_loss       | 0.17919315 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 654 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 653        |
| Time/Actor_Time         | 0.0828     |
| Time/B_Format_Time      | 0.0828     |
| Time/B_Original_Form... | 0.0849     |
| Time/Buffer             | 0.00324    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2295686  |
| Train/Action_magnitu... | 0.5163516  |
| Train/Action_magnitude  | 0.3986204  |
| Train/Action_max        | 0.18548644 |
| Train/Action_std        | 0.13879198 |
| Train/Entropy           | -0.5865645 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1666174  |
| Train/Loss              | 0.12978901 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12521431 |
| Train/Ratio             | 0.99999714 |
| Train/Return            | 1.6560514  |
| Train/V                 | 1.7812616  |
| Train/Value             | 1.7812616  |
| Train/control_penalty   | 0.39881366 |
| Train/policy_loss       | 0.12521431 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 655 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 654        |
| Time/Actor_Time         | 0.0728     |
| Time/B_Format_Time      | 0.0741     |
| Time/B_Original_Form... | 0.073      |
| Time/Buffer             | 0.00447    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22138065 |
| Train/Action_magnitu... | 0.49910465 |
| Train/Action_magnitude  | 0.3910436  |
| Train/Action_max        | 0.17048499 |
| Train/Action_std        | 0.1357243  |
| Train/Entropy           | -0.6119176 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2710447  |
| Train/Loss              | 0.22326902 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21871059 |
| Train/Ratio             | 0.9999974  |
| Train/Return            | 2.0444117  |
| Train/V                 | 2.2631092  |
| Train/Value             | 2.2631092  |
| Train/control_penalty   | 0.39465207 |
| Train/policy_loss       | 0.21871059 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 656 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 655        |
| Time/Actor_Time         | 0.0734     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.0725     |
| Time/Buffer             | 0.00396    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22260368 |
| Train/Action_magnitu... | 0.49905595 |
| Train/Action_magnitude  | 0.3874062  |
| Train/Action_max        | 0.18323417 |
| Train/Action_std        | 0.13542551 |
| Train/Entropy           | -0.6148005 |
| Train/Entropy_Loss      | 0.000615   |
| Train/Entropy_loss      | 0.000615   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2593008  |
| Train/Loss              | 0.24285766 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2383526  |
| Train/Ratio             | 1.0000021  |
| Train/Return            | 1.7466223  |
| Train/V                 | 1.9849685  |
| Train/Value             | 1.9849685  |
| Train/control_penalty   | 0.3890262  |
| Train/policy_loss       | 0.2383526  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01975    |
----------------------------------------

 ---------------- Iteration 657 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 656        |
| Time/Actor_Time         | 0.074      |
| Time/B_Format_Time      | 0.0759     |
| Time/B_Original_Form... | 0.0739     |
| Time/Buffer             | 0.00347    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23294991 |
| Train/Action_magnitu... | 0.5209181  |
| Train/Action_magnitude  | 0.40759164 |
| Train/Action_max        | 0.17213596 |
| Train/Action_std        | 0.13907729 |
| Train/Entropy           | -0.58381   |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1733179  |
| Train/Loss              | 0.18724261 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18256767 |
| Train/Ratio             | 0.9999986  |
| Train/Return            | 1.8373908  |
| Train/V                 | 2.0199552  |
| Train/Value             | 2.0199552  |
| Train/control_penalty   | 0.40911224 |
| Train/policy_loss       | 0.18256767 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 658 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 657        |
| Time/Actor_Time         | 0.0747     |
| Time/B_Format_Time      | 0.0761     |
| Time/B_Original_Form... | 0.0754     |
| Time/Buffer             | 0.00422    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22318861 |
| Train/Action_magnitu... | 0.5061932  |
| Train/Action_magnitude  | 0.39537263 |
| Train/Action_max        | 0.15553965 |
| Train/Action_std        | 0.13729589 |
| Train/Entropy           | -0.5967665 |
| Train/Entropy_Loss      | 0.000597   |
| Train/Entropy_loss      | 0.000597   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1843586  |
| Train/Loss              | 0.32300642 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.31849924 |
| Train/Ratio             | 0.9999955  |
| Train/Return            | 2.1187036  |
| Train/V                 | 2.4372144  |
| Train/Value             | 2.4372144  |
| Train/control_penalty   | 0.39104366 |
| Train/policy_loss       | 0.31849924 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02025    |
----------------------------------------

 ---------------- Iteration 659 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 658        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0732     |
| Time/B_Original_Form... | 0.0766     |
| Time/Buffer             | 0.00331    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23230742 |
| Train/Action_magnitu... | 0.52127254 |
| Train/Action_magnitude  | 0.40792578 |
| Train/Action_max        | 0.15270673 |
| Train/Action_std        | 0.13708484 |
| Train/Entropy           | -0.5961159 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2074604  |
| Train/Loss              | 0.33702895 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.3323564  |
| Train/Ratio             | 0.9999947  |
| Train/Return            | 2.2189405  |
| Train/V                 | 2.5512915  |
| Train/Value             | 2.5512915  |
| Train/control_penalty   | 0.40764484 |
| Train/policy_loss       | 0.3323564  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 660 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 659        |
| Time/Actor_Time         | 0.0743     |
| Time/B_Format_Time      | 0.0716     |
| Time/B_Original_Form... | 0.0716     |
| Time/Buffer             | 0.00291    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22988524 |
| Train/Action_magnitu... | 0.5136819  |
| Train/Action_magnitude  | 0.39945593 |
| Train/Action_max        | 0.17931911 |
| Train/Action_std        | 0.13805576 |
| Train/Entropy           | -0.5930358 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2038679  |
| Train/Loss              | 0.2019041  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19733427 |
| Train/Ratio             | 0.9999819  |
| Train/Return            | 1.8565336  |
| Train/V                 | 2.053881   |
| Train/Value             | 2.053881   |
| Train/control_penalty   | 0.39767864 |
| Train/policy_loss       | 0.19733427 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 661 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 660        |
| Time/Actor_Time         | 0.0793     |
| Time/B_Format_Time      | 0.0818     |
| Time/B_Original_Form... | 0.0828     |
| Time/Buffer             | 0.00421    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22848576 |
| Train/Action_magnitu... | 0.5160341  |
| Train/Action_magnitude  | 0.40050176 |
| Train/Action_max        | 0.18170285 |
| Train/Action_std        | 0.13905284 |
| Train/Entropy           | -0.5901124 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1847771  |
| Train/Loss              | 0.27803785 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2734559  |
| Train/Ratio             | 0.9999995  |
| Train/Return            | 1.8035438  |
| Train/V                 | 2.0770082  |
| Train/Value             | 2.0770082  |
| Train/control_penalty   | 0.39918497 |
| Train/policy_loss       | 0.2734559  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 662 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 661        |
| Time/Actor_Time         | 0.0799     |
| Time/B_Format_Time      | 0.0828     |
| Time/B_Original_Form... | 0.0837     |
| Time/Buffer             | 0.00409    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23111528 |
| Train/Action_magnitu... | 0.51621366 |
| Train/Action_magnitude  | 0.40378013 |
| Train/Action_max        | 0.177066   |
| Train/Action_std        | 0.14250813 |
| Train/Entropy           | -0.5622026 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1384352  |
| Train/Loss              | 0.28993815 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.28529388 |
| Train/Ratio             | 1.0000074  |
| Train/Return            | 1.8429079  |
| Train/V                 | 2.128196   |
| Train/Value             | 2.128196   |
| Train/control_penalty   | 0.40820983 |
| Train/policy_loss       | 0.28529388 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 663 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 662        |
| Time/Actor_Time         | 0.0726     |
| Time/B_Format_Time      | 0.0732     |
| Time/B_Original_Form... | 0.0744     |
| Time/Buffer             | 0.00487    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.2301536  |
| Train/Action_magnitu... | 0.519391   |
| Train/Action_magnitude  | 0.4030752  |
| Train/Action_max        | 0.18261854 |
| Train/Action_std        | 0.13840136 |
| Train/Entropy           | -0.5921392 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.194123   |
| Train/Loss              | 0.21888636 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21425892 |
| Train/Ratio             | 1.0000081  |
| Train/Return            | 2.1375387  |
| Train/V                 | 2.3517966  |
| Train/Value             | 2.3517966  |
| Train/control_penalty   | 0.40352973 |
| Train/policy_loss       | 0.21425892 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.032      |
----------------------------------------

 ---------------- Iteration 664 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 663        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0737     |
| Time/B_Original_Form... | 0.0739     |
| Time/Buffer             | 0.00344    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23456864 |
| Train/Action_magnitu... | 0.5264363  |
| Train/Action_magnitude  | 0.40967616 |
| Train/Action_max        | 0.19053693 |
| Train/Action_std        | 0.1392356  |
| Train/Entropy           | -0.5879322 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1806636  |
| Train/Loss              | 0.23325557 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22858426 |
| Train/Ratio             | 1.0000104  |
| Train/Return            | 1.848683   |
| Train/V                 | 2.0772617  |
| Train/Value             | 2.0772617  |
| Train/control_penalty   | 0.40833762 |
| Train/policy_loss       | 0.22858426 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 665 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 664         |
| Time/Actor_Time         | 0.0743      |
| Time/B_Format_Time      | 0.0776      |
| Time/B_Original_Form... | 0.0788      |
| Time/Buffer             | 0.00468     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22929654  |
| Train/Action_magnitu... | 0.5129413   |
| Train/Action_magnitude  | 0.39860505  |
| Train/Action_max        | 0.20183444  |
| Train/Action_std        | 0.13872668  |
| Train/Entropy           | -0.59358454 |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1908797   |
| Train/Loss              | 0.20897275  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20437112  |
| Train/Ratio             | 1.0000005   |
| Train/Return            | 1.6973774   |
| Train/V                 | 1.9017477   |
| Train/Value             | 1.9017477   |
| Train/control_penalty   | 0.40080363  |
| Train/policy_loss       | 0.20437112  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 666 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 665         |
| Time/Actor_Time         | 0.0776      |
| Time/B_Format_Time      | 0.0789      |
| Time/B_Original_Form... | 0.0817      |
| Time/Buffer             | 0.00371     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23184976  |
| Train/Action_magnitu... | 0.52195334  |
| Train/Action_magnitude  | 0.4061278   |
| Train/Action_max        | 0.19160351  |
| Train/Action_std        | 0.13997288  |
| Train/Entropy           | -0.58661747 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1772178   |
| Train/Loss              | 0.27441376  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.26973864  |
| Train/Ratio             | 1.0000015   |
| Train/Return            | 2.1221354   |
| Train/V                 | 2.391884    |
| Train/Value             | 2.391884    |
| Train/control_penalty   | 0.4088479   |
| Train/policy_loss       | 0.26973864  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02525     |
-----------------------------------------

 ---------------- Iteration 667 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 666        |
| Time/Actor_Time         | 0.0724     |
| Time/B_Format_Time      | 0.0759     |
| Time/B_Original_Form... | 0.0728     |
| Time/Buffer             | 0.00309    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23492919 |
| Train/Action_magnitu... | 0.5299824  |
| Train/Action_magnitude  | 0.4140347  |
| Train/Action_max        | 0.18412142 |
| Train/Action_std        | 0.13955526 |
| Train/Entropy           | -0.5850071 |
| Train/Entropy_Loss      | 0.000585   |
| Train/Entropy_loss      | 0.000585   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1819799  |
| Train/Loss              | 0.40375373 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.39900115 |
| Train/Ratio             | 1.0000074  |
| Train/Return            | 2.2364607  |
| Train/V                 | 2.6354587  |
| Train/Value             | 2.6354587  |
| Train/control_penalty   | 0.41675508 |
| Train/policy_loss       | 0.39900115 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 668 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 667         |
| Time/Actor_Time         | 0.0724      |
| Time/B_Format_Time      | 0.0686      |
| Time/B_Original_Form... | 0.0723      |
| Time/Buffer             | 0.00411     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23651864  |
| Train/Action_magnitu... | 0.5324353   |
| Train/Action_magnitude  | 0.4130243   |
| Train/Action_max        | 0.20609306  |
| Train/Action_std        | 0.14242868  |
| Train/Entropy           | -0.56675756 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1228493   |
| Train/Loss              | 0.21981676  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.215103    |
| Train/Ratio             | 0.9999901   |
| Train/Return            | 1.9453461   |
| Train/V                 | 2.1604393   |
| Train/Value             | 2.1604393   |
| Train/control_penalty   | 0.4147007   |
| Train/policy_loss       | 0.215103    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 669 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 668        |
| Time/Actor_Time         | 0.0734     |
| Time/B_Format_Time      | 0.0741     |
| Time/B_Original_Form... | 0.0735     |
| Time/Buffer             | 0.00484    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24133858 |
| Train/Action_magnitu... | 0.5375869  |
| Train/Action_magnitude  | 0.41839433 |
| Train/Action_max        | 0.2061595  |
| Train/Action_std        | 0.14251679 |
| Train/Entropy           | -0.5665761 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1539626  |
| Train/Loss              | 0.16207755 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15734546 |
| Train/Ratio             | 0.9999924  |
| Train/Return            | 1.6839015  |
| Train/V                 | 1.8412532  |
| Train/Value             | 1.8412532  |
| Train/control_penalty   | 0.41655096 |
| Train/policy_loss       | 0.15734546 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 670 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 669        |
| Time/Actor_Time         | 0.0737     |
| Time/B_Format_Time      | 0.0733     |
| Time/B_Original_Form... | 0.0751     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22780983 |
| Train/Action_magnitu... | 0.51855165 |
| Train/Action_magnitude  | 0.4039497  |
| Train/Action_max        | 0.21662967 |
| Train/Action_std        | 0.14261587 |
| Train/Entropy           | -0.5667343 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1194735  |
| Train/Loss              | 0.18611531 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18152064 |
| Train/Ratio             | 0.99997264 |
| Train/Return            | 1.7630032  |
| Train/V                 | 1.944529   |
| Train/Value             | 1.944529   |
| Train/control_penalty   | 0.40279347 |
| Train/policy_loss       | 0.18152064 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02125    |
----------------------------------------

 ---------------- Iteration 671 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 670        |
| Time/Actor_Time         | 0.0749     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.0753     |
| Time/Buffer             | 0.00481    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22869885 |
| Train/Action_magnitu... | 0.52060455 |
| Train/Action_magnitude  | 0.40453997 |
| Train/Action_max        | 0.19566424 |
| Train/Action_std        | 0.13997059 |
| Train/Entropy           | -0.5835766 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1506292  |
| Train/Loss              | 0.23985648 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.23524898 |
| Train/Ratio             | 0.9999886  |
| Train/Return            | 2.1995711  |
| Train/V                 | 2.4348204  |
| Train/Value             | 2.4348204  |
| Train/control_penalty   | 0.402392   |
| Train/policy_loss       | 0.23524898 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0355     |
----------------------------------------

 ---------------- Iteration 672 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 671         |
| Time/Actor_Time         | 0.0729      |
| Time/B_Format_Time      | 0.0715      |
| Time/B_Original_Form... | 0.0752      |
| Time/Buffer             | 0.00764     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24928056  |
| Train/Action_magnitu... | 0.5611353   |
| Train/Action_magnitude  | 0.43623176  |
| Train/Action_max        | 0.20885132  |
| Train/Action_std        | 0.14630948  |
| Train/Entropy           | -0.53523684 |
| Train/Entropy_Loss      | 0.000535    |
| Train/Entropy_loss      | 0.000535    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0388218   |
| Train/Loss              | 0.18024491  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17534778  |
| Train/Ratio             | 0.99996525  |
| Train/Return            | 2.002176    |
| Train/V                 | 2.177527    |
| Train/Value             | 2.177527    |
| Train/control_penalty   | 0.43618998  |
| Train/policy_loss       | 0.17534778  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.037       |
-----------------------------------------

 ---------------- Iteration 673 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 672         |
| Time/Actor_Time         | 0.0749      |
| Time/B_Format_Time      | 0.0725      |
| Time/B_Original_Form... | 0.0763      |
| Time/Buffer             | 0.00397     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23990469  |
| Train/Action_magnitu... | 0.5417396   |
| Train/Action_magnitude  | 0.42347932  |
| Train/Action_max        | 0.19249956  |
| Train/Action_std        | 0.14555101  |
| Train/Entropy           | -0.54678285 |
| Train/Entropy_Loss      | 0.000547    |
| Train/Entropy_loss      | 0.000547    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.097439    |
| Train/Loss              | 0.15439343  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14964272  |
| Train/Ratio             | 0.99999684  |
| Train/Return            | 1.8852487   |
| Train/V                 | 2.0348876   |
| Train/Value             | 2.0348876   |
| Train/control_penalty   | 0.42039314  |
| Train/policy_loss       | 0.14964272  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0355      |
-----------------------------------------

 ---------------- Iteration 674 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 673         |
| Time/Actor_Time         | 0.074       |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0761      |
| Time/Buffer             | 0.00464     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23340404  |
| Train/Action_magnitu... | 0.524913    |
| Train/Action_magnitude  | 0.40623245  |
| Train/Action_max        | 0.21431997  |
| Train/Action_std        | 0.14107645  |
| Train/Entropy           | -0.57933354 |
| Train/Entropy_Loss      | 0.000579    |
| Train/Entropy_loss      | 0.000579    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1900483   |
| Train/Loss              | 0.20530993  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20068641  |
| Train/Ratio             | 0.9999641   |
| Train/Return            | 1.869612    |
| Train/V                 | 2.0702994   |
| Train/Value             | 2.0702994   |
| Train/control_penalty   | 0.40441912  |
| Train/policy_loss       | 0.20068641  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 675 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 674         |
| Time/Actor_Time         | 0.0731      |
| Time/B_Format_Time      | 0.0753      |
| Time/B_Original_Form... | 0.0787      |
| Time/Buffer             | 0.00396     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23528454  |
| Train/Action_magnitu... | 0.5315923   |
| Train/Action_magnitude  | 0.41250104  |
| Train/Action_max        | 0.19688529  |
| Train/Action_std        | 0.14207712  |
| Train/Entropy           | -0.56702137 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1442915   |
| Train/Loss              | 0.1773013   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17261983  |
| Train/Ratio             | 1.0000241   |
| Train/Return            | 1.9426898   |
| Train/V                 | 2.1153011   |
| Train/Value             | 2.1153011   |
| Train/control_penalty   | 0.41144437  |
| Train/policy_loss       | 0.17261983  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 676 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 675        |
| Time/Actor_Time         | 0.0732     |
| Time/B_Format_Time      | 0.0737     |
| Time/B_Original_Form... | 0.0746     |
| Time/Buffer             | 0.00467    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.24131985 |
| Train/Action_magnitu... | 0.5422133  |
| Train/Action_magnitude  | 0.42356497 |
| Train/Action_max        | 0.1993956  |
| Train/Action_std        | 0.14401229 |
| Train/Entropy           | -0.5558597 |
| Train/Entropy_Loss      | 0.000556   |
| Train/Entropy_loss      | 0.000556   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0940008  |
| Train/Loss              | 0.18407968 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1792674  |
| Train/Ratio             | 0.9999837  |
| Train/Return            | 1.9133633  |
| Train/V                 | 2.0926335  |
| Train/Value             | 2.0926335  |
| Train/control_penalty   | 0.4256417  |
| Train/policy_loss       | 0.1792674  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 677 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 676         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.075       |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.00719     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.231496    |
| Train/Action_magnitu... | 0.5275562   |
| Train/Action_magnitude  | 0.41014552  |
| Train/Action_max        | 0.20789924  |
| Train/Action_std        | 0.14084326  |
| Train/Entropy           | -0.58092916 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.181892    |
| Train/Loss              | 0.20505151  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2004241   |
| Train/Ratio             | 0.9999797   |
| Train/Return            | 1.9844972   |
| Train/V                 | 2.184936    |
| Train/Value             | 2.184936    |
| Train/control_penalty   | 0.40464923  |
| Train/policy_loss       | 0.2004241   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 678 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 677         |
| Time/Actor_Time         | 0.0733      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.0767      |
| Time/Buffer             | 0.00304     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2369256   |
| Train/Action_magnitu... | 0.5287655   |
| Train/Action_magnitude  | 0.4091162   |
| Train/Action_max        | 0.20452239  |
| Train/Action_std        | 0.14358935  |
| Train/Entropy           | -0.55653566 |
| Train/Entropy_Loss      | 0.000557    |
| Train/Entropy_loss      | 0.000557    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.127689    |
| Train/Loss              | 0.19439597  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18975087  |
| Train/Ratio             | 1.0000062   |
| Train/Return            | 1.9163853   |
| Train/V                 | 2.1061332   |
| Train/Value             | 2.1061332   |
| Train/control_penalty   | 0.40885743  |
| Train/policy_loss       | 0.18975087  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------

 ---------------- Iteration 679 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 678         |
| Time/Actor_Time         | 0.075       |
| Time/B_Format_Time      | 0.0795      |
| Time/B_Original_Form... | 0.0828      |
| Time/Buffer             | 0.00435     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23755737  |
| Train/Action_magnitu... | 0.53767884  |
| Train/Action_magnitude  | 0.4173798   |
| Train/Action_max        | 0.19475815  |
| Train/Action_std        | 0.14428364  |
| Train/Entropy           | -0.55215186 |
| Train/Entropy_Loss      | 0.000552    |
| Train/Entropy_loss      | 0.000552    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.118927    |
| Train/Loss              | 0.12891646  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12421725  |
| Train/Ratio             | 1.0000161   |
| Train/Return            | 1.874158    |
| Train/V                 | 1.998359    |
| Train/Value             | 1.998359    |
| Train/control_penalty   | 0.41470596  |
| Train/policy_loss       | 0.12421725  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 680 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 679         |
| Time/Actor_Time         | 0.0756      |
| Time/B_Format_Time      | 0.0777      |
| Time/B_Original_Form... | 0.0775      |
| Time/Buffer             | 0.00432     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23689815  |
| Train/Action_magnitu... | 0.5338367   |
| Train/Action_magnitude  | 0.41238818  |
| Train/Action_max        | 0.20431708  |
| Train/Action_std        | 0.1383535   |
| Train/Entropy           | -0.59459627 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1697026   |
| Train/Loss              | 0.18789108  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18323056  |
| Train/Ratio             | 0.9999928   |
| Train/Return            | 1.8940672   |
| Train/V                 | 2.0773048   |
| Train/Value             | 2.0773048   |
| Train/control_penalty   | 0.40659186  |
| Train/policy_loss       | 0.18323056  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 681 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 680        |
| Time/Actor_Time         | 0.0761     |
| Time/B_Format_Time      | 0.0695     |
| Time/B_Original_Form... | 0.072      |
| Time/Buffer             | 0.00291    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.22538185 |
| Train/Action_magnitu... | 0.5079337  |
| Train/Action_magnitude  | 0.3948294  |
| Train/Action_max        | 0.17846982 |
| Train/Action_std        | 0.14003201 |
| Train/Entropy           | -0.584467  |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2144876  |
| Train/Loss              | 0.14877746 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14420895 |
| Train/Ratio             | 1.0000229  |
| Train/Return            | 1.7560523  |
| Train/V                 | 1.9002578  |
| Train/Value             | 1.9002578  |
| Train/control_penalty   | 0.39840367 |
| Train/policy_loss       | 0.14420895 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------

 ---------------- Iteration 682 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 681         |
| Time/Actor_Time         | 0.0754      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0767      |
| Time/Buffer             | 0.00359     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23179102  |
| Train/Action_magnitu... | 0.5188863   |
| Train/Action_magnitude  | 0.40423006  |
| Train/Action_max        | 0.20421797  |
| Train/Action_std        | 0.14131017  |
| Train/Entropy           | -0.57639384 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1635487   |
| Train/Loss              | 0.17141843  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1668216   |
| Train/Ratio             | 0.99998236  |
| Train/Return            | 1.7425098   |
| Train/V                 | 1.9093262   |
| Train/Value             | 1.9093262   |
| Train/control_penalty   | 0.40204316  |
| Train/policy_loss       | 0.1668216   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 683 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 682        |
| Time/Actor_Time         | 0.0751     |
| Time/B_Format_Time      | 0.0726     |
| Time/B_Original_Form... | 0.0722     |
| Time/Buffer             | 0.00326    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23566996 |
| Train/Action_magnitu... | 0.5241452  |
| Train/Action_magnitude  | 0.40635332 |
| Train/Action_max        | 0.20292401 |
| Train/Action_std        | 0.13795786 |
| Train/Entropy           | -0.5974399 |
| Train/Entropy_Loss      | 0.000597   |
| Train/Entropy_loss      | 0.000597   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2124653  |
| Train/Loss              | 0.20914283 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20449677 |
| Train/Ratio             | 1.0000083  |
| Train/Return            | 1.6054878  |
| Train/V                 | 1.8099824  |
| Train/Value             | 1.8099824  |
| Train/control_penalty   | 0.40486172 |
| Train/policy_loss       | 0.20449677 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 684 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 683        |
| Time/Actor_Time         | 0.073      |
| Time/B_Format_Time      | 0.0701     |
| Time/B_Original_Form... | 0.0737     |
| Time/Buffer             | 0.00326    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22744645 |
| Train/Action_magnitu... | 0.50781846 |
| Train/Action_magnitude  | 0.3943392  |
| Train/Action_max        | 0.19428022 |
| Train/Action_std        | 0.13886456 |
| Train/Entropy           | -0.5885479 |
| Train/Entropy_Loss      | 0.000589   |
| Train/Entropy_loss      | 0.000589   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1648835  |
| Train/Loss              | 0.14486246 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14030406 |
| Train/Ratio             | 0.9999687  |
| Train/Return            | 1.3841283  |
| Train/V                 | 1.5244288  |
| Train/Value             | 1.5244288  |
| Train/control_penalty   | 0.39698452 |
| Train/policy_loss       | 0.14030406 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02025    |
----------------------------------------

 ---------------- Iteration 685 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 684        |
| Time/Actor_Time         | 0.0747     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.0761     |
| Time/Buffer             | 0.00324    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23100957 |
| Train/Action_magnitu... | 0.5111048  |
| Train/Action_magnitude  | 0.39750838 |
| Train/Action_max        | 0.19689283 |
| Train/Action_std        | 0.13739258 |
| Train/Entropy           | -0.6036048 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2088857  |
| Train/Loss              | 0.17453465 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16993447 |
| Train/Ratio             | 0.9999764  |
| Train/Return            | 1.605337   |
| Train/V                 | 1.7752851  |
| Train/Value             | 1.7752851  |
| Train/control_penalty   | 0.39965835 |
| Train/policy_loss       | 0.16993447 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 686 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 685         |
| Time/Actor_Time         | 0.0754      |
| Time/B_Format_Time      | 0.0759      |
| Time/B_Original_Form... | 0.0738      |
| Time/Buffer             | 0.00416     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23890503  |
| Train/Action_magnitu... | 0.5314223   |
| Train/Action_magnitude  | 0.41262105  |
| Train/Action_max        | 0.20717055  |
| Train/Action_std        | 0.13904431  |
| Train/Entropy           | -0.59122926 |
| Train/Entropy_Loss      | 0.000591    |
| Train/Entropy_loss      | 0.000591    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1687528   |
| Train/Loss              | 0.18349549  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17879939  |
| Train/Ratio             | 1.0000126   |
| Train/Return            | 1.5931563   |
| Train/V                 | 1.7719578   |
| Train/Value             | 1.7719578   |
| Train/control_penalty   | 0.41048676  |
| Train/policy_loss       | 0.17879939  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02525     |
-----------------------------------------

 ---------------- Iteration 687 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 686        |
| Time/Actor_Time         | 0.0733     |
| Time/B_Format_Time      | 0.0746     |
| Time/B_Original_Form... | 0.0729     |
| Time/Buffer             | 0.00267    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23427582 |
| Train/Action_magnitu... | 0.5213953  |
| Train/Action_magnitude  | 0.40598544 |
| Train/Action_max        | 0.2007413  |
| Train/Action_std        | 0.14091629 |
| Train/Entropy           | -0.5767218 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1659092  |
| Train/Loss              | 0.15841416 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15374151 |
| Train/Ratio             | 1.0000224  |
| Train/Return            | 1.4161932  |
| Train/V                 | 1.569926   |
| Train/Value             | 1.569926   |
| Train/control_penalty   | 0.40959245 |
| Train/policy_loss       | 0.15374151 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01775    |
----------------------------------------

 ---------------- Iteration 688 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 687        |
| Time/Actor_Time         | 0.0798     |
| Time/B_Format_Time      | 0.0798     |
| Time/B_Original_Form... | 0.0828     |
| Time/Buffer             | 0.0036     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23175646 |
| Train/Action_magnitu... | 0.51658493 |
| Train/Action_magnitude  | 0.39985153 |
| Train/Action_max        | 0.19063228 |
| Train/Action_std        | 0.13792723 |
| Train/Entropy           | -0.5956731 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1552423  |
| Train/Loss              | 0.16139813 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15678135 |
| Train/Ratio             | 1.0000184  |
| Train/Return            | 1.5978471  |
| Train/V                 | 1.7546304  |
| Train/Value             | 1.7546304  |
| Train/control_penalty   | 0.40211117 |
| Train/policy_loss       | 0.15678135 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 689 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 688         |
| Time/Actor_Time         | 0.0776      |
| Time/B_Format_Time      | 0.0778      |
| Time/B_Original_Form... | 0.0763      |
| Time/Buffer             | 0.00348     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23030657  |
| Train/Action_magnitu... | 0.51001596  |
| Train/Action_magnitude  | 0.3944708   |
| Train/Action_max        | 0.19460008  |
| Train/Action_std        | 0.13569693  |
| Train/Entropy           | -0.61348367 |
| Train/Entropy_Loss      | 0.000613    |
| Train/Entropy_loss      | 0.000613    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.243007    |
| Train/Loss              | 0.1368965   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13232218  |
| Train/Ratio             | 0.99998146  |
| Train/Return            | 1.42426     |
| Train/V                 | 1.5565819   |
| Train/Value             | 1.5565819   |
| Train/control_penalty   | 0.39608437  |
| Train/policy_loss       | 0.13232218  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 690 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 689        |
| Time/Actor_Time         | 0.0803     |
| Time/B_Format_Time      | 0.0829     |
| Time/B_Original_Form... | 0.0843     |
| Time/Buffer             | 0.00346    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23083399 |
| Train/Action_magnitu... | 0.51307017 |
| Train/Action_magnitude  | 0.39765123 |
| Train/Action_max        | 0.20427942 |
| Train/Action_std        | 0.13621178 |
| Train/Entropy           | -0.6105782 |
| Train/Entropy_Loss      | 0.000611   |
| Train/Entropy_loss      | 0.000611   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1975724  |
| Train/Loss              | 0.16425754 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15964822 |
| Train/Ratio             | 1.0000253  |
| Train/Return            | 1.4250171  |
| Train/V                 | 1.5846617  |
| Train/Value             | 1.5846617  |
| Train/control_penalty   | 0.3998737  |
| Train/policy_loss       | 0.15964822 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01875    |
----------------------------------------

 ---------------- Iteration 691 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 690        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0722     |
| Time/Buffer             | 0.00316    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23775318 |
| Train/Action_magnitu... | 0.529701   |
| Train/Action_magnitude  | 0.409719   |
| Train/Action_max        | 0.2122501  |
| Train/Action_std        | 0.13769378 |
| Train/Entropy           | -0.5989326 |
| Train/Entropy_Loss      | 0.000599   |
| Train/Entropy_loss      | 0.000599   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1890264  |
| Train/Loss              | 0.16893783 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16428246 |
| Train/Ratio             | 1.0000002  |
| Train/Return            | 1.58005    |
| Train/V                 | 1.744327   |
| Train/Value             | 1.744327   |
| Train/control_penalty   | 0.40564358 |
| Train/policy_loss       | 0.16428246 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 692 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 691        |
| Time/Actor_Time         | 0.0732     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.0739     |
| Time/Buffer             | 0.00328    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23551463 |
| Train/Action_magnitu... | 0.5200724  |
| Train/Action_magnitude  | 0.4027692  |
| Train/Action_max        | 0.19464181 |
| Train/Action_std        | 0.13716483 |
| Train/Entropy           | -0.6042151 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2074021  |
| Train/Loss              | 0.1318268  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12714861 |
| Train/Ratio             | 0.9999945  |
| Train/Return            | 1.523678   |
| Train/V                 | 1.6508286  |
| Train/Value             | 1.6508286  |
| Train/control_penalty   | 0.40739766 |
| Train/policy_loss       | 0.12714861 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 693 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 692         |
| Time/Actor_Time         | 0.0734      |
| Time/B_Format_Time      | 0.0737      |
| Time/B_Original_Form... | 0.0739      |
| Time/Buffer             | 0.0046      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24022853  |
| Train/Action_magnitu... | 0.53356117  |
| Train/Action_magnitude  | 0.41321453  |
| Train/Action_max        | 0.19238342  |
| Train/Action_std        | 0.13917944  |
| Train/Entropy           | -0.58703655 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1957735   |
| Train/Loss              | 0.11682807  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11209785  |
| Train/Ratio             | 0.9999937   |
| Train/Return            | 1.6667869   |
| Train/V                 | 1.7788829   |
| Train/Value             | 1.7788829   |
| Train/control_penalty   | 0.41431776  |
| Train/policy_loss       | 0.11209785  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 694 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 693         |
| Time/Actor_Time         | 0.0731      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00341     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2346028   |
| Train/Action_magnitu... | 0.5232383   |
| Train/Action_magnitude  | 0.4053931   |
| Train/Action_max        | 0.18552063  |
| Train/Action_std        | 0.1385986   |
| Train/Entropy           | -0.59263587 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1886245   |
| Train/Loss              | 0.120991945 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11638825  |
| Train/Ratio             | 1.0000046   |
| Train/Return            | 1.5886824   |
| Train/V                 | 1.7050719   |
| Train/Value             | 1.7050719   |
| Train/control_penalty   | 0.40110666  |
| Train/policy_loss       | 0.11638825  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 695 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 694        |
| Time/Actor_Time         | 0.0894     |
| Time/B_Format_Time      | 0.0838     |
| Time/B_Original_Form... | 0.0808     |
| Time/Buffer             | 0.00408    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23985308 |
| Train/Action_magnitu... | 0.53726125 |
| Train/Action_magnitude  | 0.41540805 |
| Train/Action_max        | 0.2104286  |
| Train/Action_std        | 0.14083593 |
| Train/Entropy           | -0.5734585 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.140317   |
| Train/Loss              | 0.16484684 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1601217  |
| Train/Ratio             | 1.0000138  |
| Train/Return            | 1.5914177  |
| Train/V                 | 1.7515357  |
| Train/Value             | 1.7515357  |
| Train/control_penalty   | 0.4151692  |
| Train/policy_loss       | 0.1601217  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02375    |
----------------------------------------

 ---------------- Iteration 696 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 695        |
| Time/Actor_Time         | 0.0789     |
| Time/B_Format_Time      | 0.0882     |
| Time/B_Original_Form... | 0.0867     |
| Time/Buffer             | 0.00519    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23294449 |
| Train/Action_magnitu... | 0.51597124 |
| Train/Action_magnitude  | 0.40062115 |
| Train/Action_max        | 0.1885773  |
| Train/Action_std        | 0.13993706 |
| Train/Entropy           | -0.5831558 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1929822  |
| Train/Loss              | 0.10819636 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10358842 |
| Train/Ratio             | 0.9999981  |
| Train/Return            | 1.6596     |
| Train/V                 | 1.763183   |
| Train/Value             | 1.763183   |
| Train/control_penalty   | 0.4024787  |
| Train/policy_loss       | 0.10358842 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------

 ---------------- Iteration 697 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 696         |
| Time/Actor_Time         | 0.0766      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00637     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23444527  |
| Train/Action_magnitu... | 0.52590966  |
| Train/Action_magnitude  | 0.40588155  |
| Train/Action_max        | 0.19132744  |
| Train/Action_std        | 0.13929969  |
| Train/Entropy           | -0.58662826 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1813174   |
| Train/Loss              | 0.1332931   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12868018  |
| Train/Ratio             | 0.9999904   |
| Train/Return            | 1.706146    |
| Train/V                 | 1.834834    |
| Train/Value             | 1.834834    |
| Train/control_penalty   | 0.40262917  |
| Train/policy_loss       | 0.12868018  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 698 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 697        |
| Time/Actor_Time         | 0.0757     |
| Time/B_Format_Time      | 0.0836     |
| Time/B_Original_Form... | 0.0867     |
| Time/Buffer             | 0.00423    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23725547 |
| Train/Action_magnitu... | 0.5311265  |
| Train/Action_magnitude  | 0.41304272 |
| Train/Action_max        | 0.20025656 |
| Train/Action_std        | 0.14027114 |
| Train/Entropy           | -0.5816887 |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1784749  |
| Train/Loss              | 0.16448067 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15976141 |
| Train/Ratio             | 1.0000017  |
| Train/Return            | 1.7023028  |
| Train/V                 | 1.8620523  |
| Train/Value             | 1.8620523  |
| Train/control_penalty   | 0.4137582  |
| Train/policy_loss       | 0.15976141 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 699 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 698         |
| Time/Actor_Time         | 0.0732      |
| Time/B_Format_Time      | 0.0712      |
| Time/B_Original_Form... | 0.0743      |
| Time/Buffer             | 0.0041      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24540547  |
| Train/Action_magnitu... | 0.5443285   |
| Train/Action_magnitude  | 0.42144012  |
| Train/Action_max        | 0.1905205   |
| Train/Action_std        | 0.14001364  |
| Train/Entropy           | -0.58134204 |
| Train/Entropy_Loss      | 0.000581    |
| Train/Entropy_loss      | 0.000581    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1790749   |
| Train/Loss              | 0.11046003  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10569375  |
| Train/Ratio             | 0.99998194  |
| Train/Return            | 1.8174671   |
| Train/V                 | 1.9231718   |
| Train/Value             | 1.9231718   |
| Train/control_penalty   | 0.4184939   |
| Train/policy_loss       | 0.10569375  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.036       |
-----------------------------------------

 ---------------- Iteration 700 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 699        |
| Time/Actor_Time         | 0.0789     |
| Time/B_Format_Time      | 0.0817     |
| Time/B_Original_Form... | 0.0837     |
| Time/Buffer             | 0.00389    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23077829 |
| Train/Action_magnitu... | 0.51818633 |
| Train/Action_magnitude  | 0.40347895 |
| Train/Action_max        | 0.17857178 |
| Train/Action_std        | 0.13827878 |
| Train/Entropy           | -0.5919946 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.151141   |
| Train/Loss              | 0.1966568  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19204248 |
| Train/Ratio             | 1.0000205  |
| Train/Return            | 1.7355767  |
| Train/V                 | 1.9276061  |
| Train/Value             | 1.9276061  |
| Train/control_penalty   | 0.4022312  |
| Train/policy_loss       | 0.19204248 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 701 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 700        |
| Time/Actor_Time         | 0.0751     |
| Time/B_Format_Time      | 0.0722     |
| Time/B_Original_Form... | 0.0756     |
| Time/Buffer             | 0.0041     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23979314 |
| Train/Action_magnitu... | 0.53824204 |
| Train/Action_magnitude  | 0.41939288 |
| Train/Action_max        | 0.18993755 |
| Train/Action_std        | 0.14150986 |
| Train/Entropy           | -0.5682916 |
| Train/Entropy_Loss      | 0.000568   |
| Train/Entropy_loss      | 0.000568   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1446494  |
| Train/Loss              | 0.12948585 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12475606 |
| Train/Ratio             | 0.999996   |
| Train/Return            | 1.8045533  |
| Train/V                 | 1.9293162  |
| Train/Value             | 1.9293162  |
| Train/control_penalty   | 0.41614985 |
| Train/policy_loss       | 0.12475606 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0355     |
----------------------------------------

 ---------------- Iteration 702 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 701        |
| Time/Actor_Time         | 0.0806     |
| Time/B_Format_Time      | 0.0847     |
| Time/B_Original_Form... | 0.089      |
| Time/Buffer             | 0.00455    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2397525  |
| Train/Action_magnitu... | 0.5370617  |
| Train/Action_magnitude  | 0.41664132 |
| Train/Action_max        | 0.17399259 |
| Train/Action_std        | 0.13904917 |
| Train/Entropy           | -0.5851813 |
| Train/Entropy_Loss      | 0.000585   |
| Train/Entropy_loss      | 0.000585   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1688529  |
| Train/Loss              | 0.11256176 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10784539 |
| Train/Ratio             | 1.0000135  |
| Train/Return            | 1.8318467  |
| Train/V                 | 1.9396938  |
| Train/Value             | 1.9396938  |
| Train/control_penalty   | 0.4131189  |
| Train/policy_loss       | 0.10784539 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03275    |
----------------------------------------

 ---------------- Iteration 703 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 702         |
| Time/Actor_Time         | 0.0769      |
| Time/B_Format_Time      | 0.0774      |
| Time/B_Original_Form... | 0.0807      |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22938646  |
| Train/Action_magnitu... | 0.5115262   |
| Train/Action_magnitude  | 0.39605778  |
| Train/Action_max        | 0.18122849  |
| Train/Action_std        | 0.13854906  |
| Train/Entropy           | -0.59247094 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2060511   |
| Train/Loss              | 0.15482993  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15028171  |
| Train/Ratio             | 1.0000479   |
| Train/Return            | 1.5734215   |
| Train/V                 | 1.7236916   |
| Train/Value             | 1.7236916   |
| Train/control_penalty   | 0.39557508  |
| Train/policy_loss       | 0.15028171  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 704 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 703         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0776      |
| Time/B_Original_Form... | 0.0795      |
| Time/Buffer             | 0.00419     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23030555  |
| Train/Action_magnitu... | 0.5188957   |
| Train/Action_magnitude  | 0.40192375  |
| Train/Action_max        | 0.19274339  |
| Train/Action_std        | 0.13811585  |
| Train/Entropy           | -0.59751123 |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1972722   |
| Train/Loss              | 0.14321819  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13863124  |
| Train/Ratio             | 0.9999931   |
| Train/Return            | 1.4669611   |
| Train/V                 | 1.6056002   |
| Train/Value             | 1.6056002   |
| Train/control_penalty   | 0.3989443   |
| Train/policy_loss       | 0.13863124  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 705 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 704        |
| Time/Actor_Time         | 0.0727     |
| Time/B_Format_Time      | 0.078      |
| Time/B_Original_Form... | 0.0765     |
| Time/Buffer             | 0.00448    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23128994 |
| Train/Action_magnitu... | 0.5167373  |
| Train/Action_magnitude  | 0.40105844 |
| Train/Action_max        | 0.1837202  |
| Train/Action_std        | 0.13794151 |
| Train/Entropy           | -0.5961143 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1783699  |
| Train/Loss              | 0.1026045  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09797319 |
| Train/Ratio             | 1.0000037  |
| Train/Return            | 1.489994   |
| Train/V                 | 1.5879678  |
| Train/Value             | 1.5879678  |
| Train/control_penalty   | 0.40351996 |
| Train/policy_loss       | 0.09797319 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.029      |
----------------------------------------

 ---------------- Iteration 706 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 705         |
| Time/Actor_Time         | 0.0736      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0786      |
| Time/Buffer             | 0.00416     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2324371   |
| Train/Action_magnitu... | 0.5165797   |
| Train/Action_magnitude  | 0.40033638  |
| Train/Action_max        | 0.18646598  |
| Train/Action_std        | 0.13768995  |
| Train/Entropy           | -0.59512323 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1840494   |
| Train/Loss              | 0.11139941  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10678525  |
| Train/Ratio             | 1.000019    |
| Train/Return            | 1.522019    |
| Train/V                 | 1.6288049   |
| Train/Value             | 1.6288049   |
| Train/control_penalty   | 0.40190408  |
| Train/policy_loss       | 0.10678525  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 707 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 706         |
| Time/Actor_Time         | 0.0737      |
| Time/B_Format_Time      | 0.0798      |
| Time/B_Original_Form... | 0.0801      |
| Time/Buffer             | 0.00395     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24496454  |
| Train/Action_magnitu... | 0.5352781   |
| Train/Action_magnitude  | 0.41434237  |
| Train/Action_max        | 0.18864228  |
| Train/Action_std        | 0.1376724   |
| Train/Entropy           | -0.59694904 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2061837   |
| Train/Loss              | 0.11205113  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10726322  |
| Train/Ratio             | 1.0000054   |
| Train/Return            | 1.5357277   |
| Train/V                 | 1.642985    |
| Train/Value             | 1.642985    |
| Train/control_penalty   | 0.41909617  |
| Train/policy_loss       | 0.10726322  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 708 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 707        |
| Time/Actor_Time         | 0.075      |
| Time/B_Format_Time      | 0.0725     |
| Time/B_Original_Form... | 0.0751     |
| Time/Buffer             | 0.00336    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22837155 |
| Train/Action_magnitu... | 0.5098769  |
| Train/Action_magnitude  | 0.39879027 |
| Train/Action_max        | 0.16987596 |
| Train/Action_std        | 0.13867122 |
| Train/Entropy           | -0.5915295 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1915544  |
| Train/Loss              | 0.13732427 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1327162  |
| Train/Ratio             | 1.0000023  |
| Train/Return            | 1.5614635  |
| Train/V                 | 1.6941843  |
| Train/Value             | 1.6941843  |
| Train/control_penalty   | 0.40165427 |
| Train/policy_loss       | 0.1327162  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 709 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 708        |
| Time/Actor_Time         | 0.0748     |
| Time/B_Format_Time      | 0.078      |
| Time/B_Original_Form... | 0.0782     |
| Time/Buffer             | 0.0044     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23226778 |
| Train/Action_magnitu... | 0.5135383  |
| Train/Action_magnitude  | 0.3988317  |
| Train/Action_max        | 0.19359261 |
| Train/Action_std        | 0.13803875 |
| Train/Entropy           | -0.5969916 |
| Train/Entropy_Loss      | 0.000597   |
| Train/Entropy_loss      | 0.000597   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.192731   |
| Train/Loss              | 0.13087188 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1262262  |
| Train/Ratio             | 0.999989   |
| Train/Return            | 1.4693379  |
| Train/V                 | 1.5955704  |
| Train/Value             | 1.5955704  |
| Train/control_penalty   | 0.4048685  |
| Train/policy_loss       | 0.1262262  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 710 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 709        |
| Time/Actor_Time         | 0.0731     |
| Time/B_Format_Time      | 0.0715     |
| Time/B_Original_Form... | 0.0747     |
| Time/Buffer             | 0.00348    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22569862 |
| Train/Action_magnitu... | 0.5037342  |
| Train/Action_magnitude  | 0.39189917 |
| Train/Action_max        | 0.18357022 |
| Train/Action_std        | 0.1324912  |
| Train/Entropy           | -0.6339833 |
| Train/Entropy_Loss      | 0.000634   |
| Train/Entropy_loss      | 0.000634   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2815142  |
| Train/Loss              | 0.11289063 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1083232  |
| Train/Ratio             | 0.9999767  |
| Train/Return            | 1.4026676  |
| Train/V                 | 1.5109901  |
| Train/Value             | 1.5109901  |
| Train/control_penalty   | 0.39334485 |
| Train/policy_loss       | 0.1083232  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 711 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 710         |
| Time/Actor_Time         | 0.0795      |
| Time/B_Format_Time      | 0.0816      |
| Time/B_Original_Form... | 0.0833      |
| Time/Buffer             | 0.00471     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23009516  |
| Train/Action_magnitu... | 0.5160548   |
| Train/Action_magnitude  | 0.40259498  |
| Train/Action_max        | 0.17297928  |
| Train/Action_std        | 0.13853008  |
| Train/Entropy           | -0.5936506  |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2158995   |
| Train/Loss              | 0.12599495  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.121351175 |
| Train/Ratio             | 1.000026    |
| Train/Return            | 1.4600552   |
| Train/V                 | 1.5814023   |
| Train/Value             | 1.5814023   |
| Train/control_penalty   | 0.4050127   |
| Train/policy_loss       | 0.121351175 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 712 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 711        |
| Time/Actor_Time         | 0.0832     |
| Time/B_Format_Time      | 0.0893     |
| Time/B_Original_Form... | 0.0841     |
| Time/Buffer             | 0.00425    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22355555 |
| Train/Action_magnitu... | 0.50202394 |
| Train/Action_magnitude  | 0.3910203  |
| Train/Action_max        | 0.18834753 |
| Train/Action_std        | 0.13496463 |
| Train/Entropy           | -0.6182402 |
| Train/Entropy_Loss      | 0.000618   |
| Train/Entropy_loss      | 0.000618   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2396975  |
| Train/Loss              | 0.1644334  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1598674  |
| Train/Ratio             | 1.0000098  |
| Train/Return            | 1.3663235  |
| Train/V                 | 1.5261805  |
| Train/Value             | 1.5261805  |
| Train/control_penalty   | 0.39477623 |
| Train/policy_loss       | 0.1598674  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02025    |
----------------------------------------

 ---------------- Iteration 713 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 712        |
| Time/Actor_Time         | 0.0787     |
| Time/B_Format_Time      | 0.076      |
| Time/B_Original_Form... | 0.0782     |
| Time/Buffer             | 0.00411    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23500489 |
| Train/Action_magnitu... | 0.5283509  |
| Train/Action_magnitude  | 0.41092554 |
| Train/Action_max        | 0.20175616 |
| Train/Action_std        | 0.13989261 |
| Train/Entropy           | -0.58101   |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1494646  |
| Train/Loss              | 0.16211705 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15738873 |
| Train/Ratio             | 1.0000113  |
| Train/Return            | 1.6499825  |
| Train/V                 | 1.8073637  |
| Train/Value             | 1.8073637  |
| Train/control_penalty   | 0.4147313  |
| Train/policy_loss       | 0.15738873 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02125    |
----------------------------------------

 ---------------- Iteration 714 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 713         |
| Time/Actor_Time         | 0.0751      |
| Time/B_Format_Time      | 0.0791      |
| Time/B_Original_Form... | 0.0795      |
| Time/Buffer             | 0.004       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2343737   |
| Train/Action_magnitu... | 0.52120495  |
| Train/Action_magnitude  | 0.40576956  |
| Train/Action_max        | 0.18630244  |
| Train/Action_std        | 0.13669564  |
| Train/Entropy           | -0.60666126 |
| Train/Entropy_Loss      | 0.000607    |
| Train/Entropy_loss      | 0.000607    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2177057   |
| Train/Loss              | 0.1131744   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10852202  |
| Train/Ratio             | 0.9999997   |
| Train/Return            | 1.5596054   |
| Train/V                 | 1.6681272   |
| Train/Value             | 1.6681272   |
| Train/control_penalty   | 0.4045716   |
| Train/policy_loss       | 0.10852202  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 715 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 714         |
| Time/Actor_Time         | 0.0759      |
| Time/B_Format_Time      | 0.0786      |
| Time/B_Original_Form... | 0.0796      |
| Time/Buffer             | 0.00446     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23820664  |
| Train/Action_magnitu... | 0.5296541   |
| Train/Action_magnitude  | 0.41257772  |
| Train/Action_max        | 0.19055082  |
| Train/Action_std        | 0.13585198  |
| Train/Entropy           | -0.61184305 |
| Train/Entropy_Loss      | 0.000612    |
| Train/Entropy_loss      | 0.000612    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2298917   |
| Train/Loss              | 0.18853502  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18378879  |
| Train/Ratio             | 1.0000086   |
| Train/Return            | 1.586769    |
| Train/V                 | 1.7705379   |
| Train/Value             | 1.7705379   |
| Train/control_penalty   | 0.4134381   |
| Train/policy_loss       | 0.18378879  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 716 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 715        |
| Time/Actor_Time         | 0.0761     |
| Time/B_Format_Time      | 0.074      |
| Time/B_Original_Form... | 0.0783     |
| Time/Buffer             | 0.00403    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23458181 |
| Train/Action_magnitu... | 0.5328523  |
| Train/Action_magnitude  | 0.4168473  |
| Train/Action_max        | 0.18366894 |
| Train/Action_std        | 0.14178707 |
| Train/Entropy           | -0.5713827 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1404479  |
| Train/Loss              | 0.17802839 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17324151 |
| Train/Ratio             | 0.9999947  |
| Train/Return            | 1.6663257  |
| Train/V                 | 1.8395573  |
| Train/Value             | 1.8395573  |
| Train/control_penalty   | 0.42154875 |
| Train/policy_loss       | 0.17324151 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.021      |
----------------------------------------

 ---------------- Iteration 717 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 716        |
| Time/Actor_Time         | 0.0767     |
| Time/B_Format_Time      | 0.0751     |
| Time/B_Original_Form... | 0.0753     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24610913 |
| Train/Action_magnitu... | 0.546895   |
| Train/Action_magnitude  | 0.4256206  |
| Train/Action_max        | 0.20038071 |
| Train/Action_std        | 0.13996968 |
| Train/Entropy           | -0.5816621 |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.189908   |
| Train/Loss              | 0.16870707 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16389461 |
| Train/Ratio             | 1.0000008  |
| Train/Return            | 1.6456659  |
| Train/V                 | 1.8095664  |
| Train/Value             | 1.8095664  |
| Train/control_penalty   | 0.4230799  |
| Train/policy_loss       | 0.16389461 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 718 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 717         |
| Time/Actor_Time         | 0.0742      |
| Time/B_Format_Time      | 0.073       |
| Time/B_Original_Form... | 0.0758      |
| Time/Buffer             | 0.00335     |
| Time/Critic_Time        | 1.91e-06    |
| Train/Action_abs_mean   | 0.24144529  |
| Train/Action_magnitu... | 0.5342555   |
| Train/Action_magnitude  | 0.4153666   |
| Train/Action_max        | 0.18131588  |
| Train/Action_std        | 0.13914049  |
| Train/Entropy           | -0.58599883 |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1803477   |
| Train/Loss              | 0.15147133  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14673056  |
| Train/Ratio             | 0.99999404  |
| Train/Return            | 1.5024426   |
| Train/V                 | 1.6491758   |
| Train/Value             | 1.6491758   |
| Train/control_penalty   | 0.41547737  |
| Train/policy_loss       | 0.14673056  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 719 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 718        |
| Time/Actor_Time         | 0.0781     |
| Time/B_Format_Time      | 0.078      |
| Time/B_Original_Form... | 0.0824     |
| Time/Buffer             | 0.00409    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22582164 |
| Train/Action_magnitu... | 0.51463896 |
| Train/Action_magnitude  | 0.40313172 |
| Train/Action_max        | 0.1950738  |
| Train/Action_std        | 0.1415626  |
| Train/Entropy           | -0.5714598 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.132343   |
| Train/Loss              | 0.16556281 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16094917 |
| Train/Ratio             | 1.0000209  |
| Train/Return            | 1.6002442  |
| Train/V                 | 1.7611861  |
| Train/Value             | 1.7611861  |
| Train/control_penalty   | 0.40421838 |
| Train/policy_loss       | 0.16094917 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 720 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 719         |
| Time/Actor_Time         | 0.0764      |
| Time/B_Format_Time      | 0.0777      |
| Time/B_Original_Form... | 0.0786      |
| Time/Buffer             | 0.00343     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22947949  |
| Train/Action_magnitu... | 0.5168245   |
| Train/Action_magnitude  | 0.40543526  |
| Train/Action_max        | 0.1707578   |
| Train/Action_std        | 0.13821878  |
| Train/Entropy           | -0.59709626 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.186546    |
| Train/Loss              | 0.21413219  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20947191  |
| Train/Ratio             | 0.999995    |
| Train/Return            | 1.7195987   |
| Train/V                 | 1.9290694   |
| Train/Value             | 1.9290694   |
| Train/control_penalty   | 0.40631908  |
| Train/policy_loss       | 0.20947191  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.021       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 721 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 720        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0775     |
| Time/B_Original_Form... | 0.0822     |
| Time/Buffer             | 0.00363    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23826163 |
| Train/Action_magnitu... | 0.5408164  |
| Train/Action_magnitude  | 0.42315567 |
| Train/Action_max        | 0.18633096 |
| Train/Action_std        | 0.14106125 |
| Train/Entropy           | -0.5737238 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1668794  |
| Train/Loss              | 0.21150249 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20671575 |
| Train/Ratio             | 0.9999796  |
| Train/Return            | 1.8874041  |
| Train/V                 | 2.0941195  |
| Train/Value             | 2.0941195  |
| Train/control_penalty   | 0.4213019  |
| Train/policy_loss       | 0.20671575 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 722 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 721        |
| Time/Actor_Time         | 0.0766     |
| Time/B_Format_Time      | 0.0739     |
| Time/B_Original_Form... | 0.0784     |
| Time/Buffer             | 0.00372    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23667091 |
| Train/Action_magnitu... | 0.5313294  |
| Train/Action_magnitude  | 0.41663173 |
| Train/Action_max        | 0.17416352 |
| Train/Action_std        | 0.14124553 |
| Train/Entropy           | -0.5712576 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1583189  |
| Train/Loss              | 0.2267207  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.22194101 |
| Train/Ratio             | 0.9999908  |
| Train/Return            | 1.8097919  |
| Train/V                 | 2.0317414  |
| Train/Value             | 2.0317414  |
| Train/control_penalty   | 0.42084515 |
| Train/policy_loss       | 0.22194101 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02175    |
----------------------------------------

 ---------------- Iteration 723 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 722         |
| Time/Actor_Time         | 0.0741      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0733      |
| Time/Buffer             | 0.00421     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23626155  |
| Train/Action_magnitu... | 0.5366066   |
| Train/Action_magnitude  | 0.4203855   |
| Train/Action_max        | 0.17680348  |
| Train/Action_std        | 0.13958745  |
| Train/Entropy           | -0.58151656 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1303939   |
| Train/Loss              | 0.24255633  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.23778223  |
| Train/Ratio             | 1.0000048   |
| Train/Return            | 1.8187451   |
| Train/V                 | 2.0565166   |
| Train/Value             | 2.0565166   |
| Train/control_penalty   | 0.41925848  |
| Train/policy_loss       | 0.23778223  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 724 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 723        |
| Time/Actor_Time         | 0.0765     |
| Time/B_Format_Time      | 0.0747     |
| Time/B_Original_Form... | 0.0758     |
| Time/Buffer             | 0.00466    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2420228  |
| Train/Action_magnitu... | 0.53952557 |
| Train/Action_magnitude  | 0.42353773 |
| Train/Action_max        | 0.19520932 |
| Train/Action_std        | 0.14143203 |
| Train/Entropy           | -0.5707637 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1404041  |
| Train/Loss              | 0.18934731 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18448754 |
| Train/Ratio             | 1.0000056  |
| Train/Return            | 1.7012261  |
| Train/V                 | 1.8857107  |
| Train/Value             | 1.8857107  |
| Train/control_penalty   | 0.42890197 |
| Train/policy_loss       | 0.18448754 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 725 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 724        |
| Time/Actor_Time         | 0.0764     |
| Time/B_Format_Time      | 0.0784     |
| Time/B_Original_Form... | 0.0798     |
| Time/Buffer             | 0.00386    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24126507 |
| Train/Action_magnitu... | 0.531837   |
| Train/Action_magnitude  | 0.414524   |
| Train/Action_max        | 0.19077711 |
| Train/Action_std        | 0.13808681 |
| Train/Entropy           | -0.5925047 |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1713662  |
| Train/Loss              | 0.15933023 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15457341 |
| Train/Ratio             | 1.0000099  |
| Train/Return            | 1.7459112  |
| Train/V                 | 1.9004672  |
| Train/Value             | 1.9004672  |
| Train/control_penalty   | 0.41643268 |
| Train/policy_loss       | 0.15457341 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 726 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 725        |
| Time/Actor_Time         | 0.0783     |
| Time/B_Format_Time      | 0.0847     |
| Time/B_Original_Form... | 0.0843     |
| Time/Buffer             | 0.00454    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24672386 |
| Train/Action_magnitu... | 0.5526412  |
| Train/Action_magnitude  | 0.4323278  |
| Train/Action_max        | 0.2061295  |
| Train/Action_std        | 0.14138252 |
| Train/Entropy           | -0.5739137 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1738305  |
| Train/Loss              | 0.21165164 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20676874 |
| Train/Ratio             | 0.9999772  |
| Train/Return            | 1.7269547  |
| Train/V                 | 1.9337308  |
| Train/Value             | 1.9337308  |
| Train/control_penalty   | 0.43089905 |
| Train/policy_loss       | 0.20676874 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 727 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 726         |
| Time/Actor_Time         | 0.0838      |
| Time/B_Format_Time      | 0.0873      |
| Time/B_Original_Form... | 0.0903      |
| Time/Buffer             | 0.00479     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2429712   |
| Train/Action_magnitu... | 0.54487157  |
| Train/Action_magnitude  | 0.42959303  |
| Train/Action_max        | 0.16720185  |
| Train/Action_std        | 0.14102982  |
| Train/Entropy           | -0.56819403 |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1365564   |
| Train/Loss              | 0.16842023  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16355601  |
| Train/Ratio             | 0.9999773   |
| Train/Return            | 2.0769157   |
| Train/V                 | 2.240485    |
| Train/Value             | 2.240485    |
| Train/control_penalty   | 0.42960143  |
| Train/policy_loss       | 0.16355601  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02975     |
-----------------------------------------

 ---------------- Iteration 728 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 727        |
| Time/Actor_Time         | 0.0753     |
| Time/B_Format_Time      | 0.0737     |
| Time/B_Original_Form... | 0.0788     |
| Time/Buffer             | 0.00437    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24228744 |
| Train/Action_magnitu... | 0.54414546 |
| Train/Action_magnitude  | 0.42754716 |
| Train/Action_max        | 0.18240839 |
| Train/Action_std        | 0.14071402 |
| Train/Entropy           | -0.5754298 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1303152  |
| Train/Loss              | 0.20252538 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19767112 |
| Train/Ratio             | 1.0000141  |
| Train/Return            | 1.6542318  |
| Train/V                 | 1.8519233  |
| Train/Value             | 1.8519233  |
| Train/control_penalty   | 0.42788345 |
| Train/policy_loss       | 0.19767112 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03175    |
----------------------------------------

 ---------------- Iteration 729 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 728        |
| Time/Actor_Time         | 0.077      |
| Time/B_Format_Time      | 0.0805     |
| Time/B_Original_Form... | 0.0816     |
| Time/Buffer             | 0.00443    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23186739 |
| Train/Action_magnitu... | 0.52079666 |
| Train/Action_magnitude  | 0.4070336  |
| Train/Action_max        | 0.1832677  |
| Train/Action_std        | 0.13769025 |
| Train/Entropy           | -0.5961556 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2031968  |
| Train/Loss              | 0.13933393 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13464034 |
| Train/Ratio             | 0.9999939  |
| Train/Return            | 1.5688342  |
| Train/V                 | 1.7034807  |
| Train/Value             | 1.7034807  |
| Train/control_penalty   | 0.40974495 |
| Train/policy_loss       | 0.13464034 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02525    |
----------------------------------------

 ---------------- Iteration 730 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 729         |
| Time/Actor_Time         | 0.0783      |
| Time/B_Format_Time      | 0.0834      |
| Time/B_Original_Form... | 0.0849      |
| Time/Buffer             | 0.00449     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24274951  |
| Train/Action_magnitu... | 0.54368204  |
| Train/Action_magnitude  | 0.42587823  |
| Train/Action_max        | 0.1794788   |
| Train/Action_std        | 0.1397374   |
| Train/Entropy           | -0.58258003 |
| Train/Entropy_Loss      | 0.000583    |
| Train/Entropy_loss      | 0.000583    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1518269   |
| Train/Loss              | 0.18859611  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18376698  |
| Train/Ratio             | 0.9999949   |
| Train/Return            | 1.7466666   |
| Train/V                 | 1.93043     |
| Train/Value             | 1.93043     |
| Train/control_penalty   | 0.42465684  |
| Train/policy_loss       | 0.18376698  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 731 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 730        |
| Time/Actor_Time         | 0.0771     |
| Time/B_Format_Time      | 0.0825     |
| Time/B_Original_Form... | 0.0833     |
| Time/Buffer             | 0.00417    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24324974 |
| Train/Action_magnitu... | 0.54315704 |
| Train/Action_magnitude  | 0.4280822  |
| Train/Action_max        | 0.17672957 |
| Train/Action_std        | 0.14002904 |
| Train/Entropy           | -0.5774647 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1648482  |
| Train/Loss              | 0.16689405 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16202633 |
| Train/Ratio             | 0.9999993  |
| Train/Return            | 1.8938866  |
| Train/V                 | 2.0559168  |
| Train/Value             | 2.0559168  |
| Train/control_penalty   | 0.42902598 |
| Train/policy_loss       | 0.16202633 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------

 ---------------- Iteration 732 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 731        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.0785     |
| Time/B_Original_Form... | 0.0789     |
| Time/Buffer             | 0.0037     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23701014 |
| Train/Action_magnitu... | 0.5381583  |
| Train/Action_magnitude  | 0.4224776  |
| Train/Action_max        | 0.17543939 |
| Train/Action_std        | 0.14227185 |
| Train/Entropy           | -0.5624145 |
| Train/Entropy_Loss      | 0.000562   |
| Train/Entropy_loss      | 0.000562   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1339668  |
| Train/Loss              | 0.18744572 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18265907 |
| Train/Ratio             | 0.999971   |
| Train/Return            | 1.861228   |
| Train/V                 | 2.0439062  |
| Train/Value             | 2.0439062  |
| Train/control_penalty   | 0.42242303 |
| Train/policy_loss       | 0.18265907 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.034      |
----------------------------------------

 ---------------- Iteration 733 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 732         |
| Time/Actor_Time         | 0.0818      |
| Time/B_Format_Time      | 0.0843      |
| Time/B_Original_Form... | 0.0878      |
| Time/Buffer             | 0.00523     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2514409   |
| Train/Action_magnitu... | 0.55697685  |
| Train/Action_magnitude  | 0.43626872  |
| Train/Action_max        | 0.202237    |
| Train/Action_std        | 0.14303745  |
| Train/Entropy           | -0.55700547 |
| Train/Entropy_Loss      | 0.000557    |
| Train/Entropy_loss      | 0.000557    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1141346   |
| Train/Loss              | 0.12150483  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11652346  |
| Train/Ratio             | 0.9999933   |
| Train/Return            | 1.5523332   |
| Train/V                 | 1.6688622   |
| Train/Value             | 1.6688622   |
| Train/control_penalty   | 0.44243655  |
| Train/policy_loss       | 0.11652346  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 734 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 733        |
| Time/Actor_Time         | 0.0802     |
| Time/B_Format_Time      | 0.0817     |
| Time/B_Original_Form... | 0.0832     |
| Time/Buffer             | 0.00412    |
| Time/Critic_Time        | 1.43e-05   |
| Train/Action_abs_mean   | 0.23512334 |
| Train/Action_magnitu... | 0.52779824 |
| Train/Action_magnitude  | 0.41423023 |
| Train/Action_max        | 0.17715324 |
| Train/Action_std        | 0.14082547 |
| Train/Entropy           | -0.5743493 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1179409  |
| Train/Loss              | 0.10468843 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09997057 |
| Train/Ratio             | 0.9999829  |
| Train/Return            | 1.744651   |
| Train/V                 | 1.8446174  |
| Train/Value             | 1.8446174  |
| Train/control_penalty   | 0.41435045 |
| Train/policy_loss       | 0.09997057 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03525    |
----------------------------------------

 ---------------- Iteration 735 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 734        |
| Time/Actor_Time         | 0.0785     |
| Time/B_Format_Time      | 0.0823     |
| Time/B_Original_Form... | 0.0865     |
| Time/Buffer             | 0.00486    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24814057 |
| Train/Action_magnitu... | 0.5574832  |
| Train/Action_magnitude  | 0.43283862 |
| Train/Action_max        | 0.20173642 |
| Train/Action_std        | 0.1444979  |
| Train/Entropy           | -0.547757  |
| Train/Entropy_Loss      | 0.000548   |
| Train/Entropy_loss      | 0.000548   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.074401   |
| Train/Loss              | 0.10710545 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10225251 |
| Train/Ratio             | 0.9999736  |
| Train/Return            | 1.5549328  |
| Train/V                 | 1.657197   |
| Train/Value             | 1.657197   |
| Train/control_penalty   | 0.43051782 |
| Train/policy_loss       | 0.10225251 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03175    |
----------------------------------------

 ---------------- Iteration 736 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 735         |
| Time/Actor_Time         | 0.0762      |
| Time/B_Format_Time      | 0.0781      |
| Time/B_Original_Form... | 0.0816      |
| Time/Buffer             | 0.0045      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23818034  |
| Train/Action_magnitu... | 0.5354403   |
| Train/Action_magnitude  | 0.42052048  |
| Train/Action_max        | 0.18645927  |
| Train/Action_std        | 0.14299203  |
| Train/Entropy           | -0.55966604 |
| Train/Entropy_Loss      | 0.00056     |
| Train/Entropy_loss      | 0.00056     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1238314   |
| Train/Loss              | 0.065578796 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.060787555 |
| Train/Ratio             | 0.999988    |
| Train/Return            | 1.7441238   |
| Train/V                 | 1.8049102   |
| Train/Value             | 1.8049102   |
| Train/control_penalty   | 0.4231575   |
| Train/policy_loss       | 0.060787555 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.04075     |
-----------------------------------------

 ---------------- Iteration 737 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 736         |
| Time/Actor_Time         | 0.0798      |
| Time/B_Format_Time      | 0.0811      |
| Time/B_Original_Form... | 0.0815      |
| Time/Buffer             | 0.00403     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23196422  |
| Train/Action_magnitu... | 0.52231437  |
| Train/Action_magnitude  | 0.40766078  |
| Train/Action_max        | 0.18542203  |
| Train/Action_std        | 0.14475079  |
| Train/Entropy           | -0.54914355 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1012079   |
| Train/Loss              | 0.113492176 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10883557  |
| Train/Ratio             | 0.9999906   |
| Train/Return            | 1.6878064   |
| Train/V                 | 1.7966352   |
| Train/Value             | 1.7966352   |
| Train/control_penalty   | 0.41074604  |
| Train/policy_loss       | 0.10883557  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 738 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 737        |
| Time/Actor_Time         | 0.0743     |
| Time/B_Format_Time      | 0.0762     |
| Time/B_Original_Form... | 0.0745     |
| Time/Buffer             | 0.00388    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23435245 |
| Train/Action_magnitu... | 0.52853453 |
| Train/Action_magnitude  | 0.41554952 |
| Train/Action_max        | 0.17937036 |
| Train/Action_std        | 0.14193414 |
| Train/Entropy           | -0.5736468 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1177589  |
| Train/Loss              | 0.13230681 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12758613 |
| Train/Ratio             | 0.99999154 |
| Train/Return            | 1.4806708  |
| Train/V                 | 1.6082633  |
| Train/Value             | 1.6082633  |
| Train/control_penalty   | 0.4147044  |
| Train/policy_loss       | 0.12758613 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 739 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 738         |
| Time/Actor_Time         | 0.0767      |
| Time/B_Format_Time      | 0.0778      |
| Time/B_Original_Form... | 0.0787      |
| Time/Buffer             | 0.00345     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21997453  |
| Train/Action_magnitu... | 0.5012104   |
| Train/Action_magnitude  | 0.39332905  |
| Train/Action_max        | 0.17797402  |
| Train/Action_std        | 0.14218935  |
| Train/Entropy           | -0.57207036 |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1320086   |
| Train/Loss              | 0.1630908   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15854588  |
| Train/Ratio             | 0.99999845  |
| Train/Return            | 1.6840866   |
| Train/V                 | 1.8426327   |
| Train/Value             | 1.8426327   |
| Train/control_penalty   | 0.397284    |
| Train/policy_loss       | 0.15854588  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 740 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 739        |
| Time/Actor_Time         | 0.0736     |
| Time/B_Format_Time      | 0.0702     |
| Time/B_Original_Form... | 0.0734     |
| Time/Buffer             | 0.00286    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2184628  |
| Train/Action_magnitu... | 0.49308386 |
| Train/Action_magnitude  | 0.38404062 |
| Train/Action_max        | 0.18065098 |
| Train/Action_std        | 0.13596578 |
| Train/Entropy           | -0.6159326 |
| Train/Entropy_Loss      | 0.000616   |
| Train/Entropy_loss      | 0.000616   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2389512  |
| Train/Loss              | 0.09850977 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09406438 |
| Train/Ratio             | 1.0000018  |
| Train/Return            | 1.4909401  |
| Train/V                 | 1.5850105  |
| Train/Value             | 1.5850105  |
| Train/control_penalty   | 0.3829462  |
| Train/policy_loss       | 0.09406438 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 741 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 740        |
| Time/Actor_Time         | 0.0828     |
| Time/B_Format_Time      | 0.0876     |
| Time/B_Original_Form... | 0.0869     |
| Time/Buffer             | 0.00445    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21829405 |
| Train/Action_magnitu... | 0.50009155 |
| Train/Action_magnitude  | 0.39100266 |
| Train/Action_max        | 0.1777487  |
| Train/Action_std        | 0.13752979 |
| Train/Entropy           | -0.6036178 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1796622  |
| Train/Loss              | 0.1272275  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12278644 |
| Train/Ratio             | 1.0000067  |
| Train/Return            | 1.5427227  |
| Train/V                 | 1.6654996  |
| Train/Value             | 1.6654996  |
| Train/control_penalty   | 0.38374403 |
| Train/policy_loss       | 0.12278644 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 742 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 741         |
| Time/Actor_Time         | 0.0794      |
| Time/B_Format_Time      | 0.0771      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00365     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22491027  |
| Train/Action_magnitu... | 0.5166882   |
| Train/Action_magnitude  | 0.40561926  |
| Train/Action_max        | 0.17341919  |
| Train/Action_std        | 0.1449277   |
| Train/Entropy           | -0.55006105 |
| Train/Entropy_Loss      | 0.00055     |
| Train/Entropy_loss      | 0.00055     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1005063   |
| Train/Loss              | 0.14414425  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13954668  |
| Train/Ratio             | 1.000003    |
| Train/Return            | 1.6844825   |
| Train/V                 | 1.8240304   |
| Train/Value             | 1.8240304   |
| Train/control_penalty   | 0.40475112  |
| Train/policy_loss       | 0.13954668  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 743 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 742        |
| Time/Actor_Time         | 0.0758     |
| Time/B_Format_Time      | 0.0718     |
| Time/B_Original_Form... | 0.0742     |
| Time/Buffer             | 0.00417    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24007142 |
| Train/Action_magnitu... | 0.54753983 |
| Train/Action_magnitude  | 0.42990592 |
| Train/Action_max        | 0.17008601 |
| Train/Action_std        | 0.14565332 |
| Train/Entropy           | -0.5435178 |
| Train/Entropy_Loss      | 0.000544   |
| Train/Entropy_loss      | 0.000544   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0741209  |
| Train/Loss              | 0.10844096 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10361089 |
| Train/Ratio             | 0.9999929  |
| Train/Return            | 1.7210889  |
| Train/V                 | 1.8246956  |
| Train/Value             | 1.8246956  |
| Train/control_penalty   | 0.42865518 |
| Train/policy_loss       | 0.10361089 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 744 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 743         |
| Time/Actor_Time         | 0.0755      |
| Time/B_Format_Time      | 0.0745      |
| Time/B_Original_Form... | 0.0773      |
| Time/Buffer             | 0.00569     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22985773  |
| Train/Action_magnitu... | 0.51427734  |
| Train/Action_magnitude  | 0.40343714  |
| Train/Action_max        | 0.17991194  |
| Train/Action_std        | 0.14300542  |
| Train/Entropy           | -0.56123644 |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1299226   |
| Train/Loss              | 0.12536012  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12077131  |
| Train/Ratio             | 0.9999943   |
| Train/Return            | 1.5923647   |
| Train/V                 | 1.7131405   |
| Train/Value             | 1.7131405   |
| Train/control_penalty   | 0.4027561   |
| Train/policy_loss       | 0.12077131  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 745 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 744        |
| Time/Actor_Time         | 0.0775     |
| Time/B_Format_Time      | 0.0787     |
| Time/B_Original_Form... | 0.0818     |
| Time/Buffer             | 0.00371    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22274189 |
| Train/Action_magnitu... | 0.5067174  |
| Train/Action_magnitude  | 0.39488316 |
| Train/Action_max        | 0.17248607 |
| Train/Action_std        | 0.13864951 |
| Train/Entropy           | -0.5912051 |
| Train/Entropy_Loss      | 0.000591   |
| Train/Entropy_loss      | 0.000591   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1895602  |
| Train/Loss              | 0.14982128 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14525777 |
| Train/Ratio             | 1.0000068  |
| Train/Return            | 1.5691156  |
| Train/V                 | 1.7143738  |
| Train/Value             | 1.7143738  |
| Train/control_penalty   | 0.39723146 |
| Train/policy_loss       | 0.14525777 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 746 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 745         |
| Time/Actor_Time         | 0.0737      |
| Time/B_Format_Time      | 0.0751      |
| Time/B_Original_Form... | 0.0751      |
| Time/Buffer             | 0.00359     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2346064   |
| Train/Action_magnitu... | 0.52740395  |
| Train/Action_magnitude  | 0.41146562  |
| Train/Action_max        | 0.16691965  |
| Train/Action_std        | 0.14092028  |
| Train/Entropy           | -0.5761577  |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.156606    |
| Train/Loss              | 0.064991236 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.060315344 |
| Train/Ratio             | 0.99999017  |
| Train/Return            | 1.7588779   |
| Train/V                 | 1.8192043   |
| Train/Value             | 1.8192043   |
| Train/control_penalty   | 0.40997314  |
| Train/policy_loss       | 0.060315344 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.038       |
-----------------------------------------

 ---------------- Iteration 747 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 746         |
| Time/Actor_Time         | 0.0738      |
| Time/B_Format_Time      | 0.0758      |
| Time/B_Original_Form... | 0.0768      |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23197779  |
| Train/Action_magnitu... | 0.51797134  |
| Train/Action_magnitude  | 0.40470892  |
| Train/Action_max        | 0.18297179  |
| Train/Action_std        | 0.14225651  |
| Train/Entropy           | -0.5692905  |
| Train/Entropy_Loss      | 0.000569    |
| Train/Entropy_loss      | 0.000569    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1401641   |
| Train/Loss              | 0.097365215 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09272978  |
| Train/Ratio             | 1.0000159   |
| Train/Return            | 1.5872806   |
| Train/V                 | 1.6800029   |
| Train/Value             | 1.6800029   |
| Train/control_penalty   | 0.40661445  |
| Train/policy_loss       | 0.09272978  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03125     |
-----------------------------------------

 ---------------- Iteration 748 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 747        |
| Time/Actor_Time         | 0.0734     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.0725     |
| Time/Buffer             | 0.00309    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21991765 |
| Train/Action_magnitu... | 0.4988914  |
| Train/Action_magnitude  | 0.38917395 |
| Train/Action_max        | 0.17083326 |
| Train/Action_std        | 0.13686831 |
| Train/Entropy           | -0.6029201 |
| Train/Entropy_Loss      | 0.000603   |
| Train/Entropy_loss      | 0.000603   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2126763  |
| Train/Loss              | 0.13289325 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12840818 |
| Train/Ratio             | 0.99999976 |
| Train/Return            | 1.3853792  |
| Train/V                 | 1.5137883  |
| Train/Value             | 1.5137883  |
| Train/control_penalty   | 0.3882155  |
| Train/policy_loss       | 0.12840818 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 749 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 748         |
| Time/Actor_Time         | 0.0719      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00344     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23346406  |
| Train/Action_magnitu... | 0.52824694  |
| Train/Action_magnitude  | 0.41249636  |
| Train/Action_max        | 0.19060862  |
| Train/Action_std        | 0.14355518  |
| Train/Entropy           | -0.55883086 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1442004   |
| Train/Loss              | 0.1286004   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12391241  |
| Train/Ratio             | 0.9999985   |
| Train/Return            | 1.6080543   |
| Train/V                 | 1.7319714   |
| Train/Value             | 1.7319714   |
| Train/control_penalty   | 0.41291624  |
| Train/policy_loss       | 0.12391241  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 750 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 749        |
| Time/Actor_Time         | 0.0822     |
| Time/B_Format_Time      | 0.0829     |
| Time/B_Original_Form... | 0.0826     |
| Time/Buffer             | 0.00363    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22590196 |
| Train/Action_magnitu... | 0.5131927  |
| Train/Action_magnitude  | 0.4028672  |
| Train/Action_max        | 0.16338429 |
| Train/Action_std        | 0.13774356 |
| Train/Entropy           | -0.5994161 |
| Train/Entropy_Loss      | 0.000599   |
| Train/Entropy_loss      | 0.000599   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1899015  |
| Train/Loss              | 0.11405742 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10949395 |
| Train/Ratio             | 1.0000091  |
| Train/Return            | 1.4407008  |
| Train/V                 | 1.5502061  |
| Train/Value             | 1.5502061  |
| Train/control_penalty   | 0.39640588 |
| Train/policy_loss       | 0.10949395 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 751 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 750         |
| Time/Actor_Time         | 0.076       |
| Time/B_Format_Time      | 0.0757      |
| Time/B_Original_Form... | 0.0783      |
| Time/Buffer             | 0.00386     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.233914    |
| Train/Action_magnitu... | 0.5219452   |
| Train/Action_magnitude  | 0.4069325   |
| Train/Action_max        | 0.18416516  |
| Train/Action_std        | 0.13829282  |
| Train/Entropy           | -0.59414554 |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2047594   |
| Train/Loss              | 0.11434289  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10965646  |
| Train/Ratio             | 1.0000112   |
| Train/Return            | 1.4073985   |
| Train/V                 | 1.5170478   |
| Train/Value             | 1.5170478   |
| Train/control_penalty   | 0.40922847  |
| Train/policy_loss       | 0.10965646  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 752 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 751         |
| Time/Actor_Time         | 0.0727      |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.0722      |
| Time/Buffer             | 0.004       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23518236  |
| Train/Action_magnitu... | 0.52114356  |
| Train/Action_magnitude  | 0.40771243  |
| Train/Action_max        | 0.17857677  |
| Train/Action_std        | 0.13817209  |
| Train/Entropy           | -0.59202176 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2029879   |
| Train/Loss              | 0.081711255 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.077010974 |
| Train/Ratio             | 0.9999928   |
| Train/Return            | 1.579469    |
| Train/V                 | 1.6564826   |
| Train/Value             | 1.6564826   |
| Train/control_penalty   | 0.4108259   |
| Train/policy_loss       | 0.077010974 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 753 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 752        |
| Time/Actor_Time         | 0.0755     |
| Time/B_Format_Time      | 0.0837     |
| Time/B_Original_Form... | 0.0838     |
| Time/Buffer             | 0.00305    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24011487 |
| Train/Action_magnitu... | 0.5399876  |
| Train/Action_magnitude  | 0.42406836 |
| Train/Action_max        | 0.17221303 |
| Train/Action_std        | 0.14478526 |
| Train/Entropy           | -0.5423781 |
| Train/Entropy_Loss      | 0.000542   |
| Train/Entropy_loss      | 0.000542   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1052566  |
| Train/Loss              | 0.11675728 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11195529 |
| Train/Ratio             | 1.0000101  |
| Train/Return            | 1.8128313  |
| Train/V                 | 1.924781   |
| Train/Value             | 1.924781   |
| Train/control_penalty   | 0.42596057 |
| Train/policy_loss       | 0.11195529 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03225    |
----------------------------------------

 ---------------- Iteration 754 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 753         |
| Time/Actor_Time         | 0.0735      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0727      |
| Time/Buffer             | 0.00436     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23870997  |
| Train/Action_magnitu... | 0.54039633  |
| Train/Action_magnitude  | 0.42209888  |
| Train/Action_max        | 0.18267766  |
| Train/Action_std        | 0.1427433   |
| Train/Entropy           | -0.55869484 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1077236   |
| Train/Loss              | 0.08732584  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08254587  |
| Train/Ratio             | 1.0000054   |
| Train/Return            | 1.6830708   |
| Train/V                 | 1.7656194   |
| Train/Value             | 1.7656194   |
| Train/control_penalty   | 0.4221279   |
| Train/policy_loss       | 0.08254587  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03325     |
-----------------------------------------

 ---------------- Iteration 755 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 754        |
| Time/Actor_Time         | 0.0749     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.0763     |
| Time/Buffer             | 0.00379    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24107826 |
| Train/Action_magnitu... | 0.5339843  |
| Train/Action_magnitude  | 0.4174107  |
| Train/Action_max        | 0.1792808  |
| Train/Action_std        | 0.13956735 |
| Train/Entropy           | -0.5798536 |
| Train/Entropy_Loss      | 0.00058    |
| Train/Entropy_loss      | 0.00058    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1508546  |
| Train/Loss              | 0.11389935 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10913464 |
| Train/Ratio             | 0.99999434 |
| Train/Return            | 1.7928714  |
| Train/V                 | 1.9020116  |
| Train/Value             | 1.9020116  |
| Train/control_penalty   | 0.41848552 |
| Train/policy_loss       | 0.10913464 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0365     |
----------------------------------------

 ---------------- Iteration 756 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 755        |
| Time/Actor_Time         | 0.0757     |
| Time/B_Format_Time      | 0.0707     |
| Time/B_Original_Form... | 0.0741     |
| Time/Buffer             | 0.00347    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24047297 |
| Train/Action_magnitu... | 0.54308873 |
| Train/Action_magnitude  | 0.42387396 |
| Train/Action_max        | 0.17475784 |
| Train/Action_std        | 0.14371835 |
| Train/Entropy           | -0.5505579 |
| Train/Entropy_Loss      | 0.000551   |
| Train/Entropy_loss      | 0.000551   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0927713  |
| Train/Loss              | 0.03264942 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.02786879 |
| Train/Ratio             | 0.9999652  |
| Train/Return            | 1.6944952  |
| Train/V                 | 1.7223715  |
| Train/Value             | 1.7223715  |
| Train/control_penalty   | 0.423007   |
| Train/policy_loss       | 0.02786879 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03425    |
----------------------------------------

 ---------------- Iteration 757 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 756        |
| Time/Actor_Time         | 0.0711     |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0748     |
| Time/Buffer             | 0.00397    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24221095 |
| Train/Action_magnitu... | 0.5409442  |
| Train/Action_magnitude  | 0.42398375 |
| Train/Action_max        | 0.19151701 |
| Train/Action_std        | 0.14031301 |
| Train/Entropy           | -0.5763147 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1610521  |
| Train/Loss              | 0.10683199 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10199908 |
| Train/Ratio             | 0.99999154 |
| Train/Return            | 1.7532982  |
| Train/V                 | 1.8552914  |
| Train/Value             | 1.8552914  |
| Train/control_penalty   | 0.425659   |
| Train/policy_loss       | 0.10199908 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03425    |
----------------------------------------

 ---------------- Iteration 758 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 757        |
| Time/Actor_Time         | 0.0724     |
| Time/B_Format_Time      | 0.0717     |
| Time/B_Original_Form... | 0.0711     |
| Time/Buffer             | 0.00394    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24217474 |
| Train/Action_magnitu... | 0.541829   |
| Train/Action_magnitude  | 0.4229927  |
| Train/Action_max        | 0.17809692 |
| Train/Action_std        | 0.13905391 |
| Train/Entropy           | -0.5839878 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1484746  |
| Train/Loss              | 0.07524091 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07042145 |
| Train/Ratio             | 0.999988   |
| Train/Return            | 1.7423819  |
| Train/V                 | 1.8128026  |
| Train/Value             | 1.8128026  |
| Train/control_penalty   | 0.42354667 |
| Train/policy_loss       | 0.07042145 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0375     |
----------------------------------------

 ---------------- Iteration 759 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 758         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0772      |
| Time/B_Original_Form... | 0.0777      |
| Time/Buffer             | 0.00366     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24021152  |
| Train/Action_magnitu... | 0.53934276  |
| Train/Action_magnitude  | 0.42209962  |
| Train/Action_max        | 0.19509594  |
| Train/Action_std        | 0.14219111  |
| Train/Entropy           | -0.563825   |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1161449   |
| Train/Loss              | 0.06037308  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.055634324 |
| Train/Ratio             | 1.000001    |
| Train/Return            | 1.8542402   |
| Train/V                 | 1.9098647   |
| Train/Value             | 1.9098647   |
| Train/control_penalty   | 0.4174928   |
| Train/policy_loss       | 0.055634324 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.038       |
-----------------------------------------

 ---------------- Iteration 760 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 759        |
| Time/Actor_Time         | 0.0766     |
| Time/B_Format_Time      | 0.0789     |
| Time/B_Original_Form... | 0.0819     |
| Time/Buffer             | 0.00387    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23982154 |
| Train/Action_magnitu... | 0.53040403 |
| Train/Action_magnitude  | 0.4127873  |
| Train/Action_max        | 0.20327851 |
| Train/Action_std        | 0.13618061 |
| Train/Entropy           | -0.6103019 |
| Train/Entropy_Loss      | 0.00061    |
| Train/Entropy_loss      | 0.00061    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2000593  |
| Train/Loss              | 0.08269142 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07795144 |
| Train/Ratio             | 1.0000359  |
| Train/Return            | 1.6550269  |
| Train/V                 | 1.7329805  |
| Train/Value             | 1.7329805  |
| Train/control_penalty   | 0.41296846 |
| Train/policy_loss       | 0.07795144 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03025    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 761 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 760         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0749      |
| Time/Buffer             | 0.00312     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2505044   |
| Train/Action_magnitu... | 0.55669487  |
| Train/Action_magnitude  | 0.43469772  |
| Train/Action_max        | 0.18601038  |
| Train/Action_std        | 0.14033787  |
| Train/Entropy           | -0.5773035  |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1673311   |
| Train/Loss              | 0.094524115 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08968659  |
| Train/Ratio             | 0.99999857  |
| Train/Return            | 1.6800122   |
| Train/V                 | 1.7696978   |
| Train/Value             | 1.7696978   |
| Train/control_penalty   | 0.4260231   |
| Train/policy_loss       | 0.08968659  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 762 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 761         |
| Time/Actor_Time         | 0.0745      |
| Time/B_Format_Time      | 0.0737      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00376     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24948616  |
| Train/Action_magnitu... | 0.5518324   |
| Train/Action_magnitude  | 0.42959395  |
| Train/Action_max        | 0.20432702  |
| Train/Action_std        | 0.14123929  |
| Train/Entropy           | -0.5716412  |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1082253   |
| Train/Loss              | 0.040614527 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.03576474  |
| Train/Ratio             | 1.0000166   |
| Train/Return            | 1.8808084   |
| Train/V                 | 1.9165713   |
| Train/Value             | 1.9165713   |
| Train/control_penalty   | 0.42781463  |
| Train/policy_loss       | 0.03576474  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0405      |
-----------------------------------------

 ---------------- Iteration 763 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 762         |
| Time/Actor_Time         | 0.0767      |
| Time/B_Format_Time      | 0.0804      |
| Time/B_Original_Form... | 0.0823      |
| Time/Buffer             | 0.0036      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24142645  |
| Train/Action_magnitu... | 0.5448776   |
| Train/Action_magnitude  | 0.4261389   |
| Train/Action_max        | 0.19464292  |
| Train/Action_std        | 0.14215219  |
| Train/Entropy           | -0.5669071  |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1386535   |
| Train/Loss              | 0.096948735 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0921581   |
| Train/Ratio             | 1.0000023   |
| Train/Return            | 1.9222102   |
| Train/V                 | 2.0143592   |
| Train/Value             | 2.0143592   |
| Train/control_penalty   | 0.42237288  |
| Train/policy_loss       | 0.0921581   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.038       |
-----------------------------------------

 ---------------- Iteration 764 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 763         |
| Time/Actor_Time         | 0.0771      |
| Time/B_Format_Time      | 0.0829      |
| Time/B_Original_Form... | 0.0835      |
| Time/Buffer             | 0.00465     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2536308   |
| Train/Action_magnitu... | 0.5575923   |
| Train/Action_magnitude  | 0.4346027   |
| Train/Action_max        | 0.2019489   |
| Train/Action_std        | 0.14270486  |
| Train/Entropy           | -0.557495   |
| Train/Entropy_Loss      | 0.000557    |
| Train/Entropy_loss      | 0.000557    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1443621   |
| Train/Loss              | 0.06826779  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.063370615 |
| Train/Ratio             | 1.0000049   |
| Train/Return            | 1.9303873   |
| Train/V                 | 1.993762    |
| Train/Value             | 1.993762    |
| Train/control_penalty   | 0.433968    |
| Train/policy_loss       | 0.063370615 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.041       |
-----------------------------------------

 ---------------- Iteration 765 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 764        |
| Time/Actor_Time         | 0.0743     |
| Time/B_Format_Time      | 0.0702     |
| Time/B_Original_Form... | 0.0734     |
| Time/Buffer             | 0.00334    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24335633 |
| Train/Action_magnitu... | 0.54179287 |
| Train/Action_magnitude  | 0.42316848 |
| Train/Action_max        | 0.19286205 |
| Train/Action_std        | 0.14039463 |
| Train/Entropy           | -0.5755078 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.131146   |
| Train/Loss              | 0.12999466 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12516938 |
| Train/Ratio             | 1.0000271  |
| Train/Return            | 1.447182   |
| Train/V                 | 1.5723512  |
| Train/Value             | 1.5723512  |
| Train/control_penalty   | 0.42497694 |
| Train/policy_loss       | 0.12516938 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 766 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 765        |
| Time/Actor_Time         | 0.0756     |
| Time/B_Format_Time      | 0.0751     |
| Time/B_Original_Form... | 0.0798     |
| Time/Buffer             | 0.00415    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.24914064 |
| Train/Action_magnitu... | 0.55797064 |
| Train/Action_magnitude  | 0.43459365 |
| Train/Action_max        | 0.17386948 |
| Train/Action_std        | 0.13963598 |
| Train/Entropy           | -0.5805206 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1572049  |
| Train/Loss              | 0.07769892 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.0728     |
| Train/Ratio             | 0.9999897  |
| Train/Return            | 1.7942362  |
| Train/V                 | 1.8670291  |
| Train/Value             | 1.8670291  |
| Train/control_penalty   | 0.43183985 |
| Train/policy_loss       | 0.0728     |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 767 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 766        |
| Time/Actor_Time         | 0.0744     |
| Time/B_Format_Time      | 0.0767     |
| Time/B_Original_Form... | 0.0764     |
| Time/Buffer             | 0.00379    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23474415 |
| Train/Action_magnitu... | 0.52131605 |
| Train/Action_magnitude  | 0.40720245 |
| Train/Action_max        | 0.1877253  |
| Train/Action_std        | 0.1387804  |
| Train/Entropy           | -0.590009  |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1895146  |
| Train/Loss              | 0.15589269 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15121777 |
| Train/Ratio             | 1.0000032  |
| Train/Return            | 1.483522   |
| Train/V                 | 1.6347358  |
| Train/Value             | 1.6347358  |
| Train/control_penalty   | 0.40848976 |
| Train/policy_loss       | 0.15121777 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 768 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 767         |
| Time/Actor_Time         | 0.0756      |
| Time/B_Format_Time      | 0.0741      |
| Time/B_Original_Form... | 0.0755      |
| Time/Buffer             | 0.00353     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23229545  |
| Train/Action_magnitu... | 0.52220505  |
| Train/Action_magnitude  | 0.4057297   |
| Train/Action_max        | 0.1917037   |
| Train/Action_std        | 0.13794976  |
| Train/Entropy           | -0.5978986  |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1936927   |
| Train/Loss              | 0.121401    |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.116780825 |
| Train/Ratio             | 0.99999547  |
| Train/Return            | 1.4298315   |
| Train/V                 | 1.5466148   |
| Train/Value             | 1.5466148   |
| Train/control_penalty   | 0.4022271   |
| Train/policy_loss       | 0.116780825 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 769 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 768        |
| Time/Actor_Time         | 0.0727     |
| Time/B_Format_Time      | 0.0705     |
| Time/B_Original_Form... | 0.0728     |
| Time/Buffer             | 0.00328    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23630056 |
| Train/Action_magnitu... | 0.5267871  |
| Train/Action_magnitude  | 0.40949744 |
| Train/Action_max        | 0.2185129  |
| Train/Action_std        | 0.13921694 |
| Train/Entropy           | -0.5876757 |
| Train/Entropy_Loss      | 0.000588   |
| Train/Entropy_loss      | 0.000588   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.171614   |
| Train/Loss              | 0.16654702 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16187534 |
| Train/Ratio             | 0.9999794  |
| Train/Return            | 1.445344   |
| Train/V                 | 1.6072286  |
| Train/Value             | 1.6072286  |
| Train/control_penalty   | 0.40840018 |
| Train/policy_loss       | 0.16187534 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.021      |
----------------------------------------

 ---------------- Iteration 770 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 769        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.0756     |
| Time/Buffer             | 0.00349    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23532632 |
| Train/Action_magnitu... | 0.51760525 |
| Train/Action_magnitude  | 0.40275148 |
| Train/Action_max        | 0.21799278 |
| Train/Action_std        | 0.13505343 |
| Train/Entropy           | -0.6157617 |
| Train/Entropy_Loss      | 0.000616   |
| Train/Entropy_loss      | 0.000616   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2242891  |
| Train/Loss              | 0.14412765 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13948895 |
| Train/Ratio             | 0.9999868  |
| Train/Return            | 1.4282358  |
| Train/V                 | 1.5677228  |
| Train/Value             | 1.5677228  |
| Train/control_penalty   | 0.4022934  |
| Train/policy_loss       | 0.13948895 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02325    |
----------------------------------------

 ---------------- Iteration 771 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 770         |
| Time/Actor_Time         | 0.0752      |
| Time/B_Format_Time      | 0.0831      |
| Time/B_Original_Form... | 0.0808      |
| Time/Buffer             | 0.00341     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2361876   |
| Train/Action_magnitu... | 0.5233921   |
| Train/Action_magnitude  | 0.4068576   |
| Train/Action_max        | 0.19907129  |
| Train/Action_std        | 0.1371594   |
| Train/Entropy           | -0.60367376 |
| Train/Entropy_Loss      | 0.000604    |
| Train/Entropy_loss      | 0.000604    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.20344     |
| Train/Loss              | 0.11584378  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11116897  |
| Train/Ratio             | 0.99997985  |
| Train/Return            | 1.4922607   |
| Train/V                 | 1.6034342   |
| Train/Value             | 1.6034342   |
| Train/control_penalty   | 0.4071128   |
| Train/policy_loss       | 0.11116897  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02875     |
-----------------------------------------

 ---------------- Iteration 772 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 771         |
| Time/Actor_Time         | 0.0724      |
| Time/B_Format_Time      | 0.0764      |
| Time/B_Original_Form... | 0.0738      |
| Time/Buffer             | 0.00538     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23869473  |
| Train/Action_magnitu... | 0.5353561   |
| Train/Action_magnitude  | 0.41527236  |
| Train/Action_max        | 0.21511304  |
| Train/Action_std        | 0.14225873  |
| Train/Entropy           | -0.56862575 |
| Train/Entropy_Loss      | 0.000569    |
| Train/Entropy_loss      | 0.000569    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.147737    |
| Train/Loss              | 0.12743568  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12272345  |
| Train/Ratio             | 0.99999607  |
| Train/Return            | 1.4670358   |
| Train/V                 | 1.5897652   |
| Train/Value             | 1.5897652   |
| Train/control_penalty   | 0.41436076  |
| Train/policy_loss       | 0.12272345  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 773 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 772         |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.0767      |
| Time/Buffer             | 0.00353     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24115095  |
| Train/Action_magnitu... | 0.53931993  |
| Train/Action_magnitude  | 0.4163055   |
| Train/Action_max        | 0.20695184  |
| Train/Action_std        | 0.14085728  |
| Train/Entropy           | -0.57562304 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.149439    |
| Train/Loss              | 0.14525282  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14054388  |
| Train/Ratio             | 1.000004    |
| Train/Return            | 1.2646697   |
| Train/V                 | 1.4052167   |
| Train/Value             | 1.4052167   |
| Train/control_penalty   | 0.41333222  |
| Train/policy_loss       | 0.14054388  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01875     |
-----------------------------------------

 ---------------- Iteration 774 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 773         |
| Time/Actor_Time         | 0.0728      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.076       |
| Time/Buffer             | 0.00363     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23305854  |
| Train/Action_magnitu... | 0.5236048   |
| Train/Action_magnitude  | 0.40776703  |
| Train/Action_max        | 0.2095211   |
| Train/Action_std        | 0.13976248  |
| Train/Entropy           | -0.58247143 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1443721   |
| Train/Loss              | 0.12990986  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.125256    |
| Train/Ratio             | 1.0000097   |
| Train/Return            | 1.3606988   |
| Train/V                 | 1.4859514   |
| Train/Value             | 1.4859514   |
| Train/control_penalty   | 0.40713808  |
| Train/policy_loss       | 0.125256    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02525     |
-----------------------------------------

 ---------------- Iteration 775 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 774         |
| Time/Actor_Time         | 0.073       |
| Time/B_Format_Time      | 0.0751      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00387     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23550645  |
| Train/Action_magnitu... | 0.52066517  |
| Train/Action_magnitude  | 0.40280762  |
| Train/Action_max        | 0.2046234   |
| Train/Action_std        | 0.14080235  |
| Train/Entropy           | -0.57643735 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.182519    |
| Train/Loss              | 0.13521236  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13057564  |
| Train/Ratio             | 0.9999788   |
| Train/Return            | 1.429015    |
| Train/V                 | 1.5595958   |
| Train/Value             | 1.5595958   |
| Train/control_penalty   | 0.40602848  |
| Train/policy_loss       | 0.13057564  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0215      |
-----------------------------------------

 ---------------- Iteration 776 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 775        |
| Time/Actor_Time         | 0.0756     |
| Time/B_Format_Time      | 0.0761     |
| Time/B_Original_Form... | 0.0775     |
| Time/Buffer             | 0.00426    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24371088 |
| Train/Action_magnitu... | 0.53713393 |
| Train/Action_magnitude  | 0.4162439  |
| Train/Action_max        | 0.21720287 |
| Train/Action_std        | 0.13904053 |
| Train/Entropy           | -0.5873138 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1749289  |
| Train/Loss              | 0.1981414  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1934154  |
| Train/Ratio             | 0.99998677 |
| Train/Return            | 1.3918605  |
| Train/V                 | 1.5852796  |
| Train/Value             | 1.5852796  |
| Train/control_penalty   | 0.41386813 |
| Train/policy_loss       | 0.1934154  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 777 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 776         |
| Time/Actor_Time         | 0.0773      |
| Time/B_Format_Time      | 0.0815      |
| Time/B_Original_Form... | 0.0823      |
| Time/Buffer             | 0.00399     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23400797  |
| Train/Action_magnitu... | 0.5217569   |
| Train/Action_magnitude  | 0.40332192  |
| Train/Action_max        | 0.20709619  |
| Train/Action_std        | 0.13987225  |
| Train/Entropy           | -0.58383936 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1486801   |
| Train/Loss              | 0.12848441  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.123876266 |
| Train/Ratio             | 0.99999267  |
| Train/Return            | 1.3686833   |
| Train/V                 | 1.4925656   |
| Train/Value             | 1.4925656   |
| Train/control_penalty   | 0.4024303   |
| Train/policy_loss       | 0.123876266 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 778 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 777        |
| Time/Actor_Time         | 0.0726     |
| Time/B_Format_Time      | 0.0696     |
| Time/B_Original_Form... | 0.0717     |
| Time/Buffer             | 0.00434    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24092485 |
| Train/Action_magnitu... | 0.5399629  |
| Train/Action_magnitude  | 0.41937268 |
| Train/Action_max        | 0.22771432 |
| Train/Action_std        | 0.14172852 |
| Train/Entropy           | -0.5674847 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1302705  |
| Train/Loss              | 0.16572128 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16098554 |
| Train/Ratio             | 0.99997866 |
| Train/Return            | 1.4534812  |
| Train/V                 | 1.6144807  |
| Train/Value             | 1.6144807  |
| Train/control_penalty   | 0.41682595 |
| Train/policy_loss       | 0.16098554 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 779 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 778        |
| Time/Actor_Time         | 0.0721     |
| Time/B_Format_Time      | 0.0734     |
| Time/B_Original_Form... | 0.0726     |
| Time/Buffer             | 0.00438    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24495912 |
| Train/Action_magnitu... | 0.5522786  |
| Train/Action_magnitude  | 0.43041027 |
| Train/Action_max        | 0.20175251 |
| Train/Action_std        | 0.14494641 |
| Train/Entropy           | -0.5450379 |
| Train/Entropy_Loss      | 0.000545   |
| Train/Entropy_loss      | 0.000545   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.086266   |
| Train/Loss              | 0.13040754 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12555866 |
| Train/Ratio             | 1.0000063  |
| Train/Return            | 1.7475976  |
| Train/V                 | 1.8731543  |
| Train/Value             | 1.8731543  |
| Train/control_penalty   | 0.4303844  |
| Train/policy_loss       | 0.12555866 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.032      |
----------------------------------------

 ---------------- Iteration 780 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 779        |
| Time/Actor_Time         | 0.0716     |
| Time/B_Format_Time      | 0.0758     |
| Time/B_Original_Form... | 0.0754     |
| Time/Buffer             | 0.00377    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24136527 |
| Train/Action_magnitu... | 0.53633434 |
| Train/Action_magnitude  | 0.41873693 |
| Train/Action_max        | 0.20750593 |
| Train/Action_std        | 0.1427574  |
| Train/Entropy           | -0.5578466 |
| Train/Entropy_Loss      | 0.000558   |
| Train/Entropy_loss      | 0.000558   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0944272  |
| Train/Loss              | 0.15671055 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15195079 |
| Train/Ratio             | 0.9999854  |
| Train/Return            | 1.7072377  |
| Train/V                 | 1.8591934  |
| Train/Value             | 1.8591934  |
| Train/control_penalty   | 0.42019257 |
| Train/policy_loss       | 0.15195079 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.031      |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 781 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 780         |
| Time/Actor_Time         | 0.0738      |
| Time/B_Format_Time      | 0.0757      |
| Time/B_Original_Form... | 0.0765      |
| Time/Buffer             | 0.00385     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23962249  |
| Train/Action_magnitu... | 0.53498095  |
| Train/Action_magnitude  | 0.4152411   |
| Train/Action_max        | 0.21125391  |
| Train/Action_std        | 0.14054984  |
| Train/Entropy           | -0.57848865 |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1607215   |
| Train/Loss              | 0.16261783  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15787664  |
| Train/Ratio             | 0.99999344  |
| Train/Return            | 1.5914108   |
| Train/V                 | 1.7492968   |
| Train/Value             | 1.7492968   |
| Train/control_penalty   | 0.41626993  |
| Train/policy_loss       | 0.15787664  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0275      |
-----------------------------------------

 ---------------- Iteration 782 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 781         |
| Time/Actor_Time         | 0.0751      |
| Time/B_Format_Time      | 0.0755      |
| Time/B_Original_Form... | 0.0799      |
| Time/Buffer             | 0.00449     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23510286  |
| Train/Action_magnitu... | 0.52188283  |
| Train/Action_magnitude  | 0.4056275   |
| Train/Action_max        | 0.20504193  |
| Train/Action_std        | 0.13921227  |
| Train/Entropy           | -0.58482826 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1634703   |
| Train/Loss              | 0.13015601  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12549236  |
| Train/Ratio             | 0.9999843   |
| Train/Return            | 1.7886873   |
| Train/V                 | 1.9141831   |
| Train/Value             | 1.9141831   |
| Train/control_penalty   | 0.4078819   |
| Train/policy_loss       | 0.12549236  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 783 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 782         |
| Time/Actor_Time         | 0.0762      |
| Time/B_Format_Time      | 0.0783      |
| Time/B_Original_Form... | 0.0789      |
| Time/Buffer             | 0.00382     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22844717  |
| Train/Action_magnitu... | 0.5212351   |
| Train/Action_magnitude  | 0.40750295  |
| Train/Action_max        | 0.18014725  |
| Train/Action_std        | 0.1411502   |
| Train/Entropy           | -0.57313955 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1426036   |
| Train/Loss              | 0.16285442  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15823744  |
| Train/Ratio             | 1.000039    |
| Train/Return            | 1.6550834   |
| Train/V                 | 1.8133156   |
| Train/Value             | 1.8133156   |
| Train/control_penalty   | 0.40438268  |
| Train/policy_loss       | 0.15823744  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 784 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 783         |
| Time/Actor_Time         | 0.078       |
| Time/B_Format_Time      | 0.0805      |
| Time/B_Original_Form... | 0.0817      |
| Time/Buffer             | 0.00449     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23576856  |
| Train/Action_magnitu... | 0.5249283   |
| Train/Action_magnitude  | 0.41323018  |
| Train/Action_max        | 0.17368019  |
| Train/Action_std        | 0.13831075  |
| Train/Entropy           | -0.59266233 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1957812   |
| Train/Loss              | 0.15323204  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14847432  |
| Train/Ratio             | 1.0000272   |
| Train/Return            | 1.673087    |
| Train/V                 | 1.8215616   |
| Train/Value             | 1.8215616   |
| Train/control_penalty   | 0.4165052   |
| Train/policy_loss       | 0.14847432  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03075     |
-----------------------------------------

 ---------------- Iteration 785 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 784         |
| Time/Actor_Time         | 0.0745      |
| Time/B_Format_Time      | 0.0785      |
| Time/B_Original_Form... | 0.0772      |
| Time/Buffer             | 0.00464     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23270789  |
| Train/Action_magnitu... | 0.5174786   |
| Train/Action_magnitude  | 0.4039333   |
| Train/Action_max        | 0.20599152  |
| Train/Action_std        | 0.13587826  |
| Train/Entropy           | -0.6113244  |
| Train/Entropy_Loss      | 0.000611    |
| Train/Entropy_loss      | 0.000611    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2274309   |
| Train/Loss              | 0.100136265 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0954729   |
| Train/Ratio             | 0.99999595  |
| Train/Return            | 1.6274989   |
| Train/V                 | 1.7229682   |
| Train/Value             | 1.7229682   |
| Train/control_penalty   | 0.40520373  |
| Train/policy_loss       | 0.0954729   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03375     |
-----------------------------------------

 ---------------- Iteration 786 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 785         |
| Time/Actor_Time         | 0.0795      |
| Time/B_Format_Time      | 0.0804      |
| Time/B_Original_Form... | 0.0813      |
| Time/Buffer             | 0.00328     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22645925  |
| Train/Action_magnitu... | 0.504795    |
| Train/Action_magnitude  | 0.393288    |
| Train/Action_max        | 0.19738248  |
| Train/Action_std        | 0.13522936  |
| Train/Entropy           | -0.61371803 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.212178    |
| Train/Loss              | 0.1348431   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1302809   |
| Train/Ratio             | 1.0000036   |
| Train/Return            | 1.8264979   |
| Train/V                 | 1.9567833   |
| Train/Value             | 1.9567833   |
| Train/control_penalty   | 0.39484784  |
| Train/policy_loss       | 0.1302809   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 787 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 786         |
| Time/Actor_Time         | 0.0764      |
| Time/B_Format_Time      | 0.0723      |
| Time/B_Original_Form... | 0.0767      |
| Time/Buffer             | 0.0041      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23838939  |
| Train/Action_magnitu... | 0.5275781   |
| Train/Action_magnitude  | 0.41026136  |
| Train/Action_max        | 0.20021646  |
| Train/Action_std        | 0.1375196   |
| Train/Entropy           | -0.5990785  |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1998372   |
| Train/Loss              | 0.12761405  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.122893296 |
| Train/Ratio             | 1.0000223   |
| Train/Return            | 1.8566587   |
| Train/V                 | 1.9795353   |
| Train/Value             | 1.9795353   |
| Train/control_penalty   | 0.4121675   |
| Train/policy_loss       | 0.122893296 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03625     |
-----------------------------------------

 ---------------- Iteration 788 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 787         |
| Time/Actor_Time         | 0.0791      |
| Time/B_Format_Time      | 0.082       |
| Time/B_Original_Form... | 0.0846      |
| Time/Buffer             | 0.00404     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22669457  |
| Train/Action_magnitu... | 0.50650257  |
| Train/Action_magnitude  | 0.39481217  |
| Train/Action_max        | 0.18376625  |
| Train/Action_std        | 0.13335003  |
| Train/Entropy           | -0.62958425 |
| Train/Entropy_Loss      | 0.00063     |
| Train/Entropy_loss      | 0.00063     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2427274   |
| Train/Loss              | 0.1613639   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1568276   |
| Train/Ratio             | 0.9999999   |
| Train/Return            | 1.2616132   |
| Train/V                 | 1.4184254   |
| Train/Value             | 1.4184254   |
| Train/control_penalty   | 0.39067188  |
| Train/policy_loss       | 0.1568276   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01925     |
-----------------------------------------

 ---------------- Iteration 789 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 788         |
| Time/Actor_Time         | 0.0846      |
| Time/B_Format_Time      | 0.0927      |
| Time/B_Original_Form... | 0.0922      |
| Time/Buffer             | 0.00563     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22182134  |
| Train/Action_magnitu... | 0.50095856  |
| Train/Action_magnitude  | 0.39414045  |
| Train/Action_max        | 0.15747146  |
| Train/Action_std        | 0.13454361  |
| Train/Entropy           | -0.62214184 |
| Train/Entropy_Loss      | 0.000622    |
| Train/Entropy_loss      | 0.000622    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2627869   |
| Train/Loss              | 0.056702763 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.052181285 |
| Train/Ratio             | 0.99999315  |
| Train/Return            | 1.5369054   |
| Train/V                 | 1.5890945   |
| Train/Value             | 1.5890945   |
| Train/control_penalty   | 0.3899336   |
| Train/policy_loss       | 0.052181285 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0365      |
-----------------------------------------

 ---------------- Iteration 790 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 789         |
| Time/Actor_Time         | 0.0751      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0787      |
| Time/Buffer             | 0.00508     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22500113  |
| Train/Action_magnitu... | 0.5042043   |
| Train/Action_magnitude  | 0.39643037  |
| Train/Action_max        | 0.18479103  |
| Train/Action_std        | 0.13584436  |
| Train/Entropy           | -0.60928714 |
| Train/Entropy_Loss      | 0.000609    |
| Train/Entropy_loss      | 0.000609    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2145535   |
| Train/Loss              | 0.08734346  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.082722045 |
| Train/Ratio             | 0.9999889   |
| Train/Return            | 1.5656303   |
| Train/V                 | 1.6483531   |
| Train/Value             | 1.6483531   |
| Train/control_penalty   | 0.4012131   |
| Train/policy_loss       | 0.082722045 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03325     |
-----------------------------------------

 ---------------- Iteration 791 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 790        |
| Time/Actor_Time         | 0.0804     |
| Time/B_Format_Time      | 0.0844     |
| Time/B_Original_Form... | 0.0857     |
| Time/Buffer             | 0.00497    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22523835 |
| Train/Action_magnitu... | 0.50023425 |
| Train/Action_magnitude  | 0.3885106  |
| Train/Action_max        | 0.21815924 |
| Train/Action_std        | 0.13562238 |
| Train/Entropy           | -0.6196295 |
| Train/Entropy_Loss      | 0.00062    |
| Train/Entropy_loss      | 0.00062    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2347188  |
| Train/Loss              | 0.1346266  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13009308 |
| Train/Ratio             | 1.0000092  |
| Train/Return            | 1.6055368  |
| Train/V                 | 1.7356343  |
| Train/Value             | 1.7356343  |
| Train/control_penalty   | 0.39138848 |
| Train/policy_loss       | 0.13009308 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 792 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 791        |
| Time/Actor_Time         | 0.078      |
| Time/B_Format_Time      | 0.0787     |
| Time/B_Original_Form... | 0.0818     |
| Time/Buffer             | 0.00513    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22771612 |
| Train/Action_magnitu... | 0.5051847  |
| Train/Action_magnitude  | 0.39506614 |
| Train/Action_max        | 0.20190375 |
| Train/Action_std        | 0.13452896 |
| Train/Entropy           | -0.6238706 |
| Train/Entropy_Loss      | 0.000624   |
| Train/Entropy_loss      | 0.000624   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2444441  |
| Train/Loss              | 0.14534725 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1407495  |
| Train/Ratio             | 1.0000165  |
| Train/Return            | 1.6213349  |
| Train/V                 | 1.7620782  |
| Train/Value             | 1.7620782  |
| Train/control_penalty   | 0.3973889  |
| Train/policy_loss       | 0.1407495  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 793 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 792         |
| Time/Actor_Time         | 0.075       |
| Time/B_Format_Time      | 0.0799      |
| Time/B_Original_Form... | 0.0804      |
| Time/Buffer             | 0.00472     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23778859  |
| Train/Action_magnitu... | 0.53181845  |
| Train/Action_magnitude  | 0.4143725   |
| Train/Action_max        | 0.18324982  |
| Train/Action_std        | 0.13524371  |
| Train/Entropy           | -0.6136606  |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2345489   |
| Train/Loss              | 0.08702996  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.082292296 |
| Train/Ratio             | 0.9999877   |
| Train/Return            | 1.6712978   |
| Train/V                 | 1.753586    |
| Train/Value             | 1.753586    |
| Train/control_penalty   | 0.41240042  |
| Train/policy_loss       | 0.082292296 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03975     |
-----------------------------------------

 ---------------- Iteration 794 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 793        |
| Time/Actor_Time         | 0.0749     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.0773     |
| Time/Buffer             | 0.00422    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22403596 |
| Train/Action_magnitu... | 0.5021596  |
| Train/Action_magnitude  | 0.39344162 |
| Train/Action_max        | 0.18065518 |
| Train/Action_std        | 0.13442913 |
| Train/Entropy           | -0.621597  |
| Train/Entropy_Loss      | 0.000622   |
| Train/Entropy_loss      | 0.000622   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2415557  |
| Train/Loss              | 0.07972573 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07518322 |
| Train/Ratio             | 1.0000163  |
| Train/Return            | 1.4159225  |
| Train/V                 | 1.491101   |
| Train/Value             | 1.491101   |
| Train/control_penalty   | 0.39209154 |
| Train/policy_loss       | 0.07518322 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0345     |
----------------------------------------

 ---------------- Iteration 795 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 794         |
| Time/Actor_Time         | 0.0739      |
| Time/B_Format_Time      | 0.0759      |
| Time/B_Original_Form... | 0.0785      |
| Time/Buffer             | 0.004       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22466706  |
| Train/Action_magnitu... | 0.5041544   |
| Train/Action_magnitude  | 0.3919105   |
| Train/Action_max        | 0.1936773   |
| Train/Action_std        | 0.13846064  |
| Train/Entropy           | -0.5984252  |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1924756   |
| Train/Loss              | 0.109085776 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1045738   |
| Train/Ratio             | 0.9999961   |
| Train/Return            | 1.6412134   |
| Train/V                 | 1.7457856   |
| Train/Value             | 1.7457856   |
| Train/control_penalty   | 0.39135528  |
| Train/policy_loss       | 0.1045738   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03        |
-----------------------------------------

 ---------------- Iteration 796 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 795        |
| Time/Actor_Time         | 0.0783     |
| Time/B_Format_Time      | 0.0802     |
| Time/B_Original_Form... | 0.0815     |
| Time/Buffer             | 0.00338    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23370549 |
| Train/Action_magnitu... | 0.52089673 |
| Train/Action_magnitude  | 0.40572098 |
| Train/Action_max        | 0.19489248 |
| Train/Action_std        | 0.1421888  |
| Train/Entropy           | -0.5687469 |
| Train/Entropy_Loss      | 0.000569   |
| Train/Entropy_loss      | 0.000569   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.145469   |
| Train/Loss              | 0.16461338 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1599825  |
| Train/Ratio             | 1.000015   |
| Train/Return            | 1.5928324  |
| Train/V                 | 1.7528151  |
| Train/Value             | 1.7528151  |
| Train/control_penalty   | 0.40621382 |
| Train/policy_loss       | 0.1599825  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 797 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 796        |
| Time/Actor_Time         | 0.0749     |
| Time/B_Format_Time      | 0.0733     |
| Time/B_Original_Form... | 0.0745     |
| Time/Buffer             | 0.00339    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24842818 |
| Train/Action_magnitu... | 0.54532063 |
| Train/Action_magnitude  | 0.42434162 |
| Train/Action_max        | 0.22449236 |
| Train/Action_std        | 0.14012943 |
| Train/Entropy           | -0.5776625 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1766623  |
| Train/Loss              | 0.14608839 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14124355 |
| Train/Ratio             | 1.000007   |
| Train/Return            | 1.7308398  |
| Train/V                 | 1.8720919  |
| Train/Value             | 1.8720919  |
| Train/control_penalty   | 0.4267182  |
| Train/policy_loss       | 0.14124355 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.031      |
----------------------------------------

 ---------------- Iteration 798 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 797        |
| Time/Actor_Time         | 0.0748     |
| Time/B_Format_Time      | 0.0755     |
| Time/B_Original_Form... | 0.075      |
| Time/Buffer             | 0.00416    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23561135 |
| Train/Action_magnitu... | 0.52585167 |
| Train/Action_magnitude  | 0.40956557 |
| Train/Action_max        | 0.19544879 |
| Train/Action_std        | 0.14098305 |
| Train/Entropy           | -0.5754693 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1528195  |
| Train/Loss              | 0.11000817 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10530646 |
| Train/Ratio             | 1.000022   |
| Train/Return            | 1.7028533  |
| Train/V                 | 1.8081517  |
| Train/Value             | 1.8081517  |
| Train/control_penalty   | 0.412624   |
| Train/policy_loss       | 0.10530646 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 799 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 798         |
| Time/Actor_Time         | 0.0767      |
| Time/B_Format_Time      | 0.0815      |
| Time/B_Original_Form... | 0.0817      |
| Time/Buffer             | 0.00479     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24403557  |
| Train/Action_magnitu... | 0.5443244   |
| Train/Action_magnitude  | 0.42621177  |
| Train/Action_max        | 0.18623938  |
| Train/Action_std        | 0.14185542  |
| Train/Entropy           | -0.56681794 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1209431   |
| Train/Loss              | 0.08827963  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08344109  |
| Train/Ratio             | 0.99998623  |
| Train/Return            | 1.6982135   |
| Train/V                 | 1.7816656   |
| Train/Value             | 1.7816656   |
| Train/control_penalty   | 0.42717162  |
| Train/policy_loss       | 0.08344109  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03725     |
-----------------------------------------

 ---------------- Iteration 800 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 799         |
| Time/Actor_Time         | 0.0772      |
| Time/B_Format_Time      | 0.0794      |
| Time/B_Original_Form... | 0.0809      |
| Time/Buffer             | 0.0142      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23724261  |
| Train/Action_magnitu... | 0.53145933  |
| Train/Action_magnitude  | 0.41303003  |
| Train/Action_max        | 0.20316958  |
| Train/Action_std        | 0.14189066  |
| Train/Entropy           | -0.56636596 |
| Train/Entropy_Loss      | 0.000566    |
| Train/Entropy_loss      | 0.000566    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1109226   |
| Train/Loss              | 0.13694268  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13223462  |
| Train/Ratio             | 0.99999654  |
| Train/Return            | 1.7372041   |
| Train/V                 | 1.8694412   |
| Train/Value             | 1.8694412   |
| Train/control_penalty   | 0.41417012  |
| Train/policy_loss       | 0.13223462  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03225     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 801 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 800        |
| Time/Actor_Time         | 0.0756     |
| Time/B_Format_Time      | 0.0737     |
| Time/B_Original_Form... | 0.0772     |
| Time/Buffer             | 0.00399    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.2297486  |
| Train/Action_magnitu... | 0.5143238  |
| Train/Action_magnitude  | 0.399005   |
| Train/Action_max        | 0.19868384 |
| Train/Action_std        | 0.14209434 |
| Train/Entropy           | -0.5719219 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.143687   |
| Train/Loss              | 0.14713953 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14255917 |
| Train/Ratio             | 1.0000085  |
| Train/Return            | 1.7775545  |
| Train/V                 | 1.9201071  |
| Train/Value             | 1.9201071  |
| Train/control_penalty   | 0.40084448 |
| Train/policy_loss       | 0.14255917 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 802 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 801         |
| Time/Actor_Time         | 0.0749      |
| Time/B_Format_Time      | 0.0787      |
| Time/B_Original_Form... | 0.0798      |
| Time/Buffer             | 0.0045      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22568847  |
| Train/Action_magnitu... | 0.5174318   |
| Train/Action_magnitude  | 0.40308735  |
| Train/Action_max        | 0.21514007  |
| Train/Action_std        | 0.14310728  |
| Train/Entropy           | -0.56372505 |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1113684   |
| Train/Loss              | 0.22348675  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21890564  |
| Train/Ratio             | 1.0000061   |
| Train/Return            | 1.647507    |
| Train/V                 | 1.8664161   |
| Train/Value             | 1.8664161   |
| Train/control_penalty   | 0.40173805  |
| Train/policy_loss       | 0.21890564  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 803 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 802        |
| Time/Actor_Time         | 0.0736     |
| Time/B_Format_Time      | 0.0726     |
| Time/B_Original_Form... | 0.0781     |
| Time/Buffer             | 0.00403    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2329621  |
| Train/Action_magnitu... | 0.5291398  |
| Train/Action_magnitude  | 0.41104737 |
| Train/Action_max        | 0.21530464 |
| Train/Action_std        | 0.14270282 |
| Train/Entropy           | -0.5646643 |
| Train/Entropy_Loss      | 0.000565   |
| Train/Entropy_loss      | 0.000565   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1002245  |
| Train/Loss              | 0.10211277 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09747196 |
| Train/Ratio             | 0.9999958  |
| Train/Return            | 1.9499257  |
| Train/V                 | 2.0474052  |
| Train/Value             | 2.0474052  |
| Train/control_penalty   | 0.40761444 |
| Train/policy_loss       | 0.09747196 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.036      |
----------------------------------------

 ---------------- Iteration 804 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 803         |
| Time/Actor_Time         | 0.0754      |
| Time/B_Format_Time      | 0.0777      |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00368     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23092332  |
| Train/Action_magnitu... | 0.517039    |
| Train/Action_magnitude  | 0.4016118   |
| Train/Action_max        | 0.21221186  |
| Train/Action_std        | 0.13964994  |
| Train/Entropy           | -0.58161914 |
| Train/Entropy_Loss      | 0.000582    |
| Train/Entropy_loss      | 0.000582    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.180003    |
| Train/Loss              | 0.13864821  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13401107  |
| Train/Ratio             | 1.0000094   |
| Train/Return            | 1.6268934   |
| Train/V                 | 1.7609066   |
| Train/Value             | 1.7609066   |
| Train/control_penalty   | 0.40555203  |
| Train/policy_loss       | 0.13401107  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 805 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 804         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0788      |
| Time/B_Original_Form... | 0.0789      |
| Time/Buffer             | 0.00437     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23523216  |
| Train/Action_magnitu... | 0.5295938   |
| Train/Action_magnitude  | 0.4122337   |
| Train/Action_max        | 0.22063746  |
| Train/Action_std        | 0.14393318  |
| Train/Entropy           | -0.55877244 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1067137   |
| Train/Loss              | 0.11950817  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.114817664 |
| Train/Ratio             | 0.9999747   |
| Train/Return            | 1.964649    |
| Train/V                 | 2.0794709   |
| Train/Value             | 2.0794709   |
| Train/control_penalty   | 0.4131738   |
| Train/policy_loss       | 0.114817664 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.038       |
-----------------------------------------

 ---------------- Iteration 806 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 805         |
| Time/Actor_Time         | 0.0782      |
| Time/B_Format_Time      | 0.0838      |
| Time/B_Original_Form... | 0.0847      |
| Time/Buffer             | 0.00504     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23290959  |
| Train/Action_magnitu... | 0.52888983  |
| Train/Action_magnitude  | 0.4137782   |
| Train/Action_max        | 0.19591257  |
| Train/Action_std        | 0.14150423  |
| Train/Entropy           | -0.56698376 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1425996   |
| Train/Loss              | 0.13518745  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13054334  |
| Train/Ratio             | 0.99999887  |
| Train/Return            | 1.9035481   |
| Train/V                 | 2.0340793   |
| Train/Value             | 2.0340793   |
| Train/control_penalty   | 0.40771204  |
| Train/policy_loss       | 0.13054334  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0345      |
-----------------------------------------

 ---------------- Iteration 807 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 806        |
| Time/Actor_Time         | 0.0811     |
| Time/B_Format_Time      | 0.0855     |
| Time/B_Original_Form... | 0.0876     |
| Time/Buffer             | 0.00413    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22916073 |
| Train/Action_magnitu... | 0.5227994  |
| Train/Action_magnitude  | 0.4064246  |
| Train/Action_max        | 0.19875218 |
| Train/Action_std        | 0.14142345 |
| Train/Entropy           | -0.5698545 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1526208  |
| Train/Loss              | 0.20405662 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1994269  |
| Train/Ratio             | 0.9999989  |
| Train/Return            | 1.6478686  |
| Train/V                 | 1.84729    |
| Train/Value             | 1.84729    |
| Train/control_penalty   | 0.40598688 |
| Train/policy_loss       | 0.1994269  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 808 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 807         |
| Time/Actor_Time         | 0.0739      |
| Time/B_Format_Time      | 0.0766      |
| Time/B_Original_Form... | 0.08        |
| Time/Buffer             | 0.00498     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23768628  |
| Train/Action_magnitu... | 0.53348446  |
| Train/Action_magnitude  | 0.4170267   |
| Train/Action_max        | 0.1890273   |
| Train/Action_std        | 0.14395943  |
| Train/Entropy           | -0.55418336 |
| Train/Entropy_Loss      | 0.000554    |
| Train/Entropy_loss      | 0.000554    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1109959   |
| Train/Loss              | 0.15889175  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15411575  |
| Train/Ratio             | 0.999997    |
| Train/Return            | 1.9503605   |
| Train/V                 | 2.10448     |
| Train/Value             | 2.10448     |
| Train/control_penalty   | 0.4221816   |
| Train/policy_loss       | 0.15411575  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0355      |
-----------------------------------------

 ---------------- Iteration 809 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 808        |
| Time/Actor_Time         | 0.0769     |
| Time/B_Format_Time      | 0.0798     |
| Time/B_Original_Form... | 0.0802     |
| Time/Buffer             | 0.00378    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2386466  |
| Train/Action_magnitu... | 0.54134816 |
| Train/Action_magnitude  | 0.42516595 |
| Train/Action_max        | 0.18630463 |
| Train/Action_std        | 0.14816399 |
| Train/Entropy           | -0.5248419 |
| Train/Entropy_Loss      | 0.000525   |
| Train/Entropy_loss      | 0.000525   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0460361  |
| Train/Loss              | 0.18219376 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17739616 |
| Train/Ratio             | 1.0000168  |
| Train/Return            | 1.9174038  |
| Train/V                 | 2.094802   |
| Train/Value             | 2.094802   |
| Train/control_penalty   | 0.42727453 |
| Train/policy_loss       | 0.17739616 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03125    |
----------------------------------------

 ---------------- Iteration 810 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 809         |
| Time/Actor_Time         | 0.0758      |
| Time/B_Format_Time      | 0.0783      |
| Time/B_Original_Form... | 0.0803      |
| Time/Buffer             | 0.00427     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24455503  |
| Train/Action_magnitu... | 0.5584041   |
| Train/Action_magnitude  | 0.43543372  |
| Train/Action_max        | 0.2022208   |
| Train/Action_std        | 0.1468724   |
| Train/Entropy           | -0.53463614 |
| Train/Entropy_Loss      | 0.000535    |
| Train/Entropy_loss      | 0.000535    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.067413    |
| Train/Loss              | 0.15354814  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1486788   |
| Train/Ratio             | 0.9999483   |
| Train/Return            | 1.9702686   |
| Train/V                 | 2.1189582   |
| Train/Value             | 2.1189582   |
| Train/control_penalty   | 0.43347025  |
| Train/policy_loss       | 0.1486788   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03025     |
-----------------------------------------

 ---------------- Iteration 811 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 810         |
| Time/Actor_Time         | 0.0731      |
| Time/B_Format_Time      | 0.0782      |
| Time/B_Original_Form... | 0.079       |
| Time/Buffer             | 0.00413     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2427356   |
| Train/Action_magnitu... | 0.548381    |
| Train/Action_magnitude  | 0.4300846   |
| Train/Action_max        | 0.18891913  |
| Train/Action_std        | 0.14615089  |
| Train/Entropy           | -0.53661007 |
| Train/Entropy_Loss      | 0.000537    |
| Train/Entropy_loss      | 0.000537    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0876595   |
| Train/Loss              | 0.21726803  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21245338  |
| Train/Ratio             | 1.0000175   |
| Train/Return            | 1.8406937   |
| Train/V                 | 2.0531385   |
| Train/Value             | 2.0531385   |
| Train/control_penalty   | 0.4278052   |
| Train/policy_loss       | 0.21245338  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02275     |
-----------------------------------------

 ---------------- Iteration 812 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 811        |
| Time/Actor_Time         | 0.0747     |
| Time/B_Format_Time      | 0.0769     |
| Time/B_Original_Form... | 0.075      |
| Time/Buffer             | 0.00431    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24831778 |
| Train/Action_magnitu... | 0.56036353 |
| Train/Action_magnitude  | 0.43885154 |
| Train/Action_max        | 0.19477929 |
| Train/Action_std        | 0.14548105 |
| Train/Entropy           | -0.5372891 |
| Train/Entropy_Loss      | 0.000537   |
| Train/Entropy_loss      | 0.000537   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0752758  |
| Train/Loss              | 0.20710139 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20216526 |
| Train/Ratio             | 0.99998105 |
| Train/Return            | 1.927755   |
| Train/V                 | 2.129934   |
| Train/Value             | 2.129934   |
| Train/control_penalty   | 0.4398841  |
| Train/policy_loss       | 0.20216526 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 813 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 812        |
| Time/Actor_Time         | 0.0733     |
| Time/B_Format_Time      | 0.0723     |
| Time/B_Original_Form... | 0.0743     |
| Time/Buffer             | 0.0043     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2427791  |
| Train/Action_magnitu... | 0.54979664 |
| Train/Action_magnitude  | 0.42958078 |
| Train/Action_max        | 0.21181032 |
| Train/Action_std        | 0.14701907 |
| Train/Entropy           | -0.5314759 |
| Train/Entropy_Loss      | 0.000531   |
| Train/Entropy_loss      | 0.000531   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0576336  |
| Train/Loss              | 0.23557791 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.23076776 |
| Train/Ratio             | 0.9999871  |
| Train/Return            | 1.9250649  |
| Train/V                 | 2.1558359  |
| Train/Value             | 2.1558359  |
| Train/control_penalty   | 0.42786765 |
| Train/policy_loss       | 0.23076776 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------

 ---------------- Iteration 814 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 813        |
| Time/Actor_Time         | 0.0761     |
| Time/B_Format_Time      | 0.0755     |
| Time/B_Original_Form... | 0.0793     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24492106 |
| Train/Action_magnitu... | 0.54776186 |
| Train/Action_magnitude  | 0.42759418 |
| Train/Action_max        | 0.21412383 |
| Train/Action_std        | 0.14306569 |
| Train/Entropy           | -0.5595261 |
| Train/Entropy_Loss      | 0.00056    |
| Train/Entropy_loss      | 0.00056    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0958012  |
| Train/Loss              | 0.24515574 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.24033953 |
| Train/Ratio             | 0.9999967  |
| Train/Return            | 1.8030894  |
| Train/V                 | 2.043429   |
| Train/Value             | 2.043429   |
| Train/control_penalty   | 0.42566785 |
| Train/policy_loss       | 0.24033953 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 815 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 814        |
| Time/Actor_Time         | 0.0778     |
| Time/B_Format_Time      | 0.0768     |
| Time/B_Original_Form... | 0.0828     |
| Time/Buffer             | 0.00405    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22572361 |
| Train/Action_magnitu... | 0.5140183  |
| Train/Action_magnitude  | 0.4009558  |
| Train/Action_max        | 0.18969636 |
| Train/Action_std        | 0.14185622 |
| Train/Entropy           | -0.574458  |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1377378  |
| Train/Loss              | 0.1338752  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12932047 |
| Train/Ratio             | 1.0000112  |
| Train/Return            | 1.6415323  |
| Train/V                 | 1.7708634  |
| Train/Value             | 1.7708634  |
| Train/control_penalty   | 0.39802843 |
| Train/policy_loss       | 0.12932047 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 816 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 815         |
| Time/Actor_Time         | 0.0823      |
| Time/B_Format_Time      | 0.0841      |
| Time/B_Original_Form... | 0.0853      |
| Time/Buffer             | 0.00478     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2301303   |
| Train/Action_magnitu... | 0.5231161   |
| Train/Action_magnitude  | 0.4092942   |
| Train/Action_max        | 0.19723845  |
| Train/Action_std        | 0.14284782  |
| Train/Entropy           | -0.56727177 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1543796   |
| Train/Loss              | 0.2561295   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2514364   |
| Train/Ratio             | 1.0000013   |
| Train/Return            | 1.6809399   |
| Train/V                 | 1.9323709   |
| Train/Value             | 1.9323709   |
| Train/control_penalty   | 0.41258207  |
| Train/policy_loss       | 0.2514364   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01575     |
-----------------------------------------

 ---------------- Iteration 817 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 816        |
| Time/Actor_Time         | 0.075      |
| Time/B_Format_Time      | 0.0761     |
| Time/B_Original_Form... | 0.0756     |
| Time/Buffer             | 0.00421    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22992331 |
| Train/Action_magnitu... | 0.52637076 |
| Train/Action_magnitude  | 0.4131072  |
| Train/Action_max        | 0.18078715 |
| Train/Action_std        | 0.14416564 |
| Train/Entropy           | -0.5588653 |
| Train/Entropy_Loss      | 0.000559   |
| Train/Entropy_loss      | 0.000559   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.119642   |
| Train/Loss              | 0.2757129  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2710472  |
| Train/Ratio             | 0.99999374 |
| Train/Return            | 1.8767153  |
| Train/V                 | 2.147767   |
| Train/Value             | 2.147767   |
| Train/control_penalty   | 0.41068536 |
| Train/policy_loss       | 0.2710472  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 818 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 817        |
| Time/Actor_Time         | 0.0741     |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0774     |
| Time/Buffer             | 0.0034     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2462135  |
| Train/Action_magnitu... | 0.55278766 |
| Train/Action_magnitude  | 0.4329911  |
| Train/Action_max        | 0.20043564 |
| Train/Action_std        | 0.14712867 |
| Train/Entropy           | -0.5320556 |
| Train/Entropy_Loss      | 0.000532   |
| Train/Entropy_loss      | 0.000532   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0672163  |
| Train/Loss              | 0.18127608 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17639814 |
| Train/Ratio             | 0.9999935  |
| Train/Return            | 1.714472   |
| Train/V                 | 1.8908658  |
| Train/Value             | 1.8908658  |
| Train/control_penalty   | 0.43458775 |
| Train/policy_loss       | 0.17639814 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0245     |
----------------------------------------

 ---------------- Iteration 819 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 818        |
| Time/Actor_Time         | 0.0813     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.0777     |
| Time/Buffer             | 0.0042     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24839027 |
| Train/Action_magnitu... | 0.55039984 |
| Train/Action_magnitude  | 0.42651215 |
| Train/Action_max        | 0.22893316 |
| Train/Action_std        | 0.14546482 |
| Train/Entropy           | -0.5462676 |
| Train/Entropy_Loss      | 0.000546   |
| Train/Entropy_loss      | 0.000546   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1082604  |
| Train/Loss              | 0.1392028  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13435437 |
| Train/Ratio             | 0.99999577 |
| Train/Return            | 1.6687111  |
| Train/V                 | 1.8030638  |
| Train/Value             | 1.8030638  |
| Train/control_penalty   | 0.430218   |
| Train/policy_loss       | 0.13435437 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 820 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 819        |
| Time/Actor_Time         | 0.0754     |
| Time/B_Format_Time      | 0.0715     |
| Time/B_Original_Form... | 0.0721     |
| Time/Buffer             | 0.00341    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24981813 |
| Train/Action_magnitu... | 0.55972797 |
| Train/Action_magnitude  | 0.43868938 |
| Train/Action_max        | 0.20514655 |
| Train/Action_std        | 0.1508499  |
| Train/Entropy           | -0.5068428 |
| Train/Entropy_Loss      | 0.000507   |
| Train/Entropy_loss      | 0.000507   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 0.9903312  |
| Train/Loss              | 0.1943261  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18941596 |
| Train/Ratio             | 1.0000219  |
| Train/Return            | 1.751466   |
| Train/V                 | 1.9408764  |
| Train/Value             | 1.9408764  |
| Train/control_penalty   | 0.44032982 |
| Train/policy_loss       | 0.18941596 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 821 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 820        |
| Time/Actor_Time         | 0.0818     |
| Time/B_Format_Time      | 0.0839     |
| Time/B_Original_Form... | 0.0845     |
| Time/Buffer             | 0.00346    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.24942477 |
| Train/Action_magnitu... | 0.55841386 |
| Train/Action_magnitude  | 0.43514505 |
| Train/Action_max        | 0.21521886 |
| Train/Action_std        | 0.14438838 |
| Train/Entropy           | -0.553453  |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1151409  |
| Train/Loss              | 0.18791199 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1829945  |
| Train/Ratio             | 0.999992   |
| Train/Return            | 1.8637977  |
| Train/V                 | 2.0467927  |
| Train/Value             | 2.0467927  |
| Train/control_penalty   | 0.43640253 |
| Train/policy_loss       | 0.1829945  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 822 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 821         |
| Time/Actor_Time         | 0.0824      |
| Time/B_Format_Time      | 0.0832      |
| Time/B_Original_Form... | 0.0842      |
| Time/Buffer             | 0.00407     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.22985367  |
| Train/Action_magnitu... | 0.5138752   |
| Train/Action_magnitude  | 0.4002231   |
| Train/Action_max        | 0.21264166  |
| Train/Action_std        | 0.1373053   |
| Train/Entropy           | -0.60461813 |
| Train/Entropy_Loss      | 0.000605    |
| Train/Entropy_loss      | 0.000605    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2057748   |
| Train/Loss              | 0.15243362  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.147829    |
| Train/Ratio             | 1.0000151   |
| Train/Return            | 1.6917084   |
| Train/V                 | 1.8395301   |
| Train/Value             | 1.8395301   |
| Train/control_penalty   | 0.40000033  |
| Train/policy_loss       | 0.147829    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 823 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 822        |
| Time/Actor_Time         | 0.0746     |
| Time/B_Format_Time      | 0.0746     |
| Time/B_Original_Form... | 0.0732     |
| Time/Buffer             | 0.00383    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23471308 |
| Train/Action_magnitu... | 0.52788764 |
| Train/Action_magnitude  | 0.41236946 |
| Train/Action_max        | 0.21485682 |
| Train/Action_std        | 0.14116804 |
| Train/Entropy           | -0.5783174 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1811225  |
| Train/Loss              | 0.24269591 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.23800312 |
| Train/Ratio             | 1.000008   |
| Train/Return            | 1.6660696  |
| Train/V                 | 1.9040742  |
| Train/Value             | 1.9040742  |
| Train/control_penalty   | 0.41144738 |
| Train/policy_loss       | 0.23800312 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.019      |
----------------------------------------

 ---------------- Iteration 824 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 823         |
| Time/Actor_Time         | 0.0805      |
| Time/B_Format_Time      | 0.0812      |
| Time/B_Original_Form... | 0.0798      |
| Time/Buffer             | 0.00549     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24118756  |
| Train/Action_magnitu... | 0.54116625  |
| Train/Action_magnitude  | 0.42204404  |
| Train/Action_max        | 0.20956519  |
| Train/Action_std        | 0.14354457  |
| Train/Entropy           | -0.55822575 |
| Train/Entropy_Loss      | 0.000558    |
| Train/Entropy_loss      | 0.000558    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1106206   |
| Train/Loss              | 0.18402976  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17924967  |
| Train/Ratio             | 1.0000058   |
| Train/Return            | 1.5750288   |
| Train/V                 | 1.7542827   |
| Train/Value             | 1.7542827   |
| Train/control_penalty   | 0.42218632  |
| Train/policy_loss       | 0.17924967  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.022       |
-----------------------------------------

 ---------------- Iteration 825 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 824        |
| Time/Actor_Time         | 0.0745     |
| Time/B_Format_Time      | 0.0733     |
| Time/B_Original_Form... | 0.0795     |
| Time/Buffer             | 0.00386    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23319687 |
| Train/Action_magnitu... | 0.5243949  |
| Train/Action_magnitude  | 0.41110525 |
| Train/Action_max        | 0.18103817 |
| Train/Action_std        | 0.14429066 |
| Train/Entropy           | -0.5532195 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1069282  |
| Train/Loss              | 0.16855858 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16380906 |
| Train/Ratio             | 1.0000112  |
| Train/Return            | 1.6066426  |
| Train/V                 | 1.7704484  |
| Train/Value             | 1.7704484  |
| Train/control_penalty   | 0.41963002 |
| Train/policy_loss       | 0.16380906 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 826 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 825        |
| Time/Actor_Time         | 0.0766     |
| Time/B_Format_Time      | 0.0783     |
| Time/B_Original_Form... | 0.0798     |
| Time/Buffer             | 0.00337    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24290574 |
| Train/Action_magnitu... | 0.5443993  |
| Train/Action_magnitude  | 0.42611802 |
| Train/Action_max        | 0.19611375 |
| Train/Action_std        | 0.14454585 |
| Train/Entropy           | -0.548171  |
| Train/Entropy_Loss      | 0.000548   |
| Train/Entropy_loss      | 0.000548   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1123198  |
| Train/Loss              | 0.1736265  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16883114 |
| Train/Ratio             | 1.0000257  |
| Train/Return            | 1.5329815  |
| Train/V                 | 1.701799   |
| Train/Value             | 1.701799   |
| Train/control_penalty   | 0.42471853 |
| Train/policy_loss       | 0.16883114 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 827 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 826         |
| Time/Actor_Time         | 0.0734      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00332     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.25079757  |
| Train/Action_magnitu... | 0.56831545  |
| Train/Action_magnitude  | 0.44500208  |
| Train/Action_max        | 0.19505179  |
| Train/Action_std        | 0.1485895   |
| Train/Entropy           | -0.51735413 |
| Train/Entropy_Loss      | 0.000517    |
| Train/Entropy_loss      | 0.000517    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0227703   |
| Train/Loss              | 0.108171634 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.10319734  |
| Train/Ratio             | 0.99997956  |
| Train/Return            | 1.6335489   |
| Train/V                 | 1.7367516   |
| Train/Value             | 1.7367516   |
| Train/control_penalty   | 0.44569388  |
| Train/policy_loss       | 0.10319734  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 828 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 827         |
| Time/Actor_Time         | 0.0737      |
| Time/B_Format_Time      | 0.0749      |
| Time/B_Original_Form... | 0.0744      |
| Time/Buffer             | 0.00411     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.25738534  |
| Train/Action_magnitu... | 0.5760973   |
| Train/Action_magnitude  | 0.45213312  |
| Train/Action_max        | 0.19347334  |
| Train/Action_std        | 0.14723475  |
| Train/Entropy           | -0.52706116 |
| Train/Entropy_Loss      | 0.000527    |
| Train/Entropy_loss      | 0.000527    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0863279   |
| Train/Loss              | 0.115790516 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11075301  |
| Train/Ratio             | 1.000024    |
| Train/Return            | 1.6423945   |
| Train/V                 | 1.7531415   |
| Train/Value             | 1.7531415   |
| Train/control_penalty   | 0.45104468  |
| Train/policy_loss       | 0.11075301  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------

 ---------------- Iteration 829 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 828         |
| Time/Actor_Time         | 0.0742      |
| Time/B_Format_Time      | 0.0773      |
| Time/B_Original_Form... | 0.0756      |
| Time/Buffer             | 0.00423     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24812546  |
| Train/Action_magnitu... | 0.5685287   |
| Train/Action_magnitude  | 0.44633752  |
| Train/Action_max        | 0.18094827  |
| Train/Action_std        | 0.14746755  |
| Train/Entropy           | -0.52799326 |
| Train/Entropy_Loss      | 0.000528    |
| Train/Entropy_loss      | 0.000528    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0485785   |
| Train/Loss              | 0.114442185 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.109497756 |
| Train/Ratio             | 1.0000198   |
| Train/Return            | 1.830764    |
| Train/V                 | 1.9402587   |
| Train/Value             | 1.9402587   |
| Train/control_penalty   | 0.4416433   |
| Train/policy_loss       | 0.109497756 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03225     |
-----------------------------------------

 ---------------- Iteration 830 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 829        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.074      |
| Time/B_Original_Form... | 0.077      |
| Time/Buffer             | 0.00388    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25298643 |
| Train/Action_magnitu... | 0.56122786 |
| Train/Action_magnitude  | 0.43998554 |
| Train/Action_max        | 0.19363321 |
| Train/Action_std        | 0.14524527 |
| Train/Entropy           | -0.5409614 |
| Train/Entropy_Loss      | 0.000541   |
| Train/Entropy_loss      | 0.000541   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1062671  |
| Train/Loss              | 0.18743001 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18244244 |
| Train/Ratio             | 1.0000066  |
| Train/Return            | 1.7398778  |
| Train/V                 | 1.9223132  |
| Train/Value             | 1.9223132  |
| Train/control_penalty   | 0.4446618  |
| Train/policy_loss       | 0.18244244 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 831 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 830         |
| Time/Actor_Time         | 0.0779      |
| Time/B_Format_Time      | 0.0837      |
| Time/B_Original_Form... | 0.0854      |
| Time/Buffer             | 0.00638     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24599704  |
| Train/Action_magnitu... | 0.5527914   |
| Train/Action_magnitude  | 0.43139118  |
| Train/Action_max        | 0.18590762  |
| Train/Action_std        | 0.1443804   |
| Train/Entropy           | -0.54883564 |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0954078   |
| Train/Loss              | 0.089402996 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08453328  |
| Train/Ratio             | 0.9999978   |
| Train/Return            | 1.6339797   |
| Train/V                 | 1.7185147   |
| Train/Value             | 1.7185147   |
| Train/control_penalty   | 0.43208855  |
| Train/policy_loss       | 0.08453328  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 832 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 831         |
| Time/Actor_Time         | 0.0787      |
| Time/B_Format_Time      | 0.0787      |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00405     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24781576  |
| Train/Action_magnitu... | 0.54938185  |
| Train/Action_magnitude  | 0.42821398  |
| Train/Action_max        | 0.18435852  |
| Train/Action_std        | 0.1418905   |
| Train/Entropy           | -0.5640349  |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1313543   |
| Train/Loss              | 0.092233896 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.087351374 |
| Train/Ratio             | 0.99999017  |
| Train/Return            | 1.7609783   |
| Train/V                 | 1.8483373   |
| Train/Value             | 1.8483373   |
| Train/control_penalty   | 0.43184927  |
| Train/policy_loss       | 0.087351374 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 833 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 832        |
| Time/Actor_Time         | 0.0752     |
| Time/B_Format_Time      | 0.0748     |
| Time/B_Original_Form... | 0.0786     |
| Time/Buffer             | 0.00404    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24078792 |
| Train/Action_magnitu... | 0.5418725  |
| Train/Action_magnitude  | 0.42152116 |
| Train/Action_max        | 0.19471161 |
| Train/Action_std        | 0.14105263 |
| Train/Entropy           | -0.5726949 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1191376  |
| Train/Loss              | 0.15581556 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15109123 |
| Train/Ratio             | 1.0000156  |
| Train/Return            | 1.5680842  |
| Train/V                 | 1.7191758  |
| Train/Value             | 1.7191758  |
| Train/control_penalty   | 0.41516227 |
| Train/policy_loss       | 0.15109123 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 834 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 833        |
| Time/Actor_Time         | 0.0738     |
| Time/B_Format_Time      | 0.07       |
| Time/B_Original_Form... | 0.0729     |
| Time/Buffer             | 0.00412    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24189022 |
| Train/Action_magnitu... | 0.53687733 |
| Train/Action_magnitude  | 0.41615933 |
| Train/Action_max        | 0.21482016 |
| Train/Action_std        | 0.14022617 |
| Train/Entropy           | -0.5759662 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1668916  |
| Train/Loss              | 0.18542512 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18067364 |
| Train/Ratio             | 0.9999908  |
| Train/Return            | 1.5893124  |
| Train/V                 | 1.7699871  |
| Train/Value             | 1.7699871  |
| Train/control_penalty   | 0.41755083 |
| Train/policy_loss       | 0.18067364 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.021      |
----------------------------------------

 ---------------- Iteration 835 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 834        |
| Time/Actor_Time         | 0.0769     |
| Time/B_Format_Time      | 0.0782     |
| Time/B_Original_Form... | 0.0764     |
| Time/Buffer             | 0.00386    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23676874 |
| Train/Action_magnitu... | 0.5330748  |
| Train/Action_magnitude  | 0.41332138 |
| Train/Action_max        | 0.19769052 |
| Train/Action_std        | 0.13992882 |
| Train/Entropy           | -0.5826514 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1767446  |
| Train/Loss              | 0.13193902 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12725338 |
| Train/Ratio             | 1.0000056  |
| Train/Return            | 1.5611516  |
| Train/V                 | 1.6884037  |
| Train/Value             | 1.6884037  |
| Train/control_penalty   | 0.41029963 |
| Train/policy_loss       | 0.12725338 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 836 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 835        |
| Time/Actor_Time         | 0.072      |
| Time/B_Format_Time      | 0.0697     |
| Time/B_Original_Form... | 0.0729     |
| Time/Buffer             | 0.00443    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24462153 |
| Train/Action_magnitu... | 0.5464646  |
| Train/Action_magnitude  | 0.42606258 |
| Train/Action_max        | 0.20916891 |
| Train/Action_std        | 0.14282855 |
| Train/Entropy           | -0.5635143 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1239839  |
| Train/Loss              | 0.17812946 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17332831 |
| Train/Ratio             | 1.0000086  |
| Train/Return            | 1.7111943  |
| Train/V                 | 1.8845254  |
| Train/Value             | 1.8845254  |
| Train/control_penalty   | 0.4237633  |
| Train/policy_loss       | 0.17332831 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02725    |
----------------------------------------

 ---------------- Iteration 837 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 836        |
| Time/Actor_Time         | 0.0708     |
| Time/B_Format_Time      | 0.0703     |
| Time/B_Original_Form... | 0.0715     |
| Time/Buffer             | 0.00316    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.2377617  |
| Train/Action_magnitu... | 0.5326018  |
| Train/Action_magnitude  | 0.4137956  |
| Train/Action_max        | 0.21265298 |
| Train/Action_std        | 0.14334399 |
| Train/Entropy           | -0.5578441 |
| Train/Entropy_Loss      | 0.000558   |
| Train/Entropy_loss      | 0.000558   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1122707  |
| Train/Loss              | 0.1962444  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1915606  |
| Train/Ratio             | 0.9999986  |
| Train/Return            | 1.5276552  |
| Train/V                 | 1.7192289  |
| Train/Value             | 1.7192289  |
| Train/control_penalty   | 0.4125961  |
| Train/policy_loss       | 0.1915606  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01875    |
----------------------------------------

 ---------------- Iteration 838 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 837         |
| Time/Actor_Time         | 0.0701      |
| Time/B_Format_Time      | 0.0698      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00353     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24011627  |
| Train/Action_magnitu... | 0.5352116   |
| Train/Action_magnitude  | 0.41619164  |
| Train/Action_max        | 0.1840782   |
| Train/Action_std        | 0.14200853  |
| Train/Entropy           | -0.56666845 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1229113   |
| Train/Loss              | 0.1520409   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14727205  |
| Train/Ratio             | 1.0000011   |
| Train/Return            | 1.5850565   |
| Train/V                 | 1.7323161   |
| Train/Value             | 1.7323161   |
| Train/control_penalty   | 0.4202191   |
| Train/policy_loss       | 0.14727205  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 839 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 838         |
| Time/Actor_Time         | 0.0697      |
| Time/B_Format_Time      | 0.0711      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.00306     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2416889   |
| Train/Action_magnitu... | 0.5419253   |
| Train/Action_magnitude  | 0.4224533   |
| Train/Action_max        | 0.18916586  |
| Train/Action_std        | 0.14487009  |
| Train/Entropy           | -0.54452693 |
| Train/Entropy_Loss      | 0.000545    |
| Train/Entropy_loss      | 0.000545    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0943477   |
| Train/Loss              | 0.11475017  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.109954245 |
| Train/Ratio             | 1.000017    |
| Train/Return            | 1.5860506   |
| Train/V                 | 1.6959999   |
| Train/Value             | 1.6959999   |
| Train/control_penalty   | 0.42513958  |
| Train/policy_loss       | 0.109954245 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 840 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 839        |
| Time/Actor_Time         | 0.0752     |
| Time/B_Format_Time      | 0.0773     |
| Time/B_Original_Form... | 0.0796     |
| Time/Buffer             | 0.00406    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24609391 |
| Train/Action_magnitu... | 0.5529466  |
| Train/Action_magnitude  | 0.43263614 |
| Train/Action_max        | 0.18516791 |
| Train/Action_std        | 0.1440467  |
| Train/Entropy           | -0.5499503 |
| Train/Entropy_Loss      | 0.00055    |
| Train/Entropy_loss      | 0.00055    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1181145  |
| Train/Loss              | 0.08084647 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07607662 |
| Train/Ratio             | 1.0000199  |
| Train/Return            | 1.7215495  |
| Train/V                 | 1.7976304  |
| Train/Value             | 1.7976304  |
| Train/control_penalty   | 0.4219905  |
| Train/policy_loss       | 0.07607662 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0325     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 841 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 840        |
| Time/Actor_Time         | 0.0728     |
| Time/B_Format_Time      | 0.073      |
| Time/B_Original_Form... | 0.0748     |
| Time/Buffer             | 0.00353    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24252486 |
| Train/Action_magnitu... | 0.538061   |
| Train/Action_magnitude  | 0.4180765  |
| Train/Action_max        | 0.20351197 |
| Train/Action_std        | 0.13944115 |
| Train/Entropy           | -0.5830106 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1825985  |
| Train/Loss              | 0.10380232 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09902735 |
| Train/Ratio             | 1.0000008  |
| Train/Return            | 1.56402    |
| Train/V                 | 1.663046   |
| Train/Value             | 1.663046   |
| Train/control_penalty   | 0.41919658 |
| Train/policy_loss       | 0.09902735 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 842 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 841         |
| Time/Actor_Time         | 0.0757      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0767      |
| Time/Buffer             | 0.00279     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2400444   |
| Train/Action_magnitu... | 0.5382359   |
| Train/Action_magnitude  | 0.41942438  |
| Train/Action_max        | 0.19125372  |
| Train/Action_std        | 0.14069304  |
| Train/Entropy           | -0.5775658  |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1514771   |
| Train/Loss              | 0.091925174 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.087229915 |
| Train/Ratio             | 0.99999666  |
| Train/Return            | 1.87634     |
| Train/V                 | 1.9635708   |
| Train/Value             | 1.9635708   |
| Train/control_penalty   | 0.4117694   |
| Train/policy_loss       | 0.087229915 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 843 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 842         |
| Time/Actor_Time         | 0.0729      |
| Time/B_Format_Time      | 0.0763      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00352     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23901501  |
| Train/Action_magnitu... | 0.53146654  |
| Train/Action_magnitude  | 0.41146556  |
| Train/Action_max        | 0.18890607  |
| Train/Action_std        | 0.13762033  |
| Train/Entropy           | -0.5980065  |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2016027   |
| Train/Loss              | 0.096985534 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.092267305 |
| Train/Ratio             | 0.9999851   |
| Train/Return            | 1.8207647   |
| Train/V                 | 1.9130262   |
| Train/Value             | 1.9130262   |
| Train/control_penalty   | 0.41202262  |
| Train/policy_loss       | 0.092267305 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0365      |
-----------------------------------------

 ---------------- Iteration 844 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 843         |
| Time/Actor_Time         | 0.0703      |
| Time/B_Format_Time      | 0.0706      |
| Time/B_Original_Form... | 0.0717      |
| Time/Buffer             | 0.00321     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23306668  |
| Train/Action_magnitu... | 0.5170863   |
| Train/Action_magnitude  | 0.39872688  |
| Train/Action_max        | 0.19413131  |
| Train/Action_std        | 0.13631898  |
| Train/Entropy           | -0.6074265  |
| Train/Entropy_Loss      | 0.000607    |
| Train/Entropy_loss      | 0.000607    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2222404   |
| Train/Loss              | 0.091756724 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08717771  |
| Train/Ratio             | 0.99999356  |
| Train/Return            | 1.6737224   |
| Train/V                 | 1.7609043   |
| Train/Value             | 1.7609043   |
| Train/control_penalty   | 0.39715898  |
| Train/policy_loss       | 0.08717771  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 845 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 844         |
| Time/Actor_Time         | 0.0718      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00289     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24303176  |
| Train/Action_magnitu... | 0.53454024  |
| Train/Action_magnitude  | 0.41344967  |
| Train/Action_max        | 0.20881583  |
| Train/Action_std        | 0.13889565  |
| Train/Entropy           | -0.59018064 |
| Train/Entropy_Loss      | 0.00059     |
| Train/Entropy_loss      | 0.00059     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1769787   |
| Train/Loss              | 0.19358419  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18881425  |
| Train/Ratio             | 0.9999966   |
| Train/Return            | 1.5988959   |
| Train/V                 | 1.7877128   |
| Train/Value             | 1.7877128   |
| Train/control_penalty   | 0.41797677  |
| Train/policy_loss       | 0.18881425  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01975     |
-----------------------------------------

 ---------------- Iteration 846 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 845         |
| Time/Actor_Time         | 0.0707      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00339     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2391771   |
| Train/Action_magnitu... | 0.5316328   |
| Train/Action_magnitude  | 0.40996942  |
| Train/Action_max        | 0.20910992  |
| Train/Action_std        | 0.13677391  |
| Train/Entropy           | -0.60355365 |
| Train/Entropy_Loss      | 0.000604    |
| Train/Entropy_loss      | 0.000604    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2066725   |
| Train/Loss              | 0.107259706 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.102606244 |
| Train/Ratio             | 1.0000106   |
| Train/Return            | 1.6832737   |
| Train/V                 | 1.7858796   |
| Train/Value             | 1.7858796   |
| Train/control_penalty   | 0.40499014  |
| Train/policy_loss       | 0.102606244 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 847 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 846        |
| Time/Actor_Time         | 0.0707     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.0733     |
| Time/Buffer             | 0.00312    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23672226 |
| Train/Action_magnitu... | 0.52825934 |
| Train/Action_magnitude  | 0.4109236  |
| Train/Action_max        | 0.18877107 |
| Train/Action_std        | 0.14339127 |
| Train/Entropy           | -0.5628292 |
| Train/Entropy_Loss      | 0.000563   |
| Train/Entropy_loss      | 0.000563   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1047976  |
| Train/Loss              | 0.12271039 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11805054 |
| Train/Ratio             | 0.9999761  |
| Train/Return            | 1.6272576  |
| Train/V                 | 1.7453144  |
| Train/Value             | 1.7453144  |
| Train/control_penalty   | 0.40970188 |
| Train/policy_loss       | 0.11805054 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03575    |
----------------------------------------

 ---------------- Iteration 848 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 847         |
| Time/Actor_Time         | 0.0714      |
| Time/B_Format_Time      | 0.0703      |
| Time/B_Original_Form... | 0.0719      |
| Time/Buffer             | 0.00323     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24417803  |
| Train/Action_magnitu... | 0.54057205  |
| Train/Action_magnitude  | 0.4184527   |
| Train/Action_max        | 0.20146175  |
| Train/Action_std        | 0.14051145  |
| Train/Entropy           | -0.57670784 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1590416   |
| Train/Loss              | 0.1328223   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12805928  |
| Train/Ratio             | 0.9999838   |
| Train/Return            | 1.658676    |
| Train/V                 | 1.7867411   |
| Train/Value             | 1.7867411   |
| Train/control_penalty   | 0.41863182  |
| Train/policy_loss       | 0.12805928  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03125     |
-----------------------------------------

 ---------------- Iteration 849 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 848         |
| Time/Actor_Time         | 0.0772      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.0768      |
| Time/Buffer             | 0.004       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24347165  |
| Train/Action_magnitu... | 0.546764    |
| Train/Action_magnitude  | 0.4232849   |
| Train/Action_max        | 0.19906874  |
| Train/Action_std        | 0.14413458  |
| Train/Entropy           | -0.55493724 |
| Train/Entropy_Loss      | 0.000555    |
| Train/Entropy_loss      | 0.000555    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0952356   |
| Train/Loss              | 0.078264914 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.07349004  |
| Train/Ratio             | 1.0000103   |
| Train/Return            | 1.7398919   |
| Train/V                 | 1.8133706   |
| Train/Value             | 1.8133706   |
| Train/control_penalty   | 0.42199418  |
| Train/policy_loss       | 0.07349004  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03675     |
-----------------------------------------

 ---------------- Iteration 850 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 849        |
| Time/Actor_Time         | 0.0707     |
| Time/B_Format_Time      | 0.0689     |
| Time/B_Original_Form... | 0.072      |
| Time/Buffer             | 0.00365    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.24654953 |
| Train/Action_magnitu... | 0.54223865 |
| Train/Action_magnitude  | 0.42126074 |
| Train/Action_max        | 0.21175669 |
| Train/Action_std        | 0.14196852 |
| Train/Entropy           | -0.5663501 |
| Train/Entropy_Loss      | 0.000566   |
| Train/Entropy_loss      | 0.000566   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.15351    |
| Train/Loss              | 0.20611316 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2012896  |
| Train/Ratio             | 1.0000241  |
| Train/Return            | 1.6015903  |
| Train/V                 | 1.8028716  |
| Train/Value             | 1.8028716  |
| Train/control_penalty   | 0.42572245 |
| Train/policy_loss       | 0.2012896  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 851 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 850         |
| Time/Actor_Time         | 0.0717      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.0722      |
| Time/Buffer             | 0.00295     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2385315   |
| Train/Action_magnitu... | 0.5349154   |
| Train/Action_magnitude  | 0.41534778  |
| Train/Action_max        | 0.1979868   |
| Train/Action_std        | 0.14293425  |
| Train/Entropy           | -0.5609999  |
| Train/Entropy_Loss      | 0.000561    |
| Train/Entropy_loss      | 0.000561    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1113982   |
| Train/Loss              | 0.09460013  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.089916475 |
| Train/Ratio             | 0.9999992   |
| Train/Return            | 1.6750808   |
| Train/V                 | 1.7650033   |
| Train/Value             | 1.7650033   |
| Train/control_penalty   | 0.41226572  |
| Train/policy_loss       | 0.089916475 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0305      |
-----------------------------------------

 ---------------- Iteration 852 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 851         |
| Time/Actor_Time         | 0.0734      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.00437     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24580553  |
| Train/Action_magnitu... | 0.54698753  |
| Train/Action_magnitude  | 0.42432117  |
| Train/Action_max        | 0.19831756  |
| Train/Action_std        | 0.14323288  |
| Train/Entropy           | -0.55632126 |
| Train/Entropy_Loss      | 0.000556    |
| Train/Entropy_loss      | 0.000556    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1384996   |
| Train/Loss              | 0.09173276  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08694673  |
| Train/Ratio             | 1.0000021   |
| Train/Return            | 1.728144    |
| Train/V                 | 1.8150896   |
| Train/Value             | 1.8150896   |
| Train/control_penalty   | 0.42297107  |
| Train/policy_loss       | 0.08694673  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03125     |
-----------------------------------------

 ---------------- Iteration 853 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 852         |
| Time/Actor_Time         | 0.071       |
| Time/B_Format_Time      | 0.07        |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23191005  |
| Train/Action_magnitu... | 0.5177394   |
| Train/Action_magnitude  | 0.40215516  |
| Train/Action_max        | 0.1854458   |
| Train/Action_std        | 0.13691519  |
| Train/Entropy           | -0.60382885 |
| Train/Entropy_Loss      | 0.000604    |
| Train/Entropy_loss      | 0.000604    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1932194   |
| Train/Loss              | 0.21319626  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20857479  |
| Train/Ratio             | 1.0000105   |
| Train/Return            | 1.6419787   |
| Train/V                 | 1.8505592   |
| Train/Value             | 1.8505592   |
| Train/control_penalty   | 0.40176564  |
| Train/policy_loss       | 0.20857479  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.018       |
-----------------------------------------

 ---------------- Iteration 854 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 853         |
| Time/Actor_Time         | 0.0712      |
| Time/B_Format_Time      | 0.0698      |
| Time/B_Original_Form... | 0.0727      |
| Time/Buffer             | 0.00413     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23777403  |
| Train/Action_magnitu... | 0.5315571   |
| Train/Action_magnitude  | 0.41125712  |
| Train/Action_max        | 0.19775146  |
| Train/Action_std        | 0.1377703   |
| Train/Entropy           | -0.59637475 |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.19714     |
| Train/Loss              | 0.09975259  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09506379  |
| Train/Ratio             | 1.0000062   |
| Train/Return            | 1.6963354   |
| Train/V                 | 1.7913998   |
| Train/Value             | 1.7913998   |
| Train/control_penalty   | 0.4092423   |
| Train/policy_loss       | 0.09506379  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0345      |
-----------------------------------------

 ---------------- Iteration 855 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 854         |
| Time/Actor_Time         | 0.0713      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.0726      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23541197  |
| Train/Action_magnitu... | 0.52039003  |
| Train/Action_magnitude  | 0.40487242  |
| Train/Action_max        | 0.21055497  |
| Train/Action_std        | 0.140526    |
| Train/Entropy           | -0.57682574 |
| Train/Entropy_Loss      | 0.000577    |
| Train/Entropy_loss      | 0.000577    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1762611   |
| Train/Loss              | 0.16278508  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15813178  |
| Train/Ratio             | 0.99999154  |
| Train/Return            | 1.6146524   |
| Train/V                 | 1.7727859   |
| Train/Value             | 1.7727859   |
| Train/control_penalty   | 0.4076483   |
| Train/policy_loss       | 0.15813178  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 856 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 855        |
| Time/Actor_Time         | 0.0713     |
| Time/B_Format_Time      | 0.0685     |
| Time/B_Original_Form... | 0.0705     |
| Time/Buffer             | 0.00613    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23760326 |
| Train/Action_magnitu... | 0.5253594  |
| Train/Action_magnitude  | 0.4084422  |
| Train/Action_max        | 0.20264524 |
| Train/Action_std        | 0.14183573 |
| Train/Entropy           | -0.5670196 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1481011  |
| Train/Loss              | 0.173515   |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16884081 |
| Train/Ratio             | 0.9999997  |
| Train/Return            | 1.6440986  |
| Train/V                 | 1.8129373  |
| Train/Value             | 1.8129373  |
| Train/control_penalty   | 0.41071844 |
| Train/policy_loss       | 0.16884081 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02775    |
----------------------------------------

 ---------------- Iteration 857 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 856         |
| Time/Actor_Time         | 0.0695      |
| Time/B_Format_Time      | 0.0693      |
| Time/B_Original_Form... | 0.0724      |
| Time/Buffer             | 0.0027      |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23036654  |
| Train/Action_magnitu... | 0.5147908   |
| Train/Action_magnitude  | 0.39913067  |
| Train/Action_max        | 0.20790726  |
| Train/Action_std        | 0.13958593  |
| Train/Entropy           | -0.58718276 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1501402   |
| Train/Loss              | 0.1756639   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1710697   |
| Train/Ratio             | 0.99998695  |
| Train/Return            | 1.8112614   |
| Train/V                 | 1.9823372   |
| Train/Value             | 1.9823372   |
| Train/control_penalty   | 0.400703    |
| Train/policy_loss       | 0.1710697   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 858 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 857        |
| Time/Actor_Time         | 0.0711     |
| Time/B_Format_Time      | 0.0703     |
| Time/B_Original_Form... | 0.0735     |
| Time/Buffer             | 0.00292    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23397681 |
| Train/Action_magnitu... | 0.52529204 |
| Train/Action_magnitude  | 0.40871793 |
| Train/Action_max        | 0.19865705 |
| Train/Action_std        | 0.13927475 |
| Train/Entropy           | -0.5912163 |
| Train/Entropy_Loss      | 0.000591   |
| Train/Entropy_loss      | 0.000591   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1757293  |
| Train/Loss              | 0.2597124  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2550253  |
| Train/Ratio             | 0.99999636 |
| Train/Return            | 1.7098205  |
| Train/V                 | 1.964862   |
| Train/Value             | 1.964862   |
| Train/control_penalty   | 0.40958846 |
| Train/policy_loss       | 0.2550253  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 859 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 858        |
| Time/Actor_Time         | 0.0716     |
| Time/B_Format_Time      | 0.0696     |
| Time/B_Original_Form... | 0.0721     |
| Time/Buffer             | 0.00258    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24653515 |
| Train/Action_magnitu... | 0.5525052  |
| Train/Action_magnitude  | 0.43335363 |
| Train/Action_max        | 0.18026535 |
| Train/Action_std        | 0.1429056  |
| Train/Entropy           | -0.5603796 |
| Train/Entropy_Loss      | 0.00056    |
| Train/Entropy_loss      | 0.00056    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1311758  |
| Train/Loss              | 0.2733243  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.268411   |
| Train/Ratio             | 0.9999834  |
| Train/Return            | 1.9903314  |
| Train/V                 | 2.258744   |
| Train/Value             | 2.258744   |
| Train/control_penalty   | 0.43529266 |
| Train/policy_loss       | 0.268411   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.021      |
----------------------------------------

 ---------------- Iteration 860 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 859        |
| Time/Actor_Time         | 0.0728     |
| Time/B_Format_Time      | 0.0716     |
| Time/B_Original_Form... | 0.0742     |
| Time/Buffer             | 0.0037     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25645682 |
| Train/Action_magnitu... | 0.5711517  |
| Train/Action_magnitude  | 0.4491632  |
| Train/Action_max        | 0.18154944 |
| Train/Action_std        | 0.14489095 |
| Train/Entropy           | -0.544423  |
| Train/Entropy_Loss      | 0.000544   |
| Train/Entropy_loss      | 0.000544   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0996891  |
| Train/Loss              | 0.37639982 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.37136194 |
| Train/Ratio             | 1.000026   |
| Train/Return            | 2.3877115  |
| Train/V                 | 2.7590752  |
| Train/Value             | 2.7590752  |
| Train/control_penalty   | 0.44934523 |
| Train/policy_loss       | 0.37136194 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02925    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 861 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 860         |
| Time/Actor_Time         | 0.0702      |
| Time/B_Format_Time      | 0.072       |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.00277     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23978135  |
| Train/Action_magnitu... | 0.530665    |
| Train/Action_magnitude  | 0.41504768  |
| Train/Action_max        | 0.16449551  |
| Train/Action_std        | 0.13482954  |
| Train/Entropy           | -0.61643714 |
| Train/Entropy_Loss      | 0.000616    |
| Train/Entropy_loss      | 0.000616    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2288737   |
| Train/Loss              | 0.32786083  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.3230683   |
| Train/Ratio             | 1.00002     |
| Train/Return            | 2.0013323   |
| Train/V                 | 2.3243926   |
| Train/Value             | 2.3243926   |
| Train/control_penalty   | 0.41761124  |
| Train/policy_loss       | 0.3230683   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0185      |
-----------------------------------------

 ---------------- Iteration 862 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 861         |
| Time/Actor_Time         | 0.0701      |
| Time/B_Format_Time      | 0.0684      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00262     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23752002  |
| Train/Action_magnitu... | 0.53048766  |
| Train/Action_magnitude  | 0.41499886  |
| Train/Action_max        | 0.18622933  |
| Train/Action_std        | 0.13817509  |
| Train/Entropy           | -0.59214157 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1829731   |
| Train/Loss              | 0.29517797  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.29044527  |
| Train/Ratio             | 1.0000037   |
| Train/Return            | 1.7347647   |
| Train/V                 | 2.0252132   |
| Train/Value             | 2.0252132   |
| Train/control_penalty   | 0.41405505  |
| Train/policy_loss       | 0.29044527  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------

 ---------------- Iteration 863 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 862         |
| Time/Actor_Time         | 0.0704      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00382     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24321598  |
| Train/Action_magnitu... | 0.5482841   |
| Train/Action_magnitude  | 0.42883033  |
| Train/Action_max        | 0.18026152  |
| Train/Action_std        | 0.14200133  |
| Train/Entropy           | -0.56355226 |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0978813   |
| Train/Loss              | 0.22008333  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21524538  |
| Train/Ratio             | 0.9999889   |
| Train/Return            | 1.9017121   |
| Train/V                 | 2.1169648   |
| Train/Value             | 2.1169648   |
| Train/control_penalty   | 0.42743942  |
| Train/policy_loss       | 0.21524538  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 864 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 863        |
| Time/Actor_Time         | 0.0709     |
| Time/B_Format_Time      | 0.0688     |
| Time/B_Original_Form... | 0.0697     |
| Time/Buffer             | 0.00317    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23835486 |
| Train/Action_magnitu... | 0.5323358  |
| Train/Action_magnitude  | 0.4164258  |
| Train/Action_max        | 0.15680438 |
| Train/Action_std        | 0.13852876 |
| Train/Entropy           | -0.5867518 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2032901  |
| Train/Loss              | 0.21718027 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21243334 |
| Train/Ratio             | 0.9999871  |
| Train/Return            | 1.9344277  |
| Train/V                 | 2.1468546  |
| Train/Value             | 2.1468546  |
| Train/control_penalty   | 0.41601783 |
| Train/policy_loss       | 0.21243334 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02525    |
----------------------------------------

 ---------------- Iteration 865 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 864         |
| Time/Actor_Time         | 0.0712      |
| Time/B_Format_Time      | 0.0686      |
| Time/B_Original_Form... | 0.0716      |
| Time/Buffer             | 0.00341     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22798724  |
| Train/Action_magnitu... | 0.5182417   |
| Train/Action_magnitude  | 0.40611497  |
| Train/Action_max        | 0.18274932  |
| Train/Action_std        | 0.14094669  |
| Train/Entropy           | -0.57275486 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1496938   |
| Train/Loss              | 0.24745809  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.24282032  |
| Train/Ratio             | 0.99999386  |
| Train/Return            | 1.6781074   |
| Train/V                 | 1.9209169   |
| Train/Value             | 1.9209169   |
| Train/control_penalty   | 0.4065006   |
| Train/policy_loss       | 0.24282032  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------

 ---------------- Iteration 866 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 865        |
| Time/Actor_Time         | 0.0719     |
| Time/B_Format_Time      | 0.0707     |
| Time/B_Original_Form... | 0.0717     |
| Time/Buffer             | 0.00337    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23942599 |
| Train/Action_magnitu... | 0.5293055  |
| Train/Action_magnitude  | 0.41255617 |
| Train/Action_max        | 0.197042   |
| Train/Action_std        | 0.13765574 |
| Train/Entropy           | -0.5997783 |
| Train/Entropy_Loss      | 0.0006     |
| Train/Entropy_loss      | 0.0006     |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1912574  |
| Train/Loss              | 0.1634341  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1587243  |
| Train/Ratio             | 0.9999994  |
| Train/Return            | 1.7119023  |
| Train/V                 | 1.8706237  |
| Train/Value             | 1.8706237  |
| Train/control_penalty   | 0.41100404 |
| Train/policy_loss       | 0.1587243  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0305     |
----------------------------------------

 ---------------- Iteration 867 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 866        |
| Time/Actor_Time         | 0.0741     |
| Time/B_Format_Time      | 0.0702     |
| Time/B_Original_Form... | 0.072      |
| Time/Buffer             | 0.00294    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22887757 |
| Train/Action_magnitu... | 0.51711    |
| Train/Action_magnitude  | 0.40314046 |
| Train/Action_max        | 0.19644807 |
| Train/Action_std        | 0.14092949 |
| Train/Entropy           | -0.5755188 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.165688   |
| Train/Loss              | 0.20689593 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2022813  |
| Train/Ratio             | 1.0        |
| Train/Return            | 1.6191704  |
| Train/V                 | 1.8214687  |
| Train/Value             | 1.8214687  |
| Train/control_penalty   | 0.40391198 |
| Train/policy_loss       | 0.2022813  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 868 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 867        |
| Time/Actor_Time         | 0.0708     |
| Time/B_Format_Time      | 0.0698     |
| Time/B_Original_Form... | 0.074      |
| Time/Buffer             | 0.00332    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23153399 |
| Train/Action_magnitu... | 0.5238704  |
| Train/Action_magnitude  | 0.4093347  |
| Train/Action_max        | 0.21167965 |
| Train/Action_std        | 0.14565076 |
| Train/Entropy           | -0.5443158 |
| Train/Entropy_Loss      | 0.000544   |
| Train/Entropy_loss      | 0.000544   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0973381  |
| Train/Loss              | 0.18717094 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18250233 |
| Train/Ratio             | 0.9999938  |
| Train/Return            | 1.5869488  |
| Train/V                 | 1.7694383  |
| Train/Value             | 1.7694383  |
| Train/control_penalty   | 0.41242972 |
| Train/policy_loss       | 0.18250233 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0225     |
----------------------------------------

 ---------------- Iteration 869 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 868        |
| Time/Actor_Time         | 0.0722     |
| Time/B_Format_Time      | 0.0692     |
| Time/B_Original_Form... | 0.0727     |
| Time/Buffer             | 0.00264    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23050214 |
| Train/Action_magnitu... | 0.5201747  |
| Train/Action_magnitude  | 0.40606055 |
| Train/Action_max        | 0.19357325 |
| Train/Action_std        | 0.1414458  |
| Train/Entropy           | -0.5721492 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1568251  |
| Train/Loss              | 0.2183768  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21373148 |
| Train/Ratio             | 1.0000013  |
| Train/Return            | 1.6241325  |
| Train/V                 | 1.8378735  |
| Train/Value             | 1.8378735  |
| Train/control_penalty   | 0.40731776 |
| Train/policy_loss       | 0.21373148 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01825    |
----------------------------------------

 ---------------- Iteration 870 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 869         |
| Time/Actor_Time         | 0.0743      |
| Time/B_Format_Time      | 0.0779      |
| Time/B_Original_Form... | 0.0802      |
| Time/Buffer             | 0.00318     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23470311  |
| Train/Action_magnitu... | 0.52612615  |
| Train/Action_magnitude  | 0.41171858  |
| Train/Action_max        | 0.19631112  |
| Train/Action_std        | 0.14208728  |
| Train/Entropy           | -0.56805897 |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.172435    |
| Train/Loss              | 0.22321445  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21848044  |
| Train/Ratio             | 0.9999811   |
| Train/Return            | 1.6325339   |
| Train/V                 | 1.8510239   |
| Train/Value             | 1.8510239   |
| Train/control_penalty   | 0.41659534  |
| Train/policy_loss       | 0.21848044  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01875     |
-----------------------------------------

 ---------------- Iteration 871 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 870        |
| Time/Actor_Time         | 0.0692     |
| Time/B_Format_Time      | 0.0685     |
| Time/B_Original_Form... | 0.0714     |
| Time/Buffer             | 0.00251    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23724455 |
| Train/Action_magnitu... | 0.5391146  |
| Train/Action_magnitude  | 0.42139786 |
| Train/Action_max        | 0.17040159 |
| Train/Action_std        | 0.13994548 |
| Train/Entropy           | -0.5799385 |
| Train/Entropy_Loss      | 0.00058    |
| Train/Entropy_loss      | 0.00058    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1592312  |
| Train/Loss              | 0.29967275 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.29488298 |
| Train/Ratio             | 0.9999788  |
| Train/Return            | 1.891104   |
| Train/V                 | 2.1860015  |
| Train/Value             | 2.1860015  |
| Train/control_penalty   | 0.42098114 |
| Train/policy_loss       | 0.29488298 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0165     |
----------------------------------------

 ---------------- Iteration 872 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 871        |
| Time/Actor_Time         | 0.0703     |
| Time/B_Format_Time      | 0.0686     |
| Time/B_Original_Form... | 0.0735     |
| Time/Buffer             | 0.00316    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24088782 |
| Train/Action_magnitu... | 0.5363922  |
| Train/Action_magnitude  | 0.42077938 |
| Train/Action_max        | 0.17536265 |
| Train/Action_std        | 0.13986842 |
| Train/Entropy           | -0.5772777 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1527021  |
| Train/Loss              | 0.24521971 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2404423  |
| Train/Ratio             | 1.000002   |
| Train/Return            | 1.8477589  |
| Train/V                 | 2.088202   |
| Train/Value             | 2.088202   |
| Train/control_penalty   | 0.4200134  |
| Train/policy_loss       | 0.2404423  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0205     |
----------------------------------------

 ---------------- Iteration 873 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 872         |
| Time/Actor_Time         | 0.0707      |
| Time/B_Format_Time      | 0.0697      |
| Time/B_Original_Form... | 0.0737      |
| Time/Buffer             | 0.00286     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24738717  |
| Train/Action_magnitu... | 0.5429132   |
| Train/Action_magnitude  | 0.4239196   |
| Train/Action_max        | 0.18646343  |
| Train/Action_std        | 0.14166844  |
| Train/Entropy           | -0.57058316 |
| Train/Entropy_Loss      | 0.000571    |
| Train/Entropy_loss      | 0.000571    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1389384   |
| Train/Loss              | 0.19622084  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19136238  |
| Train/Ratio             | 1.0000266   |
| Train/Return            | 1.7144464   |
| Train/V                 | 1.9057953   |
| Train/Value             | 1.9057953   |
| Train/control_penalty   | 0.42878827  |
| Train/policy_loss       | 0.19136238  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 874 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 873        |
| Time/Actor_Time         | 0.0714     |
| Time/B_Format_Time      | 0.068      |
| Time/B_Original_Form... | 0.0703     |
| Time/Buffer             | 0.0041     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23897894 |
| Train/Action_magnitu... | 0.538063   |
| Train/Action_magnitude  | 0.41888785 |
| Train/Action_max        | 0.1808368  |
| Train/Action_std        | 0.14235128 |
| Train/Entropy           | -0.566439  |
| Train/Entropy_Loss      | 0.000566   |
| Train/Entropy_loss      | 0.000566   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1152248  |
| Train/Loss              | 0.15033151 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14562729 |
| Train/Ratio             | 0.99998033 |
| Train/Return            | 1.7370023  |
| Train/V                 | 1.8826275  |
| Train/Value             | 1.8826275  |
| Train/control_penalty   | 0.41377854 |
| Train/policy_loss       | 0.14562729 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 875 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 874        |
| Time/Actor_Time         | 0.0725     |
| Time/B_Format_Time      | 0.0693     |
| Time/B_Original_Form... | 0.0723     |
| Time/Buffer             | 0.00287    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22934866 |
| Train/Action_magnitu... | 0.51149434 |
| Train/Action_magnitude  | 0.3962664  |
| Train/Action_max        | 0.16839461 |
| Train/Action_std        | 0.1355764  |
| Train/Entropy           | -0.6115822 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2203403  |
| Train/Loss              | 0.12477595 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12023074 |
| Train/Ratio             | 0.9999849  |
| Train/Return            | 1.5899863  |
| Train/V                 | 1.7102165  |
| Train/Value             | 1.7102165  |
| Train/control_penalty   | 0.3933633  |
| Train/policy_loss       | 0.12023074 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 876 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 875         |
| Time/Actor_Time         | 0.0705      |
| Time/B_Format_Time      | 0.0694      |
| Time/B_Original_Form... | 0.0737      |
| Time/Buffer             | 0.00345     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23119581  |
| Train/Action_magnitu... | 0.52081215  |
| Train/Action_magnitude  | 0.4051463   |
| Train/Action_max        | 0.16492157  |
| Train/Action_std        | 0.14183359  |
| Train/Entropy           | -0.56667084 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1356599   |
| Train/Loss              | 0.09193607  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.087296076 |
| Train/Ratio             | 0.99999577  |
| Train/Return            | 1.8437396   |
| Train/V                 | 1.9310478   |
| Train/Value             | 1.9310478   |
| Train/control_penalty   | 0.4073323   |
| Train/policy_loss       | 0.087296076 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 877 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 876        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0709     |
| Time/B_Original_Form... | 0.0729     |
| Time/Buffer             | 0.00312    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23302838 |
| Train/Action_magnitu... | 0.5258747  |
| Train/Action_magnitude  | 0.4091847  |
| Train/Action_max        | 0.18728757 |
| Train/Action_std        | 0.1419752  |
| Train/Entropy           | -0.5642038 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.126644   |
| Train/Loss              | 0.1468546  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14222172 |
| Train/Ratio             | 1.0000038  |
| Train/Return            | 1.618973   |
| Train/V                 | 1.7611954  |
| Train/Value             | 1.7611954  |
| Train/control_penalty   | 0.40686718 |
| Train/policy_loss       | 0.14222172 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 878 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 877        |
| Time/Actor_Time         | 0.072      |
| Time/B_Format_Time      | 0.0699     |
| Time/B_Original_Form... | 0.073      |
| Time/Buffer             | 0.00303    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22008093 |
| Train/Action_magnitu... | 0.495689   |
| Train/Action_magnitude  | 0.38387263 |
| Train/Action_max        | 0.18460697 |
| Train/Action_std        | 0.13439535 |
| Train/Entropy           | -0.622418  |
| Train/Entropy_Loss      | 0.000622   |
| Train/Entropy_loss      | 0.000622   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2162575  |
| Train/Loss              | 0.12456641 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12009633 |
| Train/Ratio             | 0.99999    |
| Train/Return            | 1.4078528  |
| Train/V                 | 1.5279498  |
| Train/Value             | 1.5279498  |
| Train/control_penalty   | 0.3847658  |
| Train/policy_loss       | 0.12009633 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.021      |
----------------------------------------

 ---------------- Iteration 879 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 878         |
| Time/Actor_Time         | 0.0741      |
| Time/B_Format_Time      | 0.0722      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00263     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23456237  |
| Train/Action_magnitu... | 0.5238002   |
| Train/Action_magnitude  | 0.4060604   |
| Train/Action_max        | 0.2117151   |
| Train/Action_std        | 0.1386862   |
| Train/Entropy           | -0.59262025 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1962451   |
| Train/Loss              | 0.19063851  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18601038  |
| Train/Ratio             | 1.0000149   |
| Train/Return            | 1.4457791   |
| Train/V                 | 1.6317929   |
| Train/Value             | 1.6317929   |
| Train/control_penalty   | 0.40355116  |
| Train/policy_loss       | 0.18601038  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01725     |
-----------------------------------------

 ---------------- Iteration 880 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 879         |
| Time/Actor_Time         | 0.0684      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.07        |
| Time/Buffer             | 0.00298     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22890775  |
| Train/Action_magnitu... | 0.5073585   |
| Train/Action_magnitude  | 0.39525577  |
| Train/Action_max        | 0.18156579  |
| Train/Action_std        | 0.133431    |
| Train/Entropy           | -0.62829876 |
| Train/Entropy_Loss      | 0.000628    |
| Train/Entropy_loss      | 0.000628    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2845247   |
| Train/Loss              | 0.23571953  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2311345   |
| Train/Ratio             | 1.0000043   |
| Train/Return            | 1.8095013   |
| Train/V                 | 2.040629    |
| Train/Value             | 2.040629    |
| Train/control_penalty   | 0.39567378  |
| Train/policy_loss       | 0.2311345   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01825     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 881 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 880         |
| Time/Actor_Time         | 0.0705      |
| Time/B_Format_Time      | 0.0698      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00243     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2279371   |
| Train/Action_magnitu... | 0.50934225  |
| Train/Action_magnitude  | 0.39935735  |
| Train/Action_max        | 0.14543469  |
| Train/Action_std        | 0.13418218  |
| Train/Entropy           | -0.61882746 |
| Train/Entropy_Loss      | 0.000619    |
| Train/Entropy_loss      | 0.000619    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2551233   |
| Train/Loss              | 0.35861936  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.35395172  |
| Train/Ratio             | 0.9999855   |
| Train/Return            | 2.1808834   |
| Train/V                 | 2.534831    |
| Train/Value             | 2.534831    |
| Train/control_penalty   | 0.4048824   |
| Train/policy_loss       | 0.35395172  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0175      |
-----------------------------------------

 ---------------- Iteration 882 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 881         |
| Time/Actor_Time         | 0.0689      |
| Time/B_Format_Time      | 0.0688      |
| Time/B_Original_Form... | 0.0704      |
| Time/Buffer             | 0.00325     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24081726  |
| Train/Action_magnitu... | 0.54295313  |
| Train/Action_magnitude  | 0.42310378  |
| Train/Action_max        | 0.18031403  |
| Train/Action_std        | 0.13887297  |
| Train/Entropy           | -0.58660865 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1624304   |
| Train/Loss              | 0.2661603   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.261394    |
| Train/Ratio             | 0.9999938   |
| Train/Return            | 2.0518599   |
| Train/V                 | 2.3132546   |
| Train/Value             | 2.3132546   |
| Train/control_penalty   | 0.41797268  |
| Train/policy_loss       | 0.261394    |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.026       |
-----------------------------------------

 ---------------- Iteration 883 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 882         |
| Time/Actor_Time         | 0.0707      |
| Time/B_Format_Time      | 0.0698      |
| Time/B_Original_Form... | 0.0708      |
| Time/Buffer             | 0.00328     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24039899  |
| Train/Action_magnitu... | 0.5371683   |
| Train/Action_magnitude  | 0.41731405  |
| Train/Action_max        | 0.18027817  |
| Train/Action_std        | 0.13924114  |
| Train/Entropy           | -0.58498585 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1710173   |
| Train/Loss              | 0.24137872  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.23663116  |
| Train/Ratio             | 1.0000077   |
| Train/Return            | 1.9624503   |
| Train/V                 | 2.1990786   |
| Train/Value             | 2.1990786   |
| Train/control_penalty   | 0.41625774  |
| Train/policy_loss       | 0.23663116  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02325     |
-----------------------------------------

 ---------------- Iteration 884 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 883        |
| Time/Actor_Time         | 0.0716     |
| Time/B_Format_Time      | 0.0705     |
| Time/B_Original_Form... | 0.0707     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2415457  |
| Train/Action_magnitu... | 0.53792924 |
| Train/Action_magnitude  | 0.4210263  |
| Train/Action_max        | 0.16025628 |
| Train/Action_std        | 0.14108738 |
| Train/Entropy           | -0.5731545 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1495456  |
| Train/Loss              | 0.21796985 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21317066 |
| Train/Ratio             | 0.99999774 |
| Train/Return            | 2.0226424  |
| Train/V                 | 2.2358139  |
| Train/Value             | 2.2358139  |
| Train/control_penalty   | 0.4226024  |
| Train/policy_loss       | 0.21317066 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 885 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 884        |
| Time/Actor_Time         | 0.0719     |
| Time/B_Format_Time      | 0.0783     |
| Time/B_Original_Form... | 0.079      |
| Time/Buffer             | 0.00324    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24438791 |
| Train/Action_magnitu... | 0.5448691  |
| Train/Action_magnitude  | 0.42733714 |
| Train/Action_max        | 0.14567658 |
| Train/Action_std        | 0.14166002 |
| Train/Entropy           | -0.5655438 |
| Train/Entropy_Loss      | 0.000566   |
| Train/Entropy_loss      | 0.000566   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.148686   |
| Train/Loss              | 0.21968931 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2148222  |
| Train/Ratio             | 1.000012   |
| Train/Return            | 2.0552964  |
| Train/V                 | 2.2701235  |
| Train/Value             | 2.2701235  |
| Train/control_penalty   | 0.43015614 |
| Train/policy_loss       | 0.2148222  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02975    |
----------------------------------------

 ---------------- Iteration 886 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 885         |
| Time/Actor_Time         | 0.0738      |
| Time/B_Format_Time      | 0.0696      |
| Time/B_Original_Form... | 0.0713      |
| Time/Buffer             | 0.00389     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.2374469   |
| Train/Action_magnitu... | 0.52664685  |
| Train/Action_magnitude  | 0.40864593  |
| Train/Action_max        | 0.18123364  |
| Train/Action_std        | 0.13788624  |
| Train/Entropy           | -0.59443986 |
| Train/Entropy_Loss      | 0.000594    |
| Train/Entropy_loss      | 0.000594    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1856377   |
| Train/Loss              | 0.17944722  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17475335  |
| Train/Ratio             | 1.0000226   |
| Train/Return            | 1.5368265   |
| Train/V                 | 1.7115849   |
| Train/Value             | 1.7115849   |
| Train/control_penalty   | 0.40994224  |
| Train/policy_loss       | 0.17475335  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 887 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 886        |
| Time/Actor_Time         | 0.0728     |
| Time/B_Format_Time      | 0.0743     |
| Time/B_Original_Form... | 0.0722     |
| Time/Buffer             | 0.00314    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22710718 |
| Train/Action_magnitu... | 0.5093632  |
| Train/Action_magnitude  | 0.39583138 |
| Train/Action_max        | 0.1764316  |
| Train/Action_std        | 0.13637412 |
| Train/Entropy           | -0.6079168 |
| Train/Entropy_Loss      | 0.000608   |
| Train/Entropy_loss      | 0.000608   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2171687  |
| Train/Loss              | 0.15388218 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14935285 |
| Train/Ratio             | 1.0000117  |
| Train/Return            | 1.4421626  |
| Train/V                 | 1.5915058  |
| Train/Value             | 1.5915058  |
| Train/control_penalty   | 0.39214098 |
| Train/policy_loss       | 0.14935285 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 888 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 887         |
| Time/Actor_Time         | 0.079       |
| Time/B_Format_Time      | 0.0703      |
| Time/B_Original_Form... | 0.0718      |
| Time/Buffer             | 0.003       |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2352029   |
| Train/Action_magnitu... | 0.52126735  |
| Train/Action_magnitude  | 0.40570423  |
| Train/Action_max        | 0.180948    |
| Train/Action_std        | 0.13720267  |
| Train/Entropy           | -0.59945005 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2217652   |
| Train/Loss              | 0.12552398  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12088375  |
| Train/Ratio             | 1.0000179   |
| Train/Return            | 1.6853045   |
| Train/V                 | 1.8061842   |
| Train/Value             | 1.8061842   |
| Train/control_penalty   | 0.4040779   |
| Train/policy_loss       | 0.12088375  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 889 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 888        |
| Time/Actor_Time         | 0.0717     |
| Time/B_Format_Time      | 0.0691     |
| Time/B_Original_Form... | 0.0707     |
| Time/Buffer             | 0.00325    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23081477 |
| Train/Action_magnitu... | 0.5163034  |
| Train/Action_magnitude  | 0.40314972 |
| Train/Action_max        | 0.15661967 |
| Train/Action_std        | 0.13688484 |
| Train/Entropy           | -0.6000373 |
| Train/Entropy_Loss      | 0.0006     |
| Train/Entropy_loss      | 0.0006     |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1952333  |
| Train/Loss              | 0.18823634 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18359378 |
| Train/Ratio             | 0.9999892  |
| Train/Return            | 1.8593628  |
| Train/V                 | 2.0429606  |
| Train/Value             | 2.0429606  |
| Train/control_penalty   | 0.4042515  |
| Train/policy_loss       | 0.18359378 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 890 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 889         |
| Time/Actor_Time         | 0.0753      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0764      |
| Time/Buffer             | 0.00307     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23732379  |
| Train/Action_magnitu... | 0.52108717  |
| Train/Action_magnitude  | 0.40496895  |
| Train/Action_max        | 0.17513597  |
| Train/Action_std        | 0.13422756  |
| Train/Entropy           | -0.61602867 |
| Train/Entropy_Loss      | 0.000616    |
| Train/Entropy_loss      | 0.000616    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2259543   |
| Train/Loss              | 0.20759618  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20292561  |
| Train/Ratio             | 0.9999991   |
| Train/Return            | 1.8405253   |
| Train/V                 | 2.0434628   |
| Train/Value             | 2.0434628   |
| Train/control_penalty   | 0.4054552   |
| Train/policy_loss       | 0.20292561  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02125     |
-----------------------------------------

 ---------------- Iteration 891 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 890        |
| Time/Actor_Time         | 0.0764     |
| Time/B_Format_Time      | 0.076      |
| Time/B_Original_Form... | 0.0773     |
| Time/Buffer             | 0.00362    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24063352 |
| Train/Action_magnitu... | 0.5418926  |
| Train/Action_magnitude  | 0.4199605  |
| Train/Action_max        | 0.17615189 |
| Train/Action_std        | 0.14021534 |
| Train/Entropy           | -0.581496  |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1727914  |
| Train/Loss              | 0.30498508 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.30017948 |
| Train/Ratio             | 1.0000032  |
| Train/Return            | 1.9587222  |
| Train/V                 | 2.2589076  |
| Train/Value             | 2.2589076  |
| Train/control_penalty   | 0.42240793 |
| Train/policy_loss       | 0.30017948 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02175    |
----------------------------------------

 ---------------- Iteration 892 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 891         |
| Time/Actor_Time         | 0.0726      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00317     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22872491  |
| Train/Action_magnitu... | 0.5118862   |
| Train/Action_magnitude  | 0.39812693  |
| Train/Action_max        | 0.15736209  |
| Train/Action_std        | 0.13497849  |
| Train/Entropy           | -0.61434406 |
| Train/Entropy_Loss      | 0.000614    |
| Train/Entropy_loss      | 0.000614    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2308711   |
| Train/Loss              | 0.23373951  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.22913142  |
| Train/Ratio             | 1.000034    |
| Train/Return            | 2.2623184   |
| Train/V                 | 2.4914455   |
| Train/Value             | 2.4914455   |
| Train/control_penalty   | 0.39937517  |
| Train/policy_loss       | 0.22913142  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02925     |
-----------------------------------------

 ---------------- Iteration 893 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 892        |
| Time/Actor_Time         | 0.0744     |
| Time/B_Format_Time      | 0.0764     |
| Time/B_Original_Form... | 0.0769     |
| Time/Buffer             | 0.00376    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23398483 |
| Train/Action_magnitu... | 0.52630347 |
| Train/Action_magnitude  | 0.4102785  |
| Train/Action_max        | 0.17542405 |
| Train/Action_std        | 0.13928095 |
| Train/Entropy           | -0.5812197 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.184109   |
| Train/Loss              | 0.25639123 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2517114  |
| Train/Ratio             | 1.000007   |
| Train/Return            | 1.7128837  |
| Train/V                 | 1.9645753  |
| Train/Value             | 1.9645753  |
| Train/control_penalty   | 0.40986392 |
| Train/policy_loss       | 0.2517114  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 894 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 893         |
| Time/Actor_Time         | 0.0722      |
| Time/B_Format_Time      | 0.0717      |
| Time/B_Original_Form... | 0.0733      |
| Time/Buffer             | 0.00395     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23063834  |
| Train/Action_magnitu... | 0.5174545   |
| Train/Action_magnitude  | 0.40137762  |
| Train/Action_max        | 0.18166147  |
| Train/Action_std        | 0.13812494  |
| Train/Entropy           | -0.59535605 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1870598   |
| Train/Loss              | 0.07779006  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.07317175  |
| Train/Ratio             | 1.0000064   |
| Train/Return            | 1.8161113   |
| Train/V                 | 1.8892891   |
| Train/Value             | 1.8892891   |
| Train/control_penalty   | 0.40229577  |
| Train/policy_loss       | 0.07317175  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03425     |
-----------------------------------------

 ---------------- Iteration 895 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 894         |
| Time/Actor_Time         | 0.0739      |
| Time/B_Format_Time      | 0.0706      |
| Time/B_Original_Form... | 0.0709      |
| Time/Buffer             | 0.00622     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23282191  |
| Train/Action_magnitu... | 0.5157768   |
| Train/Action_magnitude  | 0.4001771   |
| Train/Action_max        | 0.1866182   |
| Train/Action_std        | 0.1338428   |
| Train/Entropy           | -0.62233454 |
| Train/Entropy_Loss      | 0.000622    |
| Train/Entropy_loss      | 0.000622    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2506984   |
| Train/Loss              | 0.16981515  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16516614  |
| Train/Ratio             | 1.000012    |
| Train/Return            | 1.4948866   |
| Train/V                 | 1.6600473   |
| Train/Value             | 1.6600473   |
| Train/control_penalty   | 0.40266752  |
| Train/policy_loss       | 0.16516614  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 896 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 895         |
| Time/Actor_Time         | 0.0737      |
| Time/B_Format_Time      | 0.0718      |
| Time/B_Original_Form... | 0.077       |
| Time/Buffer             | 0.00417     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24180341  |
| Train/Action_magnitu... | 0.53605604  |
| Train/Action_magnitude  | 0.41705647  |
| Train/Action_max        | 0.19131231  |
| Train/Action_std        | 0.13868518  |
| Train/Entropy           | -0.59058267 |
| Train/Entropy_Loss      | 0.000591    |
| Train/Entropy_loss      | 0.000591    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1786436   |
| Train/Loss              | 0.10670655  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1019676   |
| Train/Ratio             | 0.99999636  |
| Train/Return            | 1.8706228   |
| Train/V                 | 1.9725932   |
| Train/Value             | 1.9725932   |
| Train/control_penalty   | 0.41483614  |
| Train/policy_loss       | 0.1019676   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 897 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 896         |
| Time/Actor_Time         | 0.0735      |
| Time/B_Format_Time      | 0.0726      |
| Time/B_Original_Form... | 0.0752      |
| Time/Buffer             | 0.00432     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23290579  |
| Train/Action_magnitu... | 0.51868194  |
| Train/Action_magnitude  | 0.4026918   |
| Train/Action_max        | 0.18709658  |
| Train/Action_std        | 0.13921313  |
| Train/Entropy           | -0.58989835 |
| Train/Entropy_Loss      | 0.00059     |
| Train/Entropy_loss      | 0.00059     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2005328   |
| Train/Loss              | 0.1392074   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13457759  |
| Train/Ratio             | 0.99999446  |
| Train/Return            | 1.8292896   |
| Train/V                 | 1.9638665   |
| Train/Value             | 1.9638665   |
| Train/control_penalty   | 0.40399197  |
| Train/policy_loss       | 0.13457759  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.034       |
-----------------------------------------

 ---------------- Iteration 898 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 897         |
| Time/Actor_Time         | 0.0823      |
| Time/B_Format_Time      | 0.0846      |
| Time/B_Original_Form... | 0.0875      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22203206  |
| Train/Action_magnitu... | 0.49315953  |
| Train/Action_magnitude  | 0.38105792  |
| Train/Action_max        | 0.17753015  |
| Train/Action_std        | 0.1326461   |
| Train/Entropy           | -0.63712883 |
| Train/Entropy_Loss      | 0.000637    |
| Train/Entropy_loss      | 0.000637    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2819483   |
| Train/Loss              | 0.17186229  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16739757  |
| Train/Ratio             | 1.0000027   |
| Train/Return            | 1.8706975   |
| Train/V                 | 2.0381      |
| Train/Value             | 2.0381      |
| Train/control_penalty   | 0.38275898  |
| Train/policy_loss       | 0.16739757  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 899 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 898        |
| Time/Actor_Time         | 0.0834     |
| Time/B_Format_Time      | 0.0864     |
| Time/B_Original_Form... | 0.0844     |
| Time/Buffer             | 0.0036     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22848736 |
| Train/Action_magnitu... | 0.50880325 |
| Train/Action_magnitude  | 0.39072064 |
| Train/Action_max        | 0.18936114 |
| Train/Action_std        | 0.13436787 |
| Train/Entropy           | -0.6205426 |
| Train/Entropy_Loss      | 0.000621   |
| Train/Entropy_loss      | 0.000621   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2571125  |
| Train/Loss              | 0.20579249 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20128344 |
| Train/Ratio             | 1.0000135  |
| Train/Return            | 1.6925707  |
| Train/V                 | 1.8938545  |
| Train/Value             | 1.8938545  |
| Train/control_penalty   | 0.38885018 |
| Train/policy_loss       | 0.20128344 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02325    |
----------------------------------------

 ---------------- Iteration 900 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 899         |
| Time/Actor_Time         | 0.0763      |
| Time/B_Format_Time      | 0.0749      |
| Time/B_Original_Form... | 0.0749      |
| Time/Buffer             | 0.00461     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22330174  |
| Train/Action_magnitu... | 0.50338215  |
| Train/Action_magnitude  | 0.3894493   |
| Train/Action_max        | 0.1904011   |
| Train/Action_std        | 0.13604179  |
| Train/Entropy           | -0.60864913 |
| Train/Entropy_Loss      | 0.000609    |
| Train/Entropy_loss      | 0.000609    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2119875   |
| Train/Loss              | 0.16892809  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16445324  |
| Train/Ratio             | 1.0000103   |
| Train/Return            | 1.60393     |
| Train/V                 | 1.7683835   |
| Train/Value             | 1.7683835   |
| Train/control_penalty   | 0.3866203   |
| Train/policy_loss       | 0.16445324  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 901 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 900        |
| Time/Actor_Time         | 0.0743     |
| Time/B_Format_Time      | 0.0774     |
| Time/B_Original_Form... | 0.0755     |
| Time/Buffer             | 0.00453    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24323994 |
| Train/Action_magnitu... | 0.53895295 |
| Train/Action_magnitude  | 0.41846445 |
| Train/Action_max        | 0.18691021 |
| Train/Action_std        | 0.13637365 |
| Train/Entropy           | -0.6041345 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1803703  |
| Train/Loss              | 0.17775397 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1729681  |
| Train/Ratio             | 0.9999872  |
| Train/Return            | 1.7711535  |
| Train/V                 | 1.944127   |
| Train/Value             | 1.944127   |
| Train/control_penalty   | 0.41817263 |
| Train/policy_loss       | 0.1729681  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 902 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 901        |
| Time/Actor_Time         | 0.0751     |
| Time/B_Format_Time      | 0.0701     |
| Time/B_Original_Form... | 0.0719     |
| Time/Buffer             | 0.00327    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23617858 |
| Train/Action_magnitu... | 0.5345464  |
| Train/Action_magnitude  | 0.41648483 |
| Train/Action_max        | 0.19214594 |
| Train/Action_std        | 0.14206304 |
| Train/Entropy           | -0.561103  |
| Train/Entropy_Loss      | 0.000561   |
| Train/Entropy_loss      | 0.000561   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1236446  |
| Train/Loss              | 0.14260039 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13790557 |
| Train/Ratio             | 1.0000232  |
| Train/Return            | 1.6095793  |
| Train/V                 | 1.747484   |
| Train/Value             | 1.747484   |
| Train/control_penalty   | 0.4133719  |
| Train/policy_loss       | 0.13790557 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 903 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 902        |
| Time/Actor_Time         | 0.079      |
| Time/B_Format_Time      | 0.0777     |
| Time/B_Original_Form... | 0.0765     |
| Time/Buffer             | 0.00431    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24173644 |
| Train/Action_magnitu... | 0.5375522  |
| Train/Action_magnitude  | 0.41632697 |
| Train/Action_max        | 0.20365466 |
| Train/Action_std        | 0.13561976 |
| Train/Entropy           | -0.6117634 |
| Train/Entropy_Loss      | 0.000612   |
| Train/Entropy_loss      | 0.000612   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2133577  |
| Train/Loss              | 0.10993019 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10518751 |
| Train/Ratio             | 1.0000122  |
| Train/Return            | 1.6979227  |
| Train/V                 | 1.8031126  |
| Train/Value             | 1.8031126  |
| Train/control_penalty   | 0.41309124 |
| Train/policy_loss       | 0.10518751 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02825    |
----------------------------------------

 ---------------- Iteration 904 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 903         |
| Time/Actor_Time         | 0.0745      |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.0738      |
| Time/Buffer             | 0.00287     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24002132  |
| Train/Action_magnitu... | 0.53779465  |
| Train/Action_magnitude  | 0.41690785  |
| Train/Action_max        | 0.20112164  |
| Train/Action_std        | 0.13885564  |
| Train/Entropy           | -0.5856654  |
| Train/Entropy_Loss      | 0.000586    |
| Train/Entropy_loss      | 0.000586    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1567131   |
| Train/Loss              | 0.1266565   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.121926196 |
| Train/Ratio             | 0.9999923   |
| Train/Return            | 1.5838171   |
| Train/V                 | 1.7057359   |
| Train/Value             | 1.7057359   |
| Train/control_penalty   | 0.41446373  |
| Train/policy_loss       | 0.121926196 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02075     |
-----------------------------------------

 ---------------- Iteration 905 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 904        |
| Time/Actor_Time         | 0.0757     |
| Time/B_Format_Time      | 0.0792     |
| Time/B_Original_Form... | 0.0795     |
| Time/Buffer             | 0.00473    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25025618 |
| Train/Action_magnitu... | 0.55331206 |
| Train/Action_magnitude  | 0.42921412 |
| Train/Action_max        | 0.2084378  |
| Train/Action_std        | 0.137966   |
| Train/Entropy           | -0.5937537 |
| Train/Entropy_Loss      | 0.000594   |
| Train/Entropy_loss      | 0.000594   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1974713  |
| Train/Loss              | 0.10981239 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10496317 |
| Train/Ratio             | 0.99999315 |
| Train/Return            | 1.7906494  |
| Train/V                 | 1.8956127  |
| Train/Value             | 1.8956127  |
| Train/control_penalty   | 0.4255465  |
| Train/policy_loss       | 0.10496317 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03425    |
----------------------------------------

 ---------------- Iteration 906 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 905        |
| Time/Actor_Time         | 0.0756     |
| Time/B_Format_Time      | 0.079      |
| Time/B_Original_Form... | 0.0797     |
| Time/Buffer             | 0.00388    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.25073272 |
| Train/Action_magnitu... | 0.55692613 |
| Train/Action_magnitude  | 0.43395612 |
| Train/Action_max        | 0.19172718 |
| Train/Action_std        | 0.14158    |
| Train/Entropy           | -0.5668906 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1110383  |
| Train/Loss              | 0.12139488 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11653244 |
| Train/Ratio             | 1.0000322  |
| Train/Return            | 1.761053   |
| Train/V                 | 1.877572   |
| Train/Value             | 1.877572   |
| Train/control_penalty   | 0.42955494 |
| Train/policy_loss       | 0.11653244 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.033      |
----------------------------------------

 ---------------- Iteration 907 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 906        |
| Time/Actor_Time         | 0.0728     |
| Time/B_Format_Time      | 0.0722     |
| Time/B_Original_Form... | 0.0718     |
| Time/Buffer             | 0.00468    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24602048 |
| Train/Action_magnitu... | 0.54478043 |
| Train/Action_magnitude  | 0.42557868 |
| Train/Action_max        | 0.19237356 |
| Train/Action_std        | 0.14121486 |
| Train/Entropy           | -0.5706533 |
| Train/Entropy_Loss      | 0.000571   |
| Train/Entropy_loss      | 0.000571   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1323423  |
| Train/Loss              | 0.08748881 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08266816 |
| Train/Ratio             | 1.0000162  |
| Train/Return            | 1.9723997  |
| Train/V                 | 2.0550632  |
| Train/Value             | 2.0550632  |
| Train/control_penalty   | 0.42499888 |
| Train/policy_loss       | 0.08266816 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.041      |
----------------------------------------

 ---------------- Iteration 908 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 907         |
| Time/Actor_Time         | 0.0764      |
| Time/B_Format_Time      | 0.076       |
| Time/B_Original_Form... | 0.0764      |
| Time/Buffer             | 0.00465     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24395865  |
| Train/Action_magnitu... | 0.53782386  |
| Train/Action_magnitude  | 0.41759163  |
| Train/Action_max        | 0.18489502  |
| Train/Action_std        | 0.13842197  |
| Train/Entropy           | -0.59020543 |
| Train/Entropy_Loss      | 0.00059     |
| Train/Entropy_loss      | 0.00059     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1851022   |
| Train/Loss              | 0.09710055  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.092326224 |
| Train/Ratio             | 0.9999632   |
| Train/Return            | 1.8318532   |
| Train/V                 | 1.9241868   |
| Train/Value             | 1.9241868   |
| Train/control_penalty   | 0.418412    |
| Train/policy_loss       | 0.092326224 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0365      |
-----------------------------------------

 ---------------- Iteration 909 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 908        |
| Time/Actor_Time         | 0.0716     |
| Time/B_Format_Time      | 0.0712     |
| Time/B_Original_Form... | 0.0754     |
| Time/Buffer             | 0.00419    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24224396 |
| Train/Action_magnitu... | 0.53928334 |
| Train/Action_magnitude  | 0.41528645 |
| Train/Action_max        | 0.19721973 |
| Train/Action_std        | 0.14049518 |
| Train/Entropy           | -0.57729   |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1848415  |
| Train/Loss              | 0.13185842 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12718245 |
| Train/Ratio             | 0.9999847  |
| Train/Return            | 1.7819315  |
| Train/V                 | 1.9091231  |
| Train/Value             | 1.9091231  |
| Train/control_penalty   | 0.40986842 |
| Train/policy_loss       | 0.12718245 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0345     |
----------------------------------------

 ---------------- Iteration 910 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 909        |
| Time/Actor_Time         | 0.0726     |
| Time/B_Format_Time      | 0.0758     |
| Time/B_Original_Form... | 0.0764     |
| Time/Buffer             | 0.00462    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24969849 |
| Train/Action_magnitu... | 0.5501758  |
| Train/Action_magnitude  | 0.42528948 |
| Train/Action_max        | 0.22022085 |
| Train/Action_std        | 0.13809986 |
| Train/Entropy           | -0.5920286 |
| Train/Entropy_Loss      | 0.000592   |
| Train/Entropy_loss      | 0.000592   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1723684  |
| Train/Loss              | 0.12143617 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11660672 |
| Train/Ratio             | 0.9999819  |
| Train/Return            | 1.5816033  |
| Train/V                 | 1.6982198  |
| Train/Value             | 1.6982198  |
| Train/control_penalty   | 0.42374212 |
| Train/policy_loss       | 0.11660672 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 911 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 910        |
| Time/Actor_Time         | 0.0752     |
| Time/B_Format_Time      | 0.074      |
| Time/B_Original_Form... | 0.0793     |
| Time/Buffer             | 0.00449    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.25320458 |
| Train/Action_magnitu... | 0.5596185  |
| Train/Action_magnitude  | 0.4355572  |
| Train/Action_max        | 0.22127609 |
| Train/Action_std        | 0.14034353 |
| Train/Entropy           | -0.5744701 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1525627  |
| Train/Loss              | 0.12826481 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12335575 |
| Train/Ratio             | 1.0000327  |
| Train/Return            | 1.725897   |
| Train/V                 | 1.8492413  |
| Train/Value             | 1.8492413  |
| Train/control_penalty   | 0.4334591  |
| Train/policy_loss       | 0.12335575 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.031      |
----------------------------------------

 ---------------- Iteration 912 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 911         |
| Time/Actor_Time         | 0.073       |
| Time/B_Format_Time      | 0.0745      |
| Time/B_Original_Form... | 0.0739      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23834455  |
| Train/Action_magnitu... | 0.52906847  |
| Train/Action_magnitude  | 0.41112968  |
| Train/Action_max        | 0.20668669  |
| Train/Action_std        | 0.1388509   |
| Train/Entropy           | -0.59234333 |
| Train/Entropy_Loss      | 0.000592    |
| Train/Entropy_loss      | 0.000592    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1874152   |
| Train/Loss              | 0.13418712  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12946418  |
| Train/Ratio             | 1.0000082   |
| Train/Return            | 1.5702481   |
| Train/V                 | 1.6997129   |
| Train/Value             | 1.6997129   |
| Train/control_penalty   | 0.41305977  |
| Train/policy_loss       | 0.12946418  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0235      |
-----------------------------------------

 ---------------- Iteration 913 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 912         |
| Time/Actor_Time         | 0.077       |
| Time/B_Format_Time      | 0.0766      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00347     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23612213  |
| Train/Action_magnitu... | 0.52886826  |
| Train/Action_magnitude  | 0.41109473  |
| Train/Action_max        | 0.19541879  |
| Train/Action_std        | 0.1370354   |
| Train/Entropy           | -0.60085464 |
| Train/Entropy_Loss      | 0.000601    |
| Train/Entropy_loss      | 0.000601    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1844143   |
| Train/Loss              | 0.13176739  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12708853  |
| Train/Ratio             | 1.000015    |
| Train/Return            | 1.5741476   |
| Train/V                 | 1.7012261   |
| Train/Value             | 1.7012261   |
| Train/control_penalty   | 0.40779993  |
| Train/policy_loss       | 0.12708853  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 914 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 913        |
| Time/Actor_Time         | 0.072      |
| Time/B_Format_Time      | 0.0709     |
| Time/B_Original_Form... | 0.0709     |
| Time/Buffer             | 0.00359    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2448037  |
| Train/Action_magnitu... | 0.5364935  |
| Train/Action_magnitude  | 0.41570574 |
| Train/Action_max        | 0.20234033 |
| Train/Action_std        | 0.13678655 |
| Train/Entropy           | -0.6040196 |
| Train/Entropy_Loss      | 0.000604   |
| Train/Entropy_loss      | 0.000604   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1960895  |
| Train/Loss              | 0.13604715 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1312894  |
| Train/Ratio             | 0.9999894  |
| Train/Return            | 1.4538507  |
| Train/V                 | 1.5851427  |
| Train/Value             | 1.5851427  |
| Train/control_penalty   | 0.41537368 |
| Train/policy_loss       | 0.1312894  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 915 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 914         |
| Time/Actor_Time         | 0.0708      |
| Time/B_Format_Time      | 0.0732      |
| Time/B_Original_Form... | 0.0755      |
| Time/Buffer             | 0.00417     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2425541   |
| Train/Action_magnitu... | 0.535862    |
| Train/Action_magnitude  | 0.41757214  |
| Train/Action_max        | 0.19131327  |
| Train/Action_std        | 0.13648161  |
| Train/Entropy           | -0.6055677  |
| Train/Entropy_Loss      | 0.000606    |
| Train/Entropy_loss      | 0.000606    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2078264   |
| Train/Loss              | 0.1204269   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.115664475 |
| Train/Ratio             | 1.0000126   |
| Train/Return            | 1.6038404   |
| Train/V                 | 1.7195011   |
| Train/Value             | 1.7195011   |
| Train/control_penalty   | 0.415686    |
| Train/policy_loss       | 0.115664475 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 916 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 915        |
| Time/Actor_Time         | 0.0715     |
| Time/B_Format_Time      | 0.0696     |
| Time/B_Original_Form... | 0.0727     |
| Time/Buffer             | 0.0066     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24793075 |
| Train/Action_magnitu... | 0.54741967 |
| Train/Action_magnitude  | 0.42663068 |
| Train/Action_max        | 0.19194332 |
| Train/Action_std        | 0.1405245  |
| Train/Entropy           | -0.5791849 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1655405  |
| Train/Loss              | 0.0817069  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.07682866 |
| Train/Ratio             | 0.99998355 |
| Train/Return            | 1.6843095  |
| Train/V                 | 1.7611428  |
| Train/Value             | 1.7611428  |
| Train/control_penalty   | 0.42990524 |
| Train/policy_loss       | 0.07682866 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0355     |
----------------------------------------

 ---------------- Iteration 917 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 916         |
| Time/Actor_Time         | 0.078       |
| Time/B_Format_Time      | 0.0773      |
| Time/B_Original_Form... | 0.0784      |
| Time/Buffer             | 0.00489     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24360374  |
| Train/Action_magnitu... | 0.5415791   |
| Train/Action_magnitude  | 0.42085594  |
| Train/Action_max        | 0.1847139   |
| Train/Action_std        | 0.13950962  |
| Train/Entropy           | -0.58353597 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1553513   |
| Train/Loss              | 0.071691796 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0669384   |
| Train/Ratio             | 0.99998     |
| Train/Return            | 1.488653    |
| Train/V                 | 1.5555942   |
| Train/Value             | 1.5555942   |
| Train/control_penalty   | 0.41698593  |
| Train/policy_loss       | 0.0669384   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 918 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 917        |
| Time/Actor_Time         | 0.0724     |
| Time/B_Format_Time      | 0.0772     |
| Time/B_Original_Form... | 0.082      |
| Time/Buffer             | 0.00422    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23971581 |
| Train/Action_magnitu... | 0.533541   |
| Train/Action_magnitude  | 0.4155571  |
| Train/Action_max        | 0.19972567 |
| Train/Action_std        | 0.13918792 |
| Train/Entropy           | -0.584094  |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1902113  |
| Train/Loss              | 0.11174135 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10700615 |
| Train/Ratio             | 0.9999941  |
| Train/Return            | 1.4374211  |
| Train/V                 | 1.5444238  |
| Train/Value             | 1.5444238  |
| Train/control_penalty   | 0.4151105  |
| Train/policy_loss       | 0.10700615 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02725    |
----------------------------------------

 ---------------- Iteration 919 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 918         |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0762      |
| Time/Buffer             | 0.00381     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23358111  |
| Train/Action_magnitu... | 0.5209933   |
| Train/Action_magnitude  | 0.40476778  |
| Train/Action_max        | 0.19944166  |
| Train/Action_std        | 0.14121099  |
| Train/Entropy           | -0.57360333 |
| Train/Entropy_Loss      | 0.000574    |
| Train/Entropy_loss      | 0.000574    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1578838   |
| Train/Loss              | 0.120275415 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11563355  |
| Train/Ratio             | 0.9999873   |
| Train/Return            | 1.4288255   |
| Train/V                 | 1.5444651   |
| Train/Value             | 1.5444651   |
| Train/control_penalty   | 0.40682608  |
| Train/policy_loss       | 0.11563355  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 920 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 919         |
| Time/Actor_Time         | 0.0737      |
| Time/B_Format_Time      | 0.074       |
| Time/B_Original_Form... | 0.0722      |
| Time/Buffer             | 0.00419     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24065144  |
| Train/Action_magnitu... | 0.5322848   |
| Train/Action_magnitude  | 0.41345307  |
| Train/Action_max        | 0.19385189  |
| Train/Action_std        | 0.14038908  |
| Train/Entropy           | -0.57833254 |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1728518   |
| Train/Loss              | 0.11783631  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.113101915 |
| Train/Ratio             | 0.9999903   |
| Train/Return            | 1.5398862   |
| Train/V                 | 1.6529939   |
| Train/Value             | 1.6529939   |
| Train/control_penalty   | 0.4156071   |
| Train/policy_loss       | 0.113101915 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 921 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 920        |
| Time/Actor_Time         | 0.0731     |
| Time/B_Format_Time      | 0.0778     |
| Time/B_Original_Form... | 0.0763     |
| Time/Buffer             | 0.00342    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23936325 |
| Train/Action_magnitu... | 0.5440908  |
| Train/Action_magnitude  | 0.42594784 |
| Train/Action_max        | 0.20444098 |
| Train/Action_std        | 0.14580855 |
| Train/Entropy           | -0.5428198 |
| Train/Entropy_Loss      | 0.000543   |
| Train/Entropy_loss      | 0.000543   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0739748  |
| Train/Loss              | 0.12013286 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11532686 |
| Train/Ratio             | 0.99998987 |
| Train/Return            | 1.6535299  |
| Train/V                 | 1.7688686  |
| Train/Value             | 1.7688686  |
| Train/control_penalty   | 0.4263184  |
| Train/policy_loss       | 0.11532686 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02925    |
----------------------------------------

 ---------------- Iteration 922 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 921        |
| Time/Actor_Time         | 0.0714     |
| Time/B_Format_Time      | 0.0683     |
| Time/B_Original_Form... | 0.0705     |
| Time/Buffer             | 0.00374    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.2369866  |
| Train/Action_magnitu... | 0.5327586  |
| Train/Action_magnitude  | 0.413608   |
| Train/Action_max        | 0.18770316 |
| Train/Action_std        | 0.13999552 |
| Train/Entropy           | -0.5839671 |
| Train/Entropy_Loss      | 0.000584   |
| Train/Entropy_loss      | 0.000584   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1776422  |
| Train/Loss              | 0.11959708 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11489536 |
| Train/Ratio             | 1.000004   |
| Train/Return            | 1.5338823  |
| Train/V                 | 1.6487772  |
| Train/Value             | 1.6487772  |
| Train/control_penalty   | 0.41177532 |
| Train/policy_loss       | 0.11489536 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 923 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 922         |
| Time/Actor_Time         | 0.0738      |
| Time/B_Format_Time      | 0.0757      |
| Time/B_Original_Form... | 0.0788      |
| Time/Buffer             | 0.00401     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23956704  |
| Train/Action_magnitu... | 0.54327846  |
| Train/Action_magnitude  | 0.42429385  |
| Train/Action_max        | 0.19842564  |
| Train/Action_std        | 0.14458211  |
| Train/Entropy           | -0.55147064 |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0908148   |
| Train/Loss              | 0.12320457  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11842297  |
| Train/Ratio             | 1.0000077   |
| Train/Return            | 1.4115962   |
| Train/V                 | 1.5300049   |
| Train/Value             | 1.5300049   |
| Train/control_penalty   | 0.42301285  |
| Train/policy_loss       | 0.11842297  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02475     |
-----------------------------------------

 ---------------- Iteration 924 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 923         |
| Time/Actor_Time         | 0.0726      |
| Time/B_Format_Time      | 0.0748      |
| Time/B_Original_Form... | 0.0747      |
| Time/Buffer             | 0.00391     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2331118   |
| Train/Action_magnitu... | 0.5222952   |
| Train/Action_magnitude  | 0.40789625  |
| Train/Action_max        | 0.18926927  |
| Train/Action_std        | 0.13935894  |
| Train/Entropy           | -0.58882684 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1785591   |
| Train/Loss              | 0.14267215  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13799022  |
| Train/Ratio             | 1.0000044   |
| Train/Return            | 1.3108909   |
| Train/V                 | 1.4488764   |
| Train/Value             | 1.4488764   |
| Train/control_penalty   | 0.40931043  |
| Train/policy_loss       | 0.13799022  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02125     |
-----------------------------------------

 ---------------- Iteration 925 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 924         |
| Time/Actor_Time         | 0.0743      |
| Time/B_Format_Time      | 0.0789      |
| Time/B_Original_Form... | 0.0782      |
| Time/Buffer             | 0.00355     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22660068  |
| Train/Action_magnitu... | 0.50579655  |
| Train/Action_magnitude  | 0.39256617  |
| Train/Action_max        | 0.19988523  |
| Train/Action_std        | 0.13857584  |
| Train/Entropy           | -0.598771   |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1961749   |
| Train/Loss              | 0.119457364 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11491726  |
| Train/Ratio             | 0.9999887   |
| Train/Return            | 1.4484402   |
| Train/V                 | 1.5633568   |
| Train/Value             | 1.5633568   |
| Train/control_penalty   | 0.39413306  |
| Train/policy_loss       | 0.11491726  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 926 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 925        |
| Time/Actor_Time         | 0.0706     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.074      |
| Time/Buffer             | 0.00424    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2318917  |
| Train/Action_magnitu... | 0.5188629  |
| Train/Action_magnitude  | 0.4025674  |
| Train/Action_max        | 0.18468402 |
| Train/Action_std        | 0.13694502 |
| Train/Entropy           | -0.6030716 |
| Train/Entropy_Loss      | 0.000603   |
| Train/Entropy_loss      | 0.000603   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1867095  |
| Train/Loss              | 0.14770007 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14309283 |
| Train/Ratio             | 1.0000081  |
| Train/Return            | 1.4700798  |
| Train/V                 | 1.6131779  |
| Train/Value             | 1.6131779  |
| Train/control_penalty   | 0.40041822 |
| Train/policy_loss       | 0.14309283 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0215     |
----------------------------------------

 ---------------- Iteration 927 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 926        |
| Time/Actor_Time         | 0.0705     |
| Time/B_Format_Time      | 0.0764     |
| Time/B_Original_Form... | 0.0774     |
| Time/Buffer             | 0.00405    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23785748 |
| Train/Action_magnitu... | 0.5316435  |
| Train/Action_magnitude  | 0.4139088  |
| Train/Action_max        | 0.21337235 |
| Train/Action_std        | 0.14201769 |
| Train/Entropy           | -0.5694386 |
| Train/Entropy_Loss      | 0.000569   |
| Train/Entropy_loss      | 0.000569   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1489002  |
| Train/Loss              | 0.16559254 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.16087599 |
| Train/Ratio             | 1.0000267  |
| Train/Return            | 1.534165   |
| Train/V                 | 1.695033   |
| Train/Value             | 1.695033   |
| Train/control_penalty   | 0.41471124 |
| Train/policy_loss       | 0.16087599 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 928 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 927         |
| Time/Actor_Time         | 0.0749      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0756      |
| Time/Buffer             | 0.00314     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22697659  |
| Train/Action_magnitu... | 0.51152194  |
| Train/Action_magnitude  | 0.39701533  |
| Train/Action_max        | 0.20105915  |
| Train/Action_std        | 0.13765639  |
| Train/Entropy           | -0.59947777 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1980772   |
| Train/Loss              | 0.14032875  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13576418  |
| Train/Ratio             | 1.0000032   |
| Train/Return            | 1.2647649   |
| Train/V                 | 1.4005351   |
| Train/Value             | 1.4005351   |
| Train/control_penalty   | 0.39650998  |
| Train/policy_loss       | 0.13576418  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.017       |
-----------------------------------------

 ---------------- Iteration 929 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 928        |
| Time/Actor_Time         | 0.0733     |
| Time/B_Format_Time      | 0.0751     |
| Time/B_Original_Form... | 0.076      |
| Time/Buffer             | 0.00342    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23207715 |
| Train/Action_magnitu... | 0.5151232  |
| Train/Action_magnitude  | 0.39910477 |
| Train/Action_max        | 0.18844731 |
| Train/Action_std        | 0.13508314 |
| Train/Entropy           | -0.617842  |
| Train/Entropy_Loss      | 0.000618   |
| Train/Entropy_loss      | 0.000618   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2633886  |
| Train/Loss              | 0.15611878 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15152867 |
| Train/Ratio             | 0.99999374 |
| Train/Return            | 1.4535103  |
| Train/V                 | 1.6050442  |
| Train/Value             | 1.6050442  |
| Train/control_penalty   | 0.39722562 |
| Train/policy_loss       | 0.15152867 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02175    |
----------------------------------------

 ---------------- Iteration 930 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 929        |
| Time/Actor_Time         | 0.0764     |
| Time/B_Format_Time      | 0.0797     |
| Time/B_Original_Form... | 0.082      |
| Time/Buffer             | 0.00342    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23179157 |
| Train/Action_magnitu... | 0.51156265 |
| Train/Action_magnitude  | 0.39848855 |
| Train/Action_max        | 0.20802899 |
| Train/Action_std        | 0.13810019 |
| Train/Entropy           | -0.5964208 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2064333  |
| Train/Loss              | 0.1488756  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14428143 |
| Train/Ratio             | 1.000032   |
| Train/Return            | 1.55211    |
| Train/V                 | 1.6963844  |
| Train/Value             | 1.6963844  |
| Train/control_penalty   | 0.39977422 |
| Train/policy_loss       | 0.14428143 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------

 ---------------- Iteration 931 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 930        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0756     |
| Time/B_Original_Form... | 0.0776     |
| Time/Buffer             | 0.00359    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24085861 |
| Train/Action_magnitu... | 0.53248864 |
| Train/Action_magnitude  | 0.41487357 |
| Train/Action_max        | 0.18888095 |
| Train/Action_std        | 0.140922   |
| Train/Entropy           | -0.5782725 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1604475  |
| Train/Loss              | 0.18660694 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1818085  |
| Train/Ratio             | 0.9999806  |
| Train/Return            | 1.7431328  |
| Train/V                 | 1.9249504  |
| Train/Value             | 1.9249504  |
| Train/control_penalty   | 0.42201668 |
| Train/policy_loss       | 0.1818085  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 932 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 931         |
| Time/Actor_Time         | 0.072       |
| Time/B_Format_Time      | 0.0746      |
| Time/B_Original_Form... | 0.0734      |
| Time/Buffer             | 0.0034      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2280653   |
| Train/Action_magnitu... | 0.5100662   |
| Train/Action_magnitude  | 0.39666727  |
| Train/Action_max        | 0.18590194  |
| Train/Action_std        | 0.13371141  |
| Train/Entropy           | -0.62750155 |
| Train/Entropy_Loss      | 0.000628    |
| Train/Entropy_loss      | 0.000628    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2483354   |
| Train/Loss              | 0.12097594  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11638854  |
| Train/Ratio             | 0.99999464  |
| Train/Return            | 1.6525521   |
| Train/V                 | 1.7689439   |
| Train/Value             | 1.7689439   |
| Train/control_penalty   | 0.39599004  |
| Train/policy_loss       | 0.11638854  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 933 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 932        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0702     |
| Time/B_Original_Form... | 0.0722     |
| Time/Buffer             | 0.00345    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.22955838 |
| Train/Action_magnitu... | 0.51499414 |
| Train/Action_magnitude  | 0.4002335  |
| Train/Action_max        | 0.19499905 |
| Train/Action_std        | 0.13484353 |
| Train/Entropy           | -0.6206254 |
| Train/Entropy_Loss      | 0.000621   |
| Train/Entropy_loss      | 0.000621   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2406782  |
| Train/Loss              | 0.14415544 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13958117 |
| Train/Ratio             | 1.0000137  |
| Train/Return            | 1.7310021  |
| Train/V                 | 1.8705806  |
| Train/Value             | 1.8705806  |
| Train/control_penalty   | 0.39536485 |
| Train/policy_loss       | 0.13958117 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.025      |
----------------------------------------

 ---------------- Iteration 934 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 933         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0738      |
| Time/B_Original_Form... | 0.0756      |
| Time/Buffer             | 0.00387     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22825643  |
| Train/Action_magnitu... | 0.5078879   |
| Train/Action_magnitude  | 0.39393073  |
| Train/Action_max        | 0.19088072  |
| Train/Action_std        | 0.13296068  |
| Train/Entropy           | -0.63409895 |
| Train/Entropy_Loss      | 0.000634    |
| Train/Entropy_loss      | 0.000634    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2657776   |
| Train/Loss              | 0.22444175  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21986996  |
| Train/Ratio             | 0.9999865   |
| Train/Return            | 1.6957024   |
| Train/V                 | 1.9155737   |
| Train/Value             | 1.9155737   |
| Train/control_penalty   | 0.39376867  |
| Train/policy_loss       | 0.21986996  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01675     |
-----------------------------------------

 ---------------- Iteration 935 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 934         |
| Time/Actor_Time         | 0.0747      |
| Time/B_Format_Time      | 0.0804      |
| Time/B_Original_Form... | 0.0799      |
| Time/Buffer             | 0.00383     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22269632  |
| Train/Action_magnitu... | 0.5049596   |
| Train/Action_magnitude  | 0.39171547  |
| Train/Action_max        | 0.19424321  |
| Train/Action_std        | 0.13444005  |
| Train/Entropy           | -0.62600774 |
| Train/Entropy_Loss      | 0.000626    |
| Train/Entropy_loss      | 0.000626    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.243968    |
| Train/Loss              | 0.2569254   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2524146   |
| Train/Ratio             | 1.0000247   |
| Train/Return            | 1.6599839   |
| Train/V                 | 1.9123929   |
| Train/Value             | 1.9123929   |
| Train/control_penalty   | 0.38847965  |
| Train/policy_loss       | 0.2524146   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 936 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 935        |
| Time/Actor_Time         | 0.0734     |
| Time/B_Format_Time      | 0.0766     |
| Time/B_Original_Form... | 0.0749     |
| Time/Buffer             | 0.0046     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.21943429 |
| Train/Action_magnitu... | 0.49073645 |
| Train/Action_magnitude  | 0.38105127 |
| Train/Action_max        | 0.19030687 |
| Train/Action_std        | 0.13382663 |
| Train/Entropy           | -0.6294715 |
| Train/Entropy_Loss      | 0.000629   |
| Train/Entropy_loss      | 0.000629   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2669612  |
| Train/Loss              | 0.16336472 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15886092 |
| Train/Ratio             | 0.9999917  |
| Train/Return            | 1.6421548  |
| Train/V                 | 1.8010147  |
| Train/Value             | 1.8010147  |
| Train/control_penalty   | 0.38743335 |
| Train/policy_loss       | 0.15886092 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 937 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 936         |
| Time/Actor_Time         | 0.0752      |
| Time/B_Format_Time      | 0.0764      |
| Time/B_Original_Form... | 0.0788      |
| Time/Buffer             | 0.00334     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23513855  |
| Train/Action_magnitu... | 0.52176553  |
| Train/Action_magnitude  | 0.40535337  |
| Train/Action_max        | 0.21041025  |
| Train/Action_std        | 0.13708751  |
| Train/Entropy           | -0.60233474 |
| Train/Entropy_Loss      | 0.000602    |
| Train/Entropy_loss      | 0.000602    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2382061   |
| Train/Loss              | 0.21700259  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21230714  |
| Train/Ratio             | 0.99997455  |
| Train/Return            | 1.6351068   |
| Train/V                 | 1.8474326   |
| Train/Value             | 1.8474326   |
| Train/control_penalty   | 0.4093115   |
| Train/policy_loss       | 0.21230714  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 938 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 937        |
| Time/Actor_Time         | 0.0758     |
| Time/B_Format_Time      | 0.0749     |
| Time/B_Original_Form... | 0.0759     |
| Time/Buffer             | 0.00362    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23850699 |
| Train/Action_magnitu... | 0.5286359  |
| Train/Action_magnitude  | 0.4154663  |
| Train/Action_max        | 0.18691197 |
| Train/Action_std        | 0.14074197 |
| Train/Entropy           | -0.5749908 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1504443  |
| Train/Loss              | 0.13102046 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1262618  |
| Train/Ratio             | 1.0000033  |
| Train/Return            | 1.6261204  |
| Train/V                 | 1.7523724  |
| Train/Value             | 1.7523724  |
| Train/control_penalty   | 0.41836616 |
| Train/policy_loss       | 0.1262618  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 939 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 938        |
| Time/Actor_Time         | 0.0753     |
| Time/B_Format_Time      | 0.0716     |
| Time/B_Original_Form... | 0.0708     |
| Time/Buffer             | 0.00299    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23041876 |
| Train/Action_magnitu... | 0.5171782  |
| Train/Action_magnitude  | 0.40278488 |
| Train/Action_max        | 0.19022836 |
| Train/Action_std        | 0.13796121 |
| Train/Entropy           | -0.599955  |
| Train/Entropy_Loss      | 0.0006     |
| Train/Entropy_loss      | 0.0006     |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1996082  |
| Train/Loss              | 0.1352281  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1306339  |
| Train/Ratio             | 0.9999902  |
| Train/Return            | 1.6965213  |
| Train/V                 | 1.8271583  |
| Train/Value             | 1.8271583  |
| Train/control_penalty   | 0.3994246  |
| Train/policy_loss       | 0.1306339  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 940 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 939        |
| Time/Actor_Time         | 0.0742     |
| Time/B_Format_Time      | 0.0746     |
| Time/B_Original_Form... | 0.0744     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2344777  |
| Train/Action_magnitu... | 0.52320427 |
| Train/Action_magnitude  | 0.4086344  |
| Train/Action_max        | 0.19102938 |
| Train/Action_std        | 0.13770886 |
| Train/Entropy           | -0.5976937 |
| Train/Entropy_Loss      | 0.000598   |
| Train/Entropy_loss      | 0.000598   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1982819  |
| Train/Loss              | 0.20849718 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20380093 |
| Train/Ratio             | 0.9999897  |
| Train/Return            | 1.6632987  |
| Train/V                 | 1.8671045  |
| Train/Value             | 1.8671045  |
| Train/control_penalty   | 0.4098547  |
| Train/policy_loss       | 0.20380093 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02225    |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 941 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 940        |
| Time/Actor_Time         | 0.0762     |
| Time/B_Format_Time      | 0.0765     |
| Time/B_Original_Form... | 0.0771     |
| Time/Buffer             | 0.005      |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23647246 |
| Train/Action_magnitu... | 0.52788186 |
| Train/Action_magnitude  | 0.41041133 |
| Train/Action_max        | 0.19694683 |
| Train/Action_std        | 0.13782266 |
| Train/Entropy           | -0.594797  |
| Train/Entropy_Loss      | 0.000595   |
| Train/Entropy_loss      | 0.000595   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1633874  |
| Train/Loss              | 0.1263728  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12163003 |
| Train/Ratio             | 0.999981   |
| Train/Return            | 1.7703936  |
| Train/V                 | 1.8920287  |
| Train/Value             | 1.8920287  |
| Train/control_penalty   | 0.41479725 |
| Train/policy_loss       | 0.12163003 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 942 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 941         |
| Time/Actor_Time         | 0.0757      |
| Time/B_Format_Time      | 0.0788      |
| Time/B_Original_Form... | 0.081       |
| Time/Buffer             | 0.00472     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22877099  |
| Train/Action_magnitu... | 0.5146642   |
| Train/Action_magnitude  | 0.40141547  |
| Train/Action_max        | 0.189452    |
| Train/Action_std        | 0.13818109  |
| Train/Entropy           | -0.59515333 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1944205   |
| Train/Loss              | 0.12411227  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11947376  |
| Train/Ratio             | 0.99998933  |
| Train/Return            | 1.6702344   |
| Train/V                 | 1.7897156   |
| Train/Value             | 1.7897156   |
| Train/control_penalty   | 0.4043355   |
| Train/policy_loss       | 0.11947376  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02525     |
-----------------------------------------

 ---------------- Iteration 943 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 942         |
| Time/Actor_Time         | 0.0758      |
| Time/B_Format_Time      | 0.0727      |
| Time/B_Original_Form... | 0.0706      |
| Time/Buffer             | 0.0036      |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24071182  |
| Train/Action_magnitu... | 0.5424556   |
| Train/Action_magnitude  | 0.4241308   |
| Train/Action_max        | 0.18959127  |
| Train/Action_std        | 0.13866946  |
| Train/Entropy           | -0.58857054 |
| Train/Entropy_Loss      | 0.000589    |
| Train/Entropy_loss      | 0.000589    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1625848   |
| Train/Loss              | 0.17144074  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16669238  |
| Train/Ratio             | 0.9999978   |
| Train/Return            | 1.8646613   |
| Train/V                 | 2.031367    |
| Train/Value             | 2.031367    |
| Train/control_penalty   | 0.41597918  |
| Train/policy_loss       | 0.16669238  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.029       |
-----------------------------------------

 ---------------- Iteration 944 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 943         |
| Time/Actor_Time         | 0.0736      |
| Time/B_Format_Time      | 0.0721      |
| Time/B_Original_Form... | 0.0762      |
| Time/Buffer             | 0.00367     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23999803  |
| Train/Action_magnitu... | 0.5292725   |
| Train/Action_magnitude  | 0.41356307  |
| Train/Action_max        | 0.20016678  |
| Train/Action_std        | 0.13947164  |
| Train/Entropy           | -0.58408517 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1867353   |
| Train/Loss              | 0.14687677  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14213507  |
| Train/Ratio             | 1.0000081   |
| Train/Return            | 1.6134266   |
| Train/V                 | 1.7555588   |
| Train/Value             | 1.7555588   |
| Train/control_penalty   | 0.41576222  |
| Train/policy_loss       | 0.14213507  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 945 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 944        |
| Time/Actor_Time         | 0.0759     |
| Time/B_Format_Time      | 0.078      |
| Time/B_Original_Form... | 0.0775     |
| Time/Buffer             | 0.00338    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23394254 |
| Train/Action_magnitu... | 0.5243855  |
| Train/Action_magnitude  | 0.40740132 |
| Train/Action_max        | 0.19143283 |
| Train/Action_std        | 0.13531566 |
| Train/Entropy           | -0.6145226 |
| Train/Entropy_Loss      | 0.000615   |
| Train/Entropy_loss      | 0.000615   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2391611  |
| Train/Loss              | 0.1451513  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14048639 |
| Train/Ratio             | 0.99997824 |
| Train/Return            | 1.6601936  |
| Train/V                 | 1.8006783  |
| Train/Value             | 1.8006783  |
| Train/control_penalty   | 0.40503904 |
| Train/policy_loss       | 0.14048639 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.028      |
----------------------------------------

 ---------------- Iteration 946 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 945        |
| Time/Actor_Time         | 0.0741     |
| Time/B_Format_Time      | 0.0791     |
| Time/B_Original_Form... | 0.0798     |
| Time/Buffer             | 0.00439    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23423584 |
| Train/Action_magnitu... | 0.5288031  |
| Train/Action_magnitude  | 0.41232243 |
| Train/Action_max        | 0.20026328 |
| Train/Action_std        | 0.1400243  |
| Train/Entropy           | -0.57968   |
| Train/Entropy_Loss      | 0.00058    |
| Train/Entropy_loss      | 0.00058    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1512214  |
| Train/Loss              | 0.1879723  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.183302   |
| Train/Ratio             | 0.9999929  |
| Train/Return            | 1.558343   |
| Train/V                 | 1.7416452  |
| Train/Value             | 1.7416452  |
| Train/control_penalty   | 0.4090615  |
| Train/policy_loss       | 0.183302   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.022      |
----------------------------------------

 ---------------- Iteration 947 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 946        |
| Time/Actor_Time         | 0.0742     |
| Time/B_Format_Time      | 0.0717     |
| Time/B_Original_Form... | 0.0716     |
| Time/Buffer             | 0.00413    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23327266 |
| Train/Action_magnitu... | 0.5234389  |
| Train/Action_magnitude  | 0.40837634 |
| Train/Action_max        | 0.1741793  |
| Train/Action_std        | 0.13798977 |
| Train/Entropy           | -0.5955245 |
| Train/Entropy_Loss      | 0.000596   |
| Train/Entropy_loss      | 0.000596   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1735586  |
| Train/Loss              | 0.12002474 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11536887 |
| Train/Ratio             | 1.0000005  |
| Train/Return            | 1.5611924  |
| Train/V                 | 1.6765565  |
| Train/Value             | 1.6765565  |
| Train/control_penalty   | 0.40603402 |
| Train/policy_loss       | 0.11536887 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03125    |
----------------------------------------

 ---------------- Iteration 948 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 947         |
| Time/Actor_Time         | 0.0743      |
| Time/B_Format_Time      | 0.0774      |
| Time/B_Original_Form... | 0.0813      |
| Time/Buffer             | 0.00399     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23828004  |
| Train/Action_magnitu... | 0.5312247   |
| Train/Action_magnitude  | 0.41480258  |
| Train/Action_max        | 0.18560453  |
| Train/Action_std        | 0.13900658  |
| Train/Entropy           | -0.5898879  |
| Train/Entropy_Loss      | 0.00059     |
| Train/Entropy_loss      | 0.00059     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1825869   |
| Train/Loss              | 0.11680649  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.112068556 |
| Train/Ratio             | 0.9999989   |
| Train/Return            | 1.5472212   |
| Train/V                 | 1.659288    |
| Train/Value             | 1.659288    |
| Train/control_penalty   | 0.41480476  |
| Train/policy_loss       | 0.112068556 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0265      |
-----------------------------------------

 ---------------- Iteration 949 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 948        |
| Time/Actor_Time         | 0.0738     |
| Time/B_Format_Time      | 0.0755     |
| Time/B_Original_Form... | 0.0773     |
| Time/Buffer             | 0.00387    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24134263 |
| Train/Action_magnitu... | 0.535389   |
| Train/Action_magnitude  | 0.41675344 |
| Train/Action_max        | 0.20940283 |
| Train/Action_std        | 0.1383619  |
| Train/Entropy           | -0.592964  |
| Train/Entropy_Loss      | 0.000593   |
| Train/Entropy_loss      | 0.000593   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1688691  |
| Train/Loss              | 0.15426147 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14948867 |
| Train/Ratio             | 1.0000002  |
| Train/Return            | 1.6896782  |
| Train/V                 | 1.8391678  |
| Train/Value             | 1.8391678  |
| Train/control_penalty   | 0.41798425 |
| Train/policy_loss       | 0.14948867 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0275     |
----------------------------------------

 ---------------- Iteration 950 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 949         |
| Time/Actor_Time         | 0.08        |
| Time/B_Format_Time      | 0.0855      |
| Time/B_Original_Form... | 0.0898      |
| Time/Buffer             | 0.00505     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23199023  |
| Train/Action_magnitu... | 0.5182988   |
| Train/Action_magnitude  | 0.40600273  |
| Train/Action_max        | 0.17713538  |
| Train/Action_std        | 0.13797408  |
| Train/Entropy           | -0.59624857 |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1931936   |
| Train/Loss              | 0.1253226   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12069853  |
| Train/Ratio             | 0.9999892   |
| Train/Return            | 1.4806905   |
| Train/V                 | 1.6013912   |
| Train/Value             | 1.6013912   |
| Train/control_penalty   | 0.40278205  |
| Train/policy_loss       | 0.12069853  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02575     |
-----------------------------------------

 ---------------- Iteration 951 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 950        |
| Time/Actor_Time         | 0.0762     |
| Time/B_Format_Time      | 0.0754     |
| Time/B_Original_Form... | 0.0781     |
| Time/Buffer             | 0.00378    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22728163 |
| Train/Action_magnitu... | 0.5086331  |
| Train/Action_magnitude  | 0.3935456  |
| Train/Action_max        | 0.1762773  |
| Train/Action_std        | 0.13625526 |
| Train/Entropy           | -0.606036  |
| Train/Entropy_Loss      | 0.000606   |
| Train/Entropy_loss      | 0.000606   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2307296  |
| Train/Loss              | 0.13092797 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12639056 |
| Train/Ratio             | 1.0000007  |
| Train/Return            | 1.3839217  |
| Train/V                 | 1.5103135  |
| Train/Value             | 1.5103135  |
| Train/control_penalty   | 0.39313748 |
| Train/policy_loss       | 0.12639056 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01925    |
----------------------------------------

 ---------------- Iteration 952 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 951        |
| Time/Actor_Time         | 0.0736     |
| Time/B_Format_Time      | 0.0716     |
| Time/B_Original_Form... | 0.0771     |
| Time/Buffer             | 0.00416    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24012427 |
| Train/Action_magnitu... | 0.5390032  |
| Train/Action_magnitude  | 0.4210671  |
| Train/Action_max        | 0.20351353 |
| Train/Action_std        | 0.14234567 |
| Train/Entropy           | -0.5668412 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1616347  |
| Train/Loss              | 0.14701399 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14227606 |
| Train/Ratio             | 0.9999954  |
| Train/Return            | 1.7276224  |
| Train/V                 | 1.869889   |
| Train/Value             | 1.869889   |
| Train/control_penalty   | 0.41710877 |
| Train/policy_loss       | 0.14227606 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0315     |
----------------------------------------

 ---------------- Iteration 953 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 952        |
| Time/Actor_Time         | 0.0765     |
| Time/B_Format_Time      | 0.0818     |
| Time/B_Original_Form... | 0.0824     |
| Time/Buffer             | 0.00363    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23032926 |
| Train/Action_magnitu... | 0.5152223  |
| Train/Action_magnitude  | 0.40078095 |
| Train/Action_max        | 0.17944314 |
| Train/Action_std        | 0.13875498 |
| Train/Entropy           | -0.5872444 |
| Train/Entropy_Loss      | 0.000587   |
| Train/Entropy_loss      | 0.000587   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1835096  |
| Train/Loss              | 0.13635428 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13175105 |
| Train/Ratio             | 1.0000072  |
| Train/Return            | 1.5770108  |
| Train/V                 | 1.7087584  |
| Train/Value             | 1.7087584  |
| Train/control_penalty   | 0.4015999  |
| Train/policy_loss       | 0.13175105 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 954 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 953        |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0733     |
| Time/B_Original_Form... | 0.0778     |
| Time/Buffer             | 0.00447    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24040167 |
| Train/Action_magnitu... | 0.5357359  |
| Train/Action_magnitude  | 0.41831025 |
| Train/Action_max        | 0.18960279 |
| Train/Action_std        | 0.14087701 |
| Train/Entropy           | -0.5717599 |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1532743  |
| Train/Loss              | 0.10119513 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.09645083 |
| Train/Ratio             | 1.000008   |
| Train/Return            | 1.537634   |
| Train/V                 | 1.6340857  |
| Train/Value             | 1.6340857  |
| Train/control_penalty   | 0.4172541  |
| Train/policy_loss       | 0.09645083 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 955 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 954        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.0762     |
| Time/B_Original_Form... | 0.0787     |
| Time/Buffer             | 0.00353    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23440576 |
| Train/Action_magnitu... | 0.5320782  |
| Train/Action_magnitude  | 0.41529554 |
| Train/Action_max        | 0.17694758 |
| Train/Action_std        | 0.14219376 |
| Train/Entropy           | -0.56317   |
| Train/Entropy_Loss      | 0.000563   |
| Train/Entropy_loss      | 0.000563   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0984446  |
| Train/Loss              | 0.13108163 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.12639187 |
| Train/Ratio             | 1.000009   |
| Train/Return            | 1.3497133  |
| Train/V                 | 1.4760993  |
| Train/Value             | 1.4760993  |
| Train/control_penalty   | 0.41265854 |
| Train/policy_loss       | 0.12639187 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 956 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 955         |
| Time/Actor_Time         | 0.0796      |
| Time/B_Format_Time      | 0.0838      |
| Time/B_Original_Form... | 0.0837      |
| Time/Buffer             | 0.00402     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23451245  |
| Train/Action_magnitu... | 0.5296253   |
| Train/Action_magnitude  | 0.41447732  |
| Train/Action_max        | 0.17364965  |
| Train/Action_std        | 0.14395876  |
| Train/Entropy           | -0.5512033  |
| Train/Entropy_Loss      | 0.000551    |
| Train/Entropy_loss      | 0.000551    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1141883   |
| Train/Loss              | 0.12697619  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.122294724 |
| Train/Ratio             | 1.0000191   |
| Train/Return            | 1.6104844   |
| Train/V                 | 1.7327713   |
| Train/Value             | 1.7327713   |
| Train/control_penalty   | 0.41302744  |
| Train/policy_loss       | 0.122294724 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 957 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 956        |
| Time/Actor_Time         | 0.0757     |
| Time/B_Format_Time      | 0.0771     |
| Time/B_Original_Form... | 0.08       |
| Time/Buffer             | 0.00436    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24278899 |
| Train/Action_magnitu... | 0.53700095 |
| Train/Action_magnitude  | 0.41696838 |
| Train/Action_max        | 0.19661102 |
| Train/Action_std        | 0.14093833 |
| Train/Entropy           | -0.572365  |
| Train/Entropy_Loss      | 0.000572   |
| Train/Entropy_loss      | 0.000572   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.153575   |
| Train/Loss              | 0.15920591 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15446088 |
| Train/Ratio             | 0.99996996 |
| Train/Return            | 1.5800214  |
| Train/V                 | 1.734485   |
| Train/Value             | 1.734485   |
| Train/control_penalty   | 0.41726753 |
| Train/policy_loss       | 0.15446088 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 958 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 957         |
| Time/Actor_Time         | 0.0755      |
| Time/B_Format_Time      | 0.077       |
| Time/B_Original_Form... | 0.0769      |
| Time/Buffer             | 0.00402     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23344494  |
| Train/Action_magnitu... | 0.5278011   |
| Train/Action_magnitude  | 0.41188344  |
| Train/Action_max        | 0.17538194  |
| Train/Action_std        | 0.14115876  |
| Train/Entropy           | -0.5688806  |
| Train/Entropy_Loss      | 0.000569    |
| Train/Entropy_loss      | 0.000569    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1779915   |
| Train/Loss              | 0.103523724 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.09882948  |
| Train/Ratio             | 1.0000045   |
| Train/Return            | 1.4333544   |
| Train/V                 | 1.5321872   |
| Train/Value             | 1.5321872   |
| Train/control_penalty   | 0.41253635  |
| Train/policy_loss       | 0.09882948  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 959 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 958        |
| Time/Actor_Time         | 0.0753     |
| Time/B_Format_Time      | 0.0793     |
| Time/B_Original_Form... | 0.0819     |
| Time/Buffer             | 0.00382    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23522855 |
| Train/Action_magnitu... | 0.52898943 |
| Train/Action_magnitude  | 0.41367579 |
| Train/Action_max        | 0.17642032 |
| Train/Action_std        | 0.14154695 |
| Train/Entropy           | -0.5673769 |
| Train/Entropy_Loss      | 0.000567   |
| Train/Entropy_loss      | 0.000567   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1390086  |
| Train/Loss              | 0.1382826  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13359953 |
| Train/Ratio             | 1.0000029  |
| Train/Return            | 1.3042341  |
| Train/V                 | 1.4378377  |
| Train/Value             | 1.4378377  |
| Train/control_penalty   | 0.41156808 |
| Train/policy_loss       | 0.13359953 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------

 ---------------- Iteration 960 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 959         |
| Time/Actor_Time         | 0.0727      |
| Time/B_Format_Time      | 0.077       |
| Time/B_Original_Form... | 0.0769      |
| Time/Buffer             | 0.00386     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23649853  |
| Train/Action_magnitu... | 0.5318042   |
| Train/Action_magnitude  | 0.41719753  |
| Train/Action_max        | 0.18263717  |
| Train/Action_std        | 0.14469944  |
| Train/Entropy           | -0.54448104 |
| Train/Entropy_Loss      | 0.000544    |
| Train/Entropy_loss      | 0.000544    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.089191    |
| Train/Loss              | 0.087721    |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.08298607  |
| Train/Ratio             | 1.0000155   |
| Train/Return            | 1.6053463   |
| Train/V                 | 1.6883249   |
| Train/Value             | 1.6883249   |
| Train/control_penalty   | 0.41904432  |
| Train/policy_loss       | 0.08298607  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 961 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 960         |
| Time/Actor_Time         | 0.0745      |
| Time/B_Format_Time      | 0.0709      |
| Time/B_Original_Form... | 0.0731      |
| Time/Buffer             | 0.00394     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23252267  |
| Train/Action_magnitu... | 0.5306322   |
| Train/Action_magnitude  | 0.41458684  |
| Train/Action_max        | 0.18719098  |
| Train/Action_std        | 0.14473107  |
| Train/Entropy           | -0.54548365 |
| Train/Entropy_Loss      | 0.000545    |
| Train/Entropy_loss      | 0.000545    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0649595   |
| Train/Loss              | 0.1250319   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12036634  |
| Train/Ratio             | 1.000029    |
| Train/Return            | 1.5626694   |
| Train/V                 | 1.6830361   |
| Train/Value             | 1.6830361   |
| Train/control_penalty   | 0.4120076   |
| Train/policy_loss       | 0.12036634  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 962 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 961        |
| Time/Actor_Time         | 0.0807     |
| Time/B_Format_Time      | 0.0793     |
| Time/B_Original_Form... | 0.0813     |
| Time/Buffer             | 0.00493    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22656015 |
| Train/Action_magnitu... | 0.5111471  |
| Train/Action_magnitude  | 0.39931652 |
| Train/Action_max        | 0.18363263 |
| Train/Action_std        | 0.13841194 |
| Train/Entropy           | -0.5897338 |
| Train/Entropy_Loss      | 0.00059    |
| Train/Entropy_loss      | 0.00059    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1808965  |
| Train/Loss              | 0.14837514 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14382115 |
| Train/Ratio             | 1.0000055  |
| Train/Return            | 1.4420093  |
| Train/V                 | 1.5858297  |
| Train/Value             | 1.5858297  |
| Train/control_penalty   | 0.3964256  |
| Train/policy_loss       | 0.14382115 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.026      |
----------------------------------------

 ---------------- Iteration 963 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 962        |
| Time/Actor_Time         | 0.0733     |
| Time/B_Format_Time      | 0.0719     |
| Time/B_Original_Form... | 0.0764     |
| Time/Buffer             | 0.00351    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.2475705  |
| Train/Action_magnitu... | 0.54414755 |
| Train/Action_magnitude  | 0.42611766 |
| Train/Action_max        | 0.19130763 |
| Train/Action_std        | 0.1426536  |
| Train/Entropy           | -0.5578073 |
| Train/Entropy_Loss      | 0.000558   |
| Train/Entropy_loss      | 0.000558   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1319596  |
| Train/Loss              | 0.11065846 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.10580812 |
| Train/Ratio             | 1.0000054  |
| Train/Return            | 1.4033761  |
| Train/V                 | 1.5091795  |
| Train/Value             | 1.5091795  |
| Train/control_penalty   | 0.4292531  |
| Train/policy_loss       | 0.10580812 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.023      |
----------------------------------------

 ---------------- Iteration 964 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 963         |
| Time/Actor_Time         | 0.079       |
| Time/B_Format_Time      | 0.0853      |
| Time/B_Original_Form... | 0.0837      |
| Time/Buffer             | 0.00425     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.24434522  |
| Train/Action_magnitu... | 0.5385982   |
| Train/Action_magnitude  | 0.41968766  |
| Train/Action_max        | 0.18172874  |
| Train/Action_std        | 0.14046836  |
| Train/Entropy           | -0.5732749  |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1440294   |
| Train/Loss              | 0.088550664 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.0837286   |
| Train/Ratio             | 1.0000072   |
| Train/Return            | 1.6048871   |
| Train/V                 | 1.6886218   |
| Train/Value             | 1.6886218   |
| Train/control_penalty   | 0.42487898  |
| Train/policy_loss       | 0.0837286   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03275     |
-----------------------------------------

 ---------------- Iteration 965 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 964         |
| Time/Actor_Time         | 0.0725      |
| Time/B_Format_Time      | 0.071       |
| Time/B_Original_Form... | 0.0737      |
| Time/Buffer             | 0.00376     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22970203  |
| Train/Action_magnitu... | 0.51319784  |
| Train/Action_magnitude  | 0.40016282  |
| Train/Action_max        | 0.19777723  |
| Train/Action_std        | 0.13918227  |
| Train/Entropy           | -0.58439225 |
| Train/Entropy_Loss      | 0.000584    |
| Train/Entropy_loss      | 0.000584    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1713893   |
| Train/Loss              | 0.14202991  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.13744216  |
| Train/Ratio             | 0.99999803  |
| Train/Return            | 1.4722129   |
| Train/V                 | 1.6096554   |
| Train/Value             | 1.6096554   |
| Train/control_penalty   | 0.40033594  |
| Train/policy_loss       | 0.13744216  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 966 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 965         |
| Time/Actor_Time         | 0.0747      |
| Time/B_Format_Time      | 0.0729      |
| Time/B_Original_Form... | 0.0726      |
| Time/Buffer             | 0.00506     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23112863  |
| Train/Action_magnitu... | 0.5231      |
| Train/Action_magnitude  | 0.4068861   |
| Train/Action_max        | 0.18733834  |
| Train/Action_std        | 0.13932385  |
| Train/Entropy           | -0.58698267 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1759361   |
| Train/Loss              | 0.16486537  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16023913  |
| Train/Ratio             | 1.0000172   |
| Train/Return            | 1.5439444   |
| Train/V                 | 1.7041689   |
| Train/Value             | 1.7041689   |
| Train/control_penalty   | 0.40392628  |
| Train/policy_loss       | 0.16023913  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01875     |
-----------------------------------------

 ---------------- Iteration 967 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 966         |
| Time/Actor_Time         | 0.0804      |
| Time/B_Format_Time      | 0.0883      |
| Time/B_Original_Form... | 0.0897      |
| Time/Buffer             | 0.00445     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22809845  |
| Train/Action_magnitu... | 0.5145651   |
| Train/Action_magnitude  | 0.4015825   |
| Train/Action_max        | 0.18279521  |
| Train/Action_std        | 0.13940726  |
| Train/Entropy           | -0.58828974 |
| Train/Entropy_Loss      | 0.000588    |
| Train/Entropy_loss      | 0.000588    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.16529     |
| Train/Loss              | 0.11502593  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11037818  |
| Train/Ratio             | 0.99997985  |
| Train/Return            | 1.4194355   |
| Train/V                 | 1.5298189   |
| Train/Value             | 1.5298189   |
| Train/control_penalty   | 0.40594563  |
| Train/policy_loss       | 0.11037818  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------

 ---------------- Iteration 968 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 967         |
| Time/Actor_Time         | 0.0747      |
| Time/B_Format_Time      | 0.077       |
| Time/B_Original_Form... | 0.0729      |
| Time/Buffer             | 0.00429     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23462343  |
| Train/Action_magnitu... | 0.52716213  |
| Train/Action_magnitude  | 0.40812153  |
| Train/Action_max        | 0.2001535   |
| Train/Action_std        | 0.13808508  |
| Train/Entropy           | -0.59304076 |
| Train/Entropy_Loss      | 0.000593    |
| Train/Entropy_loss      | 0.000593    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1794639   |
| Train/Loss              | 0.16078556  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1561295   |
| Train/Ratio             | 0.99999994  |
| Train/Return            | 1.388707    |
| Train/V                 | 1.5448399   |
| Train/Value             | 1.5448399   |
| Train/control_penalty   | 0.40630275  |
| Train/policy_loss       | 0.1561295   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.01675     |
-----------------------------------------

 ---------------- Iteration 969 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 968         |
| Time/Actor_Time         | 0.0765      |
| Time/B_Format_Time      | 0.0811      |
| Time/B_Original_Form... | 0.0946      |
| Time/Buffer             | 0.00372     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2407924   |
| Train/Action_magnitu... | 0.5362069   |
| Train/Action_magnitude  | 0.41553438  |
| Train/Action_max        | 0.20713976  |
| Train/Action_std        | 0.138111    |
| Train/Entropy           | -0.59506613 |
| Train/Entropy_Loss      | 0.000595    |
| Train/Entropy_loss      | 0.000595    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1991045   |
| Train/Loss              | 0.17116606  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16638973  |
| Train/Ratio             | 1.0000006   |
| Train/Return            | 1.4934326   |
| Train/V                 | 1.6598169   |
| Train/Value             | 1.6598169   |
| Train/control_penalty   | 0.41812629  |
| Train/policy_loss       | 0.16638973  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02175     |
-----------------------------------------

 ---------------- Iteration 970 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 969         |
| Time/Actor_Time         | 0.0731      |
| Time/B_Format_Time      | 0.0731      |
| Time/B_Original_Form... | 0.0755      |
| Time/Buffer             | 0.00362     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.21931595  |
| Train/Action_magnitu... | 0.49545568  |
| Train/Action_magnitude  | 0.38545516  |
| Train/Action_max        | 0.1880055   |
| Train/Action_std        | 0.13452119  |
| Train/Entropy           | -0.61755186 |
| Train/Entropy_Loss      | 0.000618    |
| Train/Entropy_loss      | 0.000618    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2750951   |
| Train/Loss              | 0.16608377  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16161309  |
| Train/Ratio             | 1.000003    |
| Train/Return            | 1.4673953   |
| Train/V                 | 1.6290128   |
| Train/Value             | 1.6290128   |
| Train/control_penalty   | 0.38531324  |
| Train/policy_loss       | 0.16161309  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 971 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 970        |
| Time/Actor_Time         | 0.0824     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.0776     |
| Time/Buffer             | 0.00392    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22417752 |
| Train/Action_magnitu... | 0.5102013  |
| Train/Action_magnitude  | 0.3961895  |
| Train/Action_max        | 0.18147652 |
| Train/Action_std        | 0.13752747 |
| Train/Entropy           | -0.6013824 |
| Train/Entropy_Loss      | 0.000601   |
| Train/Entropy_loss      | 0.000601   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.162018   |
| Train/Loss              | 0.1763031  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.171807   |
| Train/Ratio             | 1.0000086  |
| Train/Return            | 1.5567319  |
| Train/V                 | 1.7285397  |
| Train/Value             | 1.7285397  |
| Train/control_penalty   | 0.3894714  |
| Train/policy_loss       | 0.171807   |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02125    |
----------------------------------------

 ---------------- Iteration 972 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 971        |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.0726     |
| Time/B_Original_Form... | 0.0723     |
| Time/Buffer             | 0.00377    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22938402 |
| Train/Action_magnitu... | 0.51410025 |
| Train/Action_magnitude  | 0.39904743 |
| Train/Action_max        | 0.17985459 |
| Train/Action_std        | 0.1368935  |
| Train/Entropy           | -0.6084725 |
| Train/Entropy_Loss      | 0.000608   |
| Train/Entropy_loss      | 0.000608   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2242455  |
| Train/Loss              | 0.0945216  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.08991496 |
| Train/Ratio             | 0.9999958  |
| Train/Return            | 1.6170605  |
| Train/V                 | 1.7069753  |
| Train/Value             | 1.7069753  |
| Train/control_penalty   | 0.3998162  |
| Train/policy_loss       | 0.08991496 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 973 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 972         |
| Time/Actor_Time         | 0.0743      |
| Time/B_Format_Time      | 0.0743      |
| Time/B_Original_Form... | 0.0742      |
| Time/Buffer             | 0.00293     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22703168  |
| Train/Action_magnitu... | 0.5081466   |
| Train/Action_magnitude  | 0.39306659  |
| Train/Action_max        | 0.19405638  |
| Train/Action_std        | 0.13371319  |
| Train/Entropy           | -0.62957114 |
| Train/Entropy_Loss      | 0.00063     |
| Train/Entropy_loss      | 0.00063     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2743101   |
| Train/Loss              | 0.20959178  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20504227  |
| Train/Ratio             | 0.9999902   |
| Train/Return            | 1.6128752   |
| Train/V                 | 1.8179072   |
| Train/Value             | 1.8179072   |
| Train/control_penalty   | 0.39199266  |
| Train/policy_loss       | 0.20504227  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.019       |
-----------------------------------------

 ---------------- Iteration 974 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 973         |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0779      |
| Time/B_Original_Form... | 0.0771      |
| Time/Buffer             | 0.00378     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.218862    |
| Train/Action_magnitu... | 0.49357703  |
| Train/Action_magnitude  | 0.38284042  |
| Train/Action_max        | 0.1869689   |
| Train/Action_std        | 0.13565117  |
| Train/Entropy           | -0.61474067 |
| Train/Entropy_Loss      | 0.000615    |
| Train/Entropy_loss      | 0.000615    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2182149   |
| Train/Loss              | 0.11731229  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.11286486  |
| Train/Ratio             | 1.0000165   |
| Train/Return            | 1.5754955   |
| Train/V                 | 1.6883548   |
| Train/Value             | 1.6883548   |
| Train/control_penalty   | 0.38326907  |
| Train/policy_loss       | 0.11286486  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 975 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 974         |
| Time/Actor_Time         | 0.0756      |
| Time/B_Format_Time      | 0.0773      |
| Time/B_Original_Form... | 0.0777      |
| Time/Buffer             | 0.00392     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2281132   |
| Train/Action_magnitu... | 0.5122354   |
| Train/Action_magnitude  | 0.39964432  |
| Train/Action_max        | 0.18659076  |
| Train/Action_std        | 0.13802692  |
| Train/Entropy           | -0.59826034 |
| Train/Entropy_Loss      | 0.000598    |
| Train/Entropy_loss      | 0.000598    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1962771   |
| Train/Loss              | 0.18549041  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18086825  |
| Train/Ratio             | 1.0000321   |
| Train/Return            | 1.8079647   |
| Train/V                 | 1.988829    |
| Train/Value             | 1.988829    |
| Train/control_penalty   | 0.40239     |
| Train/policy_loss       | 0.18086825  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 976 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 975         |
| Time/Actor_Time         | 0.076       |
| Time/B_Format_Time      | 0.0757      |
| Time/B_Original_Form... | 0.0746      |
| Time/Buffer             | 0.00454     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22335216  |
| Train/Action_magnitu... | 0.49797958  |
| Train/Action_magnitude  | 0.38703206  |
| Train/Action_max        | 0.18078107  |
| Train/Action_std        | 0.1352779   |
| Train/Entropy           | -0.61528724 |
| Train/Entropy_Loss      | 0.000615    |
| Train/Entropy_loss      | 0.000615    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2242261   |
| Train/Loss              | 0.13432118  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12979737  |
| Train/Ratio             | 1.0000254   |
| Train/Return            | 1.5396446   |
| Train/V                 | 1.6694397   |
| Train/Value             | 1.6694397   |
| Train/control_penalty   | 0.39085263  |
| Train/policy_loss       | 0.12979737  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0225      |
-----------------------------------------

 ---------------- Iteration 977 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 976        |
| Time/Actor_Time         | 0.0737     |
| Time/B_Format_Time      | 0.0721     |
| Time/B_Original_Form... | 0.0748     |
| Time/Buffer             | 0.00416    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22423166 |
| Train/Action_magnitu... | 0.50188494 |
| Train/Action_magnitude  | 0.39070305 |
| Train/Action_max        | 0.18828942 |
| Train/Action_std        | 0.13610543 |
| Train/Entropy           | -0.6114507 |
| Train/Entropy_Loss      | 0.000611   |
| Train/Entropy_loss      | 0.000611   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.2097043  |
| Train/Loss              | 0.17639259 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17183585 |
| Train/Ratio             | 0.99997956 |
| Train/Return            | 1.4848021  |
| Train/V                 | 1.6566461  |
| Train/Value             | 1.6566461  |
| Train/control_penalty   | 0.3945279  |
| Train/policy_loss       | 0.17183585 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01625    |
----------------------------------------

 ---------------- Iteration 978 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 977         |
| Time/Actor_Time         | 0.0761      |
| Time/B_Format_Time      | 0.0735      |
| Time/B_Original_Form... | 0.0763      |
| Time/Buffer             | 0.0036      |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23277059  |
| Train/Action_magnitu... | 0.5208781   |
| Train/Action_magnitude  | 0.40571064  |
| Train/Action_max        | 0.19313917  |
| Train/Action_std        | 0.14011495  |
| Train/Entropy           | -0.58480686 |
| Train/Entropy_Loss      | 0.000585    |
| Train/Entropy_loss      | 0.000585    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.176685    |
| Train/Loss              | 0.13340238  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12873712  |
| Train/Ratio             | 0.99999607  |
| Train/Return            | 1.6910548   |
| Train/V                 | 1.8197874   |
| Train/Value             | 1.8197874   |
| Train/control_penalty   | 0.40804493  |
| Train/policy_loss       | 0.12873712  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.025       |
-----------------------------------------

 ---------------- Iteration 979 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 978        |
| Time/Actor_Time         | 0.0751     |
| Time/B_Format_Time      | 0.0805     |
| Time/B_Original_Form... | 0.0816     |
| Time/Buffer             | 0.00357    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22450319 |
| Train/Action_magnitu... | 0.507963   |
| Train/Action_magnitude  | 0.3960094  |
| Train/Action_max        | 0.18034826 |
| Train/Action_std        | 0.13743274 |
| Train/Entropy           | -0.6008511 |
| Train/Entropy_Loss      | 0.000601   |
| Train/Entropy_loss      | 0.000601   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1933838  |
| Train/Loss              | 0.19558649 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.19101587 |
| Train/Ratio             | 1.0000088  |
| Train/Return            | 1.6753703  |
| Train/V                 | 1.8663803  |
| Train/Value             | 1.8663803  |
| Train/control_penalty   | 0.39697802 |
| Train/policy_loss       | 0.19101587 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02075    |
----------------------------------------

 ---------------- Iteration 980 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 979        |
| Time/Actor_Time         | 0.072      |
| Time/B_Format_Time      | 0.0684     |
| Time/B_Original_Form... | 0.0712     |
| Time/Buffer             | 0.00583    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.22963423 |
| Train/Action_magnitu... | 0.5207982  |
| Train/Action_magnitude  | 0.40688288 |
| Train/Action_max        | 0.18081942 |
| Train/Action_std        | 0.1413758  |
| Train/Entropy           | -0.5737577 |
| Train/Entropy_Loss      | 0.000574   |
| Train/Entropy_loss      | 0.000574   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1348774  |
| Train/Loss              | 0.15154336 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.14689091 |
| Train/Ratio             | 0.99997336 |
| Train/Return            | 1.918215   |
| Train/V                 | 2.0651085  |
| Train/Value             | 2.0651085  |
| Train/control_penalty   | 0.40787002 |
| Train/policy_loss       | 0.14689091 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03       |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 981 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 980        |
| Time/Actor_Time         | 0.0714     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.0712     |
| Time/Buffer             | 0.00324    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.22911423 |
| Train/Action_magnitu... | 0.5200558  |
| Train/Action_magnitude  | 0.40587804 |
| Train/Action_max        | 0.20342453 |
| Train/Action_std        | 0.14090447 |
| Train/Entropy           | -0.5769181 |
| Train/Entropy_Loss      | 0.000577   |
| Train/Entropy_loss      | 0.000577   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1346811  |
| Train/Loss              | 0.17929281 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17464049 |
| Train/Ratio             | 1.0000095  |
| Train/Return            | 1.7240285  |
| Train/V                 | 1.8986675  |
| Train/Value             | 1.8986675  |
| Train/control_penalty   | 0.40754077 |
| Train/policy_loss       | 0.17464049 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.01975    |
----------------------------------------

 ---------------- Iteration 982 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 981         |
| Time/Actor_Time         | 0.0732      |
| Time/B_Format_Time      | 0.0678      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00299     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23981081  |
| Train/Action_magnitu... | 0.5318687   |
| Train/Action_magnitude  | 0.41584796  |
| Train/Action_max        | 0.1893122   |
| Train/Action_std        | 0.13920385  |
| Train/Entropy           | -0.58729213 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1835698   |
| Train/Loss              | 0.17716119  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17237028  |
| Train/Ratio             | 1.0000093   |
| Train/Return            | 1.7832952   |
| Train/V                 | 1.9556663   |
| Train/Value             | 1.9556663   |
| Train/control_penalty   | 0.4203601   |
| Train/policy_loss       | 0.17237028  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02625     |
-----------------------------------------

 ---------------- Iteration 983 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 982         |
| Time/Actor_Time         | 0.0756      |
| Time/B_Format_Time      | 0.0741      |
| Time/B_Original_Form... | 0.0766      |
| Time/Buffer             | 0.00322     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22802266  |
| Train/Action_magnitu... | 0.5192008   |
| Train/Action_magnitude  | 0.40697294  |
| Train/Action_max        | 0.17317887  |
| Train/Action_std        | 0.14131853  |
| Train/Entropy           | -0.57256365 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1459217   |
| Train/Loss              | 0.17405984  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16949058  |
| Train/Ratio             | 0.9999844   |
| Train/Return            | 1.5799406   |
| Train/V                 | 1.7494191   |
| Train/Value             | 1.7494191   |
| Train/control_penalty   | 0.3996701   |
| Train/policy_loss       | 0.16949058  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0245      |
-----------------------------------------

 ---------------- Iteration 984 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 983        |
| Time/Actor_Time         | 0.0756     |
| Time/B_Format_Time      | 0.0744     |
| Time/B_Original_Form... | 0.0761     |
| Time/Buffer             | 0.00445    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2509731  |
| Train/Action_magnitu... | 0.5558318  |
| Train/Action_magnitude  | 0.4348628  |
| Train/Action_max        | 0.18124893 |
| Train/Action_std        | 0.14386365 |
| Train/Entropy           | -0.5482267 |
| Train/Entropy_Loss      | 0.000548   |
| Train/Entropy_loss      | 0.000548   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1132821  |
| Train/Loss              | 0.13637573 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13142084 |
| Train/Ratio             | 1.0000331  |
| Train/Return            | 1.7826071  |
| Train/V                 | 1.9140271  |
| Train/Value             | 1.9140271  |
| Train/control_penalty   | 0.4406668  |
| Train/policy_loss       | 0.13142084 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.027      |
----------------------------------------

 ---------------- Iteration 985 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 984         |
| Time/Actor_Time         | 0.0766      |
| Time/B_Format_Time      | 0.0784      |
| Time/B_Original_Form... | 0.0774      |
| Time/Buffer             | 0.00412     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.24483915  |
| Train/Action_magnitu... | 0.5438314   |
| Train/Action_magnitude  | 0.42663115  |
| Train/Action_max        | 0.1998209   |
| Train/Action_std        | 0.14040837  |
| Train/Entropy           | -0.57563525 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1649873   |
| Train/Loss              | 0.18764627  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.18282713  |
| Train/Ratio             | 1.0000038   |
| Train/Return            | 1.6067257   |
| Train/V                 | 1.789546    |
| Train/Value             | 1.789546    |
| Train/control_penalty   | 0.4243509   |
| Train/policy_loss       | 0.18282713  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0255      |
-----------------------------------------

 ---------------- Iteration 986 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 985         |
| Time/Actor_Time         | 0.0745      |
| Time/B_Format_Time      | 0.076       |
| Time/B_Original_Form... | 0.0795      |
| Time/Buffer             | 0.00415     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23922023  |
| Train/Action_magnitu... | 0.5398454   |
| Train/Action_magnitude  | 0.42372185  |
| Train/Action_max        | 0.18888779  |
| Train/Action_std        | 0.14374974  |
| Train/Entropy           | -0.55166227 |
| Train/Entropy_Loss      | 0.000552    |
| Train/Entropy_loss      | 0.000552    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0892454   |
| Train/Loss              | 0.16458808  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1597937   |
| Train/Ratio             | 1.0000006   |
| Train/Return            | 1.7744848   |
| Train/V                 | 1.9342864   |
| Train/Value             | 1.9342864   |
| Train/control_penalty   | 0.4242716   |
| Train/policy_loss       | 0.1597937   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02725     |
-----------------------------------------

 ---------------- Iteration 987 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 986        |
| Time/Actor_Time         | 0.0803     |
| Time/B_Format_Time      | 0.0823     |
| Time/B_Original_Form... | 0.0859     |
| Time/Buffer             | 0.00388    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23878294 |
| Train/Action_magnitu... | 0.5301828  |
| Train/Action_magnitude  | 0.4132991  |
| Train/Action_max        | 0.194688   |
| Train/Action_std        | 0.13997845 |
| Train/Entropy           | -0.5833157 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1750253  |
| Train/Loss              | 0.15878654 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15406251 |
| Train/Ratio             | 1.0000181  |
| Train/Return            | 1.7213347  |
| Train/V                 | 1.8754014  |
| Train/Value             | 1.8754014  |
| Train/control_penalty   | 0.41407034 |
| Train/policy_loss       | 0.15406251 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 988 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 987         |
| Time/Actor_Time         | 0.0753      |
| Time/B_Format_Time      | 0.0796      |
| Time/B_Original_Form... | 0.0816      |
| Time/Buffer             | 0.00328     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2386299   |
| Train/Action_magnitu... | 0.52690357  |
| Train/Action_magnitude  | 0.40876982  |
| Train/Action_max        | 0.20886436  |
| Train/Action_std        | 0.13773242  |
| Train/Entropy           | -0.59710854 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1770627   |
| Train/Loss              | 0.25520045  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.25052518  |
| Train/Ratio             | 1.0000062   |
| Train/Return            | 1.7235036   |
| Train/V                 | 1.9740189   |
| Train/Value             | 1.9740189   |
| Train/control_penalty   | 0.4078144   |
| Train/policy_loss       | 0.25052518  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.015       |
-----------------------------------------

 ---------------- Iteration 989 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 988        |
| Time/Actor_Time         | 0.0721     |
| Time/B_Format_Time      | 0.0747     |
| Time/B_Original_Form... | 0.0746     |
| Time/Buffer             | 0.00439    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24202386 |
| Train/Action_magnitu... | 0.5304493  |
| Train/Action_magnitude  | 0.41342655 |
| Train/Action_max        | 0.19716369 |
| Train/Action_std        | 0.13951962 |
| Train/Entropy           | -0.5817609 |
| Train/Entropy_Loss      | 0.000582   |
| Train/Entropy_loss      | 0.000582   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1865022  |
| Train/Loss              | 0.20644301 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20165858 |
| Train/Ratio             | 1.0000165  |
| Train/Return            | 1.7041425  |
| Train/V                 | 1.9057938  |
| Train/Value             | 1.9057938  |
| Train/control_penalty   | 0.42026743 |
| Train/policy_loss       | 0.20165858 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 990 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 989        |
| Time/Actor_Time         | 0.0737     |
| Time/B_Format_Time      | 0.0748     |
| Time/B_Original_Form... | 0.0757     |
| Time/Buffer             | 0.00394    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24854803 |
| Train/Action_magnitu... | 0.5586136  |
| Train/Action_magnitude  | 0.43742532 |
| Train/Action_max        | 0.1825363  |
| Train/Action_std        | 0.14364704 |
| Train/Entropy           | -0.551595  |
| Train/Entropy_Loss      | 0.000552   |
| Train/Entropy_loss      | 0.000552   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1032224  |
| Train/Loss              | 0.20939547 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2045102  |
| Train/Ratio             | 1.000004   |
| Train/Return            | 1.8734655  |
| Train/V                 | 2.0779815  |
| Train/Value             | 2.0779815  |
| Train/control_penalty   | 0.4333674  |
| Train/policy_loss       | 0.2045102  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 991 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 990        |
| Time/Actor_Time         | 0.0728     |
| Time/B_Format_Time      | 0.0706     |
| Time/B_Original_Form... | 0.0746     |
| Time/Buffer             | 0.00368    |
| Time/Critic_Time        | 7.15e-07   |
| Train/Action_abs_mean   | 0.23282474 |
| Train/Action_magnitu... | 0.52631813 |
| Train/Action_magnitude  | 0.41132715 |
| Train/Action_max        | 0.18713726 |
| Train/Action_std        | 0.14244956 |
| Train/Entropy           | -0.5611095 |
| Train/Entropy_Loss      | 0.000561   |
| Train/Entropy_loss      | 0.000561   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1380852  |
| Train/Loss              | 0.16457799 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15989242 |
| Train/Ratio             | 1.0000057  |
| Train/Return            | 1.7074912  |
| Train/V                 | 1.8673775  |
| Train/Value             | 1.8673775  |
| Train/control_penalty   | 0.41244605 |
| Train/policy_loss       | 0.15989242 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02625    |
----------------------------------------

 ---------------- Iteration 992 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 991         |
| Time/Actor_Time         | 0.0742      |
| Time/B_Format_Time      | 0.0752      |
| Time/B_Original_Form... | 0.0769      |
| Time/Buffer             | 0.00348     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24778381  |
| Train/Action_magnitu... | 0.5599872   |
| Train/Action_magnitude  | 0.43724266  |
| Train/Action_max        | 0.21151493  |
| Train/Action_std        | 0.14552595  |
| Train/Entropy           | -0.54318327 |
| Train/Entropy_Loss      | 0.000543    |
| Train/Entropy_loss      | 0.000543    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.0660936   |
| Train/Loss              | 0.20747396  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20258498  |
| Train/Ratio             | 0.99998903  |
| Train/Return            | 1.7829779   |
| Train/V                 | 1.9855583   |
| Train/Value             | 1.9855583   |
| Train/control_penalty   | 0.43458048  |
| Train/policy_loss       | 0.20258498  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 993 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 992        |
| Time/Actor_Time         | 0.0749     |
| Time/B_Format_Time      | 0.0753     |
| Time/B_Original_Form... | 0.0778     |
| Time/Buffer             | 0.0035     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24200739 |
| Train/Action_magnitu... | 0.54196453 |
| Train/Action_magnitude  | 0.41963786 |
| Train/Action_max        | 0.21564482 |
| Train/Action_std        | 0.13911718 |
| Train/Entropy           | -0.5855451 |
| Train/Entropy_Loss      | 0.000586   |
| Train/Entropy_loss      | 0.000586   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.154492   |
| Train/Loss              | 0.18113108 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17637356 |
| Train/Ratio             | 0.9999926  |
| Train/Return            | 1.8335125  |
| Train/V                 | 2.0098853  |
| Train/Value             | 2.0098853  |
| Train/control_penalty   | 0.41719812 |
| Train/policy_loss       | 0.17637356 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02875    |
----------------------------------------

 ---------------- Iteration 994 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 993         |
| Time/Actor_Time         | 0.0751      |
| Time/B_Format_Time      | 0.0734      |
| Time/B_Original_Form... | 0.0746      |
| Time/Buffer             | 0.00438     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24210721  |
| Train/Action_magnitu... | 0.5377565   |
| Train/Action_magnitude  | 0.4205655   |
| Train/Action_max        | 0.18555853  |
| Train/Action_std        | 0.14084765  |
| Train/Entropy           | -0.57315075 |
| Train/Entropy_Loss      | 0.000573    |
| Train/Entropy_loss      | 0.000573    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1176425   |
| Train/Loss              | 0.1628592   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.15807524  |
| Train/Ratio             | 1.0000031   |
| Train/Return            | 1.7364101   |
| Train/V                 | 1.8944795   |
| Train/Value             | 1.8944795   |
| Train/control_penalty   | 0.4210815   |
| Train/policy_loss       | 0.15807524  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02375     |
-----------------------------------------

 ---------------- Iteration 995 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 994        |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0775     |
| Time/B_Original_Form... | 0.0779     |
| Time/Buffer             | 0.00473    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.248233   |
| Train/Action_magnitu... | 0.54534477 |
| Train/Action_magnitude  | 0.42478698 |
| Train/Action_max        | 0.19727464 |
| Train/Action_std        | 0.13967282 |
| Train/Entropy           | -0.5793007 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1437604  |
| Train/Loss              | 0.14721048 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1423917  |
| Train/Ratio             | 1.0000138  |
| Train/Return            | 1.8811275  |
| Train/V                 | 2.023518   |
| Train/Value             | 2.023518   |
| Train/control_penalty   | 0.42394885 |
| Train/policy_loss       | 0.1423917  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03125    |
----------------------------------------

 ---------------- Iteration 996 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 995         |
| Time/Actor_Time         | 0.0732      |
| Time/B_Format_Time      | 0.0705      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00372     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.2332649   |
| Train/Action_magnitu... | 0.5307207   |
| Train/Action_magnitude  | 0.41416728  |
| Train/Action_max        | 0.18663175  |
| Train/Action_std        | 0.14278996  |
| Train/Entropy           | -0.56282383 |
| Train/Entropy_Loss      | 0.000563    |
| Train/Entropy_loss      | 0.000563    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1194357   |
| Train/Loss              | 0.21495497  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.21026039  |
| Train/Ratio             | 1.0000012   |
| Train/Return            | 1.6879866   |
| Train/V                 | 1.898249    |
| Train/Value             | 1.898249    |
| Train/control_penalty   | 0.4131757   |
| Train/policy_loss       | 0.21026039  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02225     |
-----------------------------------------

 ---------------- Iteration 997 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 996        |
| Time/Actor_Time         | 0.0822     |
| Time/B_Format_Time      | 0.086      |
| Time/B_Original_Form... | 0.0887     |
| Time/Buffer             | 0.00418    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24162716 |
| Train/Action_magnitu... | 0.5389062  |
| Train/Action_magnitude  | 0.4193431  |
| Train/Action_max        | 0.19891956 |
| Train/Action_std        | 0.13936432 |
| Train/Entropy           | -0.5829849 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1858271  |
| Train/Loss              | 0.1763358  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17155184 |
| Train/Ratio             | 0.9999999  |
| Train/Return            | 2.0675287  |
| Train/V                 | 2.2390785  |
| Train/Value             | 2.2390785  |
| Train/control_penalty   | 0.42009866 |
| Train/policy_loss       | 0.17155184 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03075    |
----------------------------------------

 ---------------- Iteration 998 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 997         |
| Time/Actor_Time         | 0.0735      |
| Time/B_Format_Time      | 0.0732      |
| Time/B_Original_Form... | 0.0778      |
| Time/Buffer             | 0.00421     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.2473141   |
| Train/Action_magnitu... | 0.55132747  |
| Train/Action_magnitude  | 0.4328346   |
| Train/Action_max        | 0.18397535  |
| Train/Action_std        | 0.14192125  |
| Train/Entropy           | -0.56354886 |
| Train/Entropy_Loss      | 0.000564    |
| Train/Entropy_loss      | 0.000564    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.137371    |
| Train/Loss              | 0.16831703  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16342813  |
| Train/Ratio             | 1.0000287   |
| Train/Return            | 1.929631    |
| Train/V                 | 2.0930414   |
| Train/Value             | 2.0930414   |
| Train/control_penalty   | 0.4325367   |
| Train/policy_loss       | 0.16342813  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.031       |
-----------------------------------------

 ---------------- Iteration 999 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 998        |
| Time/Actor_Time         | 0.074      |
| Time/B_Format_Time      | 0.0699     |
| Time/B_Original_Form... | 0.0712     |
| Time/Buffer             | 0.00378    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23503014 |
| Train/Action_magnitu... | 0.5225646  |
| Train/Action_magnitude  | 0.40826672 |
| Train/Action_max        | 0.17561948 |
| Train/Action_std        | 0.14008619 |
| Train/Entropy           | -0.5778194 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1493088  |
| Train/Loss              | 0.15505542 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15036862 |
| Train/Ratio             | 1.0000182  |
| Train/Return            | 2.0184102  |
| Train/V                 | 2.168779   |
| Train/Value             | 2.168779   |
| Train/control_penalty   | 0.41089866 |
| Train/policy_loss       | 0.15036862 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03325    |
----------------------------------------

 ---------------- Iteration 1000 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 999        |
| Time/Actor_Time         | 0.0754     |
| Time/B_Format_Time      | 0.0762     |
| Time/B_Original_Form... | 0.0802     |
| Time/Buffer             | 0.0119     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23938885 |
| Train/Action_magnitu... | 0.5342262  |
| Train/Action_magnitude  | 0.4193321  |
| Train/Action_max        | 0.18205371 |
| Train/Action_std        | 0.14158617 |
| Train/Entropy           | -0.5648202 |
| Train/Entropy_Loss      | 0.000565   |
| Train/Entropy_loss      | 0.000565   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1216165  |
| Train/Loss              | 0.20926808 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20445989 |
| Train/Ratio             | 0.99999505 |
| Train/Return            | 1.8289117  |
| Train/V                 | 2.033375   |
| Train/Value             | 2.033375   |
| Train/control_penalty   | 0.42433736 |
| Train/policy_loss       | 0.20445989 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0235     |
----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 1001 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1000        |
| Time/Actor_Time         | 0.0805      |
| Time/B_Format_Time      | 0.0817      |
| Time/B_Original_Form... | 0.0856      |
| Time/Buffer             | 0.00433     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24140309  |
| Train/Action_magnitu... | 0.54507047  |
| Train/Action_magnitude  | 0.42553616  |
| Train/Action_max        | 0.1984243   |
| Train/Action_std        | 0.1411595   |
| Train/Entropy           | -0.57006216 |
| Train/Entropy_Loss      | 0.00057     |
| Train/Entropy_loss      | 0.00057     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1016455   |
| Train/Loss              | 0.105379134 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.100608565 |
| Train/Ratio             | 1.0000194   |
| Train/Return            | 1.978115    |
| Train/V                 | 2.0787218   |
| Train/Value             | 2.0787218   |
| Train/control_penalty   | 0.42005128  |
| Train/policy_loss       | 0.100608565 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.037       |
-----------------------------------------

 ---------------- Iteration 1002 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1001       |
| Time/Actor_Time         | 0.0767     |
| Time/B_Format_Time      | 0.0766     |
| Time/B_Original_Form... | 0.0752     |
| Time/Buffer             | 0.00433    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.231926   |
| Train/Action_magnitu... | 0.51842046 |
| Train/Action_magnitude  | 0.405518   |
| Train/Action_max        | 0.18230008 |
| Train/Action_std        | 0.14120603 |
| Train/Entropy           | -0.5728593 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1539131  |
| Train/Loss              | 0.17961875 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.17500812 |
| Train/Ratio             | 0.9999823  |
| Train/Return            | 1.8050616  |
| Train/V                 | 1.9800723  |
| Train/Value             | 1.9800723  |
| Train/control_penalty   | 0.40377626 |
| Train/policy_loss       | 0.17500812 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02925    |
----------------------------------------

 ---------------- Iteration 1003 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1002        |
| Time/Actor_Time         | 0.0734      |
| Time/B_Format_Time      | 0.0739      |
| Time/B_Original_Form... | 0.0786      |
| Time/Buffer             | 0.00478     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23391119  |
| Train/Action_magnitu... | 0.5347707   |
| Train/Action_magnitude  | 0.41916808  |
| Train/Action_max        | 0.1748994   |
| Train/Action_std        | 0.14087689  |
| Train/Entropy           | -0.57186973 |
| Train/Entropy_Loss      | 0.000572    |
| Train/Entropy_loss      | 0.000572    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1145804   |
| Train/Loss              | 0.15361208  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.14887801  |
| Train/Ratio             | 0.99999094  |
| Train/Return            | 1.9510806   |
| Train/V                 | 2.0999634   |
| Train/Value             | 2.0999634   |
| Train/control_penalty   | 0.41621965  |
| Train/policy_loss       | 0.14887801  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.033       |
-----------------------------------------

 ---------------- Iteration 1004 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1003       |
| Time/Actor_Time         | 0.0754     |
| Time/B_Format_Time      | 0.077      |
| Time/B_Original_Form... | 0.0768     |
| Time/Buffer             | 0.00327    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.229343   |
| Train/Action_magnitu... | 0.5141444  |
| Train/Action_magnitude  | 0.40058517 |
| Train/Action_max        | 0.190703   |
| Train/Action_std        | 0.14129324 |
| Train/Entropy           | -0.5734518 |
| Train/Entropy_Loss      | 0.000573   |
| Train/Entropy_loss      | 0.000573   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1607927  |
| Train/Loss              | 0.1427176  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13812646 |
| Train/Ratio             | 0.9999821  |
| Train/Return            | 1.7168233  |
| Train/V                 | 1.8549507  |
| Train/Value             | 1.8549507  |
| Train/control_penalty   | 0.40176842 |
| Train/policy_loss       | 0.13812646 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02375    |
----------------------------------------

 ---------------- Iteration 1005 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1004       |
| Time/Actor_Time         | 0.0733     |
| Time/B_Format_Time      | 0.0753     |
| Time/B_Original_Form... | 0.0736     |
| Time/Buffer             | 0.00401    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22975563 |
| Train/Action_magnitu... | 0.5191418  |
| Train/Action_magnitude  | 0.40560734 |
| Train/Action_max        | 0.16831848 |
| Train/Action_std        | 0.13956523 |
| Train/Entropy           | -0.5826102 |
| Train/Entropy_Loss      | 0.000583   |
| Train/Entropy_loss      | 0.000583   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1998435  |
| Train/Loss              | 0.16173404 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15707898 |
| Train/Ratio             | 1.0000083  |
| Train/Return            | 1.8790358  |
| Train/V                 | 2.0361142  |
| Train/Value             | 2.0361142  |
| Train/control_penalty   | 0.40724602 |
| Train/policy_loss       | 0.15707898 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0295     |
----------------------------------------

 ---------------- Iteration 1006 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1005        |
| Time/Actor_Time         | 0.0738      |
| Time/B_Format_Time      | 0.0707      |
| Time/B_Original_Form... | 0.0725      |
| Time/Buffer             | 0.00404     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23033908  |
| Train/Action_magnitu... | 0.5239856   |
| Train/Action_magnitude  | 0.41011482  |
| Train/Action_max        | 0.17318271  |
| Train/Action_std        | 0.14267816  |
| Train/Entropy           | -0.55884856 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1121676   |
| Train/Loss              | 0.19914451  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19450684  |
| Train/Ratio             | 0.99998736  |
| Train/Return            | 2.1092856   |
| Train/V                 | 2.3037958   |
| Train/Value             | 2.3037958   |
| Train/control_penalty   | 0.4078821   |
| Train/policy_loss       | 0.19450684  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 1007 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1006       |
| Time/Actor_Time         | 0.0729     |
| Time/B_Format_Time      | 0.0747     |
| Time/B_Original_Form... | 0.0751     |
| Time/Buffer             | 0.00377    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23793644 |
| Train/Action_magnitu... | 0.5316592  |
| Train/Action_magnitude  | 0.41646776 |
| Train/Action_max        | 0.18621792 |
| Train/Action_std        | 0.14209038 |
| Train/Entropy           | -0.5639619 |
| Train/Entropy_Loss      | 0.000564   |
| Train/Entropy_loss      | 0.000564   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1297606  |
| Train/Loss              | 0.27104214 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.26629764 |
| Train/Ratio             | 0.99999404 |
| Train/Return            | 2.007188   |
| Train/V                 | 2.273488   |
| Train/Value             | 2.273488   |
| Train/control_penalty   | 0.41805434 |
| Train/policy_loss       | 0.26629764 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02575    |
----------------------------------------

 ---------------- Iteration 1008 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1007        |
| Time/Actor_Time         | 0.0777      |
| Time/B_Format_Time      | 0.0818      |
| Time/B_Original_Form... | 0.0811      |
| Time/Buffer             | 0.00714     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23235439  |
| Train/Action_magnitu... | 0.5282006   |
| Train/Action_magnitude  | 0.41393557  |
| Train/Action_max        | 0.18113197  |
| Train/Action_std        | 0.1417996   |
| Train/Entropy           | -0.56464964 |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1095998   |
| Train/Loss              | 0.21347415  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2087803   |
| Train/Ratio             | 1.0000077   |
| Train/Return            | 1.9596543   |
| Train/V                 | 2.168438    |
| Train/Value             | 2.168438    |
| Train/control_penalty   | 0.4129203   |
| Train/policy_loss       | 0.2087803   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 1009 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1008       |
| Time/Actor_Time         | 0.0735     |
| Time/B_Format_Time      | 0.0781     |
| Time/B_Original_Form... | 0.078      |
| Time/Buffer             | 0.00392    |
| Time/Critic_Time        | 9.54e-07   |
| Train/Action_abs_mean   | 0.23097649 |
| Train/Action_magnitu... | 0.5210414  |
| Train/Action_magnitude  | 0.4105547  |
| Train/Action_max        | 0.14574881 |
| Train/Action_std        | 0.13954261 |
| Train/Entropy           | -0.5808331 |
| Train/Entropy_Loss      | 0.000581   |
| Train/Entropy_loss      | 0.000581   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1692216  |
| Train/Loss              | 0.25220042 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.24749373 |
| Train/Ratio             | 0.99999523 |
| Train/Return            | 2.348059   |
| Train/V                 | 2.595563   |
| Train/Value             | 2.595563   |
| Train/control_penalty   | 0.41258577 |
| Train/policy_loss       | 0.24749373 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0285     |
----------------------------------------

 ---------------- Iteration 1010 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1009       |
| Time/Actor_Time         | 0.0757     |
| Time/B_Format_Time      | 0.0714     |
| Time/B_Original_Form... | 0.0716     |
| Time/Buffer             | 0.00437    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2307507  |
| Train/Action_magnitu... | 0.528748   |
| Train/Action_magnitude  | 0.4160829  |
| Train/Action_max        | 0.17759135 |
| Train/Action_std        | 0.14187531 |
| Train/Entropy           | -0.5645781 |
| Train/Entropy_Loss      | 0.000565   |
| Train/Entropy_loss      | 0.000565   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1144761  |
| Train/Loss              | 0.36899298 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.36428812 |
| Train/Ratio             | 0.99998254 |
| Train/Return            | 2.1140907  |
| Train/V                 | 2.4783778  |
| Train/Value             | 2.4783778  |
| Train/control_penalty   | 0.41402903 |
| Train/policy_loss       | 0.36428812 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02275    |
----------------------------------------

 ---------------- Iteration 1011 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1010        |
| Time/Actor_Time         | 0.0746      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0773      |
| Time/Buffer             | 0.00449     |
| Time/Critic_Time        | 1.19e-06    |
| Train/Action_abs_mean   | 0.23066638  |
| Train/Action_magnitu... | 0.51973397  |
| Train/Action_magnitude  | 0.40544558  |
| Train/Action_max        | 0.17865436  |
| Train/Action_std        | 0.1375337   |
| Train/Entropy           | -0.5964096  |
| Train/Entropy_Loss      | 0.000596    |
| Train/Entropy_loss      | 0.000596    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1755085   |
| Train/Loss              | 0.120476395 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.115849264 |
| Train/Ratio             | 1.0000167   |
| Train/Return            | 1.7622623   |
| Train/V                 | 1.8781097   |
| Train/Value             | 1.8781097   |
| Train/control_penalty   | 0.40307215  |
| Train/policy_loss       | 0.115849264 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.037       |
-----------------------------------------

 ---------------- Iteration 1012 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1011        |
| Time/Actor_Time         | 0.074       |
| Time/B_Format_Time      | 0.0758      |
| Time/B_Original_Form... | 0.0754      |
| Time/Buffer             | 0.00346     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22875495  |
| Train/Action_magnitu... | 0.51782566  |
| Train/Action_magnitude  | 0.40301332  |
| Train/Action_max        | 0.1666208   |
| Train/Action_std        | 0.13705528  |
| Train/Entropy           | -0.59697956 |
| Train/Entropy_Loss      | 0.000597    |
| Train/Entropy_loss      | 0.000597    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1851299   |
| Train/Loss              | 0.18380386  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.17915842  |
| Train/Ratio             | 0.9999976   |
| Train/Return            | 1.9597766   |
| Train/V                 | 2.138937    |
| Train/Value             | 2.138937    |
| Train/control_penalty   | 0.40484542  |
| Train/policy_loss       | 0.17915842  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0305      |
-----------------------------------------

 ---------------- Iteration 1013 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1012       |
| Time/Actor_Time         | 0.075      |
| Time/B_Format_Time      | 0.0759     |
| Time/B_Original_Form... | 0.0753     |
| Time/Buffer             | 0.00361    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.224567   |
| Train/Action_magnitu... | 0.51046187 |
| Train/Action_magnitude  | 0.40022564 |
| Train/Action_max        | 0.16737626 |
| Train/Action_std        | 0.14099142 |
| Train/Entropy           | -0.5747753 |
| Train/Entropy_Loss      | 0.000575   |
| Train/Entropy_loss      | 0.000575   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1480987  |
| Train/Loss              | 0.22211786 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21749546 |
| Train/Ratio             | 1.0000209  |
| Train/Return            | 1.7998737  |
| Train/V                 | 2.0173566  |
| Train/Value             | 2.0173566  |
| Train/control_penalty   | 0.40476176 |
| Train/policy_loss       | 0.21749546 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.024      |
----------------------------------------

 ---------------- Iteration 1014 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1013       |
| Time/Actor_Time         | 0.0755     |
| Time/B_Format_Time      | 0.0782     |
| Time/B_Original_Form... | 0.0791     |
| Time/Buffer             | 0.00379    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23215725 |
| Train/Action_magnitu... | 0.5241731  |
| Train/Action_magnitude  | 0.408662   |
| Train/Action_max        | 0.18871766 |
| Train/Action_std        | 0.14342737 |
| Train/Entropy           | -0.5533734 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1120776  |
| Train/Loss              | 0.1891153  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18446447 |
| Train/Ratio             | 1.0000157  |
| Train/Return            | 1.7017981  |
| Train/V                 | 1.8862641  |
| Train/Value             | 1.8862641  |
| Train/control_penalty   | 0.40974623 |
| Train/policy_loss       | 0.18446447 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02425    |
----------------------------------------

 ---------------- Iteration 1015 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1014        |
| Time/Actor_Time         | 0.078       |
| Time/B_Format_Time      | 0.0767      |
| Time/B_Original_Form... | 0.0768      |
| Time/Buffer             | 0.00487     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22702435  |
| Train/Action_magnitu... | 0.517542    |
| Train/Action_magnitude  | 0.40535206  |
| Train/Action_max        | 0.16231394  |
| Train/Action_std        | 0.14158846  |
| Train/Entropy           | -0.56758994 |
| Train/Entropy_Loss      | 0.000568    |
| Train/Entropy_loss      | 0.000568    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.137379    |
| Train/Loss              | 0.23799044  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.23336878  |
| Train/Ratio             | 0.9999976   |
| Train/Return            | 1.9076196   |
| Train/V                 | 2.140998    |
| Train/Value             | 2.140998    |
| Train/control_penalty   | 0.40540645  |
| Train/policy_loss       | 0.23336878  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0295      |
-----------------------------------------

 ---------------- Iteration 1016 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1015        |
| Time/Actor_Time         | 0.0727      |
| Time/B_Format_Time      | 0.0716      |
| Time/B_Original_Form... | 0.074       |
| Time/Buffer             | 0.00338     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.21345145  |
| Train/Action_magnitu... | 0.49431813  |
| Train/Action_magnitude  | 0.3876842   |
| Train/Action_max        | 0.15365022  |
| Train/Action_std        | 0.13722719  |
| Train/Entropy           | -0.59892243 |
| Train/Entropy_Loss      | 0.000599    |
| Train/Entropy_loss      | 0.000599    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1995999   |
| Train/Loss              | 0.21639813  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2119472   |
| Train/Ratio             | 1.00001     |
| Train/Return            | 1.681676    |
| Train/V                 | 1.8936062   |
| Train/Value             | 1.8936062   |
| Train/control_penalty   | 0.38520062  |
| Train/policy_loss       | 0.2119472   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.027       |
-----------------------------------------

 ---------------- Iteration 1017 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1016        |
| Time/Actor_Time         | 0.0804      |
| Time/B_Format_Time      | 0.0837      |
| Time/B_Original_Form... | 0.0833      |
| Time/Buffer             | 0.00704     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.23269741  |
| Train/Action_magnitu... | 0.52395815  |
| Train/Action_magnitude  | 0.40597606  |
| Train/Action_max        | 0.19100524  |
| Train/Action_std        | 0.13999632  |
| Train/Entropy           | -0.5781101  |
| Train/Entropy_Loss      | 0.000578    |
| Train/Entropy_loss      | 0.000578    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1506616   |
| Train/Loss              | 0.096427605 |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.091787376 |
| Train/Ratio             | 1.0000255   |
| Train/Return            | 1.7196414   |
| Train/V                 | 1.8114302   |
| Train/Value             | 1.8114302   |
| Train/control_penalty   | 0.4062117   |
| Train/policy_loss       | 0.091787376 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 1018 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1017        |
| Time/Actor_Time         | 0.0731      |
| Time/B_Format_Time      | 0.0728      |
| Time/B_Original_Form... | 0.0714      |
| Time/Buffer             | 0.00349     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23476283  |
| Train/Action_magnitu... | 0.5331553   |
| Train/Action_magnitude  | 0.41298458  |
| Train/Action_max        | 0.19605142  |
| Train/Action_std        | 0.14366323  |
| Train/Entropy           | -0.55545604 |
| Train/Entropy_Loss      | 0.000555    |
| Train/Entropy_loss      | 0.000555    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1147176   |
| Train/Loss              | 0.21070614  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.20606647  |
| Train/Ratio             | 0.9999751   |
| Train/Return            | 1.5875878   |
| Train/V                 | 1.7936631   |
| Train/Value             | 1.7936631   |
| Train/control_penalty   | 0.4084214   |
| Train/policy_loss       | 0.20606647  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02025     |
-----------------------------------------

 ---------------- Iteration 1019 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1018        |
| Time/Actor_Time         | 0.0734      |
| Time/B_Format_Time      | 0.0733      |
| Time/B_Original_Form... | 0.0764      |
| Time/Buffer             | 0.00383     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23310305  |
| Train/Action_magnitu... | 0.524776    |
| Train/Action_magnitude  | 0.40838495  |
| Train/Action_max        | 0.17371999  |
| Train/Action_std        | 0.14138652  |
| Train/Entropy           | -0.56671476 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1355565   |
| Train/Loss              | 0.1956852   |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.19105321  |
| Train/Ratio             | 0.9999977   |
| Train/Return            | 1.7176827   |
| Train/V                 | 1.908738    |
| Train/Value             | 1.908738    |
| Train/control_penalty   | 0.40652624  |
| Train/policy_loss       | 0.19105321  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.028       |
-----------------------------------------

 ---------------- Iteration 1020 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1019        |
| Time/Actor_Time         | 0.0665      |
| Time/B_Format_Time      | 0.067       |
| Time/B_Original_Form... | 0.0697      |
| Time/Buffer             | 0.00356     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22678421  |
| Train/Action_magnitu... | 0.51201004  |
| Train/Action_magnitude  | 0.39945677  |
| Train/Action_max        | 0.18836929  |
| Train/Action_std        | 0.14023384  |
| Train/Entropy           | -0.57551825 |
| Train/Entropy_Loss      | 0.000576    |
| Train/Entropy_loss      | 0.000576    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1866299   |
| Train/Loss              | 0.11539073  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.1107985   |
| Train/Ratio             | 0.99999034  |
| Train/Return            | 1.5295956   |
| Train/V                 | 1.6403878   |
| Train/Value             | 1.6403878   |
| Train/control_penalty   | 0.40167105  |
| Train/policy_loss       | 0.1107985   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02675     |
-----------------------------------------
Saving snapshot...
Saved

 ---------------- Iteration 1021 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1020        |
| Time/Actor_Time         | 0.0682      |
| Time/B_Format_Time      | 0.0705      |
| Time/B_Original_Form... | 0.0731      |
| Time/Buffer             | 0.00336     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.22636631  |
| Train/Action_magnitu... | 0.5124342   |
| Train/Action_magnitude  | 0.39910856  |
| Train/Action_max        | 0.1756605   |
| Train/Action_std        | 0.1367492   |
| Train/Entropy           | -0.60156775 |
| Train/Entropy_Loss      | 0.000602    |
| Train/Entropy_loss      | 0.000602    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.2040858   |
| Train/Loss              | 0.11491548  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.110355034 |
| Train/Ratio             | 0.99999255  |
| Train/Return            | 1.5863323   |
| Train/V                 | 1.6966819   |
| Train/Value             | 1.6966819   |
| Train/control_penalty   | 0.39588836  |
| Train/policy_loss       | 0.110355034 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03375     |
-----------------------------------------

 ---------------- Iteration 1022 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1021        |
| Time/Actor_Time         | 0.0686      |
| Time/B_Format_Time      | 0.0681      |
| Time/B_Original_Form... | 0.0707      |
| Time/Buffer             | 0.00293     |
| Time/Critic_Time        | 9.54e-07    |
| Train/Action_abs_mean   | 0.22516072  |
| Train/Action_magnitu... | 0.508711    |
| Train/Action_magnitude  | 0.39501637  |
| Train/Action_max        | 0.1806202   |
| Train/Action_std        | 0.14216869  |
| Train/Entropy           | -0.56456906 |
| Train/Entropy_Loss      | 0.000565    |
| Train/Entropy_loss      | 0.000565    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1613048   |
| Train/Loss              | 0.12652585  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.12198642  |
| Train/Ratio             | 1.0000204   |
| Train/Return            | 1.5400175   |
| Train/V                 | 1.6620042   |
| Train/Value             | 1.6620042   |
| Train/control_penalty   | 0.3974868   |
| Train/policy_loss       | 0.12198642  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02425     |
-----------------------------------------

 ---------------- Iteration 1023 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1022       |
| Time/Actor_Time         | 0.0702     |
| Time/B_Format_Time      | 0.0731     |
| Time/B_Original_Form... | 0.071      |
| Time/Buffer             | 0.00362    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24216464 |
| Train/Action_magnitu... | 0.5473912  |
| Train/Action_magnitude  | 0.42412862 |
| Train/Action_max        | 0.19863327 |
| Train/Action_std        | 0.14509723 |
| Train/Entropy           | -0.5418312 |
| Train/Entropy_Loss      | 0.000542   |
| Train/Entropy_loss      | 0.000542   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0643338  |
| Train/Loss              | 0.20482911 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.20007029 |
| Train/Ratio             | 0.9999986  |
| Train/Return            | 1.522017   |
| Train/V                 | 1.7220894  |
| Train/Value             | 1.7220894  |
| Train/control_penalty   | 0.42169818 |
| Train/policy_loss       | 0.20007029 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02025    |
----------------------------------------

 ---------------- Iteration 1024 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1023        |
| Time/Actor_Time         | 0.0688      |
| Time/B_Format_Time      | 0.0704      |
| Time/B_Original_Form... | 0.071       |
| Time/Buffer             | 0.00681     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23900777  |
| Train/Action_magnitu... | 0.5326292   |
| Train/Action_magnitude  | 0.41816366  |
| Train/Action_max        | 0.18086296  |
| Train/Action_std        | 0.14341317  |
| Train/Entropy           | -0.5491701  |
| Train/Entropy_Loss      | 0.000549    |
| Train/Entropy_loss      | 0.000549    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1192852   |
| Train/Loss              | 0.12793794  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.123209246 |
| Train/Ratio             | 0.99998486  |
| Train/Return            | 1.4666998   |
| Train/V                 | 1.58992     |
| Train/Value             | 1.58992     |
| Train/control_penalty   | 0.41795313  |
| Train/policy_loss       | 0.123209246 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02825     |
-----------------------------------------

 ---------------- Iteration 1025 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1024        |
| Time/Actor_Time         | 0.0682      |
| Time/B_Format_Time      | 0.0713      |
| Time/B_Original_Form... | 0.0746      |
| Time/Buffer             | 0.00401     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.233804    |
| Train/Action_magnitu... | 0.5288856   |
| Train/Action_magnitude  | 0.41254973  |
| Train/Action_max        | 0.1913024   |
| Train/Action_std        | 0.1426848   |
| Train/Entropy           | -0.55880076 |
| Train/Entropy_Loss      | 0.000559    |
| Train/Entropy_loss      | 0.000559    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1248796   |
| Train/Loss              | 0.11614024  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.111448005 |
| Train/Ratio             | 0.9999849   |
| Train/Return            | 1.6277531   |
| Train/V                 | 1.7391992   |
| Train/Value             | 1.7391992   |
| Train/control_penalty   | 0.4133435   |
| Train/policy_loss       | 0.111448005 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0335      |
-----------------------------------------

 ---------------- Iteration 1026 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1025        |
| Time/Actor_Time         | 0.0704      |
| Time/B_Format_Time      | 0.069       |
| Time/B_Original_Form... | 0.0736      |
| Time/Buffer             | 0.00406     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23005086  |
| Train/Action_magnitu... | 0.51745194  |
| Train/Action_magnitude  | 0.40488872  |
| Train/Action_max        | 0.18144347  |
| Train/Action_std        | 0.13985544  |
| Train/Entropy           | -0.57977605 |
| Train/Entropy_Loss      | 0.00058     |
| Train/Entropy_loss      | 0.00058     |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1755935   |
| Train/Loss              | 0.17410216  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16945514  |
| Train/Ratio             | 0.9999776   |
| Train/Return            | 1.6775976   |
| Train/V                 | 1.847057    |
| Train/Value             | 1.847057    |
| Train/control_penalty   | 0.40672463  |
| Train/policy_loss       | 0.16945514  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.024       |
-----------------------------------------

 ---------------- Iteration 1027 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1026        |
| Time/Actor_Time         | 0.0778      |
| Time/B_Format_Time      | 0.0822      |
| Time/B_Original_Form... | 0.081       |
| Time/Buffer             | 0.00361     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24041496  |
| Train/Action_magnitu... | 0.5394339   |
| Train/Action_magnitude  | 0.41781715  |
| Train/Action_max        | 0.18748161  |
| Train/Action_std        | 0.1443817   |
| Train/Entropy           | -0.54352057 |
| Train/Entropy_Loss      | 0.000544    |
| Train/Entropy_loss      | 0.000544    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1228249   |
| Train/Loss              | 0.11623137  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.111463174 |
| Train/Ratio             | 0.9999845   |
| Train/Return            | 1.7187505   |
| Train/V                 | 1.8302114   |
| Train/Value             | 1.8302114   |
| Train/control_penalty   | 0.42246732  |
| Train/policy_loss       | 0.111463174 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.038       |
-----------------------------------------

 ---------------- Iteration 1028 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1027       |
| Time/Actor_Time         | 0.0821     |
| Time/B_Format_Time      | 0.0819     |
| Time/B_Original_Form... | 0.0779     |
| Time/Buffer             | 0.00432    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23981927 |
| Train/Action_magnitu... | 0.53634095 |
| Train/Action_magnitude  | 0.4185659  |
| Train/Action_max        | 0.19805685 |
| Train/Action_std        | 0.14362061 |
| Train/Entropy           | -0.5532647 |
| Train/Entropy_Loss      | 0.000553   |
| Train/Entropy_loss      | 0.000553   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1158428  |
| Train/Loss              | 0.18557356 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.18081804 |
| Train/Ratio             | 0.99999136 |
| Train/Return            | 1.7015882  |
| Train/V                 | 1.8824087  |
| Train/Value             | 1.8824087  |
| Train/control_penalty   | 0.4202262  |
| Train/policy_loss       | 0.18081804 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02475    |
----------------------------------------

 ---------------- Iteration 1029 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1028       |
| Time/Actor_Time         | 0.0792     |
| Time/B_Format_Time      | 0.0774     |
| Time/B_Original_Form... | 0.0798     |
| Time/Buffer             | 0.00451    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.22980209 |
| Train/Action_magnitu... | 0.52091944 |
| Train/Action_magnitude  | 0.40627992 |
| Train/Action_max        | 0.17009231 |
| Train/Action_std        | 0.14196196 |
| Train/Entropy           | -0.5614968 |
| Train/Entropy_Loss      | 0.000561   |
| Train/Entropy_loss      | 0.000561   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1293621  |
| Train/Loss              | 0.2176747  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21303222 |
| Train/Ratio             | 0.99999356 |
| Train/Return            | 1.8380303  |
| Train/V                 | 2.0510669  |
| Train/Value             | 2.0510669  |
| Train/control_penalty   | 0.40810007 |
| Train/policy_loss       | 0.21303222 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02675    |
----------------------------------------

 ---------------- Iteration 1030 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1029       |
| Time/Actor_Time         | 0.0843     |
| Time/B_Format_Time      | 0.0794     |
| Time/B_Original_Form... | 0.0785     |
| Time/Buffer             | 0.00456    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.25209835 |
| Train/Action_magnitu... | 0.5561597  |
| Train/Action_magnitude  | 0.43206072 |
| Train/Action_max        | 0.21336766 |
| Train/Action_std        | 0.14535147 |
| Train/Entropy           | -0.5404983 |
| Train/Entropy_Loss      | 0.00054    |
| Train/Entropy_loss      | 0.00054    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0628026  |
| Train/Loss              | 0.12354126 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.11868135 |
| Train/Ratio             | 0.99999315 |
| Train/Return            | 1.9165192  |
| Train/V                 | 2.0351949  |
| Train/Value             | 2.0351949  |
| Train/control_penalty   | 0.43194118 |
| Train/policy_loss       | 0.11868135 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.03975    |
----------------------------------------

 ---------------- Iteration 1031 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1030        |
| Time/Actor_Time         | 0.0805      |
| Time/B_Format_Time      | 0.0816      |
| Time/B_Original_Form... | 0.0834      |
| Time/Buffer             | 0.00479     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.24720925  |
| Train/Action_magnitu... | 0.5482701   |
| Train/Action_magnitude  | 0.42764175  |
| Train/Action_max        | 0.20229071  |
| Train/Action_std        | 0.14355448  |
| Train/Entropy           | -0.5546678  |
| Train/Entropy_Loss      | 0.000555    |
| Train/Entropy_loss      | 0.000555    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1172283   |
| Train/Loss              | 0.11991706  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.115104124 |
| Train/Ratio             | 1.0000125   |
| Train/Return            | 1.6911283   |
| Train/V                 | 1.8062352   |
| Train/Value             | 1.8062352   |
| Train/control_penalty   | 0.4258266   |
| Train/policy_loss       | 0.115104124 |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.03125     |
-----------------------------------------

 ---------------- Iteration 1032 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1031       |
| Time/Actor_Time         | 0.0759     |
| Time/B_Format_Time      | 0.0784     |
| Time/B_Original_Form... | 0.0771     |
| Time/Buffer             | 0.00426    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.23395464 |
| Train/Action_magnitu... | 0.5283848  |
| Train/Action_magnitude  | 0.41076237 |
| Train/Action_max        | 0.17989615 |
| Train/Action_std        | 0.13983579 |
| Train/Entropy           | -0.5791381 |
| Train/Entropy_Loss      | 0.000579   |
| Train/Entropy_loss      | 0.000579   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1447083  |
| Train/Loss              | 0.19303443 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.1883595  |
| Train/Ratio             | 1.0000093  |
| Train/Return            | 2.0246186  |
| Train/V                 | 2.2129786  |
| Train/Value             | 2.2129786  |
| Train/control_penalty   | 0.40957883 |
| Train/policy_loss       | 0.1883595  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.032      |
----------------------------------------

 ---------------- Iteration 1033 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1032        |
| Time/Actor_Time         | 0.0811      |
| Time/B_Format_Time      | 0.0753      |
| Time/B_Original_Form... | 0.0741      |
| Time/Buffer             | 0.00327     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23653103  |
| Train/Action_magnitu... | 0.5350986   |
| Train/Action_magnitude  | 0.41826478  |
| Train/Action_max        | 0.19620967  |
| Train/Action_std        | 0.1418084   |
| Train/Entropy           | -0.56704736 |
| Train/Entropy_Loss      | 0.000567    |
| Train/Entropy_loss      | 0.000567    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1161714   |
| Train/Loss              | 0.28637356  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.2816578   |
| Train/Ratio             | 1.0000161   |
| Train/Return            | 1.9925522   |
| Train/V                 | 2.2742045   |
| Train/Value             | 2.2742045   |
| Train/control_penalty   | 0.41487092  |
| Train/policy_loss       | 0.2816578   |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.02775     |
-----------------------------------------

 ---------------- Iteration 1034 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1033       |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0754     |
| Time/B_Original_Form... | 0.0785     |
| Time/Buffer             | 0.00287    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24227935 |
| Train/Action_magnitu... | 0.54270965 |
| Train/Action_magnitude  | 0.42403668 |
| Train/Action_max        | 0.19359387 |
| Train/Action_std        | 0.14540222 |
| Train/Entropy           | -0.5428473 |
| Train/Entropy_Loss      | 0.000543   |
| Train/Entropy_loss      | 0.000543   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0835655  |
| Train/Loss              | 0.2155965  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21082038 |
| Train/Ratio             | 1.0000063  |
| Train/Return            | 1.6831322  |
| Train/V                 | 1.8939482  |
| Train/Value             | 1.8939482  |
| Train/control_penalty   | 0.4233268  |
| Train/policy_loss       | 0.21082038 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0265     |
----------------------------------------

 ---------------- Iteration 1035 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1034       |
| Time/Actor_Time         | 0.076      |
| Time/B_Format_Time      | 0.0756     |
| Time/B_Original_Form... | 0.0744     |
| Time/Buffer             | 0.0029     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24620943 |
| Train/Action_magnitu... | 0.54244876 |
| Train/Action_magnitude  | 0.4199464  |
| Train/Action_max        | 0.22370557 |
| Train/Action_std        | 0.14562774 |
| Train/Entropy           | -0.5419491 |
| Train/Entropy_Loss      | 0.000542   |
| Train/Entropy_loss      | 0.000542   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.0823699  |
| Train/Loss              | 0.1358294  |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.13105017 |
| Train/Ratio             | 0.99999636 |
| Train/Return            | 1.6077186  |
| Train/V                 | 1.7387695  |
| Train/Value             | 1.7387695  |
| Train/control_penalty   | 0.42372754 |
| Train/policy_loss       | 0.13105017 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02725    |
----------------------------------------

 ---------------- Iteration 1036 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1035       |
| Time/Actor_Time         | 0.0761     |
| Time/B_Format_Time      | 0.0779     |
| Time/B_Original_Form... | 0.0738     |
| Time/Buffer             | 0.0037     |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.24339797 |
| Train/Action_magnitu... | 0.53843856 |
| Train/Action_magnitude  | 0.4165659  |
| Train/Action_max        | 0.21305104 |
| Train/Action_std        | 0.14019634 |
| Train/Entropy           | -0.5783874 |
| Train/Entropy_Loss      | 0.000578   |
| Train/Entropy_loss      | 0.000578   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1662465  |
| Train/Loss              | 0.16227008 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.15752402 |
| Train/Ratio             | 0.9999937  |
| Train/Return            | 1.6475435  |
| Train/V                 | 1.8050714  |
| Train/Value             | 1.8050714  |
| Train/control_penalty   | 0.41676703 |
| Train/policy_loss       | 0.15752402 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0255     |
----------------------------------------

 ---------------- Iteration 1037 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1036       |
| Time/Actor_Time         | 0.0739     |
| Time/B_Format_Time      | 0.0724     |
| Time/B_Original_Form... | 0.0773     |
| Time/Buffer             | 0.00355    |
| Time/Critic_Time        | 1.19e-06   |
| Train/Action_abs_mean   | 0.23871543 |
| Train/Action_magnitu... | 0.5387711  |
| Train/Action_magnitude  | 0.41888544 |
| Train/Action_max        | 0.21467987 |
| Train/Action_std        | 0.14170343 |
| Train/Entropy           | -0.5700993 |
| Train/Entropy_Loss      | 0.00057    |
| Train/Entropy_loss      | 0.00057    |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.1470237  |
| Train/Loss              | 0.21668111 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.21192835 |
| Train/Ratio             | 1.0000194  |
| Train/Return            | 1.8625693  |
| Train/V                 | 2.0744815  |
| Train/Value             | 2.0744815  |
| Train/control_penalty   | 0.41826528 |
| Train/policy_loss       | 0.21192835 |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.0295     |
----------------------------------------

 ---------------- Iteration 1038 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
-----------------------------------------
| Itr                     | 1037        |
| Time/Actor_Time         | 0.0765      |
| Time/B_Format_Time      | 0.0724      |
| Time/B_Original_Form... | 0.08        |
| Time/Buffer             | 0.00483     |
| Time/Critic_Time        | 0           |
| Train/Action_abs_mean   | 0.23233725  |
| Train/Action_magnitu... | 0.5205913   |
| Train/Action_magnitude  | 0.4055209   |
| Train/Action_max        | 0.18921576  |
| Train/Action_std        | 0.13897257  |
| Train/Entropy           | -0.58748865 |
| Train/Entropy_Loss      | 0.000587    |
| Train/Entropy_loss      | 0.000587    |
| Train/Grad_norm_actor   | 0.0         |
| Train/LogProb           | 1.1864614   |
| Train/Loss              | 0.16864158  |
| Train/PolicyClip        | 0.0         |
| Train/Policy_loss       | 0.16399977  |
| Train/Ratio             | 0.99998564  |
| Train/Return            | 1.9025283   |
| Train/V                 | 2.0665247   |
| Train/Value             | 2.0665247   |
| Train/control_penalty   | 0.40543246  |
| Train/policy_loss       | 0.16399977  |
| Train/recon_loss        | 0.0         |
| train/batch_reward      | 0.0325      |
-----------------------------------------

 ---------------- Iteration 1039 ----------------
Obtaining samples...
RL Training...
/Users/1000ber-5078/PycharmProjects/teachable-rl/scripts/logs/persisted_models_distill/distilled_with_mlp_2
----------------------------------------
| Itr                     | 1038       |
| Time/Actor_Time         | 0.0775     |
| Time/B_Format_Time      | 0.0804     |
| Time/B_Original_Form... | 0.0805     |
| Time/Buffer             | 0.00391    |
| Time/Critic_Time        | 0          |
| Train/Action_abs_mean   | 0.2334769  |
| Train/Action_magnitu... | 0.5238068  |
| Train/Action_magnitude  | 0.41053495 |
| Train/Action_max        | 0.17035565 |
| Train/Action_std        | 0.14011942 |
| Train/Entropy           | -0.5764173 |
| Train/Entropy_Loss      | 0.000576   |
| Train/Entropy_loss      | 0.000576   |
| Train/Grad_norm_actor   | 0.0        |
| Train/LogProb           | 1.158166   |
| Train/Loss              | 0.27005437 |
| Train/PolicyClip        | 0.0        |
| Train/Policy_loss       | 0.2653331  |
| Train/Ratio             | 0.9999788  |
| Train/Return            | 2.0628262  |
| Train/V                 | 2.3281732  |
| Train/Value             | 2.3281732  |
| Train/control_penalty   | 0.41448787 |
| Train/policy_loss       | 0.2653331  |
| Train/recon_loss        | 0.0        |
| train/batch_reward      | 0.02725    |
----------------------------------------
